"ID","CLASSIFIED","TYPE","TITLE","DESCRIPTION"
"LUCENE-2719","IMPROVEMENT","IMPROVEMENT","Re-add SorterTemplate and use it to provide fast ArraySorting and replace BytesRefHash sorting","This patch adds back an optimized and rewritten SorterTemplate back to Lucene (removed after release of 3.0). It is of use for several components:

- Automaton: Automaton needs to sort States and other things. Using Arrays.sort() is slow, because it clones internally to ensure stable search. This component is much faster. This patch adds Arrays.sort() replacements in ArrayUtil that work with natural order or using a Comparator<?>. You can choose between quickSort and mergeSort.
- BytesRefHash uses another QuickSort algorithm without insertionSort for very short ord arrays. This class uses SorterTemplate to provide the same with insertionSort fallback in a very elegant way. Ideally this class can be used everywhere, where the sort algorithm needs to be separated from the underlying data and you can implement a swap() and compare() function (that get slot numbers instead of real values). This also applies to Solr (Yonik?).

SorterTemplate provides quickSort and mergeSort algorithms. Internally for short arrays, it automatically chooses insertionSort (like JDK's Arrays). The quickSort algorith was copied modified from old BytesRefHash. This new class only shares MergeSort with the original CGLIB SorterTemplate, which is no longer maintained."
"LUCENE-754","BUG","BUG","FieldCache keeps hard references to readers, doesn't prevent multiple threads from creating same instance",""
"LUCENE-1487","RFE","RFE","FieldCacheTermsFilter","This is a companion to FieldCacheRangeFilter except it operates on a set of terms rather than a range. It works best when the set is comparatively large or the terms are comparatively common.

"
"LUCENE-2216","BUG","BUG","OpenBitSet#hashCode() may return false for identical sets.","OpenBitSet uses an internal buffer of long variables to store set bits and an additional 'wlen' index that points 
to the highest used component inside {@link #bits} buffer.

Unlike in JDK, the wlen field is not continuously maintained (on clearing bits, for example). This leads to a situation when wlen may point
far beyond the last set bit. 

The hashCode implementation iterates over all long components of the bits buffer, rotating the hash even for empty components. This is against the contract of hashCode-equals. The following test case illustrates this:

{code}
// initialize two bitsets with different capacity (bits length).
BitSet bs1 = new BitSet(200);
BitSet bs2 = new BitSet(64);
// set the same bit.
bs1.set(3);
bs2.set(3);
        
// equals returns true (passes).
assertEquals(bs1, bs2);
// hashCode returns false (against contract).
assertEquals(bs1.hashCode(), bs2.hashCode());
{code}

Fix and test case attached."
"LUCENE-1061","RFE","IMPROVEMENT","Adding a factory to QueryParser to instantiate query instances","With the new efforts with Payload and scoring functions, it would be nice to plugin custom query implementations while using the same QueryParser.
Included is a patch with some refactoring the QueryParser to take a factory that produces query instances."
"LUCENE-456","BUG","BUG","Duplicate hits and missing hits in sorted search","If using a searcher that subclasses from IndexSearcher I get different result sets (besides the ordering of course). The problem only occurrs if the searcher is wrapped by (Parallel)MultiSearcher and the index is not too small. The number of hits returned by un unsorted and a sorted search are identical but the hits are referencing different documents. A closer look at the result sets revealed that the sorted search returns duplicate hits.

I created test cases for Lucene 1.4.3 as well as for the head release. The problem showed up for both, the number of duplicates beeing bigger for the head realease. The test cases are written for package org.apache.lucene.search. There are messages describing the problem written to the console. In order to see all those hints the asserts are commented out. So dont't be confused if junit reports no errors. (Sorry, beeing a novice user of the bug tracker I don't see any means to attach the test cases on this screen. Let's see.)"
"LUCENE-2243","RFE","IMPROVEMENT","FastVectorHighlighter: support DisjunctionMaxQuery","Add DisjunctionMaxQuery support in FVH. "
"LUCENE-2458","CLEANUP","BUG","queryparser makes all CJK queries phrase queries regardless of analyzer","The queryparser automatically makes *ALL* CJK, Thai, Lao, Myanmar, Tibetan, ... queries into phrase queries, even though you didn't ask for one, and there isn't a way to turn this off.

This completely breaks lucene for these languages, as it treats all queries like 'grep'.

Example: if you query for f:abcd with standardanalyzer, where a,b,c,d are chinese characters, you get a phrasequery of ""a b c d"". if you use cjk analyzer, its no better, its a phrasequery of  ""ab bc cd"", and if you use smartchinese analyzer, you get a phrasequery like ""ab cd"". But the user didn't ask for one, and they cannot turn it off.

The reason is that the code to form phrase queries is not internationally appropriate and assumes whitespace tokenization. If more than one token comes out of whitespace delimited text, its automatically a phrase query no matter what.

The proposed patch fixes the core queryparser (with all backwards compat kept) to only form phrase queries when the double quote operator is used. 

Implementing subclasses can always extend the QP and auto-generate whatever kind of queries they want that might completely break search for languages they don't care about, but core general-purpose QPs should be language independent.
"
"LUCENE-2670","RFE","IMPROVEMENT","allow automatontermsenum to work on full byte range","AutomatonTermsEnum is really agnostic to whats in your byte[], only that its in binary order.
so if you wanted to use this on some non-utf8 terms, thats just fine.

the patch just does some code cleanup and removes ""utf8"" references, etc.
additionally i changed the pkg-private, lucene-internal byte-oriented ctor, to public, lucene.experimental.
"
"LUCENE-3183","BUG","BUG","TestIndexWriter failure: AIOOBE","trunk: r1133486 
{code}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
    [junit] Testcase: testEmptyFieldName(org.apache.lucene.index.TestIndexWriter):      Caused an ERROR
    [junit] CheckIndex failed
    [junit] java.lang.RuntimeException: CheckIndex failed
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:158)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:477)
    [junit]     at org.apache.lucene.index.TestIndexWriter.testEmptyFieldName(TestIndexWriter.java:857)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1362)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1280)
    [junit] 
    [junit] 
    [junit] Tests run: 39, Failures: 0, Errors: 1, Time elapsed: 17.634 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] CheckIndex failed
    [junit] Segments file=segments_1 numSegments=1 version=FORMAT_4_0 [Lucene 4.0]
    [junit]   1 of 1: name=_0 docCount=1
    [junit]     codec=SegmentCodecs [codecs=[PreFlex], provider=org.apache.lucene.index.codecs.CoreCodecProvider@3f78807]
    [junit]     compound=false
    [junit]     hasProx=true
    [junit]     numFiles=8
    [junit]     size (MB)=0
    [junit]     diagnostics = {os.version=2.6.39-gentoo, os=Linux, lucene.version=4.0-SNAPSHOT, source=flush, os.arch=amd64, java.version=1.6.0_25, java.vendor=Sun Microsystems Inc.}
    [junit]     no deletions
    [junit]     test: open reader.........OK
    [junit]     test: fields..............OK [1 fields]
    [junit]     test: field norms.........OK [1 fields]
    [junit]     test: terms, freq, prox...ERROR: java.lang.ArrayIndexOutOfBoundsException: -1

    [junit] java.lang.ArrayIndexOutOfBoundsException: -1
    [junit]     at org.apache.lucene.index.codecs.preflex.TermInfosReader.seekEnum(TermInfosReader.java:212)
    [junit]     at org.apache.lucene.index.codecs.preflex.TermInfosReader.seekEnum(TermInfosReader.java:301)
    [junit]     at org.apache.lucene.index.codecs.preflex.TermInfosReader.get(TermInfosReader.java:234)
    [junit]     at org.apache.lucene.index.codecs.preflex.TermInfosReader.terms(TermInfosReader.java:371)
    [junit]     at org.apache.lucene.index.codecs.preflex.PreFlexFields$PreTermsEnum.reset(PreFlexFields.java:719)
    [junit]     at org.apache.lucene.index.codecs.preflex.PreFlexFields$PreTerms.iterator(PreFlexFields.java:249)
    [junit]     at org.apache.lucene.index.PerFieldCodecWrapper$FieldsReader$FieldsIterator.terms(PerFieldCodecWrapper.java:147)
    [junit]     at org.apache.lucene.index.CheckIndex.testTermIndex(CheckIndex.java:610)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:495)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:154)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:477)
    [junit]     at org.apache.lucene.index.TestIndexWriter.testEmptyFieldName(TestIndexWriter.java:857)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1362)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1280)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:758)
    [junit]     test: stored fields.......OK [0 total field count; avg 0 fields per doc]
    [junit]     test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    [junit] FAILED

    [junit]     WARNING: fixIndex() would remove reference to this segment; full exception:
    [junit] java.lang.RuntimeException: Term Index test failed
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:508)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:154)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:477)
    [junit]     at org.apache.lucene.index.TestIndexWriter.testEmptyFieldName(TestIndexWriter.java:857)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1362)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1280)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:758)
    [junit] 
    [junit] WARNING: 1 broken segments (containing 1 documents) detected
    [junit] 
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testEmptyFieldName -Dtests.seed=-3770357642070518646:-3121175410586002489 -Dtests.multiplier=3
    [junit] NOTE: test params are: codec=PreFlex, locale=zh, timezone=Indian/Antananarivo
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDateTools, TestDeletionPolicy, TestDocsAndPositions, TestFlex, TestIndexReaderCloneNorms, TestIndexWriter]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=85972280,total=232521728
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.index.TestIndexWriter FAILED
{code}"
"LUCENE-2606","IMPROVEMENT","IMPROVEMENT","optimize contrib/regex for flex","* changes RegexCapabilities match(String) to match(BytesRef)
* the jakarta and jdk impls uses CharacterIterator/CharSequence matching against the utf16result instead.
* i also reuse the matcher for jdk, i don't see why we didnt do this before but it makes sense esp since we reuse the CSQ
"
"LUCENE-609","BACKPORT","BUG","Lazy field loading breaks backward compat","Document.getField() and Document.getFields() have changed in a non backward compatible manner.
Simple code like the following no longer compiles:
 Field x = mydoc.getField(""x"");"
"LUCENE-912","BUG","BUG","DisjunctionMaxScorer.skipTo has bug that keeps it from skipping","as reported on the mailing list, DisjunctionMaxScorer.skipTo is broken if called before next in some situations...

http://www.nabble.com/Potential-issue-with-DisjunctionMaxScorer-tf3846366.html#a10894987"
"LUCENE-2144","BUG","BUG","InstantiatedIndexReader does not handle #termDocs(null) correct (AllTermDocs)","This patch contains core changes so someone else needs to commit it.

Due to the incompatible #termDocs(null) behaviour at least MatchAllDocsQuery, FieldCacheRangeFilter and ValueSourceQuery fails using II since 2.9.

AllTermDocs now has a superclass, AbstractAllTermDocs that also InstantiatedAllTermDocs extend.

Also:

 * II-tests made less plausable to pass on future incompatible changes to TermDocs and TermEnum
 * IITermDocs#skipTo and #next mimics the behaviour of document posisioning from SegmentTermDocs#dito when returning false
 * II now uses BitVector rather than sets for deleted documents
"
"LUCENE-3129","RFE","IMPROVEMENT","Single-pass grouping collector based on doc blocks","LUCENE-3112 enables adding/updating a contiguous block of documents to
the index, guaranteed (yet, experimental!) to retain adjacent docID
assignment through the full life of the index as long the app doesn't
delete individual docs from the block.

When an app does this, it can enable neat features like LUCENE-2454
(nested documents), post-group facet counting (LUCENE-3097).

It also makes single-pass grouping possible, when you group by
the ""identifier"" field shared by the doc block, since we know we will
see a given group only once with all of its docs within one block.

This should be faster than the fully general two-pass collectors we
already have.

I'm working on a patch but not quite there yet...
"
"LUCENE-1826","RFE","IMPROVEMENT","All Tokenizer implementations should have constructors that take AttributeSource and AttributeFactory","I have a TokenStream implementation that joins together multiple sub TokenStreams (i then do additional filtering on top of this, so i can't just have the indexer do the merging)

in 2.4, this worked fine.
once one sub stream was exhausted, i just started using the next stream 

however, in 2.9, this is very difficult, and requires copying Term buffers for every token being aggregated

however, if all the sub TokenStreams share the same AttributeSource, and my ""concat"" TokenStream shares the same AttributeSource, this goes back to being very simple (and very efficient)


So for example, i would like to see the following constructor added to StandardTokenizer:
{code}
  public StandardTokenizer(AttributeSource source, Reader input, boolean replaceInvalidAcronym) {
    super(source);
    ...
  }
{code}

would likewise want similar constructors added to all Tokenizer sub classes provided by lucene
"
"LUCENE-3743","TEST","BUG","LuceneTestCase's uncaught exceptions handler should check for AssumptionViolatedExceptions and then not trigger test failure","As in single-threaded tests, {{LuceneTestCase}} should not trigger test failures for {{AssumptionViolatedException}}'s when they occur in multi-threaded tests."
"LUCENE-1495","RFE","IMPROVEMENT","Allow TaskSequence to run for certain time","To help the perf testing for LUCENE-1483, I added simple ability to specify a fixed run time (seconds) for a task sequence, eg:
{code}
{ ""XSearchWithSort"" SearchWithSort(doctitle:string) > : 2.7s
{code}
iterates on that subtask until 2.7 seconds have elapsed, and then sets the repetition count to how many iterations were done.  This is useful when you are running searches whose runtime may vary drastically."
"LUCENE-3483","REFACTORING","IMPROVEMENT","Move Function grouping collectors from Solr to grouping module","Move the Function*Collectors from Solr (inside Grouping source file) to grouping module."
"LUCENE-882","IMPROVEMENT","IMPROVEMENT","Spellchecker doesn't need to store ngrams","The spellchecker in contrib stores the ngrams although this doesn't seem to be necessary. This patch changes that, I will commit it unless someone objects. This improves indexing speed and index size. Some numbers on a small test I did:

Input of the original index: 2200 text files, index size 5.3 MB, indexing took 17 seconds

Spell index before patch: about 60.000 documents, index size 13 MB, indexing took 62 seconds
Spell index after patch: about 60.000 documents, index size 6.3 MB, indexing took 52 seconds

BTW, the test case fails even before this patch. I'll probaby submit another issue about how to fix that.
"
"LUCENE-3472","BACKPORT","BUG","add back Document.getValues()","I'm porting some code to trunk's new Doc/Field apis, and i keep running into this pattern:
{noformat}
String[] values = doc.getValues(""field"");
{noformat}

But with the new apis, this becomes a little too verbose:

{noformat}
IndexableField[] fields = doc.getFields(""field"");
String[] values = new String[fields.length];
for (int i = 0; i < values.length; i++) {
  values[i] = fields[i].stringValue();
}
{noformat}

I think we should probably add back the sugar api (with the same name) ?
"
"LUCENE-2826","IMPROVEMENT","IMPROVEMENT","LineDocSource should assign stable IDs; docdate field should be NumericField","Some small enhancements when indexing docs from a line doc source:

  * Assign docid by line number instead of by number-of-docs-indexed;
    this makes the resulting ID stable when using multiple threads

  * The docdate field is now indexed as a String (possible created
    through DateTools).  I added two numeric fields: one that indexes
    .getTime() (= long msec) and another that indexes seconds since
    the day started.  This gives us two numeric fields to play
    with...
"
"LUCENE-1435","RFE","RFE","CollationKeyFilter: convert tokens into CollationKeys encoded using IndexableBinaryStringTools","Converts each token into its CollationKey using the provided collator, and then encodes the CollationKey with IndexableBinaryStringTools, to allow it to be stored as an index term.

This will allow for efficient range searches and Sorts over fields that need collation for proper ordering.
"
"LUCENE-539","CLEANUP","IMPROVEMENT","Fix for deprecations in contrib/surround","Fix for deprecations in contrib/surround."
"LUCENE-2035","BUG","BUG","TokenSources.getTokenStream() does not assign positionIncrement","TokenSources.StoredTokenStream does not assign positionIncrement information. This means that all tokens in the stream are considered adjacent. This has implications for the phrase highlighting in QueryScorer when using non-contiguous tokens.

For example:
Consider  a token stream that creates tokens for both the stemmed and unstemmed version of each word - the fox (jump|jumped)
When retrieved from the index using TokenSources.getTokenStream(tpv,false), the token stream will be - the fox jump jumped

Now try a search and highlight for the phrase query ""fox jumped"". The search will correctly find the document; the highlighter will fail to highlight the phrase because it thinks that there is an additional word between ""fox"" and ""jumped"". If we use the original (from the analyzer) token stream then the highlighter works.

Also, consider the converse - the fox did not jump
""not"" is a stop word and there is an option to increment the position to account for stop words - (the,0) (fox,1) (did,2) (jump,4)
When retrieved from the index using TokenSources.getTokenStream(tpv,false), the token stream will be - (the,0) (fox,1) (did,2) (jump,3).

So the phrase query ""did jump"" will cause the ""did"" and ""jump"" terms in the text ""did not jump"" to be highlighted. If we use the original (from the analyzer) token stream then the highlighter works correctly."
"LUCENE-3196","IMPROVEMENT","IMPROVEMENT","Optimize FixedStraightBytes for bytes size == 1","Currently we read all the bytes in a PagedBytes instance wich is unneeded for single byte values like norms. For fast access this should simply be a straight array."
"LUCENE-764","DOCUMENTATION","IMPROVEMENT","Document the temporary free space requirements of IndexWriter methods","Just opening an issue to track fixes to javadocs around Directory
space usage of optimize(), addIndexes(*), addDocument.

This came out of a recent thread on the users list around unexpectedly
high temporary disk usage during optimize():

  http://www.gossamer-threads.com/lists/lucene/java-user/43475

"
"LUCENE-1903","BUG","BUG","Incorrect ShingleFilter behavior when outputUnigrams == false","ShingleFilter isn't working as expected when outputUnigrams == false. In particular, it is outputting unigrams at least some of the time when outputUnigrams==false.

I'll attach a patch to ShingleFilterTest.java that adds some test cases that demonstrate the problem.

I haven't checked this, but I hypothesize that the behavior for outputUnigrams == false got changed when the class was upgraded to the new TokenStream API?"
"LUCENE-1843","TEST","IMPROVEMENT","Convert some tests to new TokenStream API, better support of cross-impl AttributeImpl.copyTo()","This patch converts some remaining tests to the new TokenStream API and non-deprecated classes.
This patch also enhances AttributeImpl.copyTo() of Token and TokenWrapper to also support copying e.g. TermAttributeImpl into Token. The target impl must only support all interfaces but must not be of the same type. Token and TokenWrapper use optimized coping without casting to 6 interfaces where possible.
Maybe the special tokenizers in contrib (shingle matrix and so on using tokens to cache may be enhanced by that). Also Yonik's request for optimized copying of states between incompatible AttributeSources may be enhanced by that (possibly a new issue)."
"LUCENE-661","BUILD_SYSTEM","BUG","BUILD.txt instructions wrong for JavaCC","The text in BUILD.txt for javacc says to set the property to the bin directory in the javacc installation. It should actually be set to the javacc installation directory, the directory containing the bin directory. The comments common-build.xml correctly state this."
"LUCENE-2038","DOCUMENTATION","TASK","Systemrequirements should say 1.5 instead of 1.4","The website still says Java 1.4 but it should say 1.5"
"LUCENE-771","IMPROVEMENT","IMPROVEMENT","Change default write lock file location to index directory (not java.io.tmpdir)","Now that readers are read-only, we no longer need to store lock files
in a different global lock directory than the index directory.  This
has been a source of confusion and caused problems to users in the
past.

Furthermore, once the write lock is stored in the index directory, it
no longer needs the big digest prefix that was previously required
to make sure lock files in the global lock directory, from different
indexes, did not conflict.

This way, all files related to an index will appear in a single
directory.  And you can easily list that directory to see if a
""write.lock"" is present to check whether a writer is open on the
index.

Note that this change just affects how FSDirectory creates its default
lockFactory if no lockFactory was specified.  It is still possible
(just no longer the default) to pick a different directory to store
your lock files by pre-instantiating your own LockFactory.

As part of this I would like to remove LOCK_DIR and the no-argument
constructor, in SimpleFSLockFactory and NativeFSLockFactory.  I don't
think we should have the notion of a global default lock directory
anymore.  This is actually an API change.  However, neither
SimpleFSLockFactory nor NativeFSLockFactory haver been released yet,
so I think this API removal is allowed?

Finally I want to deprecate (but not yet remove, because this has been
in the API for many releases) the static LOCK_DIR that's in
FSDirectory.  But it's now entirely unused.

See here for discussion leading to this:

  http://www.gossamer-threads.com/lists/lucene/java-dev/43940
"
"LUCENE-929","IMPROVEMENT","BUG","contrib/benchmark build doesn't handle checking if content is properly extracted","The contrib/benchmark build does not properly handle checking to see if the content (such as Reuters coll.) is properly extracted.  It only checks to see if the directory exists.  Thus, it is possible that the directory gets created and the extraction fails.  Then, the next time it is run, it skips the extraction part and tries to continue on running the benchmark.

The workaround is to manually delete the extraction directory."
"LUCENE-2254","RFE","IMPROVEMENT","Support more queries (other than just title) in Trec quality pkg","Now that we can properly parse descriptions and narratives from TREC queries (LUCENE-2210), it would be nice to allow the user to easily run quality evaluations on more than just ""Title""

This patch adds an optional commandline argument to QueryDriver (the default is Title as before), where you can specify something like:
T: Title-only
D: Description-only
N: Narrative-only
TD: Title + Description,
TDN: Title+Description+Narrative,
DN: Description+Narrative

The SimpleQQParser has an additional constructor that simply accepts a String[] of these fields, forming a booleanquery across all of them.
"
"LUCENE-2634","BUG","BUG","IndexReader.isCurrent() lies if documents were only removed by latest commit","Usecase is as following:

1. Get indexReader via indexWriter.
2. Delete documents by Term via indexWriter. 
3. Commit indexWriter.
4. indexReader.isCurrent() returns true.

Usually there is a check if index reader is current. If not then it is reopened (re-obtained via writer or ect.). But this causes the problem when documents can still be found through the search after deletion.
Testcase is attached."
"LUCENE-3186","IMPROVEMENT","IMPROVEMENT","DocValues type should be recored in FNX file to early fail if user specifies incompatible type","Currently segment merger fails if the docvalues type is not compatible across segments. We already catch this problem if somebody changes the values type for a field within one segment but not across segments. in order to do that we should record the type in the fnx fiel alone with the field numbers.

I marked this 4.0 since it should not block the landing on trunk"
"LUCENE-1449","BUG","BUG","IndexDeletionPolicy.delete behaves incorrectly when deleting latest generation ","I have been looking to provide the ability to rollback committed transactions and encountered some issues.
I appreciate IndexDeletionPolicy's main motivation is to handle cleaning away OLD commit points but it does not explicitly state that it can or cannot be used to clean NEW commit points.

If this is not supported then the documentation should ideally state this. If the intention is to support this behaviour then read on .......

There seem to be 2 issues so far:
1) The first attempt to call IndexCommit.delete on the latest commit point fails to remove any contents. The subsequent call succeeds however
2) Deleting the latest commit point fails to update the segments.gen file to point to segments_N-1. New IndexReaders that are opened are then misdirected to open segments_N which has been deleted

Junit test to follow...

"
"LUCENE-3210","RFE","IMPROVEMENT","TieredMergePolicy should expose control over how aggressively segments with deletions are targeted ","TMP today always does a linear pro-rating of a merge's score according to what pctg of the documents are deleted; I'd like to 1) put a power factor in (score is multiplicative), and 2) default it to stronger favoring of merging away deletions."
"LUCENE-207","BUG","BUG","[PATCH] npe if java.io.tmpdir does not exist","In org.apache.lucene.store.FSDirectory from Lucene-1.3-final, on line 170-171:

File tmpdir = new File(System.getProperty(""java.io.tmpdir""));
files = tmpdir.list();

if the directory specified by the property ""java.io.tmpdir"" does not exist, a
null pointer exception is thrown.  Perhaps a check to see if the directory
exists is in order, and if it doesn't, use a directory you know exists (e.g. a
/temp directory in the directory created earlier in the create() method)."
"LUCENE-1582","REFACTORING","IMPROVEMENT","Make TrieRange completely independent from Document/Field with TokenStream of prefix encoded values","TrieRange has currently the following problem:
- To add a field, that uses a trie encoding, you can manually add each term to the index or use a helper method from TrieUtils. The helper method has the problem, that it uses a fixed field configuration
- TrieUtils currently creates per default a helper field containing the lower precision terms to enable sorting (limitation of one term/document for sorting)
- trieCodeLong/Int() creates unnecessarily String[] and char[] arrays that is heavy for GC, if you index lot of numeric values. Also a lot of char[] to String copying is involved.

This issue should improve this:
- trieCodeLong/Int() returns a TokenStream. During encoding, all char[] arrays are reused by Token API, additional String[] arrays for the encoded result are not created, instead the TokenStream enumerates the trie values.
- Trie fields can be added to Documents during indexing using the standard API: new Field(name,TokenStream,...), so no extra util method needed. By using token filters, one could also add payload and so and customize everything.

The drawback is: Sorting would not work anymore. To enable sorting, a (sub-)issue can extend the FieldCache to stop iterating the terms, as soon as a lower precision one is enumerated by TermEnum. I will create a ""hack"" patch for TrieUtils-use only, that uses a non-checked Exceptionin the Parser to stop iteration. With LUCENE-831, a more generic API for this type can be used (custom parser/iterator implementation for FieldCache). I will attach the field cache patch (with the temporary solution, until FieldCache is reimplemented) as a separate patch file, or maybe open another issue for it."
"LUCENE-1325","RFE","IMPROVEMENT","add IndexCommit.isOptimized method","Spinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200807.mbox/%3C69de18140807010347s6269fea5r12c3212e0ec0a12a@mail.gmail.com%3E"
"LUCENE-1142","BUILD_SYSTEM","IMPROVEMENT","Updated Snowball package","Updated Snowball contrib package

 * New org.tartarus.snowball java package with patched SnowballProgram to be abstract to avoid using reflection.
 * Introducing Hungarian, Turkish and Romanian stemmers
 * Introducing constructor SnowballFilter(SnowballProgram)

It is possible there have been some changes made to the some of there stemmer algorithms between this patch and the current SVN trunk of Lucene, an index might thus not be compatible with new stemmers!

The API is backwards compatibile and the test pass."
"LUCENE-3690","RFE","RFE","JFlex-based HTMLStripCharFilter replacement","A JFlex-based HTMLStripCharFilter replacement would be more performant and easier to understand and maintain."
"LUCENE-1346","IMPROVEMENT","IMPROVEMENT","replace Vector with ArrayList in Queries","Replace Vector with ArrayList in Queries.  This can make a difference in heavily concurrent scenarios when Query objects are examined or compared (e.g. used as cache keys)."
"LUCENE-2961","CLEANUP","IMPROVEMENT","Remove benchmark/lib/xml-apis.jar - JVM 1.5 already contains the required JAXP 1.3 implementation","On [LUCENE-2957|https://issues.apache.org/jira/browse/LUCENE-2957?focusedCommentId=13004991&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13004991], Uwe wrote:
{quote}
xml-apis.jar is not needed with xerces-2.9 and Java 5, as Java 5 already has these interface classes (JAXP 1.3). Xerces 2.11 needs it, because it ships with Java 6's JAXP release (containing STAX & Co. not available in Java 5).
{quote}

On the #lucene IRC channel, Uwe also wrote:
{noformat}
since we are on java 5 since 3.0
we have the javax APIs already available in the JVM
xerces until 2.9.x only needs JAXP 1.3
so the only thing you need is xercesImpl.jar
and serializer.jar
serializer.jar is shared between all apache xml projects, dont know the exact version number
ok you dont need it whan you only parse xml
as soon as you want to serialize a dom tree or result of an xsl transformation you need it
[...]
but if we upgrade to latest xerces we need it [the xml-apis jar] again unless we are on java 6
so the one shipped with xerces 2.11 is the 1.4 one
because xerces 2.11 supports Stax
{noformat}"
"LUCENE-1809","RFE","BUG","highlight-vs-vector-highlight.alg is unfair","highlight-vs-vector-highlight.alg uses EnwikiQueryMaker which makes SpanQueries, but FastVectorHighlighter simply ignores SpanQueries."
"LUCENE-3644","BUG","BUG","problems with IR's readerFinishedListener","There are two major problems:
1. The listener api does not really apply all indexreaders. for example segmentreaders dont fire it on close, only segmentcorereaders. this is wrong, a segmentcorereader is *not* an indexreader. Furthermore, if you register it on a top-level reader you get events for anything under the reader tree (sometimes, unless they are segmentreaders as mentioned above, where it doesnt work correctly at all).
2. Furthermore your listener is 'passed along' in a viral fashion from clone() and reopen(). This means for example, if you are trying to listen to readers in NRT search you are just accumulating reader listeners, all potentially keeping references to old indexreaders (because, in order to deal with #1 your listener must 'keep' a reference to the IR it was registered on, so it can check if thats *really* the one).

We should discuss how to fix #1. 

I will create a patch for #2 shortly and commit it, its just plain wrong.
"
"LUCENE-3578","BUG","BUG","TestSort testParallelMultiSort reproducible seed failure","trunk r1202157
{code}
    [junit] Testsuite: org.apache.lucene.search.TestSort
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.978 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestSort -Dtestmethod=testParallelMultiSort -Dtests.seed=-2996f3e0f5d118c2:32c8e62dd9611f63:7a90f44586ae8263 -Dargs=""-Dfile.encoding=UTF-8""
    [junit] WARNING: test method: 'testParallelMultiSort' left thread running: Thread[pool-1-thread-1,5,main]
    [junit] WARNING: test method: 'testParallelMultiSort' left thread running: Thread[pool-1-thread-2,5,main]
    [junit] WARNING: test method: 'testParallelMultiSort' left thread running: Thread[pool-1-thread-3,5,main]
    [junit] NOTE: test params are: codec=Lucene40: {short=Lucene40(minBlockSize=98 maxBlockSize=214), contents=PostingsFormat(name=MockSep), byte=PostingsFormat(name=SimpleText), int=Pulsing40(freqCutoff=4 minBlockSize=58 maxBlockSize=186), string=PostingsFormat(name=NestedPulsing), i18n=Lucene40(minBlockSize=98 maxBlockSize=214), long=PostingsFormat(name=Memory), double=Pulsing40(freqCutoff=4 minBlockSize=58 maxBlockSize=186), parser=MockVariableIntBlock(baseBlockSize=88), float=Lucene40(minBlockSize=98 maxBlockSize=214), custom=PostingsFormat(name=MockRandom)}, sim=RandomSimilarityProvider(queryNorm=false,coord=false): {short=BM25(k1=1.2,b=0.75), tracer=DFR I(ne)B2, byte=DFR I(ne)B3(800.0), contents=IB LL-LZ(0.3), int=DFR I(n)BZ(0.3), string=IB LL-D3(800.0), i18n=DFR GB2, double=DFR I(ne)B2, long=DFR GB1, parser=DFR GL2, float=BM25(k1=1.2,b=0.75), custom=DFR I(ne)Z(0.3)}, locale=ga_IE, timezone=America/Louisville
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestSort]
    [junit] NOTE: Linux 3.0.6-gentoo amd64/Sun Microsystems Inc. 1.6.0_29 (64-bit)/cpus=8,threads=4,free=78022136,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testParallelMultiSort(org.apache.lucene.search.TestSort): FAILED
    [junit] expected:<[ZJ]I> but was:<[JZ]I>
    [junit] junit.framework.AssertionFailedError: expected:<[ZJ]I> but was:<[JZ]I>
    [junit]     at org.apache.lucene.search.TestSort.assertMatches(TestSort.java:1245)
    [junit]     at org.apache.lucene.search.TestSort.assertMatches(TestSort.java:1216)
    [junit]     at org.apache.lucene.search.TestSort.runMultiSorts(TestSort.java:1202)
    [junit]     at org.apache.lucene.search.TestSort.testParallelMultiSort(TestSort.java:855)
    [junit]     at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:523)
    [junit]     at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:149)
    [junit]     at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:51)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.search.TestSort FAILED
{code}"
"LUCENE-1724","IMPROVEMENT","IMPROVEMENT","Analysis package calls Java 1.5 API","I found compile errors when I tried to compile trunk with 1.4 JVM.
org.apache.lucene.analysis.NormalizeCharMap
org.apache.lucene.analysis.MappingCharFilter

uses Character.valueOf() which has been added in 1.5.
I added a CharacterCache (+ testcase) with a valueOf method as a replacement for that quite useful method.

org.apache.lucene.analysis.BaseTokenTestCase

uses StringBuilder instead of the synchronized version StringBuffer (available in 1.4)

I will attach a patch shortly."
"LUCENE-2055","CLEANUP","BUG","Fix buggy stemmers and Remove duplicate analysis functionality","would like to remove stemmers in the following packages, and instead in their analyzers use a SnowballStemFilter instead.

* analyzers/fr
* analyzers/nl
* analyzers/ru

below are excerpts from this code where they proudly proclaim they use the snowball algorithm.
I think we should delete all of this custom stemming code in favor of the actual snowball package.


{noformat}
/**
 * A stemmer for French words. 
 * <p>
 * The algorithm is based on the work of
 * Dr Martin Porter on his snowball project<br>
 * refer to http://snowball.sourceforge.net/french/stemmer.html<br>
 * (French stemming algorithm) for details
 * </p>
 */

public class FrenchStemmer {

/**
 * A stemmer for Dutch words. 
 * <p>
 * The algorithm is an implementation of
 * the <a href=""http://snowball.tartarus.org/algorithms/dutch/stemmer.html"">dutch stemming</a>
 * algorithm in Martin Porter's snowball project.
 * </p>
 */
public class DutchStemmer {

/**
 * Russian stemming algorithm implementation (see http://snowball.sourceforge.net for detailed description).
 */
class RussianStemmer
{noformat}

"
"LUCENE-3306","CLEANUP","IMPROVEMENT","disable positions for spellchecker ngram fields","In LUCENE-2391 we optimized spellchecker (re)build time/ram usage by omitting frequencies/positions/norms for single-valued fields,
among other things.

Now that we can disable positions but keep freqs, we should disable them for the n-gram fields, because the spellchecker does
not use positional queries.
"
"LUCENE-1488","RFE","RFE","multilingual analyzer based on icu","The standard analyzer in lucene is not exactly unicode-friendly with regards to breaking text into words, especially with respect to non-alphabetic scripts.  This is because it is unaware of unicode bounds properties.

I actually couldn't figure out how the Thai analyzer could possibly be working until i looked at the jflex rules and saw that codepoint range for most of the Thai block was added to the alphanum specification. defining the exact codepoint ranges like this for every language could help with the problem but you'd basically be reimplementing the bounds properties already stated in the unicode standard. 

in general it looks like this kind of behavior is bad in lucene for even latin, for instance, the analyzer will break words around accent marks in decomposed form. While most latin letter + accent combinations have composed forms in unicode, some do not. (this is also an issue for asciifoldingfilter i suppose). 

I've got a partially tested standardanalyzer that uses icu Rule-based BreakIterator instead of jflex. Using this method you can define word boundaries according to the unicode bounds properties. After getting it into some good shape i'd be happy to contribute it for contrib but I wonder if theres a better solution so that out of box lucene will be more friendly to non-ASCII text. Unfortunately it seems jflex does not support use of these properties such as [\p{Word_Break = Extend}] so this is probably the major barrier.

Thanks,
Robert



"
"LUCENE-2444","REFACTORING","TASK","move contrib/analyzers to modules/analysis","This is a patch to move contrib/analyzers under modules/analyzers

We can then continue consolidating (LUCENE-2413)... in truth this will sorta be 
an ongoing thing anyway, as we try to distance indexing from analysis, etc
"
"LUCENE-941","BUG","BUG","Benchmark alg line -  {[AddDoc(4000)]: 4} : * - causes an infinite loop","Background in http://www.mail-archive.com/java-dev@lucene.apache.org/msg10831.html 
The line  
   {[AddDoc(4000)]: 4} : * 
causes an infinite loop because the parallel sequence would mask the exhaustion from the outer sequential sequence.

To fix this the DocMaker exhaustion check should be modified to rely  on the doc maker instance only, and to be reset when the inputs are being reset. "
"LUCENE-3776","DESIGN_DEFECT","IMPROVEMENT","NRTManager shouldn't expose its private SearcherManager","Spinoff from LUCENE-3769.

To actually obtain an IndexSearcher from NRTManager, it's a 2-step process now.

You must .getSearcherManager(), then .acquire() from the returned SearcherManager.

This is very trappy... because if the app incorrectly calls maybeReopen on that private SearcherManager (instead of NRTManager.maybeReopen) then it can unexpectedly cause threads to block forever, waiting for the necessary gen to become visible.  This will be hard to debug... I don't like creating trappy APIs.

Hopefully once LUCENE-3761 is in, we can fix NRTManager to no longer expose its private SM, instead subclassing ReferenceManaager.

Or alternatively, or in addition, maybe we factor out a new interface (SearcherProvider or something...) that only has acquire and release methods, and both NRTManager and ReferenceManager/SM impl that, and we keep NRTManager's SM private."
"LUCENE-3658","BUG","BUG","NRTCachingDir has invalid asserts (if same file name is written twice)","Normally Lucene is write-once (except for segments.gen file, which NRTCachingDir never caches), but in some tests (TestDoc, TestCrash) we can write the same file more than once.

I don't think NRTCachingDir should have these asserts, and I think on createOutput it should remove any old file if present.

I also found & fixed a possible concurrency issue (if more than one thread syncs at the same time; IndexWriter doesn't ever do this today but it has in the past)."
"LUCENE-1327","BUG","BUG","TermSpans skipTo() doesn't always move forwards","In TermSpans (or the anonymous Spans class returned by SpansTermQuery, depending on the version), the skipTo() method is improperly implemented if the target doc is less than or equal to the current doc:

  public boolean skipTo(int target) throws IOException {
          // are we already at the correct position?
          if (doc >= target) {
            return true;
          }

          ...


This violates the correct behavior (as described in the Spans interface documentation), that skipTo() should always move forwards, in other words the correct implementation would be:

if (doc >= target) {
  return next();
}

This bug causes particular problems if one wants to use the payloads feature - this is because if one loads a payload, then performs a skipTo() to the same document, then tries to load the ""next"" payload, the spans hasn't changed position and it attempts to load the same payload again (which is an error)."
"LUCENE-3074","RFE","TASK","SimpleTextCodec needs SimpleText DocValues impl","currently SimpleTextCodec uses binary docValues we should move that to a simple text impl."
"LUCENE-1369","IMPROVEMENT","IMPROVEMENT","Eliminate unnecessary uses of Hashtable and Vector","Lucene uses Vector, Hashtable and Enumeration when it doesn't need to. Changing to ArrayList and HashMap may provide better performance.

There are a few places Vector shows up in the API. IMHO, List should have been used for parameters and return values.

There are a few distinct usages of these classes:
# internal but with ArrayList or HashMap would do as well. These can simply be replaced.
# internal and synchronization is required. Either leave as is or use a collections synchronization wrapper.
# As a parameter to a method where List or Map would do as well. For contrib, just replace. For core, deprecate current and add new method signature.
# Generated by JavaCC. (All *.jj files.) Nothing to be done here.
# As a base class. Not sure what to do here. (Only applies to SegmentInfos extends Vector, but it is not used in a safe manner in all places. Perhaps, implements List would be better.)
# As a return value from a package protected method, but synchronization is not used. Change return type.
# As a return value to a final method. Change to List or Map.

In using a Vector the following iteration pattern is frequently used.
for (int i = 0; i < v.size(); i++) {
  Object o = v.elementAt(i);
}

This is an indication that synchronization is unimportant. The list could change during iteration.

"
"LUCENE-3573","BUG","BUG","TaxonomyReader.refresh() is broken, replace its logic with reopen(), following IR.reopen pattern","When recreating the taxonomy index, TR's assumption that categories are only added does not hold anymore.
As result, calling TR.refresh() will be incorrect at best, but usually throw an AIOOBE."
"LUCENE-2169","IMPROVEMENT","IMPROVEMENT","Speedup of CharArraySet#copy if a CharArraySet instance is passed to copy.","the copy method should use the entries array itself to copy the set internally instead of iterating over all values. this would speedup copying even small set "
"LUCENE-1465","BUG","BUG","NearSpansOrdered.getPayload does not return the payload from the minimum match span",""
"LUCENE-2796","TEST","BUG","Tests need to clean up after themselves","I havent run 'ant clean' for a while.

The randomly generated temporarily file names just piled up from running the tests many times... so ant clean is still running after quite a long time.

We should take the logic in the base solr test cases, and push it into LuceneTestCase, so a test cleans up all its temporary stuff.
"
"LUCENE-2012","CLEANUP","TASK","Add @Override annotations","During removal of deprecated APIs, mostly the problem was, to not only remove the method in the (abstract) base class (e.g. Scorer.explain()), but also remove it in sub classes that override it. You can easily forget that (especially, if the method was not marked deprecated in the subclass). By adding @Override annotations everywhere in Lucene, such removals are simple, because the compiler throws out an error message in all subclasses which then no longer override the method.

Also it helps preventing the well-known traps like overriding hashcode() instead of hashCode().

The patch was generated automatically, and is rather large. Should I apply it, or would it break too many patches (but I think, trunk has changed so much, that this is only a minimum of additional work to merge)?"
"LUCENE-3687","RFE","RFE","Allow similarity to encode norms other than a single byte","LUCENE-3628 cut over norms to docvalues. This removes the long standing limitation that norms are a single byte. Yet, we still need to expose this functionality to Similarity to write / encode norms in a different format. "
"LUCENE-540","BUG","BUG","Merging multiple indexes does not maintain document order.","When I merge multiple indexes into a single, empty index, the document addition order is not being maintained.

Self contained test case coming (as soon as I figure out how to attach it)"
"LUCENE-2934","IMPROVEMENT","IMPROVEMENT","Alternative depth-based DOT layout ordering in FST's Utils","Utils.toDot() dumps GraphViz's DOT file, but it can be quite difficult to read. This patch provides an alternative layout that is probably a little bit easier on the eyes (well, as far as larger FSTs can be ;)"
"LUCENE-443","IMPROVEMENT","BUG","ConjunctionScorer tune-up","I just recently ran a load test on the latest code from lucene , which is using a new BooleanScore and noticed the ConjunctionScorer was crunching through objects , especially while sorting as part of the skipTo call. It turns a linked list into an array, sorts the array, then converts the array back to a linked list for further processing by the scoring engines below.

'm not sure if anyone else is experiencing this as I have a very large index (> 4 million items) and I am issuing some heavily nested queries

Anyway, I decide to change the link list into an array and use a first and last marker to ""simulate"" a linked list.

This scaled much better during my load test as the java gargbage collector was less - umm - virulent "
"LUCENE-656","CLEANUP","IMPROVEMENT","FieldsInfo uses deprecated API","The class FieldsInfo.java uses deprecated API in method ""public void add(Document doc)""
I rused the replacement and created the patch -> see attachment"
"LUCENE-3534","BACKPORT","IMPROVEMENT","Backport FilteredQuery/IndexSearcher changes to 3.x: Remove filter logic from IndexSearcher and delegate to FilteredQuery","Spinoff from LUCENE-1536: We simplified the code in IndexSearcher to no longer do the filtering there, instead wrap all Query with FilteredQuery, if a non-null filter is given. The conjunction code would then only exist in FilteredQuery which makes it easier to maintain. Currently both implementations differ in 3.x, in trunk we used the more optimized IndexSearcher variant with addition of a simplified in-order conjunction code.

This issue will backport those changes (without random access bits)."
"LUCENE-1280","BUG","BUG","NPE in PhraseQuery.toString(String f)","the section

public String toString(String f) {
    StringBuffer buffer = new StringBuffer();
    if (!field.equals(f)) {
      buffer.append(field);
      buffer.append("":"");
    }
    <snip>


should be

public String toString(String f) {
    StringBuffer buffer = new StringBuffer();
    if (field != null && !field.equals(f)) {
      buffer.append(field);
      buffer.append("":"");
    }
    <snip>


The issue arises if a phrase query is created, no terms are added, then the phrase query is added to a boolean query. Calling toString on the boolean query will result in a NPE insdie of the PhraseQuery.
"
"LUCENE-729","IMPROVEMENT","IMPROVEMENT","non-recursive MultiTermDocs","A non-recursive implementation of MultiTermDocs.next() and skipTo() would be nice as it's currently possible to get a stack overflow in very rare situations."
"LUCENE-1038","RFE","IMPROVEMENT","TermVectorMapper.setDocumentNumber()","Passes down the index of the document whose term vector is currently beeing mapped, once for each top level call to a term vector reader.  

See http://www.nabble.com/Allowing-IOExceptions-in-TermVectorMapper--tf4687704.html#a13397341"
"LUCENE-3225","IMPROVEMENT","IMPROVEMENT","Optimize TermsEnum.seek when caller doesn't need next term","Some codecs are able to save CPU if the caller is only interested in
exact matches.  EG, Memory codec and SimpleText can do more efficient
FSTEnum lookup if they know the caller doesn't need to know the term
following the seek term.

We have cases like this in Lucene, eg when IW deletes documents by
Term, if the term is not found in a given segment then it doesn't need
to know the ceiling term.  Likewise when TermQuery looks up the term
in each segment.

I had done this change as part of LUCENE-3030, which is a new terms
index that's able to save seeking for exact-only lookups, but now that
we have Memory codec that can also save CPU I think we should commit
this today.

The change adds a ""boolean onlyExact"" param to seek(BytesRef).
"
"LUCENE-3745","RFE","IMPROVEMENT","Need stopwords and stoptags lists for default Japanese configuration","Stopwords and stoptags lists for Japanese needs to be developed, tested and integrated into Lucene."
"LUCENE-2571","TEST","TASK","Indexing performance tests with realtime branch","We should run indexing performance tests with the DWPT changes and compare to trunk.

We need to test both single-threaded and multi-threaded performance.

NOTE:  flush by RAM isn't implemented just yet, so either we wait with the tests or flush by doc count."
"LUCENE-2290","IMPROVEMENT","IMPROVEMENT","Remove unnecessary String concatenation in IndexWriter","I've noticed a couple of places in IndexWriter where a boolean string is created by bool + """", or integer by int + """". There are some places (in setDiagonstics) where a string is concatenated with an empty String ...
The patch uses Boolean.toString and Integer.toString, as well as remove the unnecessary str + """"."
"LUCENE-1114","DOCUMENTATION","BUG","contrib/Highlighter javadoc example needs to be updated","The Javadoc package.html example code is outdated, as it still uses QueryParser.parse.  

http://lucene.zones.apache.org:8080/hudson/job/Lucene-Nightly/javadoc/contrib-highlighter/index.html"
"LUCENE-3401","BUG","BUG","need to ensure that sims that use collection-level stats (e.g. sumTotalTermFreq) handle non-existent field","Because of things like queryNorm, unfortunately similarities have to handle the case where they are asked to computeStats() for a term, where the field does not exist at all.
(Note they will never have to actually score anything, but unless we break how queryNorm works for TFIDF, we have to deal with this case).

I noticed this while doing some benchmarking, so i created a test to test some cases like this across all the sims."
"LUCENE-325","RFE","IMPROVEMENT","[PATCH] new method expungeDeleted() added to IndexWriter","We make use the docIDs in lucene. I need a way to compact the docIDs in segments
to remove the ""holes"" created from doing deletes. The only way to do this is by
calling IndexWriter.optimize(). This is a very heavy call, for the cases where
the index is large but with very small number of deleted docs, calling optimize
is not practical.

I need a new method: expungeDeleted(), which finds all the segments that have
delete documents and merge only those segments.

I have implemented this method and have discussed with Otis about submitting a
patch. I don't see where I can attached the patch. I will do according to the
patch guidleine and email the lucene mailing list.

Thanks

-John

I don't see a place where I can"
"LUCENE-3182","BUG","BUG","TestAddIndexes reproducible test failure on turnk","trunk: r1133385

{code}
    [junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Tests run: 2843, Failures: 1, Errors: 0, Time elapsed: 137.121 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] java.io.FileNotFoundException: _cy.fdx
    [junit]     at org.apache.lucene.store.RAMDirectory.fileLength(RAMDirectory.java:121)
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.fileLength(MockDirectoryWrapper.java:606)
    [junit]     at org.apache.lucene.index.SegmentInfo.sizeInBytes(SegmentInfo.java:294)
    [junit]     at org.apache.lucene.index.TieredMergePolicy.size(TieredMergePolicy.java:633)
    [junit]     at org.apache.lucene.index.TieredMergePolicy.useCompoundFile(TieredMergePolicy.java:611)
    [junit]     at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:2459)
    [junit]     at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:847)
    [junit]     at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:675)
    [junit] java.io.FileNotFoundException: _cx.fdx
    [junit]     at org.apache.lucene.store.RAMDirectory.fileLength(RAMDirectory.java:121)
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.fileLength(MockDirectoryWrapper.java:606)
    [junit]     at org.apache.lucene.index.SegmentInfo.sizeInBytes(SegmentInfo.java:294)
    [junit]     at org.apache.lucene.index.TieredMergePolicy.size(TieredMergePolicy.java:633)
    [junit]     at org.apache.lucene.index.TieredMergePolicy.useCompoundFile(TieredMergePolicy.java:611)
    [junit]     at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:2459)
    [junit]     at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:847)
    [junit]     at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:675)
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithRollback -Dtests.seed=9026722750295014952:2645762923088581043 -Dtests.multiplier=3
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=SimpleText, content=SimpleText, d=MockRandom, c=SimpleText}, locale=fr, timezone=Africa/Kigali
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestAddIndexes]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=68050392,total=446234624
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAddIndexesWithRollback(org.apache.lucene.index.TestAddIndexes):       FAILED
    [junit]
    [junit] junit.framework.AssertionFailedError:
    [junit]     at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithRollback(TestAddIndexes.java:932)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1362)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1280)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.index.TestAddIndexes FAILED
{code}


Fails randomly in my while(1) test run, and Fails after a few min of running: 
{code}
ant test -Dtestcase=TestAddIndexes -Dtests.seed=9026722750295014952:2645762923088581043 -Dtests.multiplier=3 -Dtests.iter=200 -Dtests.iter.min=1
{code}"
"LUCENE-2940","BUG","BUG","NPE in TestNRTThreads","I hit this when while(1)ing this test... I think it's because the logic on when we ask the SegmentReader to load stored fields is off...

{noformat}
*** Thread: Lucene Merge Thread #1 ***
org.apache.lucene.index.MergePolicy$MergeException:
java.lang.NullPointerException
       at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:507)
       at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:472)
Caused by: java.lang.NullPointerException
       at org.apache.lucene.index.SegmentReader$FieldsReaderLocal.initialValue(SegmentReader.java:245)
       at org.apache.lucene.index.SegmentReader$FieldsReaderLocal.initialValue(SegmentReader.java:242)
       at org.apache.lucene.util.CloseableThreadLocal.get(CloseableThreadLocal.java:68)
       at org.apache.lucene.index.SegmentReader.getFieldsReader(SegmentReader.java:749)
       at org.apache.lucene.index.SegmentReader.document(SegmentReader.java:838)
       at org.apache.lucene.index.IndexReader.document(IndexReader.java:951)
       at org.apache.lucene.index.TestNRTThreads$1.warm(TestNRTThreads.java:86)
       at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3311)
       at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2875)
       at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:379)
       at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:447)
{noformat}
"
"LUCENE-1987","CLEANUP","TASK","Remove rest of analysis deprecations (Token, CharacterCache)","These removes the rest of the deprecations in the analysis package:
- -Token's termText field-- (DONE)
- -eventually un-deprecate ctors of Token taking Strings (they are still useful) -> if yes remove deprec in 2.9.1- (DONE)
- -remove CharacterCache and use Character.valueOf() from Java5- (DONE)
- Stopwords lists
- Remove the backwards settings from analyzers (acronym, posIncr,...). They are deprecated, but we still have the VERSION constants. Do not know, how to proceed. Keep the settings alive for index compatibility? Or remove it together with the version constants (which were undeprecated)."
"LUCENE-1011","BUG","BUG","Two or more writers over NFS can cause index corruption","When an index is used over NFS, and, more than one machine can be a
writer such that they swap roles quickly, it's possible for the index
to become corrupt if the NFS client directory cache is stale.

Not all NFS clients will show this.  Very recent versions of Linux's
NFS client do not seem to show the issue, yet, slightly older ones do,
and the latest Mac OS X one does as well.

I've been working with Patrick Kimber, who provided a standalone test
showing the problem (thank you Patrick!).  This came out of this
thread:

  http://www.gossamer-threads.com/lists/engine?do=post_view_flat;post=50680;page=1;sb=post_latest_reply;so=ASC;mh=25;list=lucene

Note that the first issue in that discussion has been resolved
(LUCENE-948).  This is a new issue.
"
"LUCENE-2688","RFE","BUG","NativeFSLockFactory throws an exception on Android 2.2 platform as java.lang.management package is not available on android.","NativeFSLockFactory throws an exception on Android 2.2 platform as java.lang.management package is not available on android.
   Looks like FSDirectory defaults to NativeFSLockFactory, and this class refers to java.lang.management package to generate a unique lock. java.lang.management is not available in Android 2.2 and hence a runtime exception is raised. The workaround is to use another custom LockFactory or SimpleFSLockFactory, but Fixing NativeFSLockFactroy will help.

Thanks,
Surinder"
"LUCENE-1272","RFE","RFE","Support for boost factor in MoreLikeThis","This is a patch I made to be able to boost the terms with a specific factor beside the relevancy returned by MoreLikeThis. This is helpful when having more then 1 MoreLikeThis in the query, so words in the field A (i.e. Title) can be boosted more than words in the field B (i.e. Description)."
"LUCENE-822","RFE","IMPROVEMENT","Make FieldSelector usable from Searchable ","Seems reasonable that you would want to be able to specify a FieldSelector from Searchable because many systems do not use IndexSearcher (where you can get a Reader), but instead use Searchable or Searcher so that Searchers and MultiSearchers can be used in a polymorphic manner."
"LUCENE-3296","RFE","IMPROVEMENT","Enable passing a config into PKIndexSplitter","I need to be able to pass the IndexWriterConfig into the IW used by PKIndexSplitter."
"LUCENE-2293","RFE","BUG","IndexWriter has hard limit on max concurrency","DocumentsWriter has this nasty hardwired constant:

{code}
private final static int MAX_THREAD_STATE = 5;
{code}

which probably I should have attached a //nocommit to the moment I
wrote it ;)

That constant sets the max number of thread states to 5.  This means,
if more than 5 threads enter IndexWriter at once, they will ""share""
only 5 thread states, meaning we gate CPU concurrency to 5 running
threads inside IW (each thread must first wait for the last thread to
finish using the thread state before grabbing it).

This is bad because modern hardware can make use of more than 5
threads.  So I think an immediate fix is to make this settable
(expert), and increase the default (8?).

It's tricky, though, because the more thread states, the less RAM
efficiency you have, meaning the worse indexing throughput.  So you
shouldn't up and set this to 50: you'll be flushing too often.

But... I think a better fix is to re-think how threads write state
into DocumentsWriter.  Today, a single docID stream is assigned across
threads (eg one thread gets docID=0, next one docID=1, etc.), and each
thread writes to a private RAM buffer (living in the thread state),
and then on flush we do a merge sort.  The merge sort is inefficient
(does not currently use a PQ)... and, wasteful because we must
re-decode every posting byte.

I think we could change this, so that threads write to private RAM
buffers, with a private docID stream, but then instead of merging on
flush, we directly flush each thread as its own segment (and, allocate
private docIDs to each thread).  We can then leave merging to CMS
which can already run merges in the BG without blocking ongoing
indexing (unlike the merge we do in flush, today).

This would also allow us to separately flush thread states.  Ie, we
need not flush all thread states at once -- we can flush one when it
gets too big, and then let the others keep running.  This should be a
good concurrency gain since is uses IO & CPU resources ""throughout""
indexing instead of ""big burst of CPU only"" then ""big burst of IO
only"" that we have today (flush today ""stops the world"").

One downside I can think of is... docIDs would now be ""less
monotonic"", meaning if N threads are indexing, you'll roughly get
in-time-order assignment of docIDs.  But with this change, all of one
thread state would get 0..N docIDs, the next thread state'd get
N+1...M docIDs, etc.  However, a single thread would still get
monotonic assignment of docIDs.
"
"LUCENE-879","RFE","IMPROVEMENT","Document number integrity merge policy","This patch allows for document numbers stays the same even after merge of segments with deletions.

Consumer needs to do this:
indexWriter.setSkipMergingDeletedDocuments(false);

The effect will be that deleted documents are replaced by a new Document() in the merged segment, but not marked as deleted. This should probably be some policy thingy that allows for different solutions such as keeping the old document, et c.

Also see http://www.nabble.com/optimization-behaviour-tf3723327.html#a10418880
"
"LUCENE-2011","CLEANUP","TASK","Remove deprecated Scorer.explain(int) method","This is the only remaining deprecation in core, but is not so easy to handle, because lot's of code in core still uses the explain() method in Scorer. So e.g. in PhraseQuery, the explain method has to be moved from Scorer to the Weight."
"LUCENE-288","BUILD_SYSTEM","BUG","[patch] better support gcj compilation","In order to workaround http://gcc.gnu.org/bugzilla/show_bug.cgi?id=15411 the
attached patch is necessary."
"LUCENE-1016","RFE","RFE","TermVectorAccessor, transparent vector space access ","This class visits TermVectorMapper and populates it with information transparent by either passing it down to the default terms cache (documents indexed with Field.TermVector) or by resolving the inverted index."
"LUCENE-1314","RFE","RFE","IndexReader.clone","Based on discussion http://www.nabble.com/IndexReader.reopen-issue-td18070256.html.  The problem is reopen returns the same reader if there are no changes, so if docs are deleted from the new reader, they are also reflected in the previous reader which is not always desired behavior."
"LUCENE-967","RFE","IMPROVEMENT","Add ""tokenize documents only"" task to contrib/benchmark","I've been looking at performance improvements to tokenization by
re-using Tokens, and to help benchmark my changes I've added a new
task called ReadTokens that just steps through all fields in a
document, gets a TokenStream, and reads all the tokens out of it.

EG this alg just reads all Tokens for all docs in Reuters collection:

  doc.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersDocMaker
  doc.maker.forever=false
  {ReadTokens > : *
"
"LUCENE-3800","BUG","BUG","Readers wrapping other readers don't prevent usage if any of their subreaders was closed","On recent trunk test we got this problem:
org.apache.lucene.index.TestReaderClosed.test
fails because the inner reader is closed but the wrapped outer ones are still open.

I fixed the issue partially for SlowCompositeReaderWrapper and ParallelAtomicReader but it failed again. The cool thing with this test is the following:

The test opens an DirectoryReader and then creates a searcher, closes the reader and executes a search. This is not an issue, if the reader is closed that the search is running on. This test uses LTC.newSearcher(wrap=true), which randomly wraps the passed Reader with SlowComposite or ParallelReader - or with both!!! If you then close the original inner reader, the close is not detected when excuting search. This can cause SIGSEGV when MMAP is used.

The problem in (in Slow* and Parallel*) is, that both have their own Fields instances thats are kept alive until the reader itsself is closed. If the child reader is closed, the wrapping reader does not know and still uses its own Fields instance that delegates to the inner readers. On this step no more ensureOpen checks are done, causing the failures.

The first fix done in Slow and Parallel was to call ensureOpen() on the subReader, too when requesting fields(). This works fine until you wrap two times: ParallelAtomicReader(SlowCompositeReaderWrapper(StandardDirectoryReader(segments_1:3:nrt _0(4.0):C42)))

One solution would be to make ensureOpen also check all subreaders, but that would do the volatile checks way too often (with n is the total number of subreaders and m is the number of hierarchical levels this is n^m) - we cannot do this. Currently we only have n*m which is fine.

The proposal how to solve this (closing subreaders under the hood of parent readers is to use the readerClosedListeners. Whenever a composite or slow reader wraps another readers, it registers itself as interested in readerClosed events. When a subreader is then forcefully closed (e.g by a programming error or this crazy test), we automatically close the parents, too.

We should also fix this in 3.x, if we have similar problems there (needs investigation)."
"LUCENE-223","CLEANUP","IMPROVEMENT","[PATCH] remove unused variables","Seems I'm the only person who has the ""unused variable"" warning turned on in 
Eclipse :-) This patch removes those unused variables and imports (for now 
only in the ""search"" package). This doesn't introduce changes in 
functionality, but it should be reviewed anyway: there might be cases where 
the variables *should* be used, but they are not because of a bug."
"LUCENE-1550","RFE","RFE","Add N-Gram String Matching for Spell Checking","N-Gram version of edit distance based on paper by Grzegorz Kondrak, ""N-gram similarity and distance"". Proceedings of the Twelfth International Conference on String Processing and Information Retrieval (SPIRE 2005), pp. 115-126,  Buenos Aires, Argentina, November 2005. 
http://www.cs.ualberta.ca/~kondrak/papers/spire05.pdf
"
"LUCENE-387","RFE","BUG","Contrib: Main memory based SynonymMap and SynonymTokenFilter","- Contrib: Main memory based SynonymMap and SynonymTokenFilter
- applies to SVN trunk as well as 1.4.3"
"LUCENE-354","TEST","BUG","FIXME in src/test/org/apache/lucene/IndexTest.java","Index: src/test/org/apache/lucene/IndexTest.java
===============================================================
====
--- src/test/org/apache/lucene/IndexTest.java   (revision 155945)
+++ src/test/org/apache/lucene/IndexTest.java   (working copy)
@@ -27,8 +27,7 @@   
public static void main(String[] args) {
     try {
       Date start = new Date();
-      // FIXME: OG: what's with this hard-coded dirs??
-      IndexWriter writer = new IndexWriter(""F:\\test"", new SimpleAnalyzer(),
+      IndexWriter writer = new IndexWriter(File.createTempFile(""luceneTest"",""idx""), new 
SimpleAnalyzer(),
                                           true);
        writer.setMergeFactor(20);"
"LUCENE-2059","RFE","IMPROVEMENT","benchmark pkg: allow TrecContentSource not to change the docname","TrecContentSource currently appends 'iteration number' to the docname field.
Example: if the original docname is DOC0001 then it will be indexed as DOC0001_0

this presents a problem for relevance testing, because when judging results, the expected docname will never be present.
This patch adds an option to disable this behavior, defaulting to the existing behavior (which is to append the iteration number).
"
"LUCENE-348","DOCUMENTATION","BUG","fileformats.xml doesn't document compound file streams","Current versions of Lucene generate segments in compound file stream format
files, but the fileformats documentation does not have any description of the
format for those files."
"LUCENE-1405","BUILD_SYSTEM","IMPROVEMENT","Support for new Resources model in ant 1.7 in Lucene ant task.","Ant Task for Lucene should use modern Resource model (not only FileSet child element).
There is a patch with required changes.

Supported by old (ant 1.6) and new (ant 1.7) resources model:
<index ....> <!-- Lucene Ant Task -->
  <fileset ... />
</index> 

Supported only by new (ant 1.7) resources model:
<index ....> <!-- Lucene Ant Task -->
  <filelist ... />
</index> 

<index ....> <!-- Lucene Ant Task -->
  <userdefinied-filesource ... />
</index> "
"LUCENE-3345","BUG","BUG","docvalues FNFE","I created a test for LUCENE-3335, and it found an unrelated bug in docvalues."
"LUCENE-866","IMPROVEMENT","IMPROVEMENT","Multi-level skipping on posting lists","To accelerate posting list skips (TermDocs.skipTo(int)) Lucene uses skip lists. 
The default skip interval is set to 16. If we want to skip e. g. 100 documents, 
then it is not necessary to read 100 entries from the posting list, but only 
100/16 = 6 skip list entries plus 100%16 = 4 entries from the posting list. This 
speeds up conjunction (AND) and phrase queries significantly.

However, the skip interval is always a compromise. If you have a very big index 
with huge posting lists and you want to skip over lets say 100k documents, then 
it is still necessary to read 100k/16 = 6250 entries from the skip list. For big 
indexes the skip interval could be set to a higher value, but then after a big 
skip a long scan to the target doc might be necessary.

A solution for this compromise is to have multi-level skip lists that guarantee a 
logarithmic amount of skips to any target in the posting list. This patch 
implements such an approach in the following way:

  Example for skipInterval = 3:
                                                      c            (skip level 2)
                  c                 c                 c            (skip level 1) 
      x     x     x     x     x     x     x     x     x     x      (skip level 0)
  d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d  (posting list)
      3     6     9     12    15    18    21    24    27    30     (df)
 
  d - document
  x - skip data
  c - skip data with child pointer
 
Skip level i contains every skipInterval-th entry from skip level i-1. Therefore the 
number of entries on level i is: floor(df / ((skipInterval ^ (i + 1))).
 
Each skip entry on a level i>0 contains a pointer to the corresponding skip entry in 
list i-1. This guarantees a logarithmic amount of skips to find the target document.


Implementations details:

   * I factored the skipping code out of SegmentMerger and SegmentTermDocs to 
     simplify those classes. The two new classes AbstractSkipListReader and 
	 AbstractSkipListWriter implement the skipping functionality.
   * While AbstractSkipListReader and Writer take care of writing and reading the 
     multiple skip levels, they do not implement an actual skip data format. The two 
	 new subclasses DefaultSkipListReader and Writer implement the skip data format 
	 that is currently used in Lucene (with two file pointers for the freq and prox 
	 file and with payload length information). I added this extra layer to be 
	 prepared for flexible indexing and different posting list formats. 
      
   
File format changes: 

   * I added the new parameter 'maxSkipLevels' to the term dictionary and increased the
     version of this file. If maxSkipLevels is set to one, then the format of the freq 
	 file does not change at all, because we only have one skip level as before. For 
	 backwards compatibility maxSkipLevels is set to one automatically if an index 
	 without the new parameter is read. 
   * In case maxSkipLevels > 1, then the frq file changes as follows:
     FreqFile (.frq) --> <TermFreqs, SkipData>^TermCount
	 SkipData        --> <<SkipLevelLength, SkipLevel>^(Min(maxSkipLevels, 
	                       floor(log(DocFreq/log(skipInterval))) - 1)>, SkipLevel>
	 SkipLevel       --> <SkipDatum>^DocFreq/(SkipInterval^(Level + 1))

	 Remark: The length of the SkipLevel is not stored for level 0, because 1) it is not 
	 needed, and 2) the format of this file does not change for maxSkipLevels=1 then.
	 
	 
All unit tests pass with this patch."
"LUCENE-1742","REFACTORING","IMPROVEMENT","Wrap SegmentInfos in public class ","Wrap SegmentInfos in a public class so that subclasses of MergePolicy do not need to be in the org.apache.lucene.index package.  "
"LUCENE-2989","TEST","BUG","TestCollectionUtil fails on IBM JRE","    [junit] Testcase: testEmptyArraySort(org.apache.lucene.util.TestCollectionUtil):    Caused an ERROR
    [junit] CollectionUtil can only sort random access lists in-place."
"LUCENE-873","BUILD_SYSTEM","BUG","nightly builds depend on clover","as reported by Michael Pelz Sherman on java-dev@lucene and solr-user@lucene the nightly builds coming out of hudson current depend on clover...

  [root@crm.test.bbhmedia.net tmp]# strings lucene-core-nightly.jar | grep -i clover|more
org/apache/lucene/LucenePackage$__CLOVER_0_0.class
org/apache/lucene/analysis/Analyzer$__CLOVER_1_0.class
...

the old nightly.sh dealt with this by running ant nightly twice, first without clover to get the jars and then with clover to get the report.  it loks like maybe this logic never made it into the hudson setup.

someone with hudson admin access/knowledge will need to look into this."
"LUCENE-2067","RFE","RFE","Czech Stemmer","Currently, the CzechAnalyzer is merely stopwords, and there isn't a czech stemmer in snowball.

This patch implements the light stemming algorithm described in: http://portal.acm.org/citation.cfm?id=1598600

In their measurements, it improves MAP 42%

The analyzer does not use this stemmer if LUCENE_VERSION <= 3.0, for back compat.
"
"LUCENE-3556","DESIGN_DEFECT","IMPROVEMENT","Make DirectoryTaxonomyWriter's indexWriter member private","DirectoryTaxonomyWriter has a protected indexWriter member. As far as I can tell, for two reasons:

# protected openIndexWriter method which lets you open your own IW (e.g. with a custom IndexWriterConfig).
# protected closeIndexWriter which is a hook for letting you close the IW you opened in the previous one.

The fixes are trivial IMO:
# Modify the method to return IW, and have the calling code set DTW's indexWriter member
# Eliminate closeIW. DTW already has a protected closeResources() which lets you clean other resources you've allocated, so I think that's enough.

I'll post a patch shortly."
"LUCENE-2561","BUG","BUG","Fix exception handling and thread safety in realtime branch","Several tests are currently failing in the realtime branch - most of them due to thread safety problems (often exceptions in ConcurrentMergeScheduler) and in tests that test for aborting and non-aborting exceptions."
"LUCENE-1913","BUG","BUG","FastVectorHighlighter: AIOOBE occurs if one PhraseQuery is contained by another PhraseQuery","I'm very sorry but this is another one. If q=""a b c d"" OR ""b c"", then ArrayIndexOutOfBoundsException occurs in FieldQuery.checkOverlap(). I'm working on this and fix with test case soon to be posted.
Thank you for your patient!
"
"LUCENE-2624","RFE","RFE","add new snowball languages","Snowball added new languages. This patch adds support for them.

http://snowball.tartarus.org/algorithms/armenian/stemmer.html
http://snowball.tartarus.org/algorithms/catalan/stemmer.html
http://snowball.tartarus.org/algorithms/basque/stemmer.html
"
"LUCENE-272","CLEANUP","IMPROVEMENT","[PATCH] Remove equals() from internal Comparator of ConjunctionScorer","As written, the equals() method is not used. 
The docs of java.util.Comparator have an equals() with a single 
arg to compare the Comparator itself to another one, which is 
hardly ever useful. 
Patch follows"
"LUCENE-488","BUG","BUG","adding docs with large (binary) fields of 5mb causes OOM regardless of heap size","as reported by George Washington in a message to java-user@lucene.apache.org with subect ""Storing large text or binary source documents in the index and memory usage"" arround 2006-01-21 there seems to be a problem with adding docs containing really large fields.

I'll attach a test case in a moment, note that (for me) regardless of how big i make my heap size, and regardless of what value I set  MIN_MB to, once it starts trying to make documents of containing 5mb of data, it can only add 9 before it rolls over and dies.

here's the output from the code as i will attach in a moment...

    [junit] Testsuite: org.apache.lucene.document.TestBigBinary
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 78.656 sec

    [junit] ------------- Standard Output ---------------
    [junit] NOTE: directory will not be cleaned up automatically...
    [junit] Dir: /tmp/org.apache.lucene.document.TestBigBinary.97856146.100iters.4mb
    [junit] iters completed: 100
    [junit] totalBytes Allocated: 419430400
    [junit] NOTE: directory will not be cleaned up automatically...
    [junit] Dir: /tmp/org.apache.lucene.document.TestBigBinary.97856146.100iters.5mb
    [junit] iters completed: 9
    [junit] totalBytes Allocated: 52428800
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testBigBinaryFields(org.apache.lucene.document.TestBigBinary):    Caused an ERROR
    [junit] Java heap space
    [junit] java.lang.OutOfMemoryError: Java heap space


    [junit] Test org.apache.lucene.document.TestBigBinary FAILED
"
"LUCENE-1558","IMPROVEMENT","IMPROVEMENT","Make IndexReader/Searcher ctors readOnly=true by default","Another ""change the defaults"" in 3.0.

Right now you get a read/write reader from IndexReader.open and new IndexSearcher(...), and reserving the right to write causes thread contention (on isDeleted).

In 3.0 let's make readOnly reader the default, but still allow opening a read/write IndexReader."
"LUCENE-2742","RFE","IMPROVEMENT","Enable native per-field codec support ","Currently the codec name is stored for every segment and PerFieldCodecWrapper is used to enable codecs per fields which has recently brought up some issues (LUCENE-2740 and LUCENE-2741). When a codec name is stored lucene does not respect the actual codec used to encode a fields postings but rather the ""top-level"" Codec in such a case the name of the top-level codec is  ""PerField"" instead of ""Pulsing"" or ""Standard"" etc. The way this composite pattern works make the indexing part of codecs simpler but also limits its capabilities. By recoding the top-level codec in the segments file we rely on the user to ""configure"" the PerFieldCodecWrapper correctly to open a SegmentReader. If a fields codec has changed in the meanwhile we won't be able to open the segment.

The issues LUCENE-2741 and LUCENE-2740 are actually closely related to the way PFCW is implemented right now. PFCW blindly creates codecs per field on request and at the same time doesn't have any control over the file naming nor if a two codec instances are created for two distinct fields even if the codec instance is the same. If so FieldsConsumer will throw an exception since the files it relies on are already created.

Having PerFieldCodecWrapper AND a CodecProvider overcomplicates things IMO. In order to use per field codec a user should on the one hand register its custom codecs AND needs to build a PFCW which needs to be maintained in the ""user-land"" an must not change incompatible once a new IW of IR is created. What I would expect from Lucene is to enable me to register a codec in a provider and then tell the Field which codec it should use for indexing. For reading lucene should determ the codec automatically once a segment is opened. if the codec is not available in the provider that is a different story. Once we instantiate the composite codec in SegmentsReader we only have the codecs which are really used in this segment for free which in turn solves LUCENE-2740. 

Yet, instead of relying on the user to configure PFCW I suggest to move composite codec functionality inside the core an record the distinct codecs per segment in the segments info. We only really need the distinct codecs used in that segment since the codec instance should be reused to prevent additional files to be created. Lets say we have the follwing codec mapping :
{noformat}
field_a:Pulsing
field_b:Standard
field_c:Pulsing
{noformat}

then we create the following mapping:
{noformat}
SegmentInfo:
[Pulsing, Standard]

PerField:
[field_a:0, field_b:1, field_c:0]
{noformat}

that way we can easily determ which codec is used for which field an build the composite - codec internally on opening SegmentsReader. This ordering has another advantage, if like in that case pulsing and standard use really the same type of files we need a way to distinguish the used files per codec within a segment. We can in turn pass the codec's ord (implicit in the SegmentInfo) to the FieldConsumer on creation to create files with segmentname_ord.ext (or something similar). This solvel LUCENE-2741). 
"
"LUCENE-1427","IMPROVEMENT","IMPROVEMENT","QueryWrapperFilter should not do scoring","The purpose of QueryWrapperFilter is to simply filter to include the docIDs that match the query.

Its implementation is wasteful now because it computes scores for those matching docs even though the score is unused.  We could fix this by getting a Scorer and iterating through the docs without asking for the score:

{code}
Index: src/java/org/apache/lucene/search/QueryWrapperFilter.java
===================================================================
--- src/java/org/apache/lucene/search/QueryWrapperFilter.java	(revision 707060)
+++ src/java/org/apache/lucene/search/QueryWrapperFilter.java	(working copy)
@@ -62,11 +62,9 @@
   public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
     final OpenBitSet bits = new OpenBitSet(reader.maxDoc());
 
-    new IndexSearcher(reader).search(query, new HitCollector() {
-      public final void collect(int doc, float score) {
-        bits.set(doc);  // set bit for hit
-      }
-    });
+    final Scorer scorer = query.weight(new IndexSearcher(reader)).scorer(reader);
+    while(scorer.next())
+      bits.set(scorer.doc());
     return bits;
   }
{code}

Maybe I'm missing something, but this seams like a simple win?
"
"LUCENE-2124","REFACTORING","TASK","move JDK collation to core, ICU collation to ICU contrib","As mentioned on the list, I propose we move the JDK-based CollationKeyFilter/CollationKeyAnalyzer, currently located in contrib/collation into core for collation support (language-sensitive sorting)

These are not much code (the heavy duty stuff is already in core, IndexableBinaryString). 

And I would also like to move the ICUCollationKeyFilter/ICUCollationKeyAnalyzer (along with the jar file they depend on) also currently located in contrib/collation into a contrib/icu.

This way, we can start looking at integrating other functionality from ICU into a fully-fleshed out icu contrib.
"
"LUCENE-3249","REFACTORING","","Move Solr's FunctionQuery impls to Queries Module","Now that we have the main interfaces in the Queries module, we can move the actual impls over.

Impls that won't be moved are:

function/distance/* (to be moved to a spatial module)
function/FileFloatSource.java (depends on Solr's Schema, data directories and exposes a RequestHandler)"
"LUCENE-1048","BUG","BUG","Lock.obtain(timeout) behaves incorrectly for large timeouts","Because timeout is a long, but internal values derived from timeout
are ints, its possible to overflow those internal values into negative
numbers and cause incorrect behavior.

Spinoff from this thread:

  http://www.gossamer-threads.com/lists/lucene/java-user/54376

"
"LUCENE-3051","BUG","BUG","don't call SegmentInfo.sizeInBytes for the merging segments","Selckin has been running Lucene's tests on the RT branch, and hit this:
{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
    [junit] Testcase: testDeleteAllSlowly(org.apache.lucene.index.TestIndexWriter):	FAILED
    [junit] Some threads threw uncaught exceptions!
    [junit] junit.framework.AssertionFailedError: Some threads threw uncaught exceptions!
    [junit] 	at org.apache.lucene.util.LuceneTestCase.tearDown(LuceneTestCase.java:535)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1246)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1175)
    [junit] 
    [junit] 
    [junit] Tests run: 67, Failures: 1, Errors: 0, Time elapsed: 38.357 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testDeleteAllSlowly -Dtests.seed=-4291771462012978364:4550117847390778918
    [junit] The following exceptions were thrown by threads:
    [junit] *** Thread: Lucene Merge Thread #1 ***
    [junit] org.apache.lucene.index.MergePolicy$MergeException: java.io.FileNotFoundException: _4_1.del
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:507)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:472)
    [junit] Caused by: java.io.FileNotFoundException: _4_1.del
    [junit] 	at org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:290)
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.fileLength(MockDirectoryWrapper.java:549)
    [junit] 	at org.apache.lucene.index.SegmentInfo.sizeInBytes(SegmentInfo.java:287)
    [junit] 	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3280)
    [junit] 	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2956)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:379)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:447)
    [junit] NOTE: test params are: codec=RandomCodecProvider: {=SimpleText, f6=Pulsing(freqCutoff=15), f7=MockFixedIntBlock(blockSize=1606), f8=SimpleText, f9=MockSep, f1=MockVariableIntBlock(baseBlockSize=99), f0=MockFixedIntBlock(blockSize=1606), f3=Pulsing(freqCutoff=15), f2=MockSep, f5=SimpleText, f4=Standard, f=MockFixedIntBlock(blockSize=1606), c=MockSep, termVector=MockRandom, d9=MockFixedIntBlock(blockSize=1606), d8=Pulsing(freqCutoff=15), d5=SimpleText, d4=Standard, d7=MockRandom, d6=MockVariableIntBlock(baseBlockSize=99), d25=MockRandom, d0=MockRandom, c29=MockFixedIntBlock(blockSize=1606), d24=MockVariableIntBlock(baseBlockSize=99), d1=Standard, c28=Standard, d23=SimpleText, d2=MockFixedIntBlock(blockSize=1606), c27=MockRandom, d22=Standard, d3=MockVariableIntBlock(baseBlockSize=99), d21=Pulsing(freqCutoff=15), d20=MockSep, c22=MockFixedIntBlock(blockSize=1606), c21=Pulsing(freqCutoff=15), c20=MockRandom, d29=MockFixedIntBlock(blockSize=1606), c26=Standard, d28=Pulsing(freqCutoff=15), c25=MockRandom, d27=MockRandom, c24=MockSep, d26=MockVariableIntBlock(baseBlockSize=99), c23=SimpleText, e9=MockRandom, e8=MockSep, e7=SimpleText, e6=MockFixedIntBlock(blockSize=1606), e5=Pulsing(freqCutoff=15), c17=MockFixedIntBlock(blockSize=1606), e3=Standard, d12=MockVariableIntBlock(baseBlockSize=99), c16=Pulsing(freqCutoff=15), e4=SimpleText, d11=MockFixedIntBlock(blockSize=1606), c19=MockSep, e1=MockSep, d14=Pulsing(freqCutoff=15), c18=SimpleText, e2=Pulsing(freqCutoff=15), d13=MockSep, e0=MockVariableIntBlock(baseBlockSize=99), d10=Standard, d19=MockVariableIntBlock(baseBlockSize=99), c11=SimpleText, c10=Standard, d16=Pulsing(freqCutoff=15), c13=MockRandom, c12=MockVariableIntBlock(baseBlockSize=99), d15=MockSep, d18=SimpleText, c15=MockFixedIntBlock(blockSize=1606), d17=Standard, c14=Pulsing(freqCutoff=15), b3=MockSep, b2=SimpleText, b5=Standard, b4=MockRandom, b7=MockVariableIntBlock(baseBlockSize=99), b6=MockFixedIntBlock(blockSize=1606), d50=MockFixedIntBlock(blockSize=1606), b9=Pulsing(freqCutoff=15), b8=MockSep, d43=MockSep, d42=SimpleText, d41=MockFixedIntBlock(blockSize=1606), d40=Pulsing(freqCutoff=15), d47=MockVariableIntBlock(baseBlockSize=99), d46=MockFixedIntBlock(blockSize=1606), b0=MockVariableIntBlock(baseBlockSize=99), d45=Standard, b1=MockRandom, d44=MockRandom, d49=MockVariableIntBlock(baseBlockSize=99), d48=MockFixedIntBlock(blockSize=1606), c6=Pulsing(freqCutoff=15), c5=MockSep, c4=MockVariableIntBlock(baseBlockSize=99), c3=MockFixedIntBlock(blockSize=1606), c9=MockVariableIntBlock(baseBlockSize=99), c8=SimpleText, c7=Standard, d30=SimpleText, d32=MockRandom, d31=MockVariableIntBlock(baseBlockSize=99), c1=SimpleText, d34=MockFixedIntBlock(blockSize=1606), c2=MockSep, d33=Pulsing(freqCutoff=15), d36=MockSep, c0=MockFixedIntBlock(blockSize=1606), d35=SimpleText, d38=MockSep, d37=SimpleText, d39=MockRandom, e92=MockFixedIntBlock(blockSize=1606), e93=MockVariableIntBlock(baseBlockSize=99), e90=MockRandom, e91=Standard, e89=MockVariableIntBlock(baseBlockSize=99), e88=SimpleText, e87=Standard, e86=Pulsing(freqCutoff=15), e85=MockSep, e84=MockVariableIntBlock(baseBlockSize=99), e83=MockFixedIntBlock(blockSize=1606), e80=MockFixedIntBlock(blockSize=1606), e81=SimpleText, e82=MockSep, e77=MockVariableIntBlock(baseBlockSize=99), e76=MockFixedIntBlock(blockSize=1606), e79=Pulsing(freqCutoff=15), e78=MockSep, e73=MockSep, e72=SimpleText, e75=Standard, e74=MockRandom, binary=MockFixedIntBlock(blockSize=1606), f98=Pulsing(freqCutoff=15), f97=MockSep, f99=Standard, f94=Standard, f93=MockRandom, f96=MockVariableIntBlock(baseBlockSize=99), f95=MockFixedIntBlock(blockSize=1606), e95=SimpleText, e94=Standard, e97=MockRandom, e96=MockVariableIntBlock(baseBlockSize=99), e99=MockFixedIntBlock(blockSize=1606), e98=Pulsing(freqCutoff=15), id=MockFixedIntBlock(blockSize=1606), f34=MockSep, f33=SimpleText, f32=MockFixedIntBlock(blockSize=1606), f31=Pulsing(freqCutoff=15), f30=MockRandom, f39=MockFixedIntBlock(blockSize=1606), f38=Standard, f37=MockRandom, f36=MockSep, f35=SimpleText, f43=Standard, f42=MockRandom, f45=MockVariableIntBlock(baseBlockSize=99), f44=MockFixedIntBlock(blockSize=1606), f41=MockSep, f40=SimpleText, f47=MockVariableIntBlock(baseBlockSize=99), f46=MockFixedIntBlock(blockSize=1606), f49=Pulsing(freqCutoff=15), f48=MockSep, content=Pulsing(freqCutoff=15), e19=Standard, e18=MockRandom, e17=MockSep, f12=Pulsing(freqCutoff=15), e16=SimpleText, f11=MockSep, f10=MockVariableIntBlock(baseBlockSize=99), e15=MockFixedIntBlock(blockSize=1606), e14=Pulsing(freqCutoff=15), f16=SimpleText, e13=MockFixedIntBlock(blockSize=1606), f15=Standard, e12=Pulsing(freqCutoff=15), e11=MockRandom, f14=Pulsing(freqCutoff=15), e10=MockVariableIntBlock(baseBlockSize=99), f13=MockSep, f19=Pulsing(freqCutoff=15), f18=MockRandom, f17=MockVariableIntBlock(baseBlockSize=99), e29=MockSep, e26=Standard, f21=SimpleText, e25=MockRandom, f20=Standard, e28=MockVariableIntBlock(baseBlockSize=99), f23=MockRandom, e27=MockFixedIntBlock(blockSize=1606), f22=MockVariableIntBlock(baseBlockSize=99), f25=MockRandom, e22=MockSep, f24=MockVariableIntBlock(baseBlockSize=99), e21=SimpleText, f27=MockFixedIntBlock(blockSize=1606), e24=Standard, f26=Pulsing(freqCutoff=15), e23=MockRandom, f29=MockSep, f28=SimpleText, e20=MockFixedIntBlock(blockSize=1606), field=Pulsing(freqCutoff=15), string=MockVariableIntBlock(baseBlockSize=99), e30=Pulsing(freqCutoff=15), e31=MockFixedIntBlock(blockSize=1606), a98=MockFixedIntBlock(blockSize=1606), e34=MockRandom, a99=MockVariableIntBlock(baseBlockSize=99), e35=Standard, f79=Pulsing(freqCutoff=15), e32=SimpleText, e33=MockSep, b97=Pulsing(freqCutoff=15), f77=Pulsing(freqCutoff=15), e38=MockFixedIntBlock(blockSize=1606), b98=MockFixedIntBlock(blockSize=1606), f78=MockFixedIntBlock(blockSize=1606), e39=MockVariableIntBlock(baseBlockSize=99), b99=SimpleText, f75=MockVariableIntBlock(baseBlockSize=99), e36=MockRandom, f76=MockRandom, e37=Standard, f73=Standard, f74=SimpleText, f71=MockSep, f72=Pulsing(freqCutoff=15), f81=Pulsing(freqCutoff=15), f80=MockSep, e40=MockSep, e41=MockRandom, e42=Standard, e43=MockFixedIntBlock(blockSize=1606), e44=MockVariableIntBlock(baseBlockSize=99), e45=MockSep, e46=Pulsing(freqCutoff=15), f86=SimpleText, e47=MockSep, f87=MockSep, e48=Pulsing(freqCutoff=15), f88=MockRandom, e49=Standard, f89=Standard, f82=MockVariableIntBlock(baseBlockSize=99), f83=MockRandom, f84=Pulsing(freqCutoff=15), f85=MockFixedIntBlock(blockSize=1606), f90=SimpleText, f92=MockRandom, f91=MockVariableIntBlock(baseBlockSize=99), str=MockFixedIntBlock(blockSize=1606), a76=MockVariableIntBlock(baseBlockSize=99), e56=MockVariableIntBlock(baseBlockSize=99), f59=MockSep, a77=MockRandom, e57=MockRandom, a78=Pulsing(freqCutoff=15), e54=Standard, f57=MockFixedIntBlock(blockSize=1606), a79=MockFixedIntBlock(blockSize=1606), e55=SimpleText, f58=MockVariableIntBlock(baseBlockSize=99), e52=MockSep, e53=Pulsing(freqCutoff=15), e50=MockFixedIntBlock(blockSize=1606), e51=MockVariableIntBlock(baseBlockSize=99), f51=SimpleText, f52=MockSep, f50=MockFixedIntBlock(blockSize=1606), f55=MockFixedIntBlock(blockSize=1606), f56=MockVariableIntBlock(baseBlockSize=99), f53=MockRandom, e58=MockVariableIntBlock(baseBlockSize=99), f54=Standard, e59=MockRandom, a80=MockVariableIntBlock(baseBlockSize=99), e60=MockVariableIntBlock(baseBlockSize=99), a82=Pulsing(freqCutoff=15), a81=MockSep, a84=SimpleText, a83=Standard, a86=MockRandom, a85=MockVariableIntBlock(baseBlockSize=99), a89=MockRandom, f68=Standard, e65=Pulsing(freqCutoff=15), f69=SimpleText, e66=MockFixedIntBlock(blockSize=1606), a87=SimpleText, e67=SimpleText, a88=MockSep, e68=MockSep, e61=Standard, e62=SimpleText, e63=MockVariableIntBlock(baseBlockSize=99), e64=MockRandom, f60=MockRandom, f61=Standard, f62=MockFixedIntBlock(blockSize=1606), f63=MockVariableIntBlock(baseBlockSize=99), e69=SimpleText, f64=MockSep, f65=Pulsing(freqCutoff=15), f66=Standard, f67=SimpleText, f70=Standard, a93=MockRandom, a92=MockVariableIntBlock(baseBlockSize=99), a91=SimpleText, e71=SimpleText, a90=Standard, e70=Standard, a97=MockSep, a96=SimpleText, a95=MockFixedIntBlock(blockSize=1606), a94=Pulsing(freqCutoff=15), c58=MockRandom, a63=Pulsing(freqCutoff=15), a64=MockFixedIntBlock(blockSize=1606), c59=Standard, c56=SimpleText, d59=MockVariableIntBlock(baseBlockSize=99), a61=MockVariableIntBlock(baseBlockSize=99), c57=MockSep, a62=MockRandom, c54=Pulsing(freqCutoff=15), c55=MockFixedIntBlock(blockSize=1606), a60=SimpleText, c52=MockVariableIntBlock(baseBlockSize=99), c53=MockRandom, d53=MockSep, d54=Pulsing(freqCutoff=15), d51=MockFixedIntBlock(blockSize=1606), d52=MockVariableIntBlock(baseBlockSize=99), d57=MockVariableIntBlock(baseBlockSize=99), b62=MockSep, d58=MockRandom, b63=Pulsing(freqCutoff=15), d55=Standard, b60=MockFixedIntBlock(blockSize=1606), d56=SimpleText, b61=MockVariableIntBlock(baseBlockSize=99), b56=SimpleText, b55=Standard, b54=Pulsing(freqCutoff=15), b53=MockSep, d61=MockVariableIntBlock(baseBlockSize=99), b59=Pulsing(freqCutoff=15), d60=MockFixedIntBlock(blockSize=1606), b58=MockRandom, b57=MockVariableIntBlock(baseBlockSize=99), c62=MockRandom, c61=MockVariableIntBlock(baseBlockSize=99), a59=Standard, c60=SimpleText, a58=MockRandom, a57=MockSep, a56=SimpleText, a55=MockFixedIntBlock(blockSize=1606), a54=Pulsing(freqCutoff=15), a72=SimpleText, c67=MockFixedIntBlock(blockSize=1606), a73=MockSep, c68=MockVariableIntBlock(baseBlockSize=99), a74=MockRandom, c69=MockSep, a75=Standard, c63=SimpleText, c64=MockSep, a70=Pulsing(freqCutoff=15), c65=MockRandom, a71=MockFixedIntBlock(blockSize=1606), c66=Standard, d62=Standard, d63=SimpleText, d64=MockVariableIntBlock(baseBlockSize=99), b70=Pulsing(freqCutoff=15), d65=MockRandom, b71=Standard, d66=Pulsing(freqCutoff=15), b72=SimpleText, d67=MockFixedIntBlock(blockSize=1606), b73=MockVariableIntBlock(baseBlockSize=99), d68=SimpleText, b74=MockRandom, d69=MockSep, b65=MockRandom, b64=MockVariableIntBlock(baseBlockSize=99), b67=MockFixedIntBlock(blockSize=1606), b66=Pulsing(freqCutoff=15), d70=Pulsing(freqCutoff=15), b69=MockSep, b68=SimpleText, d72=SimpleText, d71=Standard, c71=MockFixedIntBlock(blockSize=1606), c70=Pulsing(freqCutoff=15), a69=MockSep, c73=MockSep, c72=SimpleText, a66=Standard, a65=MockRandom, a68=MockVariableIntBlock(baseBlockSize=99), a67=MockFixedIntBlock(blockSize=1606), c32=MockFixedIntBlock(blockSize=1606), c33=MockVariableIntBlock(baseBlockSize=99), c30=MockRandom, c31=Standard, c36=Standard, a41=MockFixedIntBlock(blockSize=1606), c37=SimpleText, a42=MockVariableIntBlock(baseBlockSize=99), a0=MockSep, c34=MockSep, c35=Pulsing(freqCutoff=15), a40=Standard, b84=SimpleText, d79=MockFixedIntBlock(blockSize=1606), b85=MockSep, b82=Pulsing(freqCutoff=15), d77=MockRandom, c38=Standard, b83=MockFixedIntBlock(blockSize=1606), d78=Standard, c39=SimpleText, b80=MockVariableIntBlock(baseBlockSize=99), d75=SimpleText, b81=MockRandom, d76=MockSep, d73=Pulsing(freqCutoff=15), d74=MockFixedIntBlock(blockSize=1606), d83=MockFixedIntBlock(blockSize=1606), a9=MockVariableIntBlock(baseBlockSize=99), d82=Pulsing(freqCutoff=15), d81=MockRandom, d80=MockVariableIntBlock(baseBlockSize=99), b79=MockFixedIntBlock(blockSize=1606), b78=Standard, b77=MockRandom, b76=MockSep, b75=SimpleText, a1=MockFixedIntBlock(blockSize=1606), a35=Pulsing(freqCutoff=15), a2=MockVariableIntBlock(baseBlockSize=99), a34=MockSep, a3=MockSep, a33=MockVariableIntBlock(baseBlockSize=99), a4=Pulsing(freqCutoff=15), a32=MockFixedIntBlock(blockSize=1606), a5=Standard, a39=MockRandom, c40=Standard, a6=SimpleText, a38=MockVariableIntBlock(baseBlockSize=99), a7=MockVariableIntBlock(baseBlockSize=99), a37=SimpleText, a8=MockRandom, a36=Standard, c41=MockSep, c42=Pulsing(freqCutoff=15), c43=Standard, c44=SimpleText, c45=MockVariableIntBlock(baseBlockSize=99), a50=MockSep, c46=MockRandom, a51=Pulsing(freqCutoff=15), c47=Pulsing(freqCutoff=15), a52=Standard, c48=MockFixedIntBlock(blockSize=1606), a53=SimpleText, b93=MockRandom, d88=MockSep, c49=Pulsing(freqCutoff=15), b94=Standard, d89=Pulsing(freqCutoff=15), b95=MockFixedIntBlock(blockSize=1606), b96=MockVariableIntBlock(baseBlockSize=99), d84=MockRandom, b90=MockFixedIntBlock(blockSize=1606), d85=Standard, b91=SimpleText, d86=MockFixedIntBlock(blockSize=1606), b92=MockSep, d87=MockVariableIntBlock(baseBlockSize=99), d92=MockSep, d91=SimpleText, d94=Standard, d93=MockRandom, b87=MockVariableIntBlock(baseBlockSize=99), b86=MockFixedIntBlock(blockSize=1606), d90=MockFixedIntBlock(blockSize=1606), b89=Pulsing(freqCutoff=15), b88=MockSep, a44=SimpleText, a43=Standard, a46=MockRandom, a45=MockVariableIntBlock(baseBlockSize=99), a48=MockFixedIntBlock(blockSize=1606), a47=Pulsing(freqCutoff=15), c51=Pulsing(freqCutoff=15), a49=SimpleText, c50=MockSep, d98=MockVariableIntBlock(baseBlockSize=99), d97=MockFixedIntBlock(blockSize=1606), d96=Standard, d95=MockRandom, d99=MockSep, a20=MockSep, c99=MockRandom, c98=MockVariableIntBlock(baseBlockSize=99), c97=SimpleText, c96=Standard, b19=MockRandom, a16=MockSep, a17=Pulsing(freqCutoff=15), b17=SimpleText, a14=MockFixedIntBlock(blockSize=1606), b18=MockSep, a15=MockVariableIntBlock(baseBlockSize=99), a12=MockRandom, a13=Standard, a10=SimpleText, a11=MockSep, b11=MockVariableIntBlock(baseBlockSize=99), b12=MockRandom, b10=SimpleText, b15=SimpleText, b16=MockSep, a18=MockSep, b13=Pulsing(freqCutoff=15), a19=Pulsing(freqCutoff=15), b14=MockFixedIntBlock(blockSize=1606), b30=MockFixedIntBlock(blockSize=1606), a31=MockVariableIntBlock(baseBlockSize=99), a30=MockFixedIntBlock(blockSize=1606), b28=MockFixedIntBlock(blockSize=1606), a25=Standard, b29=MockVariableIntBlock(baseBlockSize=99), a26=SimpleText, a27=MockVariableIntBlock(baseBlockSize=99), a28=MockRandom, a21=MockFixedIntBlock(blockSize=1606), a22=MockVariableIntBlock(baseBlockSize=99), a23=MockSep, a24=Pulsing(freqCutoff=15), b20=Pulsing(freqCutoff=15), b21=MockFixedIntBlock(blockSize=1606), b22=SimpleText, b23=MockSep, a29=MockVariableIntBlock(baseBlockSize=99), b24=MockRandom, b25=Standard, b26=MockFixedIntBlock(blockSize=1606), b27=MockVariableIntBlock(baseBlockSize=99), b41=Standard, b40=MockRandom, c77=Standard, c76=MockRandom, c75=MockSep, c74=SimpleText, c79=MockVariableIntBlock(baseBlockSize=99), c78=MockFixedIntBlock(blockSize=1606), c80=MockRandom, c83=SimpleText, c84=MockSep, c81=Pulsing(freqCutoff=15), b39=Standard, c82=MockFixedIntBlock(blockSize=1606), b37=Standard, b38=SimpleText, b35=MockSep, b36=Pulsing(freqCutoff=15), b33=MockFixedIntBlock(blockSize=1606), b34=MockVariableIntBlock(baseBlockSize=99), b31=MockRandom, b32=Standard, str2=MockFixedIntBlock(blockSize=1606), b50=MockVariableIntBlock(baseBlockSize=99), b52=Pulsing(freqCutoff=15), str3=SimpleText, b51=MockSep, c86=MockVariableIntBlock(baseBlockSize=99), tvtest=MockSep, c85=MockFixedIntBlock(blockSize=1606), c88=Pulsing(freqCutoff=15), c87=MockSep, c89=Standard, c90=SimpleText, c91=MockSep, c92=MockRandom, c93=Standard, c94=MockFixedIntBlock(blockSize=1606), c95=MockVariableIntBlock(baseBlockSize=99), content1=Pulsing(freqCutoff=15), b46=MockVariableIntBlock(baseBlockSize=99), b47=MockRandom, content3=MockVariableIntBlock(baseBlockSize=99), b48=Pulsing(freqCutoff=15), content4=MockFixedIntBlock(blockSize=1606), b49=MockFixedIntBlock(blockSize=1606), content5=Standard, b42=MockSep, b43=Pulsing(freqCutoff=15), b44=Standard, b45=SimpleText}, locale=tr, timezone=MET
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestMergeSchedulerExternal, TestCharTokenizers, TestCodecs, TestFieldInfos, TestFlushByRamOrCountsPolicy, TestIndexReaderReopen, TestIndexWriter]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=69508608,total=127336448
{noformat}

Simon dug and it looks like this is a trunk issue, caused by LUCENE-1076 (only committed to trunk so far)."
"LUCENE-1442","BUG","BUG","NOT_ANALYZED fields can double-count offsets","If the same field name has 2 NOT_ANALYZED field instances then the offsets are double-counted."
"LUCENE-1918","BUG","BUG","Adding empty ParallelReader indexes to an IndexWriter may cause ArrayIndexOutOfBoundsException or NoSuchElementException","Hi,
I recently stumbled upon this:

It is possible (and perfectly legal) to add empty indexes (IndexReaders) to an IndexWriter. However, when using ParallelReaders in this context, in two situations RuntimeExceptions may occur for no good reason.

Condition 1:
The indexes within the ParallelReader are just empty.

When adding them to the IndexWriter, we get a java.util.NoSuchElementException triggered by ParallelTermEnum's constructor. The reason for that is the TreeMap#firstKey() method which was assumed to return null if there is no entry (which is not true, apparently -- it only returns null if the first key in the Map is null).


Condition 2 (Assuming the aforementioned bug is fixed):
The indexes within the ParallelReader originally contained one or more fields with TermVectors, but all documents have been marked as deleted.

When adding the indexes to the IndexWriter, we get a java.lang.ArrayIndexOutOfBoundsException triggered by TermVectorsWriter#addAllDocVectors. The reason here is that TermVectorsWriter assumes that if the index is marked to have TermVectors, at least one field actually exists for that. This unfortunately is not true, either.

Patches and a testcase demonstrating the two bugs are provided.

Cheers,
Christian"
"LUCENE-3709","BUG","BUG","norms reading fails with FileNotFound in exceptional case","If we can't get to the bottom of this, we can always add the fileExists check back...

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterExceptions
    [junit] Testcase: testRandomExceptionsThreads(org.apache.lucene.index.TestIndexWriterExceptions):	Caused an ERROR
    [junit] No sub-file with id _nrm.cfs found (fileName=_19_nrm.cfs files: [.fdt, .fnm, .per, .fdx])
    [junit] java.io.FileNotFoundException: No sub-file with id _nrm.cfs found (fileName=_19_nrm.cfs files: [.fdt, .fnm, .per, .fdx])
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.createSlicer(CompoundFileDirectory.java:313)
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.<init>(CompoundFileDirectory.java:65)
    [junit] 	at org.apache.lucene.codecs.lucene40.Lucene40DocValuesProducer.<init>(Lucene40DocValuesProducer.java:48)
    [junit] 	at org.apache.lucene.codecs.lucene40.Lucene40NormsFormat$Lucene40NormsDocValuesProducer.<init>(Lucene40NormsFormat.java:70)
    [junit] 	at org.apache.lucene.codecs.lucene40.Lucene40NormsFormat.docsProducer(Lucene40NormsFormat.java:49)
    [junit] 	at org.apache.lucene.codecs.lucene40.Lucene40NormsFormat.docsProducer(Lucene40NormsFormat.java:62)
    [junit] 	at org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:122)
    [junit] 	at org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:54)
    [junit] 	at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:65)
    [junit] 	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:660)
    [junit] 	at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:55)
    [junit] 	at org.apache.lucene.index.IndexReader.open(IndexReader.java:242)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:304)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:530)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 
    [junit] 
    [junit] Tests run: 22, Failures: 0, Errors: 1, Time elapsed: 3.439 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testRandomExceptionsThreads -Dtests.seed=-4ea45cb40d17460b:-459bfb455a2351b9:1abd8f0f3a0611b9 -Dargs=""-Dfile.encoding=UTF-8""
    [junit] NOTE: test params are: codec=Lucene40: {field=MockVariableIntBlock(baseBlockSize=31), id=PostingsFormat(name=NestedPulsing), content=Pulsing40(freqCutoff=2 minBlockSize=58 maxBlockSize=186), contents=MockVariableIntBlock(baseBlockSize=31), content1=MockVariableIntBlock(baseBlockSize=31), content2=PostingsFormat(name=MockSep), content4=Pulsing40(freqCutoff=2 minBlockSize=58 maxBlockSize=186), content5=MockFixedIntBlock(blockSize=964), content6=PostingsFormat(name=Memory), content7=PostingsFormat(name=MockRandom), crash=PostingsFormat(name=NestedPulsing), subid=PostingsFormat(name=NestedPulsing)}, sim=RandomSimilarityProvider(queryNorm=false,coord=true): {other=DFR GB3(800.0), contents=IB SPL-L3(800.0), content=DFR GL3(800.0), id=DFR I(F)L1, field=IB LL-DZ(0.3), content1=DFR I(ne)BZ(0.3), content2=DFR I(n)3(800.0), content3=DFR GZ(0.3), content4=DFR I(ne)B2, content5=IB LL-L3(800.0), content6=IB SPL-D2, crash=DFR I(F)3(800.0), content7=DFR I(F)B3(800.0), subid=IB LL-L1}, locale=de_CH, timezone=Canada/Saskatchewan
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestAssertions, TestNumericTokenStream, TestSimpleAttributeImpl, TestImpersonation, TestPulsingReuse, TestDocument, TestAddIndexes, TestAtomicUpdate, TestByteSlices, TestCheckIndex, TestConcurrentMergeScheduler, TestConsistentFieldNumbers, TestCrashCausesCorruptIndex, TestDocCount, TestDocumentWriter, TestFlex, TestForceMergeForever, TestIndexInput, TestIndexReader, TestIndexWriterConfig, TestIndexWriterExceptions]
    [junit] NOTE: Linux 3.0.0-14-generic amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=186661872,total=245104640
{noformat}
"
"LUCENE-2700","RFE","RFE","Expose DocValues via Fields","DocValues Reader are currently exposed / accessed directly via IndexReader. To integrate the new feature in a more ""native"" way we should expose the DocValues via Fields on a perSegment level and on MultiFields in the multi reader case. DocValues should be side by side with Fields.terms  enabling access to Source, SortedSource and ValuesEnum something like that:

{code}
public abstract class Fields {
...

  public DocValues values();

}

public abstract class DocValues {
  /** on disk enum based API */
  public abstract ValuesEnum getEnum() throws IOException;
  /** in memory Random Access API - with enum support - first call loads values in ram*/
  public abstract Source getSource() throws IOException;
  /** sorted in memory Random Access API - optional operation */
  public SortedSource getSortedSource(Comparator<BytesRef> comparator) throws IOException, UnsupportedOperationException;
  /** unloads previously loaded source only but keeps the doc values open */
  public abstract unload();
  /** closes the doc values */
  public abstract close();
}
{code}

"
"LUCENE-2797","RFE","TASK","upgrade icu to 4.6","version 4.6 supports unicode 6, new collators (search collators) etc."
"LUCENE-1728","REFACTORING","IMPROVEMENT","Move SmartChineseAnalyzer & resources to own contrib project","SmartChineseAnalyzer depends on  a large dictionary that causes the analyzer jar to grow up to 3MB. The dictionary is quite big compared to all the other resouces / class files contained in that jar. 
Having a separate analyzer-cn contrib project enables footprint-sensitive users (e.g. using lucene on a mobile phone) to include analyzer.jar without getting into trouble with disk space.

Moving SmartChineseAnalyzer to a separate project could also include a small refactoring as Robert mentioned in [LUCENE-1722|https://issues.apache.org/jira/browse/LUCENE-1722] several classes should be package protected, members and classes could be final, commented syserr and logging code should be removed etc.

I set this issue target to 2.9 - if we can not make it until then feel free to move it to 3.0
"
"LUCENE-3068","BUG","BUG","The repeats mechanism in SloppyPhraseScorer is broken when doc has tokens at same position","In LUCENE-736 we made fixes to SloppyPhraseScorer, because it was
matching docs that it shouldn't; but I think those changes caused it
to fail to match docs that it should, specifically when the doc itself
has tokens at the same position.
"
"LUCENE-3827","DESIGN_DEFECT","IMPROVEMENT","Make term offsets work in MemoryIndex","Fix the logic for retrieving term offsets from DocsAndPositionsEnum on a MemoryIndex, and allow subclasses to access them."
"LUCENE-3692","BUG","BUG","DocumentsWriter blocks flushes when applyDeletes takes forever - memory not released","In DocumentsWriter we have a safety check that applies all deletes if the deletes consume too much RAM to prevent too-frequent flushing of a long tail of tiny segments. If we enter applyAllDeletes we essentially lock on IW -> BufferedDeletes which is fine since this usually doesn't take long and doesn't keep DWPTs from indexing. Yet, if that takes long and at the same time a semgent is flushed and subsequently published to the IW we take the lock on the ticket queue and the IW. Now this prevents all other threads to append to the ticketQueue which is done BEFORE we actually flush the segment concurrently and free up the RAM.

Essentially its ok to block on the IW lock but we should not keep concurrent flushed from execution just because we apply deletes. The threads will block once they try to execute maybeMerge after the segment is flushed so we don't pile up subsequent memory but we should actually allow the DWPT to be flushed since we actually try to get rid of memory.

I ran into this by accident due to a coding bug using delete queries instead of terms for each document. This thread dump show the problem:

{noformat}
""Application Worker Thread"" prio=10 tid=0x00007fdda0238000 nid=0x3256 waiting for
monitor entry [0x00007fddad3c2000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)
       - waiting to lock <0x00007fddb74ff990> (a
org.apache.lucene.index.DocumentsWriter$TicketQueue)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda0236000 nid=0x3255 waiting for
monitor entry [0x00007fddad4c3000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.IndexWriter.updatePendingMerges(IndexWriter.java:1854)
       - waiting to lock <0x00007fddb74fe350> (a
org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1848)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1843)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1493)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda0234000 nid=0x3254 waiting for
monitor entry [0x00007fddad5c4000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.IndexWriter.updatePendingMerges(IndexWriter.java:1854)
       - waiting to lock <0x00007fddb74fe350> (a
org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1848)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1843)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1493)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda0232000 nid=0x3253 waiting for
monitor entry [0x00007fddad6c5000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)
       - waiting to lock <0x00007fddb74ff990> (a
org.apache.lucene.index.DocumentsWriter$TicketQueue)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda0230800 nid=0x3252 waiting for
monitor entry [0x00007fddad7c6000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.IndexWriter.updatePendingMerges(IndexWriter.java:1854)
       - waiting to lock <0x00007fddb74fe350> (a
org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1848)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1843)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1493)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda022e800 nid=0x3251 runnable
[0x00007fddad8c6000]
  java.lang.Thread.State: RUNNABLE
       at java.nio.Bits.copyToArray(Bits.java:715)
       at java.nio.DirectByteBuffer.get(DirectByteBuffer.java:233)
       at org.apache.lucene.store.MMapDirectory$MMapIndexInput.readBytes(MMapDirectory.java:319)
       at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum$Frame.loadBlock(BlockTreeTermsReader.java:2283)
       at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum.seekExact(BlockTreeTermsReader.java:1600)
       at org.apache.lucene.util.TermContext.build(TermContext.java:97)
       at org.apache.lucene.search.TermQuery.createWeight(TermQuery.java:180)
       at org.apache.lucene.search.BooleanQuery$BooleanWeight.<init>(BooleanQuery.java:186)
       at org.apache.lucene.search.BooleanQuery.createWeight(BooleanQuery.java:423)
       at org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:583)
       at org.apache.lucene.search.QueryWrapperFilter.getDocIdSet(QueryWrapperFilter.java:55)
       at org.apache.lucene.index.BufferedDeletesStream.applyQueryDeletes(BufferedDeletesStream.java:431)
       at org.apache.lucene.index.BufferedDeletesStream.applyDeletes(BufferedDeletesStream.java:268)
       - locked <0x00007fddb751e1e8> (a
org.apache.lucene.index.BufferedDeletesStream)
       at org.apache.lucene.index.IndexWriter.applyAllDeletes(IndexWriter.java:2852)
       - locked <0x00007fddb74fe350> (a org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.DocumentsWriter.applyAllDeletes(DocumentsWriter.java:188)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:470)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)
       

""Application Worker Thread"" prio=10 tid=0x00007fdda022d800 nid=0x3250 waiting for
monitor entry [0x00007fddad9c8000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)
       - waiting to lock <0x00007fddb74ff990> (a
org.apache.lucene.index.DocumentsWriter$TicketQueue)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)
   
""Application Worker Thread"" prio=10 tid=0x00007fdda022d000 nid=0x324f waiting for
monitor entry [0x00007fddadac9000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.IndexWriter.useCompoundFile(IndexWriter.java:2274)
       - waiting to lock <0x00007fddb74fe350> (a
org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.IndexWriter.prepareFlushedSegment(IndexWriter.java:2156)
       at org.apache.lucene.index.DocumentsWriter.publishFlushedSegment(DocumentsWriter.java:526)
       at org.apache.lucene.index.DocumentsWriter.finishFlush(DocumentsWriter.java:506)
       at org.apache.lucene.index.DocumentsWriter.applyFlushTickets(DocumentsWriter.java:483)
       - locked <0x00007fddb74ff990> (a
org.apache.lucene.index.DocumentsWriter$TicketQueue)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:449)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

{noformat}"
"LUCENE-2477","CLEANUP","BUG","remove MoreLikeThis's default analyzer","MoreLikeThis has the following:

{code}
public static final Analyzer DEFAULT_ANALYZER = new StandardAnalyzer(Version.LUCENE_CURRENT);
{code}"
"LUCENE-1090","IMPROVEMENT","IMPROVEMENT","remove relative paths assumptions from benchmark code","Also see Eric comments in:
   http://www.nabble.com/forum/ViewPost.jtp?post=14347924&framed=y

Benchmark's config.xml relies on relative paths, more or less like this;
- base-dir
   -- conf-dir
   -- work-dir
       --- docs-dir
       --- indexes-dir

These assumptions are also in the Java code, and so it is inconvenient for
using absolute paths, e.g. for specifying a docs dir that is not under work-dir.

Relax this by modifying in build.xml to replace ""value"" and ""line"" props by 
""location"" and ""file"" and by requiring absolute paths in the Java code."
"LUCENE-864","TASK","IMPROVEMENT","contrib/benchmark files need eol-style set","The following files in contrib/benchmark don't have eol-style set to native, so when they are checked out, they don't get converted.

./build.xml:                    
./CHANGES.txt:                                                             
./conf/sample.alg:                                                                                
./conf/standard.alg:                                                                           
./conf/sloppy-phrase.alg:                                                                                 
./conf/deletes.alg:                                                                                         
./conf/micro-standard.alg:                                                                   
./conf/compound-penalty.alg:                                                                          
"
"LUCENE-728","CLEANUP","TASK","Remove or deprecate contrib/similarity","Classes under contrib/similarity seem to be duplicates of classes under contrib/queries.
I'd like to remove *.java from contrib/similarity without bothering with deprecation, since the same functionality exists in contrib/queries.
Anyone minds?
"
"LUCENE-3058","RFE","IMPROVEMENT","FST should allow more than one output for the same input","For the block tree terms dict, it turns out I need this case."
"LUCENE-1251","BUG","BUG","on disk full during close, FSIndexOutput fails to close descriptor","The close method just does this:

{code}
      if (isOpen) {
        super.close();
        file.close();
        isOpen = false;
      }
{code}

But super.close = BufferedIndexOutput.close, which tries to flush its buffer.  If disk is full (or something else is wrong) then we hit an exception and don't actually close the descriptor.

I will put a try/finally in so we always close, taking care to preserve the original exception. I'll commit shortly & backport to 2.3.2"
"LUCENE-1339","RFE","RFE","Add IndexReader.acquire() and release() methods using IndexReader's ref counting","From: http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200807.mbox/%3cPine.OSX.4.64.0807170752080.1708@c5850-a3-2-62-147-22-102.dial.proxad.net%3e

I have a server where a bunch of threads are handling search requests. I
have a another process that updates the index used by the search server and
that asks the searcher server to reopen its index reader after the updates
completed.

When I reopen() the index reader, I also close the old one (if the reopen()
yielded a new instance). This causes problems for the other threads that
are currently in the middle of a search request.

I'd like to propose the addition of two methods, acquire() and release() 
(attached to this bug report), that increment/decrement the ref count that IndexReader 
instances currently maintain for related purposes. That ref count prevents 
the index reader from being actually closed until it reaches zero.

My server's search threads, thus acquiring and releasing the index reader 
can be sure that the index reader they're currently using is good until 
they're done with the current request, ie, until they release() it.
"
"LUCENE-3024","BUG","BUG","If index has more than Integer.MAX_VALUE terms, seeking can it AIOOBE due to long/int overflow","Tom hit a new long/int overflow case: http://markmail.org/thread/toyl2ujcl4suqvf3

This is a regression, in 3.1, introduced with LUCENE-2075.

Worse, our Test2BTerms failed to catch this, so I've fixed that test to show the failure."
"LUCENE-1383","IMPROVEMENT","BUG","Work around ThreadLocal's ""leak""","Java's ThreadLocal is dangerous to use because it is able to take a
surprisingly very long time to release references to the values you
store in it.  Even when a ThreadLocal instance itself is GC'd, hard
references to the values you had stored in it are easily kept for
quite some time later.

While this is not technically a ""memory leak"", because eventually
(when the underlying Map that stores the values cleans up its ""stale""
references) the hard reference will be cleared, and GC can proceed,
its end behavior is not different from a memory leak in that under the
right situation you can easily tie up far more memory than you'd
expect, and then hit unexpected OOM error despite allocating an
extremely large heap to your JVM.

Lucene users have hit this many times.  Here's the most recent thread:

  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200809.mbox/%3C6e3ae6310809091157j7a9fe46bxcc31f6e63305fcdc%40mail.gmail.com%3E

And here's another:

  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200807.mbox/%3CF5FC94B2-E5C7-40C0-8B73-E12245B91CEE%40mikemccandless.com%3E

And then there's LUCENE-436 and LUCENE-529 at least.

A google search for ""ThreadLocal leak"" yields many compelling hits.

Sun does this for performance reasons, but I think it's a terrible
trap and we should work around it with Lucene."
"LUCENE-3528","BUG","BUG","TestNRTManager hang","didn't check 3.x yet, just encountered this one running the tests"
"LUCENE-867","BUILD_SYSTEM","BUG","Incomplete lucene-core-1.9.1 in Maven2 repository","I'm new to Lucene and am setting up a project using v1.9.1 to use Maven2 instead of ANT.
The project would not build with Maven2 due to lacking lucene classes.
I tracked the problem down to that the lucene-core-1.9.1 jar file that Maven2 downloaded from the repository was smaller (2.3KB) than the one I got from the local ANT repository (408KB).
Can you please update the v1.9.1 file on the Maven2 [1], [2] repositories so other developers don't get frustrated by the incomplete jar?


[1] http://repo1.maven.org/maven2/org/apache/lucene/lucene-core/1.9.1/
[2] http://mirrors.ibiblio.org/pub/mirrors/maven2/org/apache/lucene/lucene-core/1.9.1/

This issue is a copy of a mail sent to the java-dev@lucene.apache.org list april 4. 2007."
"LUCENE-1365","DOCUMENTATION","IMPROVEMENT","deprecate IndexWriter.addIndexes(Directory[])","Since addIndexesNoOptimize accomplishes the same thing, more efficiently, and you can always then call optimize() if you really wanted to, I think we should deprecate the older addIndexes(Directory[])."
"LUCENE-2689","BUG","BUG","remove NativeFSLockFactory's attempt to acquire a test lock","NativeFSLockFactory tries to acquire a test lock the first time a lock is created.  It's the only LF to do this, and, it's caused us hassle (LUCENE-2421,  LUCENE-2688).

I think we should just remove it.  The caller of .makeLock will presumably immediately thereafter acquire the lock and at the point hit any exception that acquireTestLock would've hit."
"LUCENE-2298","RFE","RFE","Polish Analyzer","Andrzej Bialecki has written a Polish stemmer and provided stemming tables for it under Apache License.

You can read more about it here: http://www.getopt.org/stempel/

In reality, the stemmer is general code and we could use it for more languages too perhaps."
"LUCENE-653","BUG","IMPROVEMENT","GData-server storage fix activation depth","Fixed nullpointer exception while rendering feeds with big amount of extensions. DB4O context.

"
"LUCENE-3757","DESIGN_DEFECT","IMPROVEMENT","Change AtomicReaderContext.leaves() to return itsself as only leave to simplify code and remove an otherwise unneeded ReaderUtil method","The documentation of IndexReaderContext.leaves() states that it returns (for convenience) all leave nodes, if the context is top-level (directly got from IndexReader), otherwise returns null. This is not correct for AtomicReaderContext, where it returns null always.

To make it consistent, the convenience method should simply return itsself as only leave for atomic contexts. This makes the utility method ReaderUtil.leaves() obsolete and simplifies code."
"LUCENE-834","RFE","RFE","Payload Queries","Now that payloads have been implemented, it will be good to make them searchable via one or more Query mechanisms.  See http://wiki.apache.org/lucene-java/Payload_Planning for some background information and https://issues.apache.org/jira/browse/LUCENE-755 for the issue that started it all.  "
"LUCENE-2824","IMPROVEMENT","IMPROVEMENT","optimizations for bufferedindexinput","along the same lines as LUCENE-2816:
* the readVInt/readVLong/readShort/readInt/readLong are not optimal here since they defer to readByte. for example this means checking the buffer's bounds per-byte in readVint instead of per-vint.
* its an easy win to speed this up, even for the vint case: its essentially always faster, the only slower case is 1024 single-byte vints in a row, in this case we would do a single extra bounds check (1025 instead of 1024)
"
"LUCENE-3552","REFACTORING","IMPROVEMENT","TaxonomyReader/Writer and their Lucene* implementation","The facet module contains two interfaces TaxonomyWriter and TaxonomyReader, with two implementations Lucene*. We've never actually implemented two TaxonomyWriters/Readers, so I'm not sure if these interfaces are useful anymore. Therefore I'd like to propose that we do either of the following:

# Remove the interfaces and remove the Lucene part from the implementation classes (to end up with TW/TR impls). Or,
# Keep the interfaces, but rename the Lucene* impls to Directory*.

Whatever we do, I'd like to make the impls/interfaces impl also TwoPhaseCommit.

Any preferences?"
"LUCENE-2664","RFE","IMPROVEMENT","Add SimpleText codec","Inspired by Sahin Buyrukbilen's question here:

  http://www.lucidimagination.com/search/document/b68846e383824653/how_to_export_lucene_index_to_a_simple_text_file#b68846e383824653

I made a simple read/write codec that stores all postings data into a
single text file (_X.pst), looking like this:

{noformat}
field contents
  term file
    doc 0
      pos 5
  term is
    doc 0
      pos 1
  term second
    doc 0
      pos 3
  term test
    doc 0
      pos 4
  term the
    doc 0
      pos 2
  term this
    doc 0
      pos 0
END
{noformat}

The codec is fully funtional -- all Lucene & Solr tests pass with
-Dtests.codec=SimpleText -- but, its performance is obviously poor.

However, it should be useful for debugging, transparency,
understanding just what Lucene stores in its index, etc.  And it's a
quick way to gain some understanding on how a codec works...
"
"LUCENE-1764","BUG","BUG","SampleComparable doesn't work well in contrib/remote tests","As discovered in LUCENE-1749, when using identical instances of a SortComparator you get multiple entries in the FieldCache.

demonstrating this bug currently requires the patches in LUCENE-1749.

See markmiller's comment here...
https://issues.apache.org/jira/browse/LUCENE-1749?focusedCommentId=12735190#action_12735190"
"LUCENE-2591","BUILD_SYSTEM","BUG","TestNLS fails with ja locale","set ANT_ARGS=""-Dargs=-Duser.language=ja -Duser.country=JP""
ant test-core -Dtestcase=TestNLS

The test has 2 sets of message, one fallback, and one ja.
The tests assume if it asks for a non-ja locale, that it will get the fallback message,
but this is not how ResourceBundle.getBundle works:
{noformat}
Otherwise, the following sequence is generated from the attribute values of the specified locale 
(language1, country1, and variant1) and of the default locale (language2, country2, and variant2):

baseName + ""_"" + language1 + ""_"" + country1 + ""_"" + variant1
baseName + ""_"" + language1 + ""_"" + country1
baseName + ""_"" + language1
baseName + ""_"" + language2 + ""_"" + country2 + ""_"" + variant2
baseName + ""_"" + language2 + ""_"" + country2
baseName + ""_"" + language2
baseName
{noformat}

So in the case of ja default locale, you get a japanese message instead from the baseName + ""_"" + language2 match"
"LUCENE-3416","RFE","IMPROVEMENT","Allow to pass an instance of RateLimiter to FSDirectory allowing to rate limit merge IO across several directories / instances","This can come in handy when running several Lucene indices in the same VM, and wishing to rate limit merge across all of them."
"LUCENE-969","IMPROVEMENT","IMPROVEMENT","Optimize the core tokenizers/analyzers & deprecate Token.termText","There is some ""low hanging fruit"" for optimizing the core tokenizers
and analyzers:

  - Re-use a single Token instance during indexing instead of creating
    a new one for every term.  To do this, I added a new method ""Token
    next(Token result)"" (Doron's suggestion) which means TokenStream
    may use the ""Token result"" as the returned Token, but is not
    required to (ie, can still return an entirely different Token if
    that is more convenient).  I added default implementations for
    both next() methods in TokenStream.java so that a TokenStream can
    choose to implement only one of the next() methods.

  - Use ""char[] termBuffer"" in Token instead of the ""String
    termText"".

    Token now maintains a char[] termBuffer for holding the term's
    text.  Tokenizers & filters should retrieve this buffer and
    directly alter it to put the term text in or change the term
    text.

    I only deprecated the termText() method.  I still allow the ctors
    that pass in String termText, as well as setTermText(String), but
    added a NOTE about performance cost of using these methods.  I
    think it's OK to keep these as convenience methods?

    After the next release, when we can remove the deprecated API, we
    should clean up Token.java to no longer maintain ""either String or
    char[]"" (and the initTermBuffer() private method) and always use
    the char[] termBuffer instead.

  - Re-use TokenStream instances across Fields & Documents instead of
    creating a new one for each doc.  To do this I added an optional
    ""reusableTokenStream(...)"" to Analyzer which just defaults to
    calling tokenStream(...), and then I implemented this for the core
    analyzers.

I'm using the patch from LUCENE-967 for benchmarking just
tokenization.

The changes above give 21% speedup (742 seconds -> 585 seconds) for
LowerCaseTokenizer -> StopFilter -> PorterStemFilter chain, tokenizing
all of Wikipedia, on JDK 1.6 -server -Xmx1024M, Debian Linux, RAID 5
IO system (best of 2 runs).

If I pre-break Wikipedia docs into 100 token docs then it's 37% faster
(1236 sec -> 774 sec), I think because of re-using TokenStreams across
docs.

I'm just running with this alg and recording the elapsed time:

  analyzer=org.apache.lucene.analysis.LowercaseStopPorterAnalyzer
  doc.tokenize.log.step=50000
  docs.file=/lucene/wikifull.txt
  doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker
  doc.tokenized=true
  doc.maker.forever=false

  {ReadTokens > : *

See this thread for discussion leading up to this:

  http://www.gossamer-threads.com/lists/lucene/java-dev/51283

I also fixed Token.toString() to work correctly when termBuffer is
used (and added unit test).
"
"LUCENE-3241","CLEANUP","","Remove Lucene core's FunctionQuery impls","As part of the consolidation of FunctionQuerys, we want to remove Lucene core's impls.  Included in this work, we will make sure that all the functionality provided by the core impls is also provided by the new module.  Any tests will be ported across too, to increase the test coverage."
"LUCENE-2069","IMPROVEMENT","IMPROVEMENT","fix LowerCaseFilter for unicode 4.0","lowercase suppl. characters correctly. 

this only fixes the filter, the LowerCaseTokenizer is part of a more complex issue (CharTokenizer)
"
"LUCENE-1976","BUG","BUG","isCurrent() and getVersion() on an NRT reader are broken","Right now isCurrent() will always return true for an NRT reader and getVersion() will always return the version of the last commit.  This is because the NRT reader holds the live segmentInfos.

I think isCurrent() should return ""false"" when any further changes have occurred with the writer, else true.   This is actually fairly easy to determine, since the writer tracks how many docs & deletions are buffered in RAM and these counters only increase with each change.

getVersion should return the version as of when the reader was created."
"LUCENE-3727","BUG","TASK","fix assertions/checks that use File.length() to use getFilePointer()","This came up on this thread ""Getting RuntimeException: after flush: fdx size mismatch while Indexing"" 
(http://www.lucidimagination.com/search/document/a8db01a220f0a126)

In trunk, a side effect of the codec refactoring is that these assertions were pushed into codecs as finish() before close().
they check getFilePointer() instead in this computation, which checks that lucene did its part (instead of falsely tripping if directory metadata is stale).

I think we should fix these checks/asserts on 3.x too
"
"LUCENE-1993","RFE","IMPROVEMENT","MoreLikeThis - allow to exclude terms that appear in too many documents (patch included)","The MoreLikeThis class allows to generate a likeness query based on a given document. So far, it is impossible to suppress words from the likeness query, that appear in almost all documents, making it necessary to use extensive lists of stop words.

Therefore I suggest to allow excluding words for which a certain absolute document count or a certain percentage of documents is exceeded. Depending on the corpus of text, words that appear in more than 50 or even 70% of documents can usually be considered insignificant for classifying a document.      "
"LUCENE-3053","TEST","TASK","improve test coverage for Multi*","It seems like an easy win that when the test calls newSearcher(), 
it should sometimes wrap the reader with a SlowMultiReaderWrapper.
"
"LUCENE-2380","IMPROVEMENT","IMPROVEMENT","Add FieldCache.getTermBytes, to load term data as byte[]","With flex, a term is now an opaque byte[] (typically, utf8 encoded unicode string, but not necessarily), so we need to push this up the search stack.

FieldCache now has getStrings and getStringIndex; we need corresponding methods to load terms as native byte[], since in general they may not be representable as String.  This should be quite a bit more RAM efficient too, for US ascii content since each character would then use 1 byte not 2."
"LUCENE-1469","IMPROVEMENT","IMPROVEMENT","isValid should be invoked after analyze rather than before it so it can validate the output of analyze","The Synonym map has a protected method String analyze(String word) designed for custom stemming.

However, before analyze is invoked on a word, boolean isValid(String str) is used to validate the word - which causes the program to discard words that maybe useable by the custom analyze method. 

I think that isValid should be invoked after analyze rather than before it so it can validate the output of analyze and allow implemters to decide what is valid for the overridden analyze method. (In fact, if you look at code snippet below, isValid should really go after the empty string check)

This is a two line change in org.apache.lucene.index.memory.SynonymMap

      /*
       * Part B: ignore phrases (with spaces and hyphens) and
       * non-alphabetic words, and let user customize word (e.g. do some
       * stemming)
       */
      if (!isValid(word)) continue; // ignore
      word = analyze(word);
      if (word == null || word.length() == 0) continue; // ignore"
"LUCENE-3671","RFE","RFE","Add a TypeTokenFilter","It would be convenient to have a TypeTokenFilter that filters tokens by its type, either with an exclude or include list. This might be a stupid thing to provide for people who use Lucene directly, but it would be very useful to later expose it to Solr and other Lucene-backed search solutions."
"LUCENE-444","BUG","BUG","StandardTokenizer loses Korean characters","While using StandardAnalyzer, exp. StandardTokenizer with Korean text stream, StandardTokenizer ignores the Korean characters. This is because the definition of CJK token in StandardTokenizer.jj JavaCC file doesn't have enough range covering Korean syllables described in Unicode character map.
This patch adds one line of 0xAC00~0xD7AF, the Korean syllables range to the StandardTokenizer.jj code."
"LUCENE-450","BUG","BUG","MatchAllDocsQuery doesn't honor boost or queryNorm","MatchAllDocsQuery doesn't pay attention to either it's own boost, or lucene's query normalization factor."
"LUCENE-1786","IMPROVEMENT","TEST","improve performance of contrib/TestCompoundWordTokenFilter","contrib/analyzers/compound has some tests that use a hyphenation grammar file.

The tests are currently for german, and they actually are nice, they show how the combination of the hyphenation rules and dictionary work in tandem.
The issue is that the german grammar file is not apache licensed: http://offo.sourceforge.net/hyphenation/licenses.html
So the test must download the entire offo zip file from sourceforge to execute.

I happen to think the test is a great example of how this thing works (with a language where it matters), but we could consider using a different grammar file, for a language that is apache licensed.
This way it could be included in the source with the test and would be more practical.
"
"LUCENE-1444","DOCUMENTATION","BUG","Broken javadocs->site docs links","See the java-dev mailing list discussion: [http://www.nabble.com/Broken-javadocs-%3Esite-docs-links-to20369092.html].

When the Lucene Java website transitioned to versioning some of the documentation, links from some javadocs were not modified to follow the resources.  I found broken links to gettingstarted.html and queryparsersyntax.html.  Here is one example, to gettingstarted.html (the link text is ""demo""): 

[http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/document/package-summary.html]

The attached patch converts absolute URLs from javadocs to versioned docs to be relative, and modifies the ""javadocs-all"" target in build.xml to add a path element named ""all"", so that both versions of the javadocs (all: core+contrib; and separated: core, contribs) can use the same relative URLs.  Adding a path element to the ""javadocs-all"" target is necessary because currently the ""all"" javadocs have one fewer path element than the separated javadocs.

I left as-is one absolute URL, in the o.a.l.index.SegmentInfos javadocs, to fileformats.html, because SegmentInfos is a package-private class, and the javadocs targets in build.xml only generate javadocs for public classes.
"
"LUCENE-3589","BUG","BUG","BytesRef copy short missed the length setting","when storing a short type integer to BytesRef, BytesRef missed the length setting. then it will cause the storage size is ZERO if no continuous options on this BytesRef"
"LUCENE-2512","BUG","BUG","DeleteByPercentTask hits NPE","I'm building up Wiki indices for testing search perf across 3.x/4.0, but hit NPE when creating deletions in 4.0 due to flex cutover..."
"LUCENE-3239","CLEANUP","TASK","drop java 5 ""support""","its been discussed here and there, but I think we need to drop java 5 ""support"", for these reasons:
* its totally untested by any continual build process. Testing java5 only when there is a release candidate ready is not enough. If we are to claim ""support"" then we need a hudson actually running the tests with java 5.
* its now unmaintained, so bugs have to either be hacked around, tests disabled, warnings placed, but some things simply cannot be fixed... we cannot actually ""support"" something that is no longer maintained: we do find JRE bugs (http://wiki.apache.org/lucene-java/SunJavaBugs) and its important that bugs actually get fixed: cannot do everything with hacks.
* because of its limitations, we do things like allow 20% slower grouping speed. I find it hard to believe we are sacrificing performance for this.

So, in summary: because we don't test it at all, because its buggy and unmaintained, and because we are sacrificing performance, I think we need to cutover the build system for the next release to require java 6.
"
"LUCENE-3254","BUG","BUG","BitVector.isSparse is sometimes wrong","In working on LUCENE-3246, I found a few problems with
BitVector.isSparse:

  * Its math can overflow int, such that if there are enough deleted
    docs and maxDoc() is largish, isSparse may incorrectly return true

  * It over-estimates the size of the sparse file, since when
    estimating number of bytes for the vInt dgaps it uses bits.length
    instead of bits.length divided by number of set bits (ie, the
    ""average"" gap between set bits)

This is relatively harmless (just affects performance / size of .del
file on disk, not correctness).
"
"LUCENE-1718","BUG","BUG","IndexReader.setTermInfosIndexDivisor doesn't carry over to reopened readers","When you reopen a reader, some segments are shared (and thus properly inherit the index divisor) but others are newly opened and use the default index divisor.  You then have no way to change the index divisor of those newly opened ones.  The only workaround is to not use reopen (always open a new reader).

I'd like to make termInfosDivisor an up-front param to IndexReader, anyway, for LUCENE-1609, so likely I'll fix both of these issues there."
"LUCENE-900","BUILD_SYSTEM","TEST","Enable Java asserts in the Junit tests","For background see http://www.mail-archive.com/java-dev@lucene.apache.org/msg10307.html"
"LUCENE-1602","RFE","RFE","Rewrite TrieRange to use MultiTermQuery","Issue for discussion here: http://www.lucidimagination.com/search/document/46a548a79ae9c809/move_trierange_to_core_module_and_integration_issues

This patch is a rewrite of TrieRange using MultiTermQuery like all other core queries. This should make TrieRange identical in functionality to core range queries."
"LUCENE-1484","IMPROVEMENT","IMPROVEMENT","Remove SegmentReader.document synchronization","This is probably the last synchronization issue in Lucene.  It is the document method in SegmentReader.  It is avoidable by using a threadlocal for FieldsReader.  "
"LUCENE-2277","BUG","BUG","QueryNodeImpl throws ConcurrentModificationException on add(List<QueryNode>)","on adding a List of children to a QueryNodeImplemention a ConcurrentModificationException is thrown.
This is due to the fact that QueryNodeImpl instead of iteration over the supplied list, iterates over its internal clauses List.

Patch:
Index: QueryNodeImpl.java
===================================================================
--- QueryNodeImpl.java    (revision 911642)
+++ QueryNodeImpl.java    (working copy)
@@ -74,7 +74,7 @@
           .getLocalizedMessage(QueryParserMessages.NODE_ACTION_NOT_SUPPORTED));
     }
 
-    for (QueryNode child : getChildren()) {
+    for (QueryNode child : children) {
       add(child);
     }
 "
"LUCENE-3045","BUG","BUG","QueryNodeImpl.containsTag(String) should lowercase the tag key","QueryNodeImpl.containsTag(String key): tag keys are  supposed to be case insensitive, however QueryNodeImpl.containsTag method is considering the case when looking up for tag.

*Bug found by Karsten Fissmer"
"LUCENE-1744","BUG","BUG","BooleanScorer2 fails to update this.doc when its the top scorer","When BooleanScorer2 runs the top collection loop (one of its
score(Collector)) methods, it uses a local ""doc"" var, ie:

{code}
public void score(Collector collector) throws IOException {
    collector.setScorer(this);
    int doc;
    while ((doc = countingSumScorer.nextDoc()) != NO_MORE_DOCS) {
      collector.collect(doc);
    }
}
{code}

The problem is, if the child collector calls scorer.doc() it will
always get -1.  Most Collectors don't actually call scorer.doc(), but
one important one that does is ScoreCachingWrapperScorer, as it uses
the doc to know when to invalidate its cache.  Since this always
returns -1, the ScoreCachingWrapperScorer keeps returning score=0.0 to
its caller, thus messing up a SortField.SCORE comparator instance if
it's included in the sort fields.
"
"LUCENE-1539","IMPROVEMENT","IMPROVEMENT","Improve Benchmark","Benchmark can be improved by incorporating recent suggestions posted
on java-dev. M. McCandless' Python scripts that execute multiple
rounds of tests can either be incorporated into the codebase or
converted to Java."
"LUCENE-536","BUG","BUG","JEDirectory delete issue","JEDirectory is not deleting files properly.  Blocks are left behind due to an error in cursor operations."
"LUCENE-1124","IMPROVEMENT","IMPROVEMENT","short circuit FuzzyQuery.rewrite when input token length is small compared to minSimilarity","I found this (unreplied to) email floating around in my Lucene folder from during the holidays...

{noformat}
From: Timo Nentwig
To: java-dev
Subject: Fuzzy makes no sense for short tokens
Date: Mon, 31 Dec 2007 16:01:11 +0100
Message-Id: <200712311601.12255.lucene@nitwit.de>

Hi!

it generally makes no sense to search fuzzy for short tokens because changing
even only a single character of course already results in a high edit
distance. So it actually only makes sense in this case:

           if( token.length() > 1f / (1f - minSimilarity) )

E.g. changing one character in a 3-letter token (foo) results in an edit
distance of 0.6. And if minSimilarity (which is by default: 0.5 :-) is higher
we can save all the expensive rewrite() logic.
{noformat}

I don't know much about FuzzyQueries, but this reasoning seems sound ... FuzzyQuery.rewrite should be able to completely skip all TermEnumeration in the event that the input token is shorter then some simple math on the minSimilarity.  (i'm not smart enough to be certain that the math above is right however ... it's been a while since i looked at Levenstein distances ... tests needed)
"
"LUCENE-2109","REFACTORING","IMPROVEMENT","Make DocsEnum subclass of DocIdSetIterator","Spinoff from LUCENE-1458:

One thing I came along long time ago, but now with a new API it get's interesting again: 
DocsEnum should extend DocIdSetIterator, that would make it simplier to use and implement e.g. in MatchAllDocQuery.Scorer, FieldCacheRangeFilter and so on. You could e.g. write a filter for all documents that simply returns the docs enumeration from IndexReader.

So it should be an abstract class that extends DocIdSetIterator. It has the same methods, only some methods must be a little bit renamed. The problem is, because java does not support multiple inheritace, we cannot also extends attributesource  Would DocIdSetIterator be an interface it would work (this is one of the cases where interfaces for really simple patterns can be used, like iterators).

The problem with multiple inheritance could be solved by an additional method attributes() that creates a new AttributeSource on first access then (because constructing an AttributeSource is costly).  The same applies for the other *Enums, it should be separated for lazy init.

DocsEnum could look like this:

{code}
public abstract class DocsEnum extends DocIdSetIterator {
  private AttributeSource atts = null;
  public int freq()
  public DontKnowClassName positions()
  public final AttributeSource attributes() {
   if (atts==null) atts=new AttributeSource();
   return atts;
  }
  ...default impl of the bulk access using the abstract methods from DocIdSetIterator
}
{code}
"
"LUCENE-2353","BUG","BUG","Config incorrectly handles Windows absolute pathnames","I have no idea how no one ran into this so far, but I tried to execute an .alg file which used ReutersContentSource and referenced both docs.dir and work.dir as Windows absolute pathnames (e.g. d:\something). Surprisingly, the run reported an error of missing content under benchmark\work\something.

I've traced the problem back to Config, where get(String, String) includes the following code:
{code}
    if (sval.indexOf("":"") < 0) {
      return sval;
    }
    // first time this prop is extracted by round
    int k = sval.indexOf("":"");
    String colName = sval.substring(0, k);
    sval = sval.substring(k + 1);
    ...
{code}

It detects "":"" in the value and so it thinks it's a per-round property, thus stripping ""d:"" from the value ... fix is very simple:
{code}
    if (sval.indexOf("":"") < 0) {
      return sval;
    } else if (sval.indexOf("":\\"") >= 0) {
      // this previously messed up absolute path names on Windows. Assuming
      // there is no real value that starts with \\
      return sval;
    }
    // first time this prop is extracted by round
    int k = sval.indexOf("":"");
    String colName = sval.substring(0, k);
    sval = sval.substring(k + 1);
{code}

I'll post a patch w/ the above fix + test shortly."
"LUCENE-430","IMPROVEMENT","IMPROVEMENT","Reducing buffer sizes for TermDocs.","From java-dev: 
 
On Friday 09 September 2005 00:34, Doug Cutting wrote: 
> Paul Elschot wrote: 
> > I suppose one of these cases are when many terms are used in a query.  
> > Would it be easily possible to make the buffer size for a term iterator 
> > depend on the numbers of documents to be iterated? 
> > Many terms only occur in a few documents, so this could be a  
> > nice win on total buffer size for the many terms case. 
>  
> This would not be too difficult. 
>  
> Look in SegmentTermDocs.java. The buffer may be allocated when the  
> parent's stream is first cloned, but clone() won't allocate a buffer if  
> the source hasn't had a buffer allocated yet, and nothing should perform  
> i/o directly on the parent's freqStream, so in practice a buffer should  
> not be allocated until the first read is performed on the clone. 
 
I tried delaying the buffer allocation in BufferedIndexInput by 
using this clone() method: 
 
 public Object clone() { 
  BufferedIndexInput clone = (BufferedIndexInput)super.clone(); 
  clone.buffer = null; 
  clone.bufferLength = 0; 
  clone.bufferPosition = 0; 
  clone.bufferStart = getFilePointer();  
  return clone; 
 } 
 
With this all term document iterators seem to be empty, no 
query in the test cases gives any results, for example TestDemo 
and TestBoolean2. 
As far as I can see, this delaying should work, but it doesn't and 
I have no idea why. 
 
End of quote from java-dev. 
 
Doug replied that at a glance this clone method looks good. 
Without this delayed buffer allocation, a reduced buffer size 
for TermDocs cannot be implemented easily."
"LUCENE-1548","BUG","BUG","LevenshteinDistance code normalization is incorrect","The normalization of the edit distance should use the maximum of the two string being compared instead of the minimum.  Otherwise negative distances are possible.  The spell checker filters out edits below a certain threshold so this hasn't been a problem in practice."
"LUCENE-1125","IMPROVEMENT","BUG","Excessive Arrays.fill(0) in DocumentsWriter drastically slows down small docs (3.9X slowdown!)","I've been doing some ""final"" performance testing of 2.3RC1 and
uncovered a fairly serious bug that adds a large fixed CPU cost when
documents have any term vector enabled fields.

The bug does not affect correctness, just performance.

Basically, for every document, we were calling Arrays.fill(0) on a
large (32 KB) byte array when in fact we only needed to zero a small
part of it.  This only happens if term vectors are turned on, and is
especially devastating for small documents."
"LUCENE-608","DOCUMENTATION","IMPROVEMENT","deprecate Document.fields(), add getFields()","A simple API improvement that I'm going to commit if nobody objects."
"LUCENE-1810","RFE","","Add a LATENT FieldSelectorResult","I propose adding LATENT FieldSelectorResult

this would be similar to LAZY_LOAD except that it would NEVER cache the stored value

This will be useful for very large fields that should always go direct to disk (because they will take so much memory)
when caching Documents returned from a Searcher, the large field may be initially requested as LAZY_LOAD, however once someone reads this field, it will then get locked into memory. if this Document (and others like it) are cached, it can start to use a very large amount of memory for these fields

Contract for FieldSelectorResult.LATENT should be that it will always be pulled direct from the IndexInput and never be persisted in memory as part of a Fieldable

I could prepare a patch if desired

"
"LUCENE-1451","BUG","BUG","Can't create NIOFSDirectory w/o setting a system property","NIOFSDirectory.getDirectory() returns a FSDirectory object"
"LUCENE-3218","RFE","IMPROVEMENT","Make CFS appendable  ","Currently CFS is created once all files are written during a flush / merge. Once on disk the files are copied into the CFS format which is basically a unnecessary for some of the files. We can at any time write at least one file directly into the CFS which can save a reasonable amount of IO. For instance stored fields could be written directly during indexing and during a Codec Flush one of the written files can be appended directly. This optimization is a nice sideeffect for lucene indexing itself but more important for DocValues and LUCENE-3216 we could transparently pack per field files into a single file only for docvalues without changing any code once LUCENE-3216 is resolved."
"LUCENE-2699","RFE","IMPROVEMENT","Update StandardTokenizer and UAX29Tokenizer to Unicode 6.0.0","Newly released Unicode 6.0.0 contains some character property changes from the previous release (5.2.0) that affect word segmentation (UAX#29), and JFlex 1.5.0-SNAPSHOT now supports Unicode 6.0.0, so Lucene's UAX#29-based tokenizers should be updated accordingly.

Note that the UAX#29 word break rules themselves did not change between Unicode versions 5.2.0 and 6.0.0."
"LUCENE-2782","BUG","BUG","Possible rare thread hazard in IW.commit","I was seeing a very rare intermittent failure in TestIndexWriter.testCommitThreadSafety.

The issue happens if one thread calls commit while another is flushing, and is exacerbated at high flush rates (eg maxBufferedDocs=2).  The thread doing commit will first flush, and then it syncs the files.  However in between those two, if other threads manage to add enough docs and trigger another flush, a 2nd new segment can sneak into the SegmentInfos before we sync.

This is normally harmless, in that it just means the commit includes a few more docs that had been added by other threads, so it's fine. But, it can mean that a committed segment references the still-open doc store files.  Our tests now catch this (I changed MockDirWrapper to throw an exception in this case), and so testCommitThreadSafety can fail with this exception.  If you hardwire the maxBufferedDocs to 2 it happens quite often.

It's not clear this is really a problem in real apps vs just our anal MockDirWrapper but I think we should fix it..."
"LUCENE-1472","IMPROVEMENT","IMPROVEMENT","DateTools.stringToDate() can cause lock contention under load","Load testing our application (the JIRA Issue Tracker) has shown that threads spend a lot of time blocked in DateTools.stringToDate().

The stringToDate() method uses a singleton SimpleDateFormat object to parse the dates.
Each call to SimpleDateFormat.parse() is *synchronized* because SimpleDateFormat is not thread safe.

"
"LUCENE-2579","DOCUMENTATION","BUG","Small imprecision in Search package Javadocs","Search package Javadocs states that Scorer#score(Collector) will be abstract in Lucene 3.0, which is not accurate anymore."
"LUCENE-239","REFACTORING","BUG","[PATCH] cleaner API for Field.Text","Currently there are four methods named Field.Text(). As those methods have 
the same name and a very similar method signature, everyone will think these 
are just convenience methods that do the same thing. But they behave 
differently: the one that takes a Reader doesn't store the data, the one that 
takes a String does. I know that this is documented, but it's still not a nice 
API. Methods that behave differently should have diffent names. The attached 
patch deprecates two of the old methods and adds two new ones named 
Field.StoredText(). I think this is much easier to understand from the 
programmer's point-of-view and will help avoid bugs."
"LUCENE-3374","REFACTORING","TASK","move nrtcachingdir to core in 4.0","in 4.0 with the IOContext changes this implementation is clean and I think we should move it to core and use it in our tests etc."
"LUCENE-2588","IMPROVEMENT","IMPROVEMENT","terms index should not store useless suffixes","This idea came up when discussing w/ Robert how to improve our terms index...

The terms dict index today simply grabs whatever term was at a 0 mod 128 index (by default).

But this is wasteful because you often don't need the suffix of the term at that point.

EG if the 127th term is aa and the 128th (indexed) term is abcd123456789, instead of storing that full term you only need to store ab.  The suffix is useless, and uses up RAM since we load the terms index into RAM.

The patch is very simple.  The optimization is particularly easy because terms are now byte[] and we sort in binary order.

I tested on first 10M 1KB Wikipedia docs, and this reduces the terms index (tii) file from 3.9 MB -> 3.3 MB = 16% smaller (using StandardAnalyzer, indexing body field tokenized but title / date fields untokenized).  I expect on noisier terms dicts, especially ones w/ bad terms accidentally indexed, that the savings will be even more.

In the future we could do crazier things.  EG there's no real reason why the indexed terms must be regular (every N terms), so, we could instead pick terms more carefully, say ""approximately"" every N, but favor terms that have a smaller net prefix.  We can also index more sparsely in regions where the net docFreq is lowish, since we can afford somewhat higher seek+scan time to these terms since enuming their docs will be much faster."
"LUCENE-1145","IMPROVEMENT","IMPROVEMENT","DisjunctionSumScorer small tweak","Move ScorerDocQueue initialization from next() and skipTo() methods to the Constructor. Makes DisjunctionSumScorer a bit faster (less than 1% on my tests). 

Downside (if this is one, I cannot judge) would be throwing IOException from DisjunctionSumScorer constructors as we touch HardDisk there. I see no problem as this IOException does not propagate too far (the only modification I made is in BooleanScorer2)

if (scorerDocQueue == null) {
      initScorerDocQueue();
}
 

Attached test is just quick & dirty rip of  TestScorerPerf from standard Lucene test package. Not included as patch as I do not like it.


All test pass, patch made on trunk revision 613923
"
"LUCENE-1665","CLEANUP","IMPROVEMENT","Remove SortField.AUTO","I'd like to remove SortField.AUTO... it's dangerous for Lucene to
guess the type of your field, based on the first term it encounters.
It can easily be wrong, and, whether it's wrong or right could
suddenly change as you index different documents.

It unexepctedly binds SortField to needing an IndexReader to do the
guessing.

It's caused various problems in the past (most recently, for me on
LUCENE-1656) as we fix other issues/make improvements.

I'd prefer that users of Lucene's field sort be explicit about the
type that Lucene should cast the field to.  Someday, if we have
optional strong[er] typing of Lucene's fields, such type information
would already be known.  But in the meantime, I think users should be
explicit.
"
"LUCENE-2737","DESIGN_DEFECT","BUG","Codec is not consistently passed in internal API","While working on SOLR-1942 I ran into a couple of glitches with codec which is not consistently passed to SegmentsInfo and friends. Codecs should really be consistently passed though. I have fixed the pieces which lead to errors in Solr but I  guess there might be others too. Patch is coming up... "
"LUCENE-2936","BUG","BUG","score and explain don't match","I've faced this problem recently. I'll attach a program to reproduce the problem soon. The program outputs the following:

{noformat}
** score = 0.10003257
** explain
0.050016284 = (MATCH) product of:
  0.15004885 = (MATCH) sum of:
    0.15004885 = weight(f1:""note book"" in 0), product of:
      0.3911943 = queryWeight(f1:""note book""), product of:
        0.61370564 = idf(f1: note=1 book=1)
        0.6374299 = queryNorm
      0.38356602 = fieldWeight(f1:""note book"" in 0), product of:
        1.0 = tf(phraseFreq=1.0)
        0.61370564 = idf(f1: note=1 book=1)
        0.625 = fieldNorm(field=f1, doc=0)
  0.33333334 = coord(1/3)
{noformat}
"
"LUCENE-1354","RFE","RFE","Provide Programmatic Access to CheckIndex","Would be nice to have programmatic access to the CheckIndex tool, so that it can be used in applications like Solr.  

See SOLR-566"
"LUCENE-2384","IMPROVEMENT","","Reset zzBuffer in StandardTokenizerImpl* when lexer is reset.","When indexing large documents, the lexer buffer may stay large forever. This sub-issue resets the lexer buffer back to the default on reset(Reader).

This is done on the enclosing issue."
"LUCENE-237","IMPROVEMENT","BUG","[PATCH] fix compile errors in sandbox","Here's a patch that fixes the compile problems in sandbox/analyzers starting 
shortly before the 1.4 release. The deprecation warnings are also fixed. I 
have not tested the changes (I don't use those analyzers) but the changes 
should be trivial enough so they don't break anything. 
 
Could someone apply the patch and also fix FrenchAnalyzer? It's the same 
change as for the other files, but I didn't manage to make a clean diff 
because of encoding problems."
"LUCENE-400","RFE","IMPROVEMENT","NGramFilter -- construct n-grams from a TokenStream","This filter constructs n-grams (token combinations up to a fixed size, sometimes
called ""shingles"") from a token stream.

The filter sets start offsets, end offsets and position increments, so
highlighting and phrase queries should work.

Position increments > 1 in the input stream are replaced by filler tokens
(tokens with termText ""_"" and endOffset - startOffset = 0) in the output
n-grams. (Position increments > 1 in the input stream are usually caused by
removing some tokens, eg. stopwords, from a stream.)

The filter uses CircularFifoBuffer and UnboundedFifoBuffer from Apache
Commons-Collections.

Filter, test case and an analyzer are attached."
"LUCENE-2170","TEST","BUG","Thread starvation problems in some tests","In some of the tests, a time limit is set and the tests have a ""while (inTime)"" loop. If creation of thread under heavy load is too slow, the tasks are not done. Most tests are only useful, if the task is at least done once (most would even fail).

This thread changes the loops to be do...while, so the task is run at least one time."
"LUCENE-2332","RFE","RFE","Merge CharTermAttribute and deprecations to stable","This should be merged to trunk until flex lands, so the analyzers can be ported to new api."
"LUCENE-788","BUG","BUG","contrib/benchmark assumes Locale.US for parsing dates in Reuters collection","SimpleDateFormat used for parsing dates in Reuters documents is instantiated without specifying a locale. So it is using the default locale. If that happens to be US, it will work. But for another locale a parse exception is likely.

Affects both StandardBenchmarker and ReutersDocMaker.

Fix is trivial - specify Locale.US for SimpleDateFormat's constructor.
"
"LUCENE-1497","REFACTORING","IMPROVEMENT","Minor changes to SimpleHTMLFormatter","I'd like to make few minor changes to SimpleHTMLFormatter.

1. Define DEFAULT_PRE_TAG and DEFAULT_POST_TAG and use them in the default constructor. This will not trigger String lookups by the JVM whenever the highlighter is instantiated.

2. Create the StringBuffer in highlightTerm with the right number of characters from the beginning. Even though StringBuffer's default constructor allocates 16 chars, which will probably be enough for most highlighted terms (pre + post tags are 7 chars, which leaves 9 chars for terms), I think it's better to allocate SB with the right # of chars in advance, to avoid char[] allocations in the middle."
"LUCENE-3272","REFACTORING","IMPROVEMENT","Consolidate Lucene's QueryParsers into a module","Lucene has a lot of QueryParsers and we should have them all in a single consistent place.  

The following are QueryParsers I can find that warrant moving to the new module:

- Lucene Core's QueryParser
- AnalyzingQueryParser
- ComplexPhraseQueryParser
- ExtendableQueryParser
- Surround's QueryParser
- PrecedenceQueryParser
- StandardQueryParser
- XML-Query-Parser's CoreParser

All seem to do a good job at their kind of parsing with extensive tests.

One challenge of consolidating these is that many tests use Lucene Core's QueryParser.  One option is to just replicate this class in src/test and call it TestingQueryParser.  Another option is to convert all tests over to programmatically building their queries (seems like alot of work)."
"LUCENE-1341","RFE","IMPROVEMENT","BoostingNearQuery class (prototype)","This patch implements term boosting for SpanNearQuery. Refer to: http://www.gossamer-threads.com/lists/lucene/java-user/62779

This patch works but probably needs more work. I don't like the use of 'instanceof', but I didn't want to touch Spans or TermSpans. Also, the payload code is mostly a copy of what's in BoostingTermQuery and could be common-sourced somewhere. Feel free to throw darts at it :)
"
"LUCENE-1867","BUILD_SYSTEM","TASK","replace collation/lib/icu4j.jar with a smaller icu jar","Collation does not need all the icu data.
we can shrink the jar file a bit by using the data customizer, and excluding things like character set conversion tables."
"LUCENE-795","DOCUMENTATION","BUG","deprecate Directory.renameFile()","Copied from my mailing list post so this issue can be tracked (if necessary). I will commit a patch.

I see that Directory.renameFile() isn't used anymore. I assume it has only 
been public for technical reasons, not because we expect this to be used 
from outside of Lucene? Should we deprecate this method? Its 
implementation e.g. in FSDirectory looks a bit scary anyway (the comment 
correctly says ""This is not atomic"" while the abstract class says ""This 
replacement should be atomic"").
"
"LUCENE-1752","BUG","BUG","incorrect snippet returned with SpanScorer","This problem was reported by my customer. They are using Solr 1.3 and uni-gram, but it can be reproduced with Lucene 2.9 and WhitespaceAnalyzer.

{panel:title=Query}
(f1:""a b c d"" OR f2:""a b c d"") AND (f1:""b c g"" OR f2:""b c g"")
{panel}

The snippet we expected is:
{panel}
x y z <B>a</B> <B>b</B> <B>c</B> <B>d</B> e f g <B>b</B> <B>c</B> <B>g</B>
{panel}

but we got:
{panel}
x y z <B>a</B> b c <B>d</B> e f g <B>b</B> <B>c</B> <B>g</B>
{panel}

Program to reproduce the problem:
{code}
public class TestHighlighter {

  static final String CONTENT = ""x y z a b c d e f g b c g"";
  static final String PH1 = ""\""a b c d\"""";
  static final String PH2 = ""\""b c g\"""";
  static final String F1 = ""f1"";
  static final String F2 = ""f2"";
  static final String F1C = F1 + "":"";
  static final String F2C = F2 + "":"";
  static final String QUERY_STRING =
    ""("" + F1C + PH1 + "" OR "" + F2C + PH1 + "") AND (""
    + F1C + PH2 + "" OR "" + F2C + PH2 + "")"";
  static Analyzer analyzer = new WhitespaceAnalyzer();
  
  public static void main(String[] args) throws Exception {
    QueryParser qp = new QueryParser( F1, analyzer );
    Query query = qp.parse( QUERY_STRING );
    CachingTokenFilter stream = new CachingTokenFilter( analyzer.tokenStream( F1, new StringReader( CONTENT ) ) );
    Scorer scorer = new SpanScorer( query, F1, stream, false );
    Highlighter h = new Highlighter( scorer );
    System.out.println( ""query : "" + QUERY_STRING );
    System.out.println( h.getBestFragment( analyzer, F1,  CONTENT ) );
  }
}
{code}
"
"LUCENE-203","BUG","BUG","[PATCH] GermanAnalyzer fails silently + doesn't close files","As mentioned on the developer list, the German analyzer will assume an empty list of 
stopwords if the stopword file isn't found. I'll attach a patch that makes it throw an 
IOException instead. Also the patch makes sure the file readers are closed."
"LUCENE-381","RFE","BUG","Contributing a High-performance single-document main memory Apache Lucene fulltext search index.","Here is my contribution: a High-performance single-document main memory Apache Lucene fulltext 
search index. I'll try to attach the files, hoping for comments on how to proceed with this..."
"LUCENE-1567","RFE","RFE","New flexible query parser","From ""New flexible query parser"" thread by Micheal Busch

in my team at IBM we have used a different query parser than Lucene's in
our products for quite a while. Recently we spent a significant amount
of time in refactoring the code and designing a very generic
architecture, so that this query parser can be easily used for different
products with varying query syntaxes.

This work was originally driven by Andreas Neumann (who, however, left
our team); most of the code was written by Luis Alves, who has been a
bit active in Lucene in the past, and Adriano Campos, who joined our
team at IBM half a year ago. Adriano is Apache committer and PMC member
on the Tuscany project and getting familiar with Lucene now too.

We think this code is much more flexible and extensible than the current
Lucene query parser, and would therefore like to contribute it to
Lucene. I'd like to give a very brief architecture overview here,
Adriano and Luis can then answer more detailed questions as they're much
more familiar with the code than I am.
The goal was it to separate syntax and semantics of a query. E.g. 'a AND
b', '+a +b', 'AND(a,b)' could be different syntaxes for the same query.
We distinguish the semantics of the different query components, e.g.
whether and how to tokenize/lemmatize/normalize the different terms or
which Query objects to create for the terms. We wanted to be able to
write a parser with a new syntax, while reusing the underlying
semantics, as quickly as possible.
In fact, Adriano is currently working on a 100% Lucene-syntax compatible
implementation to make it easy for people who are using Lucene's query
parser to switch.

The query parser has three layers and its core is what we call the
QueryNodeTree. It is a tree that initially represents the syntax of the
original query, e.g. for 'a AND b':
  AND
 /   \
A     B

The three layers are:
1. QueryParser
2. QueryNodeProcessor
3. QueryBuilder

1. The upper layer is the parsing layer which simply transforms the
query text string into a QueryNodeTree. Currently our implementations of
this layer use javacc.
2. The query node processors do most of the work. It is in fact a
configurable chain of processors. Each processors can walk the tree and
modify nodes or even the tree's structure. That makes it possible to
e.g. do query optimization before the query is executed or to tokenize
terms.
3. The third layer is also a configurable chain of builders, which
transform the QueryNodeTree into Lucene Query objects.

Furthermore the query parser uses flexible configuration objects, which
are based on AttributeSource/Attribute. It also uses message classes that
allow to attach resource bundles. This makes it possible to translate
messages, which is an important feature of a query parser.

This design allows us to develop different query syntaxes very quickly.
Adriano wrote the Lucene-compatible syntax in a matter of hours, and the
underlying processors and builders in a few days. We now have a 100%
compatible Lucene query parser, which means the syntax is identical and
all query parser test cases pass on the new one too using a wrapper.


Recent posts show that there is demand for query syntax improvements,
e.g improved range query syntax or operator precedence. There are
already different QP implementations in Lucene+contrib, however I think
we did not keep them all up to date and in sync. This is not too
surprising, because usually when fixes and changes are made to the main
query parser, people don't make the corresponding changes in the contrib
parsers. (I'm guilty here too)
With this new architecture it will be much easier to maintain different
query syntaxes, as the actual code for the first layer is not very much.
All syntaxes would benefit from patches and improvements we make to the
underlying layers, which will make supporting different syntaxes much
more manageable.
"
"LUCENE-2037","TEST","IMPROVEMENT","Allow Junit4 tests in our environment.","Now that we're dropping Java 1.4 compatibility for 3.0, we can incorporate Junit4 in testing. Junit3 and junit4 tests can coexist, so no tests should have to be rewritten. We should start this for the 3.1 release so we can get a clean 3.0 out smoothly.

It's probably worthwhile to convert a small set of tests as an exemplar.


"
"LUCENE-2374","RFE","IMPROVEMENT","Add reflection API to AttributeSource/AttributeImpl","AttributeSource/TokenStream inspection in Solr needs to have some insight into the contents of AttributeImpls. As LUCENE-2302 has some problems with toString() [which is not structured and conflicts with CharSequence's definition for CharTermAttribute], I propose an simple API that get a default implementation in AttributeImpl (just like toString() current):

- Iterator<Map.Entry<String,?>> AttributeImpl.contentsIterator() returns an iterator (for most attributes its a singleton) of a key-value pair, e.g. ""term""->""foobar"",""startOffset""->Integer.valueOf(0),...
- AttributeSource gets the same method, it just concat the iterators of each getAttributeImplsIterator() AttributeImpl

No backwards problems occur, as the default toString() method will work like before (it just gets iterator and lists), but we simply remove the documentation for the format. (Char)TermAttribute gets a special impl fo toString() according to CharSequence and a corresponding iterator.

I also want to remove the abstract hashCode() and equals() methods from AttributeImpl, as they are not needed and just create work for the implementor."
"LUCENE-3470","REFACTORING","TASK","reorder arguments of Field constructor to be more intuitive","I think Field should take (name, value, type) not (name, type, value) ?

This seems more intuitive and consistent with previous releases

Take this change to some code I had for example:
{noformat}
-    d1.add(new Field(""foo"", ""bar"", Field.Store.YES, Field.Index.ANALYZED));
+    d1.add(new Field(""foo"", TextField.TYPE_STORED, ""bar""));
{noformat}

I think it would be better if it was
{noformat}
document.add(new Field(""foo"", ""bar"", TextField.TYPE_STORED));
{noformat}"
"LUCENE-2701","REFACTORING","IMPROVEMENT","Factor maxMergeSize into findMergesForOptimize in LogMergePolicy","LogMergePolicy allows you to specify a maxMergeSize in MB, which is taken into consideration in regular merges, yet ignored by findMergesForOptimze. I think it'd be good if we take that into consideration even when optimizing. This will allow the caller to specify two constraints: maxNumSegments and maxMergeMB. Obviously both may not be satisfied, and therefore we will guarantee that if there is any segment above the threshold, the threshold constraint takes precedence and therefore you may end up w/ <maxNumSegments (if it's not 1) after optimize. Otherwise, maxNumSegments is taken into consideration.

As part of this change, I plan to change some methods to protected (from private) and members as well. I realized that if one wishes to implement his own LMP extension, he needs to either put it under o.a.l.index or copy some code over to his impl.

I'll attach a patch shortly."
"LUCENE-2048","RFE","IMPROVEMENT","Omit positions but keep termFreq","it would be useful to have an option to discard positional information but still keep the term frequency - currently setOmitTermFreqAndPositions discards both. Even though position-dependent queries wouldn't work in such case, still any other queries would work fine and we would get the right scoring."
"LUCENE-1329","IMPROVEMENT","IMPROVEMENT","Remove synchronization in SegmentReader.isDeleted","Removes SegmentReader.isDeleted synchronization by using a volatile deletedDocs variable on Java 1.5 platforms.  On Java 1.4 platforms synchronization is limited to obtaining the deletedDocs reference."
"LUCENE-1693","IMPROVEMENT","IMPROVEMENT","AttributeSource/TokenStream API improvements","This patch makes the following improvements to AttributeSource and
TokenStream/Filter:

- introduces interfaces for all Attributes. The corresponding
  implementations have the postfix 'Impl', e.g. TermAttribute and
  TermAttributeImpl. AttributeSource now has a factory for creating
  the Attribute instances; the default implementation looks for
  implementing classes with the postfix 'Impl'. Token now implements
  all 6 TokenAttribute interfaces.

- new method added to AttributeSource:
  addAttributeImpl(AttributeImpl). Using reflection it walks up in the
  class hierarchy of the passed in object and finds all interfaces
  that the class or superclasses implement and that extend the
  Attribute interface. It then adds the interface->instance mappings
  to the attribute map for each of the found interfaces.

- removes the set/getUseNewAPI() methods (including the standard
  ones). Instead it is now enough to only implement the new API,
  if one old TokenStream implements still the old API (next()/next(Token)),
  it is wrapped automatically. The delegation path is determined via
  reflection (the patch determines, which of the three methods was
  overridden).

- Token is no longer deprecated, instead it implements all 6 standard
  token interfaces (see above). The wrapper for next() and next(Token)
  uses this, to automatically map all attribute interfaces to one
  TokenWrapper instance (implementing all 6 interfaces), that contains
  a Token instance. next() and next(Token) exchange the inner Token
  instance as needed. For the new incrementToken(), only one
  TokenWrapper instance is visible, delegating to the currect reusable
  Token. This API also preserves custom Token subclasses, that maybe
  created by very special token streams (see example in Backwards-Test).

- AttributeImpl now has a default implementation of toString that uses
  reflection to print out the values of the attributes in a default
  formatting. This makes it a bit easier to implement AttributeImpl,
  because toString() was declared abstract before.

- Cloning is now done much more efficiently in
  captureState. The method figures out which unique AttributeImpl
  instances are contained as values in the attributes map, because
  those are the ones that need to be cloned. It creates a single
  linked list that supports deep cloning (in the inner class
  AttributeSource.State). AttributeSource keeps track of when this
  state changes, i.e. whenever new attributes are added to the
  AttributeSource. Only in that case will captureState recompute the
  state, otherwise it will simply clone the precomputed state and
  return the clone. restoreState(AttributeSource.State) walks the
  linked list and uses the copyTo() method of AttributeImpl to copy
  all values over into the attribute that the source stream
  (e.g. SinkTokenizer) uses. 

- Tee- and SinkTokenizer were deprecated, because they use
Token instances for caching. This is not compatible to the new API
using AttributeSource.State objects. You can still use the old
deprecated ones, but new features provided by new Attribute types
may get lost in the chain. A replacement is a new TeeSinkTokenFilter,
which has a factory to create new Sink instances, that have compatible
attributes. Sink instances created by one Tee can also be added to
another Tee, as long as the attribute implementations are compatible
(it is not possible to add a sink from a tee using one Token instance
to a tee using the six separate attribute impls). In this case UOE is thrown.

The cloning performance can be greatly improved if not multiple
AttributeImpl instances are used in one TokenStream. A user can
e.g. simply add a Token instance to the stream instead of the individual
attributes. Or the user could implement a subclass of AttributeImpl that
implements exactly the Attribute interfaces needed. I think this
should be considered an expert API (addAttributeImpl), as this manual
optimization is only needed if cloning performance is crucial. I ran
some quick performance tests using Tee/Sink tokenizers (which do
cloning) and the performance was roughly 20% faster with the new
API. I'll run some more performance tests and post more numbers then.

Note also that when we add serialization to the Attributes, e.g. for
supporting storing serialized TokenStreams in the index, then the
serialization should benefit even significantly more from the new API
than cloning. 

This issue contains one backwards-compatibility break:
TokenStreams/Filters/Tokenizers should normally be final
(see LUCENE-1753 for the explaination). Some of these core classes are 
not final and so one could override the next() or next(Token) methods.
In this case, the backwards-wrapper would automatically use
incrementToken(), because it is implemented, so the overridden
method is never called. To prevent users from errors not visible
during compilation or testing (the streams just behave wrong),
this patch makes all implementation methods final
(next(), next(Token), incrementToken()), whenever the class
itsself is not final. This is a BW break, but users will clearly see,
that they have done something unsupoorted and should better
create a custom TokenFilter with their additional implementation
(instead of extending a core implementation).

For further changing contrib token streams the following procedere should be used:

    *  rewrite and replace next(Token)/next() implementations by new API
    * if the class is final, no next(Token)/next() methods needed (must be removed!!!)
    * if the class is non-final add the following methods to the class:
{code:java}
      /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should
       * not be overridden. Delegates to the backwards compatibility layer. */
      public final Token next(final Token reusableToken) throws java.io.IOException {
        return super.next(reusableToken);
      }

      /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should
       * not be overridden. Delegates to the backwards compatibility layer. */
      public final Token next() throws java.io.IOException {
        return super.next();
      }
{code}
Also the incrementToken() method must be final in this case
(and the new method end() of LUCENE-1448)
"
"LUCENE-468","DOCUMENTATION","IMPROVEMENT","Searchable.java: The info in the @deprecated tags do not refer to the search(Weight, etc...) versions...","E.g.

The javadoc for 
          void search(Query query, Filter filter, HitCollector results)
states:
          Deprecated. use search(Query, Filter, HitCollector) instead.
instead of:
          Deprecated. use search(Weight, Filter, HitCollector) instead.
"
"LUCENE-3820","IMPROVEMENT","BUG","Wrong trailing index calculation in PatternReplaceCharFilter","Reimplementation of PatternReplaceCharFilter to pass randomized tests (used to throw exceptions previously). Simplified code, dropped boundary characters, full input buffered for pattern matching."
"LUCENE-2860","BUG","BUG","SegmentInfo.sizeInBytes ignore includeDocStore when caching","I noticed that SegmentInfo's sizeInBytes cache is potentially buggy -- it doesn't take into account 'includeDocStores'. I.e., if you call it once w/ 'false' (sizeInBytes won't include the store files) and then with 'true' (or vice versa), you won't get the right sizeInBytes (it won't re-compute, with the store files).

I'll fix and add a test case demonstrating the bug."
"LUCENE-2777","REFACTORING","TASK","Revise PagedBytes#fillUsingLengthPrefix* methods names","PagedBytes has 3 different variants of fillUsingLengthPrefix. We need better names for that since CSFBranch already added a 4th one.


here are some suggestions:

{code}
/** Reads length as 1 or 2 byte vInt prefix, starting @ start */
    public BytesRef fillLengthAndOffset(BytesRef b, long start) 
//    was: public BytesRef fillUsingLengthPrefix(BytesRef b, long start) 


 /** @lucene.internal  Reads length as 1 or 2 byte vInt prefix, starting @ start.  Returns the block number of the term. */
    public int getBlockAndFill(BytesRef b, long start) 
//    was: public BytesRef fillUsingLengthPrefix2(BytesRef b, long start) 

/** @lucene.internal  Reads length as 1 or 2 byte vInt prefix, starting @ start. 
     * Returns the start offset of the next part, suitable as start parameter on next call
     * to sequentially read all BytesRefs. */
    public long getNextOffsetAndFill(BytesRef b, long start) 
//    was: public BytesRef fillUsingLengthPrefix3(BytesRef b, long start) 

{code}"
"LUCENE-229","IMPROVEMENT","IMPROVEMENT","[PATCH] Binary stored fields","Provides a binary Field type that can be used to store byte arrays in the Lucene
index. Can be used for a variety of applications from compressed text storage,
image storage or as a basis for implementing typed storage (e.g: Integers,
Floats, etc.)

Based on discussion from lucene-dev list started here:
http://marc.theaimsgroup.com/?l=lucene-dev&m=108455161204687&w=2

Directly based on design fleshed out here:
http://marc.theaimsgroup.com/?l=lucene-dev&m=108456898230542&w=2

Patch includes updated code and unit tests not included in the patch sent do the
lucene-dev list."
"LUCENE-651","IMPROVEMENT","IMPROVEMENT","Poor performance race condition in FieldCacheImpl","A race condition exists in FieldCacheImpl that causes a significant performance degradation if multiple threads concurrently request a value that is not yet cached. The degradation is particularly noticable in large indexes and when there a many concurent requests for the cached value.

For the full discussion see the mailing list thread 'Poor performance ""race condition"" in FieldSortedHitQueue' (http://www.gossamer-threads.com/lists/lucene/java-user/38717)."
"LUCENE-1624","BUG","IMPROVEMENT","Don't commit an empty segments_N when IW is opened with create=true","If IW is opened with create=true, it forcefully commits an empty
segments_N.  But really it should not: if autoCommit is false, nothing
should be committed until commit or close is explicitly called.

Spinoff from http://www.nabble.com/no-segments*-file-found:-files:-Error-on-opening-index-td23219520.html
"
"LUCENE-2417","IMPROVEMENT","BUG","Fix IndexCommit hashCode() and equals() to be consistent","IndexCommit's impl of hashCode() and equals() is inconsistent. One uses Dir + version and the other uses Dir + equals. According to hashCode()'s javadoc, if o1.equals(o2), then o1.hashCode() == o2.hashCode(). Simple fix, and I'll add a test case."
"LUCENE-2759","BUG","BUG","We should never open an IndexInput when an IndexOutput is still open","I modified MockDirWrapper to assert this (except for
segments_N/segments.gen, where it's expected), and, it uncovered a
couple of places involving NRT readers where we open a shared doc
store file that's still open for writing.

First, if you install a merged segment warmer, we were failing to
force the merge of the doc stores in this case, thus potentially
opening the same doc stores that are also still open for writing.

Second, if you're actively adding docs in other threads when you call
IW.getReader(), the other threads could sneak in and flush new
segments sharing the doc stores.  The returned reader then opens the
doc store files that are still open for writing.
"
"LUCENE-3897","BUG","BUG","KuromojiTokenizer fails with large docs","just shoving largeish random docs triggers asserts like:

{noformat}
    [junit] Caused by: java.lang.AssertionError: backPos=4100 vs lastBackTracePos=5120
    [junit] 	at org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.backtrace(KuromojiTokenizer.java:907)
    [junit] 	at org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.parse(KuromojiTokenizer.java:756)
    [junit] 	at org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.incrementToken(KuromojiTokenizer.java:403)
    [junit] 	at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:404)
{noformat}

But, you get no seed...

I'll commit the test case and @Ignore it."
"LUCENE-2647","REFACTORING","IMPROVEMENT","Move & rename the terms dict, index, abstract postings out of oal.index.codecs.standard","The terms dict components that current live under Standard codec
(oal.index.codecs.standard.*) are in fact very generic, and in no way
particular to the Standard codec.  Already we have many other codecs
(sep, fixed int block, var int block, pulsing, appending) that re-use
the terms dict writer/reader components.

So I'd like to move these out into oal.index.codecs, and rename them:

  * StandardTermsDictWriter/Reader -> PrefixCodedTermsWriter/Reader
  * StandardTermsIndexWriter/Reader -> AbstractTermsIndexWriter/Reader
  * SimpleStandardTermsIndexWriter/Reader -> SimpleTermsIndexWriter/Reader
  * StandardPostingsWriter/Reader -> AbstractPostingsWriter/Reader
  * StandardPostingsWriterImpl/ReaderImple -> StandardPostingsWriter/Reader

With this move we have a nice reusable terms dict impl.  The terms
index impl is still well-decoupled so eg we could [in theory] explore
a variable gap terms index.

Many codecs, I expect, don't need/want to implement their own terms
dict....

There are no code/index format changes here, besides the renaming &
fixing all imports/usages of the renamed class.
"
"LUCENE-232","BUG","BUG","Version 1.3 reports IOException when re-creating an index","Version: Lucene 1.3 final 
Error reported when I am (re-)doing an initialization on the index created 
previously:
java.io.IOException: couldn't delete _26a.f1

The problem disappearred after a re-start of the jvm, some files may be locked 
after the index writer action !
Problem does not appear in Version 1.2."
"LUCENE-1658","REFACTORING","IMPROVEMENT","Absorb NIOFSDirectory into FSDirectory","I think whether one uses java.io.* vs java.nio.* or eventually
java.nio2.*, or some other means, is an under-the-hood implementation
detail of FSDirectory and doesn't merit a whole separate class.

I think FSDirectory should be the core class one uses when one's index
is in the filesystem.

So, I'd like to deprecate NIOFSDirectory, absorbing it into
FSDirectory, and add a setting ""useNIO"" to FSDirectory.  It should
default to ""true"" for non-Windows OSs, because it gives far better
concurrent performance on all platforms but Windows (due to known Sun
JRE issue http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6265734).
"
"LUCENE-2554","TEST","TEST","preflex codec doesn't order terms correctly","The surrogate dance in the preflex codec (which must dynamically remap terms from UTF16 order to unicode code point order) is buggy.

To better test it, I want to add a test-only codec, preflexrw, that is able to write indices in the pre-flex format.  Then we should also fix tests to randomly pick codecs (including preflexrw) so we better test all of our codecs."
"LUCENE-1453","BUG","BUG","When reopen returns a new IndexReader, both IndexReaders may now control the lifecycle of the underlying Directory which is managed by reference counting","Rough summary. Basically, FSDirectory tracks references to FSDirectory and when IndexReader.reopen shares a Directory with a created IndexReader and closeDirectory is true, FSDirectory's ref management will see two decrements for one increment. You can end up getting an AlreadyClosed exception on the Directory when the IndexReader is open.

I have a test I'll put up. A solution seems fairly straightforward (at least in what needs to be accomplished)."
"LUCENE-1923","RFE","","Add toString() or getName() method to IndexReader","It would be very useful for debugging if IndexReader either had a getName() method, or a toString() implementation that would get a string identification for the reader.

for SegmentReader, this would return the same as getSegmentName()
for Directory readers, this would return the ""generation id""?
for MultiReader, this could return something like ""multi(sub reader name, sub reader name, sub reader name, ...)

right now, i have to check instanceof for SegmentReader, then call getSegmentName(), and for all other IndexReader types, i would have to do something like get the IndexCommit and get the generation off it (and this may throw UnsupportedOperationException, at which point i have would have to recursively walk sub readers and try again)

I could work up a patch if others like this idea"
"LUCENE-3712","CLEANUP","TASK","Remove unused (and untested) methods from ReaderUtil that are also veeeeery ineffective","ReaderUtil contains two methods that are nowhere used and not even tested. Additionally those are implemented with useless List->array copying; ineffective docStart calculation for a binary search later instead directly returning the reader while scanning -- and I am not sure if they really work as expected. As ReaderUtil is @lucene.internal we should remove them in 3.x and trunk, alternatively the useless array copy / docStarts handling should be removed and tests added:

{code:java}
public static IndexReader subReader(int doc, IndexReader reader)
public static IndexReader subReader(IndexReader reader, int subIndex)
{code}
"
"LUCENE-2303","CLEANUP","","Remove code duplication from Token class, just extend TermAttributeImpl","This issue removes the code duplication from Token, as it shares the whole char[] buffer handling code with TermAttributeImpl. This issue removes this duplication by just extending TermAttributeImpl.

When the parent issue LUCENE-2302 will extend TermAttribute to support CharSequence and Appendable and also the new BytesRefAttribute gets added, Token will automatically provide this too, so no further code duplication.

This code should also be committed to trunk, as it has nothing to do with flex."
"LUCENE-1312","BUG","BUG","InstantiatedIndexReader does not implement getFieldNames properly","Causes error in org.apache.lucene.index.SegmentMerger.mergeFields"
"LUCENE-964","CLEANUP","IMPROVEMENT","Remove DocumentWriter","DocumentWriter has been replaced by DocumentsWriter (from LUCENE-843) so we need to remove it & fix the unit tests that directly use it..."
"LUCENE-2166","BUG","BUG","If you hit the ""max term prefix"" warning when indexing, it never goes away","Silly bug.

If IW's infoStream is on, we warn whenever we hit a ridiculously long term (> 16 KB in length).  The problem is, we never reset this warning, so, once one doc contains such a massive term, we then keep warning over and over about that same term for future docs."
"LUCENE-2637","BUG","BUG","FSDirectory.copyBytes isn't safe for SimpleFSDirectory","the copyBytes optimization from LUCENE-2574 is not safe for SimpleFSDirectory, but works fine for NIOFSDirectory.

With SimpleFSDirectory, the copyBytes optimization causes index corruption.

see http://www.lucidimagination.com/search/document/36d2dbfc691909d5/bug_triggered_by_testindexwriter_testrandomstoredfields for background

here are my steps to reproduce (most of the time, at least on windows):
{noformat}
1. edit line 87 of TestIndexWriter to plugin the seed:
    random = newRandom(3312389322103990899L);
2. edit line 5138 of TestIndexWriter to force SimpleFSDirectory:
    Directory dir = new SimpleFSDirectory(index);
3. run this command:
    ant clean test-core -Dtestcase=TestIndexWriter
-Dtestmethod=testRandomStoredFields -Dtests.iter=10
-Dtests.codec=""MockVariableIntBlock(29)""
{noformat}
"
"LUCENE-1425","RFE","RFE","Add ConstantScore highlighting support to SpanScorer","Its actually easy enough to support the family of constantscore queries with the new SpanScorer. This will also remove the requirement that you rewrite queries against the main index before highlighting (in fact, if you do, the constantscore queries will not highlight)."
"LUCENE-452","DESIGN_DEFECT","IMPROVEMENT","PrefixQuery is missing the equals() method","The PrefixQuery is inheriting the java.lang.Object's object default equals method. This makes it hard to have test working of PrefixFilter or any other task requiring equals to work proerply (insertion in Set, etc.). The equal method should be very similar, not to say identical except for class casting, to the equals() of TermQuery. "
"LUCENE-3357","TEST","","Unit and integration test cases for the new Similarities","Write test cases to test the new Similarities added in [LUCENE-3220|https://issues.apache.org/jira/browse/LUCENE-3220]. Two types of test cases will be created:
 * unit tests, in which mock statistics are provided to the Similarities and the score is validated against hand calculations;
 * integration tests, in which a small collection is indexed and then searched using the Similarities.

Performance tests will be performed in a separate issue."
"LUCENE-1569","BUG","BUG","IndexReader.clone can leave files open","I hit this in working on LUCENE-1516.

When not using compound file format, if you clone an IndexReader, then close the original, then close the clone, the stored fields files (_X.fdt, _X.fdx) remain incorrectly open.

I have a test showing it; fix is trivial.  Will post patch & commit shortly."
"LUCENE-247","IMPROVEMENT","IMPROVEMENT","Confusing code line","Line 81 of TermScorer:

      if (!(target > docs[pointer])) {

Could be replaced with the more readable:

      if (docs[pointer] >= target) {

Sorry for nit picking!"
"LUCENE-1817","DOCUMENTATION","BUG","it is impossible to use a custom dictionary for SmartChineseAnalyzer","it is not possible to use a custom dictionary, even though there is a lot of code and javadocs to allow this.

This is because the custom dictionary is only loaded if it cannot load the built-in one (which is of course, in the jar file and should load)
{code}
public synchronized static WordDictionary getInstance() {
    if (singleInstance == null) {
      singleInstance = new WordDictionary(); // load from jar file
      try {
        singleInstance.load();
      } catch (IOException e) { // loading from jar file must fail before it checks the AnalyzerProfile (where this can be configured)
        String wordDictRoot = AnalyzerProfile.ANALYSIS_DATA_DIR;
        singleInstance.load(wordDictRoot);
      } catch (ClassNotFoundException e) {
        throw new RuntimeException(e);
      }
    }
    return singleInstance;
  }
{code}

I think we should either correct this, document this, or disable custom dictionary support..."
"LUCENE-3721","BUG","BUG","CharFilters not being invoked in Solr","
On Solr trunk, *all* CharFilters have been non-functional since LUCENE-3396 was committed in r1175297 on 25 Sept 2011, until Yonik's fix today in r1235810; Solr 3.x was not affected - CharFilters have been working there all along."
"LUCENE-1723","BUG","BUG","KeywordTokenizer does not properly set the end offset","KeywordTokenizer sets the Token's term length attribute but appears to omit the end offset. The issue was discovered while using a highlighter with the KeywordAnalyzer. KeywordAnalyzer delegates to KeywordTokenizer propagating the bug. 

Below is a JUnit test (source is also attached) that exercises various analyzers via a Highlighter instance. Every analyzer but the KeywordAnazlyzer successfully wraps the text with the highlight tags, such as ""<b>thetext</b>"". When using KeywordAnalyzer the tags appear before the text, for example: ""<b></b>thetext"". 

Please note NewKeywordAnalyzer and NewKeywordTokenizer classes below. When using NewKeywordAnalyzer the tags are properly placed around the text. The NewKeywordTokenizer overrides the next method of the KeywordTokenizer setting the end offset for the returned Token. NewKeywordAnalyzer utilizes KeywordTokenizer to produce proper token.

Unless there is an objection I will gladly post a patch in the very near future . 

-----------------------------
package lucene;

import java.io.IOException;
import java.io.Reader;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.KeywordAnalyzer;
import org.apache.lucene.analysis.KeywordTokenizer;
import org.apache.lucene.analysis.SimpleAnalyzer;
import org.apache.lucene.analysis.StopAnalyzer;
import org.apache.lucene.analysis.Token;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.Tokenizer;
import org.apache.lucene.analysis.WhitespaceAnalyzer;
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.search.highlight.Highlighter;
import org.apache.lucene.search.highlight.QueryScorer;
import org.apache.lucene.search.highlight.SimpleHTMLFormatter;
import org.apache.lucene.search.highlight.WeightedTerm;
import org.junit.Test;
import static org.junit.Assert.*;

public class AnalyzerBug {

	@Test
	public void testWithHighlighting() throws IOException {
		String text = ""thetext"";
		WeightedTerm[] terms = { new WeightedTerm(1.0f, text) };

		Highlighter highlighter = new Highlighter(new SimpleHTMLFormatter(
				""<b>"", ""</b>""), new QueryScorer(terms));

		Analyzer[] analazers = { new StandardAnalyzer(), new SimpleAnalyzer(),
				new StopAnalyzer(), new WhitespaceAnalyzer(),
				new NewKeywordAnalyzer(), new KeywordAnalyzer() };

		// Analyzers pass except KeywordAnalyzer
		for (Analyzer analazer : analazers) {
			String highighted = highlighter.getBestFragment(analazer,
					""CONTENT"", text);
			assertEquals(""Failed for "" + analazer.getClass().getName(), ""<b>""
					+ text + ""</b>"", highighted);
			System.out.println(analazer.getClass().getName()
					+ "" passed, value highlighted: "" + highighted);
		}
	}
}

class NewKeywordAnalyzer extends KeywordAnalyzer {

	@Override
	public TokenStream reusableTokenStream(String fieldName, Reader reader)
			throws IOException {
		Tokenizer tokenizer = (Tokenizer) getPreviousTokenStream();
		if (tokenizer == null) {
			tokenizer = new NewKeywordTokenizer(reader);
			setPreviousTokenStream(tokenizer);
		} else
			tokenizer.reset(reader);
		return tokenizer;
	}

	@Override
	public TokenStream tokenStream(String fieldName, Reader reader) {
		return new NewKeywordTokenizer(reader);
	}
}

class NewKeywordTokenizer extends KeywordTokenizer {
	public NewKeywordTokenizer(Reader input) {
		super(input);
	}

	@Override
	public Token next(Token t) throws IOException {
		Token result = super.next(t);
		if (result != null) {
			result.setEndOffset(result.termLength());
		}
		return result;
	}
}
"
"LUCENE-1625","RFE","BUG","openReaderPassed not populated in CheckIndex.Status.SegmentInfoStatus","When using CheckIndex programatically, the openReaderPassed flag on the SegmentInfoStatus is never populated (so it always comes back false)

looking at the code, its clear that openReaderPassed is defined, but never used

furthermore, it appears that not all information that is propagated to the ""InfoStream"" is available via SegmentIinfoStatus

All of the following information should be able to be gather from public properties on the SegmentInfoStatus:
test: open reader.........OK
test: fields, norms.......OK [2 fields]
test: terms, freq, prox...OK [101 terms; 133 terms/docs pairs; 133 tokens]
test: stored fields.......OK [100 total field count; avg 1 fields per doc]
test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
"
"LUCENE-2813","BUG","BUG","TestIndexWriterDelete fails randomly","10 out of 9 runs with that see fail on my trunk:

ant test-core -Dtestcase=TestIndexWriterDelete -Dtestmethod=testErrorAfterApplyDeletes -Dtests.seed=4269712067829708991:1888184886355172227 -Dtests.codec=randomPerField


with this result:

{code}

junit-sequential:
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterDelete
    [junit] Tests run: 1, Failures: 2, Errors: 0, Time elapsed: 1.725 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterDelete -Dtestmethod=testErrorAfterApplyDeletes -Dtests.seed=4269712067829708991:1888184886355172227 -Dtests.codec=randomPerField
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=Standard, contents=SimpleText, city=MockSep}, locale=ar_QA, timezone=VST
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestIndexWriterDelete]
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testErrorAfterApplyDeletes(org.apache.lucene.index.TestIndexWriterDelete):	FAILED
    [junit] 
    [junit] junit.framework.AssertionFailedError: 
    [junit] 	at org.apache.lucene.index.TestIndexWriterDelete.testErrorAfterApplyDeletes(TestIndexWriterDelete.java:736)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1043)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:981)
    [junit] 
    [junit] 
    [junit] Testcase: testErrorAfterApplyDeletes(org.apache.lucene.index.TestIndexWriterDelete):	FAILED
    [junit] ConcurrentMergeScheduler hit unhandled exceptions
    [junit] junit.framework.AssertionFailedError: ConcurrentMergeScheduler hit unhandled exceptions
    [junit] 	at org.apache.lucene.util.LuceneTestCase.tearDown(LuceneTestCase.java:503)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1043)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:981)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestIndexWriterDelete FAILED
{code}"
"LUCENE-2787","IMPROVEMENT","IMPROVEMENT","disable atime for DirectIOLinuxDirectory","In Linux's open():
O_NOATIME
    (Since Linux 2.6.8) Do not update the file last access time (st_atime in the inode) when the file is read(2). This flag is intended for use by indexing or backup programs, where its use can significantly reduce the amount of disk activity. This flag may not be effective on all filesystems. One example is NFS, where the server maintains the access time.

So we should do this in our linux-specific DirectIOLinuxDirectory.

Separately (offtopic), it would be better if this was a LinuxDirectory that only uses O_DIRECT when it should :)
It would be nice to think about an optional modules/native for common platforms similar to what tomcat provides
Its easier to test directories like this now (-Dtests.directory)...
"
"LUCENE-3476","BUG","BUG","SearcherManager misses to close IR if manager is closed during reopen","if we close SM while there is a thread calling maybReopen() and swapSearcher throws already closed exception we miss to close the searcher / reader."
"LUCENE-2884","BUG","BUG","StandardCodec sometimes supplies skip pointers past EOF","Pretty sure this is 4.0-only:
I added an assertion, the test to reproduce is:

ant test-core -Dtestcase=TestPayloadNearQuery -Dtestmethod=testMinFunction -Dtests.seed=4841190615781133892:3888521539169738727 -Dtests.multiplier=3

{noformat}
    [junit] Testcase: testMinFunction(org.apache.lucene.search.payloads.TestPayloadNearQuery):  FAILED
    [junit] invalid skip pointer: 404, length=337
    [junit] junit.framework.AssertionFailedError: invalid skip pointer: 404, length=337
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1127)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1059)
    [junit]     at org.apache.lucene.index.codecs.MultiLevelSkipListReader.init(MultiLevelSkipListReader.java:176)
    [junit]     at org.apache.lucene.index.codecs.standard.DefaultSkipListReader.init(DefaultSkipListReader.java:50)
    [junit]     at org.apache.lucene.index.codecs.standard.StandardPostingsReader$SegmentDocsAndPositionsAndPayloadsEnum.advance(StandardPostingsReader.java:742)
    [junit]     at org.apache.lucene.search.spans.TermSpans.skipTo(TermSpans.java:72)
{noformat}"
"LUCENE-1257","OTHER","IMPROVEMENT","Port to Java5","For my needs I've updated Lucene so that it uses Java 5 constructs. I know Java 5 migration had been planned for 2.1 someday in the past, but don't know when it is planned now. This patch against the trunk includes :

- most obvious generics usage (there are tons of usages of sets, ... Those which are commonly used have been generified)
- PriorityQueue generification
- replacement of indexed for loops with for each constructs
- removal of unnececessary unboxing

The code is to my opinion much more readable with those features (you actually *know* what is stored in collections reading the code, without the need to lookup for field definitions everytime) and it simplifies many algorithms.

Note that this patch also includes an interface for the Query class. This has been done for my company's needs for building custom Query classes which add some behaviour to the base Lucene queries. It prevents multiple unnnecessary casts. I know this introduction is not wanted by the team, but it really makes our developments easier to maintain. If you don't want to use this, replace all /Queriable/ calls with standard /Query/.

"
"LUCENE-2068","BUG","BUG","fix reverseStringFilter for unicode 4.0","ReverseStringFilter is not aware of supplementary characters: when it reverses it will create unpaired surrogates, which will be replaced by U+FFFD by the indexer (but not at query time).
The wrong words will conflate to each other, and the right words won't match, basically the whole thing falls apart.

This patch implements in-place reverse with the algorithm from apache harmony AbstractStringBuilder.reverse0()
"
"LUCENE-2747","CLEANUP","IMPROVEMENT","Deprecate/remove language-specific tokenizers in favor of StandardTokenizer","As of Lucene 3.1, StandardTokenizer implements UAX#29 word boundary rules to provide language-neutral tokenization.  Lucene contains several language-specific tokenizers that should be replaced by UAX#29-based StandardTokenizer (deprecated in 3.1 and removed in 4.0).  The language-specific *analyzers*, by contrast, should remain, because they contain language-specific post-tokenization filters.  The language-specific analyzers should switch to StandardTokenizer in 3.1.

Some usages of language-specific tokenizers will need additional work beyond just replacing the tokenizer in the language-specific analyzer.  

For example, PersianAnalyzer currently uses ArabicLetterTokenizer, and depends on the fact that this tokenizer breaks tokens on the ZWNJ character (zero-width non-joiner; U+200C), but in the UAX#29 word boundary rules, ZWNJ is not a word boundary.  Robert Muir has suggested using a char filter converting ZWNJ to spaces prior to StandardTokenizer in the converted PersianAnalyzer."
"LUCENE-2933","IMPROVEMENT","IMPROVEMENT","Two-stage state expansion for the FST: distance-from-root and child-count criteria.","In the current implementation FST states are expanded into a binary search on labels (from a vector of transitions) when the child count of a state exceeds a given predefined threshold (NUM_ARCS_FIXED_ARRAY). This threshold affects automaton size and traversal speed (as it turns out when benchmarked). For some degenerate (?) data sets, close-to-the-root nodes could have a small number of children (below the threshold) and yet be traversed on every single seek.

A fix of this is to introduce two control thresholds: 
  EXPAND state if (distance-from-root <= MIN_DISTANCE || children-count >= NUM_ARCS_FIXED_ARRAY)

My plan is to create a data set that will prove this first and then to implement the workaround above."
"LUCENE-3680","CLEANUP","TASK","exception consistency in o.a.l.store","just some minor improvements:
* always use EOFException when its eof
* always include the inputstream too so we know filename etc
* use FileNotFoundException consistently in CFS when a sub-file is not found
"
"LUCENE-3728","IMPROVEMENT","IMPROVEMENT","better handling of files inside/outside CFS by codec","Since norms and deletes were moved under Codec (LUCENE-3606, LUCENE-3661),
we never really properly addressed the issue of how Codec.files() should work,
considering these files are always stored outside of CFS.

LUCENE-3606 added a hack, LUCENE-3661 cleaned up the hack a little bit more,
but its still a hack.

Currently the logic in SegmentInfo.files() is:
{code}
clearCache()

if (compoundFile) {
  // don't call Codec.files(), hardcoded CFS extensions, etc
} else {
  Codec.files()
}

// always add files stored outside CFS regardless of CFS setting
Codec.separateFiles()

if (sharedDocStores) {
  // hardcoded shared doc store extensions, etc
}
{code}

Also various codec methods take a Directory parameter, but its inconsistent
what this Directory is in the case of CFS: for some parts of the index its
the CFS directory, for others (deletes, separate norms) its not.

I wonder if instead we could restructure this so that SegmentInfo.files() logic is:
{code}
clearCache()
Codec.files()
{code}

and so that Codec is instead responsible.

instead Codec.files logic by default would do the if (compoundFile) thing, and
Lucene3x codec itself would only have the if (sharedDocStores) thing, and any
part of the codec that wants to put stuff always outside of CFS (e.g. Lucene3x separate norms, deletes) 
could just use SegmentInfo.dir. Directory parameters in the case of CFS would always
consistently be the CFSDirectory.

I haven't totally tested if this will work but there is definitely some cleanups 
we can do either way, and I think it would be a good step to try to clean this up
and simplify it.
"
"LUCENE-3682","CLEANUP","TASK","Add deprecated 'transition' api for Document/Field","I think for 4.0 we should have a deprecated transition api for Field so you can do new Field(..., Field.Store.xxx, Field.Index.yyy) like before.

These combinations would just be some predefined fieldtypes that are used behind the scenes if you use these deprecated ctors

Sure it wouldn't be 'totally' backwards binary compat for Field.java, but why must it be all or nothing? I think this would eliminate a big
hurdle for people that want to check out 4.x"
"LUCENE-250","DOCUMENTATION","IMPROVEMENT","Javadocs for Scorer.java and TermScorer.java","Javadocs for Scorer.java and TermScorer.java 
Also changed build.xml to use package access for the 
javadocs target. That caused some minor error javadoc messages 
in CompoundFileReader.java and FieldInfos.java, which are also fixed. 
 
The patch posted earlier for Weight.java 
(a broken javadoc link) is also included. 
 
The attached patch is for all 5 files against the CVS top directory 
of 28 July 2004. The only dependency is that package access 
is needed for TermScorer.java. 
 
This might be changed by declaring TermScorer as a public class, 
but I preferred to use javadoc package access in build.xml 
over changing java code. 
 
Using package access for javadocs shows some more undocumented 
classes, eg. in the doc page of the search package. This might 
encourage folks to write more javadocs... 
 
Regards, 
Paul"
"LUCENE-845","BUG","BUG","If you ""flush by RAM usage"" then IndexWriter may over-merge","I think a good way to maximize performance of Lucene's indexing for a
given amount of RAM is to flush (writer.flush()) the added documents
whenever the RAM usage (writer.ramSizeInBytes()) has crossed the max
RAM you can afford.

But, this can confuse the merge policy and cause over-merging, unless
you set maxBufferedDocs properly.

This is because the merge policy looks at the current maxBufferedDocs
to figure out which segments are level 0 (first flushed) or level 1
(merged from <mergeFactor> level 0 segments).

I'm not sure how to fix this.  Maybe we can look at net size (bytes)
of a segment and ""infer"" level from this?  Still we would have to be
resilient to the application suddenly increasing the RAM allowed.

The good news is to workaround this bug I think you just need to
ensure that your maxBufferedDocs is less than mergeFactor *
typical-number-of-docs-flushed.
"
"LUCENE-3596","RFE","IMPROVEMENT","DirectoryTaxonomyWriter extensions should be able to set internal index writer config attributes such as info stream","Current protected openIndexWriter(Directory directory, OpenMode openMode) does not provide access to the IWC it creates.
So extensions must reimplement this method completely in order to set e.f. info stream for the internal index writer.
This came up in [user question: Taxonomy indexer debug |http://lucene.472066.n3.nabble.com/Taxonomy-indexer-debug-td3533341.html]"
"LUCENE-947","IMPROVEMENT","IMPROVEMENT","Some improvements to contrib/benchmark","I've made some small improvements to the contrib/benchmark, mostly
merging in the ad-hoc benchmarking code I've been using in LUCENE-843:

  - Fixed thread safety of DirDocMaker's usage of SimpleDateFormat

  - Print the props in sorted order

  - Added new config ""autocommit=true|false"" to CreateIndexTask

  - Added new config ""ram.flush.mb=int"" to AddDocTask

  - Added new configs ""doc.term.vector.positions=true|false"" and
    ""doc.term.vector.offsets=true|false"" to BasicDocMaker

  - Added WriteLineDocTask.java, so you can make an alg that uses this
    to build up a single file containing one document per line in a
    single file.  EG this alg converts the reuters-out tree into a
    single file that has ~1000 bytes per body field, saved to
    work/reuters.1000.txt:

      docs.dir=reuters-out
      doc.maker=org.apache.lucene.benchmark.byTask.feeds.DirDocMaker
      line.file.out=work/reuters.1000.txt
      doc.maker.forever=false
      {WriteLineDoc(1000)}: *

    Each line has tab-separted TITLE, DATE, BODY fields.

  - Created feeds/LineDocMaker.java that creates documents read from
    the file created by WriteLineDocTask.java.  EG this alg indexes
    all documents created above:

      analyzer=org.apache.lucene.analysis.SimpleAnalyzer
      directory=FSDirectory
      doc.add.log.step=500

      docs.file=work/reuters.1000.txt
      doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker
      doc.tokenized=true
      doc.maker.forever=false

      ResetSystemErase
      CreateIndex
      {AddDoc}: *
      CloseIndex

      RepSumByPref AddDoc

I'll attach initial patch shortly.
"
"LUCENE-750","IMPROVEMENT","IMPROVEMENT","don't use finalizers for FSIndexInput clones","finalizers are expensive, and we should avoid using them where possible.
It looks like this helped to tickle some kind of bug (looks like a JVM bug?)
http://www.nabble.com/15-minute-hang-in-IndexInput.clone%28%29-involving-finalizers-tf2826906.html#a7891015"
"LUCENE-3417","BUG","BUG","DictionaryCompoundWordTokenFilter does not properly add tokens from the end compound word.","Due to an off-by-one error, a subword placed at the end of a compound word will not get a token added to the token stream.


For example (from the unit test in the attached patch):
Dictionary: {""ab"", ""cd"", ""ef""}
Input: ""abcdef""
Created tokens: {""abcdef"", ""ab"", ""cd""}
Expected tokens: {""abcdef"", ""ab"", ""cd"", ""ef""}


Additionally, it could produce tokens that were shorter than the minSubwordSize due to another off-by-one error. For example (again, from the attached patch):


Dictionary: {""abc"", ""d"", ""efg""}
Minimum subword length: 2
Input: ""abcdefg""
Created tokens: {""abcdef"", ""abc"", ""d"", ""efg""}
Expected tokens: {""abcdef"", ""abc"", ""efg""}
"
"LUCENE-587","BUG","BUG","Explanation.toHtml outputs invalid HTML","If you want an HTML representation of an Explanation, you might call the toHtml() method.  However, the output of this method looks like the following:

<ul>
  <li>some value = some description</li>
  <ul>
    <li>some nested value = some description</li>
  </ul>
</ul>

As it is illegal in HTML to nest a UL directly inside a UL, this method will always output unparseable HTML if there are nested explanations.

What Lucene probably means to output is the following, which is valid HTML:

<ul>
  <li>some value = some description
    <ul>
      <li>some nested value = some description</li>
    </ul>
  </li>
</ul>
"
"LUCENE-3432","BUG","BUG","TieredMergePolicy expungeDeletes should not enforce maxMergedSegmentMB",""
"LUCENE-1117","BUG","BUG","Intermittent thread safety issue with EnwikiDocMaker","Intermittent thread safety issue with EnwikiDocMaker

When I run the conf/wikipediaOneRound.alg, sometimes it gets started
OK, other times (about 1/3rd the time) I see this:

     Exception in thread ""Thread-0"" java.lang.RuntimeException: java.io.IOException: Bad file descriptor
     	at org.apache.lucene.benchmark.byTask.feeds.EnwikiDocMaker$Parser.run(EnwikiDocMaker.java:76)
     	at java.lang.Thread.run(Thread.java:595)
     Caused by: java.io.IOException: Bad file descriptor
     	at java.io.FileInputStream.readBytes(Native Method)
     	at java.io.FileInputStream.read(FileInputStream.java:194)
     	at org.apache.xerces.impl.XMLEntityManager$RewindableInputStream.read(Unknown Source)
     	at org.apache.xerces.impl.io.UTF8Reader.read(Unknown Source)
     	at org.apache.xerces.impl.XMLEntityScanner.load(Unknown Source)
     	at org.apache.xerces.impl.XMLEntityScanner.scanQName(Unknown Source)
     	at org.apache.xerces.impl.XMLNSDocumentScannerImpl.scanStartElement(Unknown Source)
     	at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown Source)
     	at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)
     	at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
     	at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
     	at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)
     	at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)
     	at org.apache.lucene.benchmark.byTask.feeds.EnwikiDocMaker$Parser.run(EnwikiDocMaker.java:60)
     	... 1 more

The problem is that the thread that pulls the XML docs is started as
soon as EnwikiDocMaker class is instantiated.  When it's started, it
uses the fileIS (FileInputStream) to feed the XML Parser.  But,
openFile is actually called twice on starting the alg, if you use any
task deriving from ResetInputsTask, which closes the original fileIS
that the XML parser may be using.

I changed the thread to instead start on-demand the first time next()
is called.  I also removed a redundant resetInputs() call (which was
opening the file more frequently than needed).  Finally, I added logic
in the thread to detect that the input stream was closed (because
LineDocMaker.resetInputs() was called, eg, if we are not running the
doc maker to exhaustion).

"
"LUCENE-2734","RFE","IMPROVEMENT","Use IndexWriterConfig in benchmark","We should use IndexWriterConfig instead of deprecated methods in benchmark. "
"LUCENE-3458","RFE","TASK","Change BooleanFilter to have only a single clauses ArrayList (so toString() works fine, clauses() method could be added) so it behaves more lik BooleanQuery","This is unrelated to the other BF changes, but should be done"
"LUCENE-1113","DOCUMENTATION","BUG","fix for Document.getBoost() documentation","The attached patch fixes the javadoc to make clear that getBoost() will never return a useful value in most cases. I will commit this unless someone has a better wording or a real fix."
"LUCENE-796","DESIGN_DEFECT","IMPROVEMENT","Change Visibility of fields[] in MultiFieldQueryParser","In MultiFieldQueryParser the two methods 

  protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException
  protected Query getWildcardQuery(String field, String termStr) throws ParseException

are intended to be overwritten if one would like to avoid fuzzy and wildcard queries. However, the String[] fields attribute of this class is private and hence it is not accessible in subclasses of MFQParser. If you just change it to protected this issue should be solved."
"LUCENE-594","DOCUMENTATION","IMPROVEMENT","Javadoc - Field constructor with Reader needs comment about retained reference","If you don't dig into the Lucene internals, it isn't obvious the Field constructor http://lucene.apache.org/java/docs/api/org/apache/lucene/document/Field.html#Field%28java.lang.String,%20java.io.Reader%29 retains a reference to the reader for use later on. It would be useful to have a comment added to the Javadoc saying something like:

Note: A reference to java.io.Reader is retained by the field. Reader is read from when the Document which this field is added to is itself added to the index.

Without this, the caller is liable to do silly things like closing the stream after constructing the org.apache.lucene.document.Field."
"LUCENE-2265","IMPROVEMENT","IMPROVEMENT","improve automaton performance by running on byte[]","Currently, when enumerating terms, automaton must convert entire terms from flex's native utf-8 byte[] to char[] first, then step each char thru the state machine.

we can make this more efficient, by allowing the state machine to run on byte[], so it can return true/false faster."
"LUCENE-1180","BUG","BUG","Syns2Index fails","Running Syns2Index fails with a
java.lang.IllegalArgumentException: maxBufferedDocs must at least be 2 when enabled exception.
at org.apache.lucene.index.IndexWriter.setMaxBufferedDocs(IndexWriter.java:883)
at org.apache.lucene.wordnet.Syns2Index.index(Syns2Index.java:249)
at org.apache.lucene.wordnet.Syns2Index.main(Syns2Index.java:208)

The code is here
		// blindly up these parameters for speed
		writer.setMergeFactor( writer.getMergeFactor() * 2);
		writer.setMaxBufferedDocs( writer.getMaxBufferedDocs() * 2);

It looks like getMaxBufferedDocs used to return 10, and now it returns -1, not sure when that started happening.

My suggestion would be to just remove these three lines.  Since speed has already improved vastly, there isn't a need to speed things up.

To run this, Syns2Index requires two args.  The first is the location of the wn_s.pl file, and the second is the directory to create the index in."
"LUCENE-3368","BUG","BUG","IndexWriter commits update documents without corresponding delete","while backporting the testcase from LUCENE-3348 I ran into this thread hazard in the 3.x branch. We actually fixed this issue in LUCENE-3348 for Lucene 4.0 but since DWPT has a slightly different behavior when committing segments I create a new issue to track this down in 3.x. when we prepare a commit we sync on IW flush the DW and apply all deletes then release the lock, maybeMerge and start the commit (IW#startCommit(userdata)). Yet, a new segment could be flushed via getReader and sneak into the SegementInfos which are cloned in IW#startCommit instead of in prepareCommit right after the flush. "
"LUCENE-880","BUG","BUG","DocumentWriter closes TokenStreams too early","The DocumentWriter closes a TokenStream as soon as it has consumed its tokens. The javadoc of TokenStream.close() says that it releases resources associated with the stream. However, the DocumentWriter keeps references of the resources (i. e. payload byte arrays, term strings) until it writes the postings to the new segment, which means that DocumentWriter should call TokenStream.close() after it has written the postings.

This problem occurs in multithreaded applications where e. g. pooling is used for the resources. My patch adds a new test to TestPayloads which shows this problem. Multiple threads add documents with payloads to an index and use a pool of byte arrays for the payloads. TokenStream.close() puts the byte arrays back into the pool. The test fails with the old version but runs successfully with the patched version. 

All other units tests pass as well.
"
"LUCENE-393","BUG","BUG","Inconsistent scoring with SpanTermQuery in BooleanQuery","When a SpanTermQuery is added to a BooleanQuery, incorrect results are 
returned.

I am running Lucene 1.9 RC1 on Windows XP.  I have a test case which has 
several tests.  It has an index with 4 identical documents in it.

When two TermQuerys are used in a BooleanQuery, the score looks like this:
  4 hits for search: two term queries
    ID:1 (score:0.54932046)
    ID:2 (score:0.54932046)
    ID:3 (score:0.54932046)
    ID:4 (score:0.54932046)

Notice how it is correctly setting the score to be the same for each document.

When two SpanQuerys are used in a BooleanQuery, the score looks like this:
  2 hits for search: two span queries
    ID:1 (score:0.3884282)
    ID:4 (score:0.1942141)

Notice how it only returned two documents instead of four.  And the two it did 
return have differing scores.

I believe that there is an error in the scoring algorithm that is making the 
other two documents not show up."
"LUCENE-550","RFE","RFE","InstantiatedIndex - faster but memory consuming index","Represented as a coupled graph of class instances, this all-in-memory index store implementation delivers search results up to a 100 times faster than the file-centric RAMDirectory at the cost of greater RAM consumption.

Performance seems to be a little bit better than log2n (binary search). No real data on that, just my eyes.

Populated with a single document InstantiatedIndex is almost, but not quite, as fast as MemoryIndex.    

At 20,000 document 10-50 characters long InstantiatedIndex outperforms RAMDirectory some 30x,
15x at 100 documents of 2000 charachters length,
and is linear to RAMDirectory at 10,000 documents of 2000 characters length.

Mileage may vary depending on term saturation.


"
"LUCENE-1953","BUG","BUG","FastVectorHighlighter: small fragCharSize can cause StringIndexOutOfBoundsException ","If fragCharSize is smaller than Query string, StringIndexOutOfBoundsException is thrown."
"LUCENE-1176","BUG","BUG","TermVectors corruption case when autoCommit=false","I took Yonik's awesome test case (TestStressIndexing2) and extended it to also compare term vectors, and, it's failing.

I still need to track down why, but it seems likely a separate issue."
"LUCENE-3508","BUG","BUG","Decompounders based on CompoundWordTokenFilterBase cannot be used with custom attributes","The CompoundWordTokenFilterBase.setToken method will call clearAttributes() and then will reset only the default Token attributes (term, position, flags, etc) resulting in any custom attributes losing their value. Commenting out clearAttributes() seems to do the trick, but will fail the TestCompoundWordTokenFilter tests.."
"LUCENE-1592","DOCUMENTATION","IMPROVEMENT","fix or deprecate TermsEnum.skipTo","This method is a trap: it looks legitimate but it has hideously poor performance (simple linear scan implemented in the TermsEnum base class since none of the concrete impls override it with a more efficient implementation).

The least we should do for 2.9 is deprecate the method with  a strong warning about its performance.

See here for background: http://www.lucidimagination.com/search/document/77dc4f8e893d3cf3/possible_terminfosreader_speedup

And, here for historical context: 

http://www.lucidimagination.com/search/document/88f1b95b404ebf16/remove_termenum_skipto_term_target"
"LUCENE-1529","BUILD_SYSTEM","RFE","back-compat tests (""ant test-tag"") should test JAR drop-in-ability","
We now test back-compat with ""ant test-tag"", which is very useful for
catching breaks in back compat before committing.

However, that currently checks out ""src/test"" sources and then
compiles them against the trunk JAR, and runs the tests.  Whereas our
back compat policy:

  http://wiki.apache.org/lucene-java/BackwardsCompatibility

states that no recompilation is required on upgrading to a new JAR.
Ie you should be able to drop in the new JAR in place of your old one
and things should work fine.

So... we should fix ""ant test-tag"" to:

  * Do full checkout of core sources & tests from the back-compat-tag

  * Compile the JAR from the back-compat sources

  * Compile the tests against that back-compat JAR

  * Swap in the trunk JAR

  * Run the tests

"
"LUCENE-2247","IMPROVEMENT","IMPROVEMENT","Add CharArrayMap to lucene and make CharAraySet an proxy on the keySet() of it","This patch adds a CharArrayMap<V> to Lucene's analysis package as compagnon of CharArraySet. It supports fast retrieval of char[] keys like CharArraySet does. This is important for some stemmers and other places in Lucene.

Stemers generally use CharArrayMap<String>, which has then get(char[]) returning String. Strings are compact and can be easily copied into termBuffer. A Map<String,String> would be slow as the termBuffer would be first converted to String, then looked up. The return value as String is perfectly legal, as it can be copied easily into termBuffer.

This class borrows lots of code from Solr's pendant, but has additional features and more consistent API according to CharArraySet. The key is always <?>, because as of CharArraySet, anything that has a toString() representation can be used as key (of course with overhead). It also defines a unmodifiable map and correct iterators (returning the native char[]).

CharArraySet was made consistent and now returns for matchVersion>=3.1 also an iterator on char[]. CharArraySet's code was almost completely copied to CharArrayMap and removed in the Set. CharArraySet is now a simple proxy on the keySet().

In future we can think of making CharArraySet/CharArrayMap/CharArrayCollection an interface so the whole API would be more consistent to the Java collections API. But this would be a backwards break. But it would be possible to use better impl instead of hashing (like prefix trees)."
"LUCENE-1653","IMPROVEMENT","IMPROVEMENT","Change DateTools to not create a Calendar in every call to dateToString or timeToString","DateTools creates a Calendar instance on every call to dateToString and timeToString. Specifically:

# timeToString calls Calendar.getInstance on every call.
# dateToString calls timeToString(date.getTime()), which then instantiates a new Date(). I think we should change the order of the calls, or not have each call the other.
# round(), which is called from timeToString (after creating a Calendar instance) creates another (!) Calendar instance ...

Seems that if we synchronize the methods and create the Calendar instance once (static), it should solve it."
"LUCENE-370","BUG","BUG","BooleanQuery assumes everything else implements skipTo","skipTo seems to be optional functionality on the Scorer class (BooleanScorer
doesn't implement it).  BooleanQuery.scorer() tests all subclauses using
""instanceof BooleanQuery"" to determine if it can use a ConjunctionScorer that
requires skipTo functionality.

This means that any other new Query/Scorer that don't implement skipTo will get
into trouble when included in a BooleanQuery.

If skipTo is really optional, then there should be some way of telling by the
Scorer or the Query in a more generic manner.

Some options:
1) have a ""boolean Scorer.hasSkipTo()"" method
2) have a ""boolean Query.hasSkipTo()"" method
3) remove Scorer.skipTo and have a ""public interface ScorerSkipTo{boolean
skipTo(int doc)}"" that scorers may implement"
"LUCENE-679","DOCUMENTATION","BUG","CLONE -QueryParser is not applicable for the arguments (String, String, Analyzer) error in results.jsp when executing search in the browser (demo from Lucene 2.0)","When executing search in the browser (as described in demo3.html Lucene demo) I get error, because the demo uses the method (QueryParser with three arguments) which is deleted (it was deprecated).
I checked the demo from Lucene 1.4-final it with Lucene 1.4-final - it works, because those time the method was there.
But demo from Lucene 2.0 does not work with Lucene 2.0

The error stack is here:
TTP Status 500 -

type Exception report

message

description The server encountered an internal error () that prevented it from fulfilling this request.

exception

org.apache.jasper.JasperException: Unable to compile class for JSP

An error occurred at line: 60 in the jsp file: /results.jsp
Generated servlet error:
The method parse(String) in the type QueryParser is not applicable for the arguments (String, String, Analyzer)


org.apache.jasper.servlet.JspServletWrapper.handleJspException(JspServletWrapper.java:510)
org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:375)
org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:314)
org.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)
javax.servlet.http.HttpServlet.service(HttpServlet.java:802)

root cause

org.apache.jasper.JasperException: Unable to compile class for JSP

An error occurred at line: 60 in the jsp file: /results.jsp
Generated servlet error:
The method parse(String) in the type QueryParser is not applicable for the arguments (String, String, Analyzer)


org.apache.jasper.compiler.DefaultErrorHandler.javacError(DefaultErrorHandler.java:84)
org.apache.jasper.compiler.ErrorDispatcher.javacError(ErrorDispatcher.java:328)
org.apache.jasper.compiler.JDTCompiler.generateClass(JDTCompiler.java:409)
org.apache.jasper.compiler.Compiler.compile(Compiler.java:297)
org.apache.jasper.compiler.Compiler.compile(Compiler.java:276)
org.apache.jasper.compiler.Compiler.compile(Compiler.java:264)
org.apache.jasper.JspCompilationContext.compile(JspCompilationContext.java:563)
org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:303)
org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:314)
org.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)
javax.servlet.http.HttpServlet.service(HttpServlet.java:802)

note The full stack trace of the root cause is available in the Apache Tomcat/5.5.15 logs."
"LUCENE-1547","BUG","BUG","Rare thread hazard in IndexWriter.commit()","The nightly build 2 nights ago hit this:

{code}
 NOTE: random seed of testcase 'testAtomicUpdates' was: -5065675995121791051
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAtomicUpdates(org.apache.lucene.index.TestAtomicUpdate):	FAILED
    [junit] expected:<100> but was:<91>
    [junit] junit.framework.AssertionFailedError: expected:<100> but was:<91>
    [junit] 	at org.apache.lucene.index.TestAtomicUpdate.runTest(TestAtomicUpdate.java:142)
    [junit] 	at org.apache.lucene.index.TestAtomicUpdate.testAtomicUpdates(TestAtomicUpdate.java:194)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)
{code}

It's an intermittant failure that only happens when multiple threads
are calling commit() at once.  With autoComit=true and
ConcurrentMergeScheduler, this can happen more often because each
merge thread calls commit after it's done.

The problem happens when one thread has already begun the commit
process, but another two or more threads then come along wanting to
also commit after further changes have happened.  Those two or more
threads would wait until the currently committing thread finished, and
then they'd wake up and do their commit.  The problem was, after
waking up they would fail to check whether they had been superseded,
ie whether another thread had already committed more up-to-date
changes.

The fix is simple -- after waking up, check again if your commit has
been superseded, and skip your commit if so.
"
"LUCENE-510","IMPROVEMENT","IMPROVEMENT","IndexOutput.writeString() should write length in bytes","We should change the format of strings written to indexes so that the length of the string is in bytes, not Java characters.  This issue has been discussed at:

http://www.mail-archive.com/java-dev@lucene.apache.org/msg01970.html

We must increment the file format number to indicate this change.  At least the format number in the segments file should change.

I'm targetting this for 2.1, i.e., we shouldn't commit it to trunk until after 2.0 is released, to minimize incompatible changes between 1.9 and 2.0 (other than removal of deprecated features)."
"LUCENE-739","IMPROVEMENT","IMPROVEMENT","Performance improvement for SegmentMerger.mergeNorms()","This patch makes two improvements to SegmentMerger.mergeNorms():

1) When the SegmentMerger merges the norm values it allocates a new byte array to buffer the values for every field of every segment. The size of such an array equals the size of the corresponding segment, so if large segments are being merged, those arrays become very large, too.
We can easily reduce the number of array allocations by reusing a byte array to buffer the norm values that only grows, if a segment is larger than the previous one.

2) Before a norm value is written it is checked if the corresponding document is deleted. If not, the norm is written using IndexOutput.writeByte(byte[]). This patch introduces an optimized case for segments that do not have any deleted docs. In this case the frequent call of IndexReader.isDeleted(int) can be avoided and the more efficient method IndexOutput.writeBytes(byte[], int) can be used.


This patch only changes the method SegmentMerger.mergeNorms(). All unit tests pass."
"LUCENE-2115","RFE","IMPROVEMENT","Port to Generics - test cases in contrib ","LUCENE-1257 in Lucene 3.0 addressed porting to generics across public api-s . LUCENE-2065 addressed across src/test . 

This would be a placeholder JIRA for any remaining pending generic conversions across the code base. 

Please keep it open after commiting and we can close it when we are near a 3.1 release , so that this could be a placeholder ticket. 

"
"LUCENE-601","RFE","IMPROVEMENT","RAMDirectory implements Serializable","RAMDirectory is for some reason not serializable."
"LUCENE-1970","CLEANUP","TASK","Remove deprecated DocIdSetIterator methods",""
"LUCENE-2793","RFE","IMPROVEMENT","Directory createOutput and openInput should take an IOContext","Today for merging we pass down a larger readBufferSize than for searching because we get better performance.

I think we should generalize this to a class (IOContext), which would hold the buffer size, but then could hold other flags like DIRECT (bypass OS's buffer cache), SEQUENTIAL, etc.

Then, we can make the DirectIOLinuxDirectory fully usable because we would only use DIRECT/SEQUENTIAL during merging.

This will require fixing how IW pools readers, so that a reader opened for merging is not then used for searching, and vice/versa.  Really, it's only all the open file handles that need to be different -- we could in theory share del docs, norms, etc, if that were somehow possible."
"LUCENE-379","IMPROVEMENT","IMPROVEMENT","Contribution: Efficient Sorting of DateField/DateTools Encoded Timestamp Long Values","Hello Tim,

As promised, the sort functionality for ""long"" values is included in the
attached files.

patchTestSort.txt contains the diff info. for my modifications to the
TestSort.java class

org.apache.lucene.search.ZIP contains the three new class files for
efficient sorting of ""long"" field values and of encoded timestamp
field values as ""long"" values.

Let me know if you have any questions.

Regards,
Rus"
"LUCENE-1848","DOCUMENTATION","IMPROVEMENT","Remove references to older versions of Lucene in ""per-release"" documentation","Some of the documentation that is ""per release"" contains references to older versions, which is often confusing.  This is most noticeable in the file formats docs, but there might be other places too."
"LUCENE-1461","RFE","RFE","Cached filter for a single term field","These classes implement inexpensive range filtering over a field containing a single term. They do this by building an integer array of term numbers (storing the term->number mapping in a TreeMap) and then implementing a fast integer comparison based DocSetIdIterator.

This code is currently being used to do age range filtering, but could also be used to do other date filtering or in any application where there need to be multiple filters based on the same single term field. I have an untested implementation of single term filtering and have considered but not yet implemented term set filtering (useful for location based searches) as well. 

The code here is fairly rough; it works but lacks javadocs and toString() and hashCode() methods etc. I'm posting it here to discover if there is other interest in this feature; I don't mind fixing it up but would hate to go to the effort if it's not going to make it into Lucene.

"
"LUCENE-2609","BUILD_SYSTEM","IMPROVEMENT","Generate jar containing test classes.","The test classes are useful for writing unit tests for code external to the Lucene project. It would be helpful to build a jar of these classes and publish them as a maven dependency."
"LUCENE-1837","IMPROVEMENT","BUG","Remove Searcher from Weight#explain","Explain needs to calculate corpus wide stats in a way that is consistent with MultiSearcher."
"LUCENE-2778","RFE","IMPROVEMENT","Allow easy extension of RAMDirectory","RAMDirectory uses RAMFiles to store the data. RAMFile offers a newBuffer() method for extensions to override and allocate buffers differently, from e.g. a pool or something. However, RAMDirectory always allocates RAMFile and doesn't allow allocating a RAMFile extension, which makes RAMFile.newBuffer() unusable.

I think we can simply introduce a newRAMFile() method on RAMDirectory and make the RAMFiles map protected, and it will allow really extending RAMDir.

I will post a patch later."
"LUCENE-3394","BUG","BUG","TestIndexFileDeleter checkIndex fail","found on 3.x

{noformat}
ant test-tag -Dtestcase=TestIndexFileDeleter -Dtestmethod=testDeleteLeftoverFiles -Dtests.seed=7631088157098800527:4270221915205524915
{noformat}"
"LUCENE-1701","RFE","RFE","Add NumericField, make plain text numeric parsers public in FieldCache, move trie parsers to FieldCache","In discussions about LUCENE-1673, Mike & me wanted to add a new NumericField to o.a.l.document specific for easy indexing. An alternative would be to add a NumericUtils.newXxxField() factory, that creates a preconfigured Field instance with norms and tf off, optionally a stored text (LUCENE-1699) and the TokenStream already initialized. On the other hand NumericUtils.newXxxSortField could be moved to NumericSortField.

I and Yonik tend to use the factory for both, Mike tends to create the new classes.

Also the parsers for string-formatted numerics are not public in FieldCache. As the new SortField API (LUCENE-1478) makes it possible to support a parser in SortField instantiation, it would be good to have the static parsers in FieldCache public available. SortField would init its member variable to them (instead of NULL), so making code a lot easier (FieldComparator has this ugly null checks when retrieving values from the cache).

Moving the Trie parsers also as static instances into FieldCache would make the code cleaner and we would be able to hide the ""hack"" StopFillCacheException by making it private to FieldCache (currently its public because NumericUtils is in o.a.l.util)."
"LUCENE-2648","RFE","IMPROVEMENT","Allow PackedInts.ReaderIterator to advance more than one value","The iterator-like API in LUCENE-2186 makes effective use of PackedInts.ReaderIterator but frequently skips multiple values. ReaderIterator currently requires to loop over ReaderInterator#next() to advance to a certain value. We should allow ReaderIterator to expose a #advance(ord) method to make use-cases like that more efficient. 

This issue is somewhat part of my efforts to make LUCENE-2186 smaller while breaking it up in little issues for parts which can be generally useful."
"LUCENE-3174","RFE","","Similarity.Stats class for term & collection statistics","In order to support ranking methods besides TF-IDF, we need to make the statistics they need available. These statistics could be computed in computeWeight (soon to become computeStats) and stored in a separate object for easy access. Since this object will be used solely by subclasses of Similarity, it should be implented as a static inner class, i.e. Similarity.Stats.

There are two ways this could be implemented:
- as a single Similarity.Stats class, reused by all ranking algorithms. In this case, this class would have a member field for all statistics;
- as a hierarchy of Stats classes, one for each ranking algorithm. Each subclass would define only the statistics needed for the ranking algorithm.

In the second case, the Stats class in DefaultSimilarity would have a single field, idf, while the one in e.g. BM25Similarity would have idf and average field/document length."
"LUCENE-3215","BUG","BUG","SloppyPhraseScorer sometimes computes Infinite freq","reported on user list:
http://www.lucidimagination.com/search/document/400cbc528ed63db9/score_of_infinity_on_dismax_query
"
"LUCENE-3521","BUG","IMPROVEMENT","upgrade icu jar to 4.8.1.1 / remove lucenetestcase hack","This bug fix release fixes problems with icu under java7: http://bugs.icu-project.org/trac/ticket/8734"
"LUCENE-952","TEST","BUG","GData's TestGdataIndexer.testDestroy() intermittently hits spin loop & causes build timeout","Several nightly builds (at least #136, #143 and #144) have failed due
to timeout at 45 minutes while running the TestGdataIndexer.testDestroy()
test case.

I tracked it down to this line:

      // wait active for the commit
      while(this.indexer.writer != null){}

Intermittently, that while loop will spin forever.  I can only get the
failure to happen on Linux: it doesn't happen on Mac OS X (haven't
tried windows).  The nightly build runs on Solaris 10, so it also
happens there.

It turns out, this is due to the fact that ""writer"" is not declared as
""volatile"".  This is because one thread is closing the indexer, which
sets writer to null, but another thread is running the while loop.
If this.indexer.writer was set to null before that while loop starts,
the test will run through fine; else, it won't.

I plan to fix this by adding this method to GDataIndexer class:

    // Used only for testing
    protected synchronized IndexWriter getWriter() {
      return this.writer;
    }

and changing unit test to call that method."
"LUCENE-3703","BUG","BUG","DirectoryTaxonomyReader.refresh misbehaves with ref counts","DirectoryTaxonomyReader uses the internal IndexReader in order to track its own reference counting. However, when you call refresh(), it reopens the internal IndexReader, and from that point, all previous reference counting gets lost (since the new IndexReader's refCount is 1).

The solution is to track reference counting in DTR itself. I wrote a simple unit test which exposes the bug (will be attached with the patch shortly)."
"LUCENE-1283","REFACTORING","IMPROVEMENT","Factor out ByteSliceWriter from DocumentsWriterFieldData","DocumentsWriter uses byte slices into shared byte[]'s to hold the
growing postings data for many different terms in memory.  This is
probably the trickiest (most confusing) part of DocumentsWriter.

Right now it's not cleanly factored out and not easy to separately
test.  In working on this issue:

  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200805.mbox/%3c126142c0805061426n1168421ya5594ef854fae5e4@mail.gmail.com%3e

which eventually turned out to be a bug in Oracle JRE's JIT compiler,
I factored out ByteSliceWriter and created a unit test to stress test
the writing & reading of byte slices.  The test just randomly writes N
streams interleaved into shared byte[]'s, then reads them back
verifying the results are correct.

I created the stress test to try to find any bugs in that code.  The
test ran fine (no bugs were found) but I think the refactoring is
still very much worthwhile.

I expected the changes to reduce indexing throughput, so I ran a test
indexing first 200K Wikipedia docs using this alg:

{code}
analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker

docs.file=/Volumes/External/lucene/wiki.txt
doc.stored = true
doc.term.vector = true
doc.add.log.step=2000

directory=FSDirectory
autocommit=false
compound=true

ram.flush.mb=256

{ ""Rounds""
  ResetSystemErase
  { ""BuildIndex""
    - CreateIndex
     { ""AddDocs"" AddDoc > : 200000
    - CloseIndex
  }
  NewRound
} : 4

RepSumByPrefRound BuildIndex

{code}

Ok trunk it produces these results:
{code}
Operation   round   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem
BuildIndex      0        1       200000        791.7      252.63   338,552,096  1,061,814,272
BuildIndex -  - 1 -  -   1 -  -  200000 -  -   793.1 -  - 252.18 - 605,262,080  1,061,814,272
BuildIndex      2        1       200000        794.8      251.63   601,966,528  1,061,814,272
BuildIndex -  - 3 -  -   1 -  -  200000 -  -   782.5 -  - 255.58 - 608,699,712  1,061,814,272
{code}

and with the patch:

{code}
Operation   round   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem
BuildIndex      0        1       200000        745.0      268.47   338,318,784  1,061,814,272
BuildIndex -  - 1 -  -   1 -  -  200000 -  -   792.7 -  - 252.30 - 605,331,776  1,061,814,272
BuildIndex      2        1       200000        786.7      254.24   602,915,712  1,061,814,272
BuildIndex -  - 3 -  -   1 -  -  200000 -  -   795.3 -  - 251.48 - 602,378,624  1,061,814,272
{code}

So it looks like the performance cost of this change is negligible (in
the noise).

"
"LUCENE-3790","BUG","BUG","benchmark cannot parse highlight-vs-vector-highlight.alg, but only on 3.x?!","A new test (TestPerfTasksParse.testParseExamples) was added in LUCENE-3768 that 
guarantees all .alg files in the conf/ directory can actually be parsed...

But highlight-vs-vector-highlight.alg cannot be parsed on 3.x (NumberFormatException), 
however it works fine on trunk... and the .alg is exactly the same in both cases.

{noformat}
    [junit] ------------- Standard Error -----------------
    [junit] java.lang.NumberFormatException: For input string: ""maxFrags[3.0],fields[body]""
    [junit] 	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1222)
    [junit] 	at java.lang.Float.parseFloat(Float.java:422)
    [junit] 	at org.apache.lucene.benchmark.byTask.tasks.SearchTravTask.setParams(SearchTravTask.java:76)
    [junit] 	at org.apache.lucene.benchmark.byTask.tasks.SearchTravRetVectorHighlightTask.setParams(SearchTravRetVectorHighlightTask.java:124)
    [junit] 	at org.apache.lucene.benchmark.byTask.utils.Algorithm.<init>(Algorithm.java:112)
    [junit] 	at org.apache.lucene.benchmark.byTask.TestPerfTasksParse.testParseExamples(TestPerfTasksParse.java:132)
{noformat}
"
"LUCENE-3730","IMPROVEMENT","IMPROVEMENT","Improved Kuromoji search mode segmentation/decompounding","Kuromoji has a segmentation mode for search that uses a heuristic to promote additional segmentation of long candidate tokens to get a decompounding effect.  This heuristic has been improved.  Patch is coming up."
"LUCENE-3506","TEST","BUG","tests for verifying that assertions are enabled do nothing since they ignore AssertionError","Follow-up from LUCENE-3501"
"LUCENE-3436","RFE","IMPROVEMENT","Spellchecker ""Suggest Mode"" Support","This is a spin-off from SOLR-2585.

Currently o.a.l.s.s.SpellChecker and o.a.l.s.s.DirectSpellChecker support two ""Suggest Modes"":
1. Suggest for terms that are not in the index.
2. Suggest ""more popular"" terms.

This issue is to add a third Suggest Mode:
3. Suggest always.

This will assist users in developing context-sensitive spell suggestions and/or did-you-mean suggestions.  See SOLR-2585 for a full discussion.

Note that o.a.l.s.s.SpellChecker already can support this functionality, if the user passes in a NULL term & IndexReader.  This, however, is not intutive.  o.a.l.s.s.DirectSpellChecker currently has no support for this third Suggest Mode."
"LUCENE-3719","BUG","BUG","FVH: slow performance on very large queries","The change from HashSet to ArrayList for flatQueries in LUCENE-3019 resulted in very significant slowdown in some of our e-discovery queries after upgrade from 3.4.0 to 3.5.0. Our queries sometime contain tens of thousands of terms. As a result, major portion of execution time for such queries is now spent in the flatQueries.contains( sourceQuery ) method calls."
"LUCENE-2663","IMPROVEMENT","BUG","wrong exception from NativeFSLockFactory (LIA2 test case)","As part of integrating Lucene In Action 2 test cases (LUCENE-2661), I found one of the test cases fail

the test is pretty simple, and passes on 3.0. The exception you get instead (LockReleaseFailedException) is 
pretty confusing and I think we should fix it.
"
"LUCENE-2043","IMPROVEMENT","IMPROVEMENT","Fix CommitIndexTask to also commit IndexReader changes","I'm setting up a benchmark for LUCENE-1458, and one limitation I hit is that the CommitIndexTask doesn't commit pending changes in the IndexReader (eg via DeleteByPercent), using a named commit point."
"LUCENE-1483","RFE","IMPROVEMENT","Change IndexSearcher multisegment searches to search each individual segment using a single HitCollector","This issue changes how an IndexSearcher searches over multiple segments. The current method of searching multiple segments is to use a MultiSegmentReader and treat all of the segments as one. This causes filters and FieldCaches to be keyed to the MultiReader and makes reopen expensive. If only a few segments change, the FieldCache is still loaded for all of them.

This patch changes things by searching each individual segment one at a time, but sharing the HitCollector used across each segment. This allows FieldCaches and Filters to be keyed on individual SegmentReaders, making reopen much cheaper. FieldCache loading over multiple segments can be much faster as well - with the old method, all unique terms for every segment is enumerated against each segment - because of the likely logarithmic change in terms per segment, this can be very wasteful. Searching individual segments avoids this cost. The term/document statistics from the multireader are used to score results for each segment.

When sorting, its more difficult to use a single HitCollector for each sub searcher. Ordinals are not comparable across segments. To account for this, a new field sort enabled HitCollector is introduced that is able to collect and sort across segments (because of its ability to compare ordinals across segments). This TopFieldCollector class will collect the values/ordinals for a given segment, and upon moving to the next segment, translate any ordinals/values so that they can be compared against the values for the new segment. This is done lazily.

All and all, the switch seems to provide numerous performance benefits, in both sorted and non sorted search. We were seeing a good loss on indices with lots of segments (1000?) and certain queue sizes / queries, but the latest results seem to show thats been mostly taken care of (you shouldnt be using such a large queue on such a segmented index anyway).

* Introduces
** MultiReaderHitCollector - a HitCollector that can collect across multiple IndexReaders. Old HitCollectors are wrapped to support multiple IndexReaders.
** TopFieldCollector - a HitCollector that can compare values/ordinals across IndexReaders and sort on fields.
** FieldValueHitQueue - a Priority queue that is part of the TopFieldCollector implementation.
** FieldComparator - a new Comparator class that works across IndexReaders. Part of the TopFieldCollector implementation.
** FieldComparatorSource - new class to allow for custom Comparators.
* Alters
** IndexSearcher uses a single HitCollector to collect hits against each individual SegmentReader. All the other changes stem from this ;)
* Deprecates
** TopFieldDocCollector
** FieldSortedHitQueue
"
"LUCENE-1126","IMPROVEMENT","IMPROVEMENT","Simplify StandardTokenizer JFlex grammar","Summary of thread entitled ""Fullwidth alphanumeric characters, plus a question on Korean ranges"" begun by Daniel Noll on java-user, and carried over to java-dev:

On 01/07/2008 at 5:06 PM, Daniel Noll wrote:
> I wish the tokeniser could just use Character.isLetter and
> Character.isDigit instead of having to know all the ranges itself, since
> the JRE already has all this information.  Character.isLetter does
> return true for CJK characters though, so the ranges would still come in
> handy for determining what kind of letter they are.  I don't support
> JFlex has a way to do this...

The DIGIT macro could be replaced by JFlex's predefined character class [:digit:], which has the same semantics as java.lang.Character.isDigit().

Although JFlex's predefined character class [:letter:] (same semantics as java.lang.Character.isLetter()) includes CJK characters, there is a way to handle this using JFlex's regex negation syntax {{!}}.  From [the JFlex documentation|http://jflex.de/manual.html]:

bq. [T]he expression that matches everything of {{a}} not matched by {{b}} is !(!{{a}}|{{b}}) 

So to exclude CJ characters from the LETTER macro:

{code}
    LETTER = ! ( ! [:letter:] | {CJ} )
{code}
 
Since [:letter:] includes all of the Korean ranges, there's no reason (AFAICT) to treat them separately; unlike Chinese and Japanese characters, which are individually tokenized, the Korean characters should participate in the same token boundary rules as all of the other letters.

I looked at some of the differences between Unicode 3.0.0, which Java 1.4.2 supports, and Unicode 5.0, the latest version, and there are lots of new and modified letter and digit ranges.  This stuff gets tweaked all the time, and I don't think Lucene should be in the business of trying to track it, or take a position on which Unicode version users' data should conform to.  

Switching to using JFlex's [:letter:] and [:digit:] predefined character classes ties (most of) these decisions to the user's choice of JVM version, and this seems much more reasonable to me than the current status quo.

I will attach a patch shortly.
"
"LUCENE-3142","CLEANUP","BUG","benchmark/stats package is obsolete and unused - remove it","This seems like a leftover from the original benchmark implementation and can thus be removed.
"
"LUCENE-1656","IMPROVEMENT","IMPROVEMENT","When sorting by field, IndexSearcher should not compute scores by default","In 2.9 we've added the ability to turn off scoring (maxScore &
trackScores, separately) when sorting by field.

I expect most apps don't use the scores when sorting by field, and
there's a sizable performance gain when scoring is off, so I think for
2.9 we should not score by default, and add show in CHANGES how to
enable scoring if you rely on it.

If there are no objections, I'll commit that change in a day or two
(it's trivial).
"
"LUCENE-3541","BUG","BUG","remove IndexInput.copyBuf","this looks really broken/dangerous as an instance variable.

what happens on clone() ?! copyBytes can instead make its own array inside the method.

its protected, so ill list in the 3.x backwards breaks section since its technically a backwards break."
"LUCENE-1260","RFE","IMPROVEMENT","Norm codec strategy in Similarity","The static span and resolution of the 8 bit norms codec might not fit with all applications. 

My use case requires that 100f-250f is discretized in 60 bags instead of the default.. 10?
"
"LUCENE-1500","BUG","BUG","Highlighter throws StringIndexOutOfBoundsException","Using the canonical Solr example (ant run-example) I added this document (using exampledocs/post.sh):

<add><doc>
  <field name=""id"">Test for Highlighting StringIndexOutOfBoundsExcdption</field>
  <field name=""name"">Some Name</field>
  <field name=""manu"">Acme, Inc.</field>
  <field name=""features"">Description of the features, mentioning various things</field>
  <field name=""features"">Features also is multivalued</field>
  <field name=""popularity"">6</field>
  <field name=""inStock"">true</field>
</doc></add>

and then the URL http://localhost:8983/solr/select/?q=features&hl=true&hl.fl=features caused the exception.

I have a patch.  I don't know if it is completely correct, but it avoids this exception.
"
"LUCENE-2370","RFE","TASK","Reintegrate flex branch into trunk","This issue is for reintegrating the flex branch into current trunk. I will post the patch here for review and commit, when all contributors to flex have reviewed the patch.

Before committing, I will tag both trunk and flex."
"LUCENE-1118","IMPROVEMENT","IMPROVEMENT","core analyzers should not produce tokens > N (100?) characters in length","Discussion that led to this:

  http://www.gossamer-threads.com/lists/lucene/java-dev/56103

I believe nearly any time a token > 100 characters in length is
produced, it's a bug in the analysis that the user is not aware of.

These long tokens cause all sorts of problems, downstream, so it's
best to catch them early at the source.

We can accomplish this by tacking on a LengthFilter onto the chains
for StandardAnalyzer, SimpleAnalyzer, WhitespaceAnalyzer, etc.

Should we do this in 2.3?  I realize this is technically a break in
backwards compatibility, however, I think it must be incredibly rare
that this break would in fact break something real in the application?"
"LUCENE-489","RFE","","Allow QP subclasses to support Wildcard Queries with leading ""*""","It would be usefull for some users if the logic that prevents QueryParser from creating WIldcardQueries with leading wildcard characters (""?"" or ""*"") be moved from the grammer into the base implimentation of getWildcardQuery so that it may be overridden in subclasses without needing to modifiy the grammer directly.
"
"LUCENE-380","RFE","BUG","A new Greek Analyzer for Lucene","I would like to contribute a greek analyzer for lucene. It is based on the
existing Russian analyzer and features:

- most common greek character sets, such as Unicode, ISO-8859-7 and Windows-1253
- a collection of common greek stop words
- conversion of characters with diacritics (accent, diaeresis) in the lower case
filter, as well as handling of special characters, such as small final sigma

For the character sets I used RFC 1947 (Greek Character Encoding for Electronic
Mail Messages) as a reference. I have incorporated this analyzer in Luke as well
as used it successfully in a recent project of my company (EBS Ltd.).

I hope you will find it a useful addition to the project."
"LUCENE-513","CLEANUP","IMPROVEMENT","Remove superfluous comment in MMapDirectory.java","See title, and I prefer my name to be removed from the source code."
"LUCENE-2784","REFACTORING","IMPROVEMENT","Change all FilteredTermsEnum impls into TermsEnum decorators","Currently, FilteredTermsEnum has two ctors:
* FilteredTermsEnum(IndexReader reader, String field)
* FilteredTermsEnum(TermsEnum tenum)

But most of our concrete implementations (e.g. TermsRangeEnum) use the IndexReader+field ctor

In my opinion we should remove this ctor, and switch over all FilteredTermsEnum implementations to just take a TermsEnum.

Advantages:
* This simplifies FilteredTermsEnum and its subclasses, where they are more decorator-like (perhaps in the future we could compose them)
* Removes silly checks such as if (tenum == null) in every next()
* Allows for consumers to pass in enumerators however they want: e.g. its their responsibility if they want to use MultiFields or not, it shouldnt be buried in FilteredTermsEnum.

I created a quick patch (all core+contrib+solr tests pass), but i think this opens up more possibilities for refactoring improvements that haven't yet been done in the patch: we should explore these too.
"
"LUCENE-3679","REFACTORING","IMPROVEMENT","Replace IndexReader.getFieldNames with IndexReader.getFieldInfos",""
"LUCENE-786","DOCUMENTATION","IMPROVEMENT","Extended javadocs in spellchecker","Added some javadocs that explains why the spellchecker does not work as one might expect it to.

http://www.nabble.com/SpellChecker%3A%3AsuggestSimilar%28%29-Question-tf3118660.html#a8640395

> Without having looked at the code for a long time, I think the problem is what the
> lucene scoring consider to be best. First the grams are searched, resulting in a number
> of hits. Then the edit-distance is calculated on each hit. ""Genetics"" is appearently the
> third most similar hit according to Lucene, but the best according to Levenshtein.
>
> I.e. Lucene does not use edit-distance as similarity. You need to get a bunch of best hits
> in order to find the one with the smallest edit-distance.

I took a look at the code, and my assessment seems to be right."
"LUCENE-1165","DOCUMENTATION","IMPROVEMENT","Reduce exposure of nightly build documentation","From LUCENE-1157  -

 ..the nightly build documentation is too prominent. A search for ""indexwriter api"" on Google or Yahoo! returns nightly documentation before released documentation.

(https://issues.apache.org/jira/browse/LUCENE-1157?focusedCommentId=12565820#action_12565820)
"
"LUCENE-1349","DOCUMENTATION","BUG","Mark Fieldable as allowing some changes in 2.x future releases","See http://lucene.markmail.org/message/4k2gqs3n7coh4lmd?q=Fieldable

1. We mark Fieldable as being subject to change. We heavily advertise (on java-dev and java-user and maybe general) that in the next minor release of Lucene (2.4), Fieldable will be changing. It is also marked at the top of CHANGES.txt very clearly for all the world to see. Since 2.4 is probably at least a month away, I think this gives anyone with a pulse enough time to react. "
"LUCENE-821","IMPROVEMENT","BUG","single norm file still uses up descriptors","The new index file format with a single .nrm file for all norms does not decrease file descriptor usage.
The .nrm file is opened once for each field with norms in the index segment."
"LUCENE-3393","REFACTORING","","Rename EasySimilarity to SimilarityBase",""
"LUCENE-2532","TEST","IMPROVEMENT","improve test coverage of multi-segment indices","Simple patch that adds a test-only helper class, RandomIndexWriter, that lets you add docs, but it will randomly do things like use a different merge policy/scheduler, flush by doc count instead of RAM, flush randomly (so we get multi-segment indices) but also randomly optimize in the end (so we also sometimes test single segment indices)."
"LUCENE-2830","IMPROVEMENT","IMPROVEMENT","Use StringBuilder instead of StringBuffer in benchmark","Minor change - use StringBuilder instead of StringBuffer in benchmark's code. We don't need the synchronization of StringBuffer in all the places that I've checked.

The only place where it _could_ be a problem is in HtmlParser's API - one method accepts a StringBuffer and it's an interface. But I think it's ok to change benchmark's API, back-compat wise and so I'd like to either change it to accept a String, or remove the method altogether -- no code in benchmark uses it, and if anyone needs it, he can pass StringReader to the other method."
"LUCENE-3711","BUG","BUG","small SentinelIntSet can cause infinite loop on resize","A small initial size of <=4 can cause the set to not rehash soon enough and thus go into an infinite loop searching the table for an open space."
"LUCENE-1704","RFE","IMPROVEMENT","org.apache.lucene.ant.HtmlDocument added Tidy config file passthrough availability","Parsing HTML documents using the org.apache.lucene.ant.HtmlDocument.Document method resulted in many error messages such as this:

    line 152 column 725 - Error: <as-html> is not recognized!
    This document has errors that must be fixed before
    using HTML Tidy to generate a tidied up version.

The solution is to configure Tidy to accept these abnormal tags by adding the tag name to the ""new-inline-tags"" option in the Tidy config file (or the command line which does not make sense in this context), like so:

    new-inline-tags: as-html

Tidy needs to know where the configuration file is, so a new constructor and Document method can be added.  Here is the code:

{code}
    /**                                                                                                                                                                                            
     *  Constructs an <code>HtmlDocument</code> from a {@link                                                                                                                                      
     *  java.io.File}.                                                                                                                                                                             
     *                                                                                                                                                                                             
     *@param  file             the <code>File</code> containing the                                                                                                                                
     *      HTML to parse                                                                                                                                                                          
     *@param  tidyConfigFile   the <code>String</code> containing                                                                                                                                  
     *      the full path to the Tidy config file                                                                                                                                                  
     *@exception  IOException  if an I/O exception occurs                                                                                                                                          
     */
    public HtmlDocument(File file, String tidyConfigFile) throws IOException {
        Tidy tidy = new Tidy();
        tidy.setConfigurationFromFile(tidyConfigFile);
        tidy.setQuiet(true);
        tidy.setShowWarnings(false);
        org.w3c.dom.Document root =
                tidy.parseDOM(new FileInputStream(file), null);
        rawDoc = root.getDocumentElement();
    }

    /**                                                                                                                                                                                            
     *  Creates a Lucene <code>Document</code> from a {@link                                                                                                                                       
     *  java.io.File}.                                                                                                                                                                             
     *                                                                                                                                                                                             
     *@param  file                                                                                                                                                                                 
     *@param  tidyConfigFile the full path to the Tidy config file                                                                                                                                 
     *@exception  IOException                                                                                                                                                                      
     */
    public static org.apache.lucene.document.Document
        Document(File file, String tidyConfigFile) throws IOException {

        HtmlDocument htmlDoc = new HtmlDocument(file, tidyConfigFile);

        org.apache.lucene.document.Document luceneDoc = new org.apache.lucene.document.Document();

        luceneDoc.add(new Field(""title"", htmlDoc.getTitle(), Field.Store.YES, Field.Index.ANALYZED));
        luceneDoc.add(new Field(""contents"", htmlDoc.getBody(), Field.Store.YES, Field.Index.ANALYZED));

        String contents = null;
        BufferedReader br =
            new BufferedReader(new FileReader(file));
        StringWriter sw = new StringWriter();
        String line = br.readLine();
        while (line != null) {
            sw.write(line);
            line = br.readLine();
        }
        br.close();
        contents = sw.toString();
        sw.close();

        luceneDoc.add(new Field(""rawcontents"", contents, Field.Store.YES, Field.Index.NO));

        return luceneDoc;
    }
{code}

I am using this now and it is working fine.  The configuration file is being passed to Tidy and now I am able to index thousands of HTML pages with no more Tidy tag errors.

"
"LUCENE-2491","RFE","IMPROVEMENT","Extend Codec with a SegmentInfos writer / reader","I'm trying to implement a Codec that works with append-only filesystems (HDFS). It's _almost_ done, except for the SegmentInfos.write(dir), which uses ChecksumIndexOutput, which in turn uses IndexOutput.seek() - and seek is not supported on append-only output. I propose to extend the Codec interface to encapsulate also the details of SegmentInfos writing / reading. Patch to follow after some feedback ;)"
"LUCENE-2297","RFE","IMPROVEMENT","IndexWriter should let you optionally enable reader pooling","For apps using a large index and frequently need to commit and resolve deletes, the cost of opening the SegmentReaders on demand for every commit can be prohibitive.

We an already pool readers (NRT does so), but, we only turn it on if NRT readers are in use.

We should allow separate control.

We should do this after LUCENE-2294."
"LUCENE-2401","IMPROVEMENT","IMPROVEMENT","Improve performance of CharTermAttribute(Impl) and also fully implement Appendable","The Appendable.append(CharSequence) method in CharTermAttributes is good for general use. But like StringBuilder has for some common use cases specialized methods, this does the same and adds separate append methods for String, StringBuilder and CharTermAttribute itsself. This methods enable the compiler to directly link the specialized methods and don't use the instanceof checks. The unspecialized method only does the instanceof checks for longer CharSequences (>8 chars), else it simply iterates.

This patch also fixes the required special ""null"" handling. append() methods are required by Appendable to append ""null"", if the argument is null. I dont like this, but its required. Maybe we should document, that we dont dont support it. Otherwise, JDK's formatter fails with formatting null."
"LUCENE-1819","BUG","BUG","MatchAllDocsQuery.toString(String field) does not honor the javadoc contract","Should be 

public String toString(String field){
  return ""*:*"";
}

QueryParser needs to be able to parse the String form of this query."
"LUCENE-3083","BUG","BUG","MockRandomMergePolicy optimizes segments not in the Set passed in","The test class MockRandomMergePolicy shuffles the whole SegmentInfos passed to the optimize callback and returns random segments for optimizing. This is fine, but it also returns segments, that are not listed in the Set<SegmentInfo> that is also passed in, containing the subset of segments to optimize.

This bug was found when writing a testcase for LUCENE-3082: The wrapper MergePolicy (when wrapped around MockRandomMergePolicy) only passes a subset of the segments to the delegate (the ones that are in old index format). But MockRandom created OneMerge in its return MergeSpecification having segments outside this set."
"LUCENE-3430","BUG","BUG","TestParser.testSpanTermXML fails with some sims","here is why this test sometimes fails (my explanation in the test i wrote):

{noformat}
  /** make sure all sims work with spanOR(termX, termY) where termY does not exist */
  public void testCrazySpans() throws Exception {
    // The problem: ""normal"" lucene queries create scorers, returning null if terms dont exist
    // This means they never score a term that does not exist.
    // however with spans, there is only one scorer for the whole hierarchy:
    // inner queries are not real queries, their boosts are ignored, etc.
{noformat}

Basically, SpanQueries aren't really queries, you just get one scorer. it calls extractTerms on the whole hierarchy and computes weights (e.g. IDF) on
the whole bag of terms, even if they don't exist.

This is fine, we already have tests that sim's won't bug-out in computeStats() here: however they don't expect to actually score documents based on
these terms that don't exist... however this is exactly what happens in Spans because it doesn't use sub-scorers.

Lucene's sim avoids this with the (docFreq + 1)
"
"LUCENE-2733","CLEANUP","TASK","Add private ctors to static utility classes","During development in 3.x and trunk we added some new classes like IOUtils and CodecUtils that are only providing static methods, but have no ctor at all. This adds the default empty public ctor, which is wrong, the classes should never be instantiated.

We should add private dummy ctors to prevent creating instances."
"LUCENE-1860","IMPROVEMENT","IMPROVEMENT","switch MultiTermQuery to ""constant score auto"" rewrite by default","Right now it defaults to scoring BooleanQuery, and that's inconsistent w/ QueryParser which does constant score auto.

The new multi-term queries already set this default, so the only core queries this will impact are PrefixQuery and WildcardQuery.  FuzzyQuery, which has its own rewrite to BooleanQuery, will keep doing so."
"LUCENE-1754","CLEANUP","IMPROVEMENT","Get rid of NonMatchingScorer from BooleanScorer2","Over in LUCENE-1614 Mike has made a comment about removing NonMatchinScorer from BS2, and return null in BooleanWeight.scorer(). I've checked and this can be easily done, so I'm going to post a patch shortly. For reference: https://issues.apache.org/jira/browse/LUCENE-1614?focusedCommentId=12715064&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12715064.

I've marked the issue as 2.9 just because it's small, and kind of related to all the search enhancements done for 2.9."
"LUCENE-1084","IMPROVEMENT","IMPROVEMENT","increase default maxFieldLength?","To my understanding, Lucene 2.3 will easily index large documents. So shouldn't we get rid of the 10,000 default limit for the field length? 10,000 isn't that much and as Lucene doesn't have any error logging by default, this is a common problem for users that is difficult to debug if you don't know where to look.

A better new default might be Integer.MAX_VALUE.
"
"LUCENE-1640","BUG","BUG","MockRAMDirectory (used only by unit tests) has some synchronization problems","Coming out of a failure that Earwin noted on java-dev this morning, I reworked the synchronization on MockRAMDirectory."
"LUCENE-545","RFE","RFE","Field Selection and Lazy Field Loading","The patch to come shortly implements a Field Selection and Lazy Loading mechanism for Document loading on the IndexReader.

It introduces a FieldSelector interface that defines the accept method:
FieldSelectorResult accept(String fieldName);

(Perhaps we want to expand this to take in other parameters such as the field metadata (term vector, etc.))

Anyone can implement a FieldSelector to define how they want to load fields for a Document.  
The FieldSelectorResult can be one of four values: LOAD, LAZY_LOAD, NO_LOAD, LOAD_AND_BREAK.  
The FieldsReader, as it is looping over the FieldsInfo, applies the FieldSelector to determine what should be done with the current field.

I modeled this after the java.io.FileFilter mechanism.  There are two implementations to date: SetBasedFieldSelector and LoadFirstFieldSelector.  The former takes in two sets of field names, one to load immed. and one to load lazily.  The latter returns LOAD_AND_BREAK on the first field encountered.  See TestFieldsReader for examples.

It should support UTF-8 (I borrowed code from Issue 509, thanks!).  See TestFieldsReader for examples

I added an expert method on IndexInput  named skipChars that takes in the number of characters to skip.  This is a compromise on changing the file format of the fields to better support seeking.  It does some of the work of readChars, but not all of it.  It doesn't require buffer storage and it doesn't do the bitwise operations.  It just reads in the appropriate number of bytes and promptly ignores them.  This is useful for skipping non-binary, non-compressed stored fields.

The biggest change is by far the introduction of the Fieldable interface (as per Doug's suggestion from a mailing list email on Lazy Field loading from a while ago).  Field is now a Fieldable.  All uses of Field have been changed to use Fieldable.  FieldsReader.LazyField also implements Fieldable.

Lazy Field loading is now implemented.  It has a major caveat (that is Documented) in that it clones the underlying IndexInput upon lazy access to the Field value.  IT IS UNDEFINED whether a Lazy Field can be loaded after the IndexInput parent has been closed (although, from what I saw, it does work).  I thought about adding a reattach method, but it seems just as easy to reload the document.  See the TestFieldsReader and DocHelper for examples.

I updated a couple of other tests to reflect the new fields that are on the DocHelper document.

All tests pass."
"LUCENE-317","BUG","IMPROVEMENT","[PATCH] When locks are disabled, IndexWriter.close() throws NullPointerException","If locks are disabled (via setting the System property 'disableLuceneLocks' to
true), IndexWriter throws a NullPointerException on closing. The reason is that
the attempt to call writeLock.release() fails because writeLock is null.
To correct this, just check for this case before releasing. A (trivial) patch is
attached."
"LUCENE-1506","RFE","IMPROVEMENT","Adding FilteredDocIdSet and FilteredDocIdSetIterator","Adding 2 convenience classes: FilteredDocIdSet and FilteredDocIDSetIterator."
"LUCENE-1160","IMPROVEMENT","IMPROVEMENT","MergeException from CMS threads should record the Directory","When you hit an unhandled exception in ConcurrentMergeScheduler, it
throws a MergePolicy.MergeException, but there's no easy way to figure
out which index caused this (if you have more than one).

I plan to add the Directory to the MergeException.  I also made a few
other small changes to ConcurrentMergeScheduler:

  * Added handleMergeException method, which is called on exception,
    so that you can subclass ConcurrentMergeScheduler to do something
    when an exception occurs.

  * Added getMergeThread() method so you can override how the threads
    are created (eg, if you want to make them in a different thread
    group, use a pool, change priorities, etc.).

  * Added doMerge(...) to actually do this merge, so you can do
    something before starting and after finishing a merge.

  * Changed private -> protected on a few attrs

I plan to commit in a day or two.
"
"LUCENE-2908","CLEANUP","TASK","clean up serialization in the codebase","We removed contrib/remote, but forgot to cleanup serialization hell everywhere.

this is no longer needed, never really worked (e.g. across versions), and slows 
development (e.g. i wasted a long time debugging stupid serialization of 
Similarity.idfExplain when trying to make a patch for the scoring system).
"
"LUCENE-768","BUG","BUG","Exception in deleteDocument, undeleteAll or setNorm in IndexReader can fail to release write lock on close","I hit this while working on LUCENE-140

We have 3 cases in the IndexReader methods above where we have this pattern:

  if (directoryOwner) acquireWriteLock();
  doSomething();
  hasChanges = true;

The problem is if you hit an exception in doSomething(), and hasChanges was not already true, then hasChanges will not have been set to true yet the write lock is held.  If you then try to close the reader without making any other changes, then the write lock is not released because in IndexReader.close() (well, in commit()) we only release write lock if hasChanges is true.

I think the simple fix is to swap the order of hasChanges = true and doSomething().  I already fixed one case of this under LUCENE-140 commit yesterday; I will fix the other two under this issue."
"LUCENE-249","DOCUMENTATION","BUG","Demo: DeleteFiles doesn't delete files by their path names","It appears that delete(term) fails to delete the last document containing term, which
for a unique match means that you can't remove an individual document.

Code attempting to remove document with specific 'path' (slightly modified version of demo code):

Directory directory = FSDirectory.getDirectory(""index"", false);
IndexReader reader = IndexReader.open(directory);
Term term = new Term(""path"", args[0]);  // path passed via command line arg
int deleted = reader.delete(term);
reader.close();
directory.close();

System.out.println(""deleted "" + deleted + "" documents containing "" + term);

Executing this always returns ""deleted 0 documents containing <path entered>""

In IndexReader.java, delete() has:

public final int delete(Term term) throws IOException {
  TermDocs docs = termDocs(term);
  if (docs == null) return 0;
  int n = 0;
  try {
    while (docs.next()) {
      delete(docs.doc());
      n++;
    }
  } finally {
    docs.close();
  }
  return n;
}

It appears that docs.next() always returns false when there is only one doc, hence
delete() is never called and 0 is always returned.  I assume that this also means that
if there are multiple matches, the last doc will not be deleted either, but I have not tested
that.

I modified the code as follows:

    boolean more = true;
    try {
      docs.next();
      while (more) {
        delete(docs.doc());
        n++;
        more = docs.next();
      }
    } finally {
      docs.close();
    }

and then it worked as expected (at least attempts to delete a single document from the
index succeeded whereas previously they did not)."
"LUCENE-289","OTHER","BUG","[patch] better support gcj compilation","There are two methods in IndexReader.java called 'delete'. That is a reserved
keyword in C++ and these methods cause trouble for gcj which implements a clever
workaround in renaming them delete$ but the OS X dynamic linker doesn't pick-up
on it.
The attached patch renames delete(int) to deleteDocument(int) and delete(Term)
to deleteDocuments(Term) and deprecates the delete methods (as requested by Doug
Cutting)."
"LUCENE-898","BUILD_SYSTEM","BUG","contrib/javascript is not packaged into releases","the contrib/javascript directory is (apparently) a collection of javascript utilities for lucene .. but it has not build files or any mechanism to package it, so it is excluded form releases.

"
"LUCENE-1152","BUG","BUG","SpellChecker does not work properly on calling indexDictionary after clearIndex","We have to call clearIndex and indexDictionary to rebuild dictionary from fresh. The call to SpellChecker clearIndex() function does not reset the searcher. Hence, when we call indexDictionary after that, many entries that are already in the stale searcher will not be indexed.

Also, I see that IndexReader reader is used for the sole purpose of obtaining the docFreq of a given term in exist() function. This functionality can also be obtained by using just the searcher by calling searcher.docFreq. Thus, can we get away completely with reader and code associated with it like
      if (IndexReader.isLocked(spellIndex)){
	IndexReader.unlock(spellIndex);
      }
and the reader related code in finalize?

"
"LUCENE-532","RFE","IMPROVEMENT","[PATCH] Indexing on Hadoop distributed file system","In my current project we needed a way to create very large Lucene indexes on Hadoop distributed file system. When we tried to do it directly on DFS using Nutch FsDirectory class - we immediately found that indexing fails because DfsIndexOutput.seek() method throws UnsupportedOperationException. The reason for this behavior is clear - DFS does not support random updates and so seek() method can't be supported (at least not easily).
 
Well, if we can't support random updates - the question is: do we really need them? Search in the Lucene code revealed 2 places which call IndexOutput.seek() method: one is in TermInfosWriter and another one in CompoundFileWriter. As we weren't planning to use CompoundFileWriter - the only place that concerned us was in TermInfosWriter.
 
TermInfosWriter uses IndexOutput.seek() in its close() method to write total number of terms in the file back into the beginning of the file. It was very simple to change file format a little bit and write number of terms into last 8 bytes of the file instead of writing them into beginning of file. The only other place that should be fixed in order for this to work is in SegmentTermEnum constructor - to read this piece of information at position = file length - 8.
 
With this format hack - we were able to use FsDirectory to write index directly to DFS without any problems. Well - we still don't index directly to DFS for performance reasons, but at least we can build small local indexes and merge them into the main index on DFS without copying big main index back and forth. 

"
"LUCENE-1073","TEST","IMPROVEMENT","Add unit test showing how to do a ""live backup"" of an index","The question of how to backup an index comes up every so often on the
lists.  Backing up and index is also clearly an important fundamental
admin task that many applications need to do for fault tolerance.

In the past you were forced to stop & block all changes to your index,
perform the backup, and then resume changes.  But many applications
cannot afford a potentially long pause in their indexing.

With the addition of DeletionPolicy (LUCENE-710), it's now possible to
do a ""live backup"", which means backup your index in the background
without pausing ongoing changes to the index.  This
SnapshotDeletionPolicy just has to mark the chosen commit point as not
deletable, until the backup finishes.
"
"LUCENE-3351","BUG","BUG","DirectSpellChecker throws NPE if field doesn't exist","DirectSpellchecker doesn't check that the resulting Terms is null,
it should return an empty list here."
"LUCENE-3484","BUG","BUG","TaxonomyWriter parents array creation is not thread safe, can cause NPE","Following user list thread [TaxWriter leakage? | http://markmail.org/thread/jkkhemfzpnbdzoft] it appears that if two threads or more are asking for the parent array for the first time, a context switch after the first thread created the empty parents array but before it initialized it would cause the other array to use an uninitialized array, causing an NPE. Fix is simple: synchronize the method getParentArray()"
"LUCENE-3661","IMPROVEMENT","TASK","move deletes under codec","After LUCENE-3631, this should be easier I think.

I haven't looked at it much myself but i'll play around a bit, but at a glance:
* SegmentReader to have Bits liveDocs instead of BitVector
* address the TODO in the IW-using ctors so that SegmentReader doesn't take a parent but just an existing core.
* we need some type of minimal ""MutableBits"" or similar subinterface of bits. BitVector and maybe Fixed/OpenBitSet could implement it
* BitVector becomes an impl detail and moves to codec (maybe we have a shared base class and split the 3.x/4.x up rather than the conditional backwards)
* I think the invertAll should not be used by IndexWriter, instead we define the codec interface to say ""give me a new MutableBits, by default all are set"" ?
* redundant internally-consistent checks in checkLiveCounts should be done in the codec impl instead of in SegmentReader.
* plain text impl in SimpleText."
"LUCENE-1634","IMPROVEMENT","IMPROVEMENT","LogMergePolicy should use the number of deleted docs when deciding which segments to merge","I found that IndexWriter.optimize(int) method does not pick up large segments with a lot of deletes even when most of the docs are deleted. And the existence of such segments affected the query performance significantly.

I created an index with 1 million docs, then went over all docs and updated a few thousand at a time.  I ran optimize(20) occasionally. What saw were large segments with most of docs deleted. Although these segments did not have valid docs they remained in the directory for a very long time until more segments with comparable or bigger sizes were created.

This is because LogMergePolicy.findMergeForOptimize uses the size of segments but does not take the number of deleted documents into consideration when it decides which segments to merge. So, a simple fix is to use the delete count to calibrate the segment size. I can create a patch for this.

"
"LUCENE-2715","IMPROVEMENT","IMPROVEMENT","optimize fuzzytermsenum per-segment","we can make fuzzyquery about 3% faster by not creating DFA(s) for each segment.

creating the DFAs is still somewhat heavy: i can address this here too, but this is easy."
"LUCENE-2711","IMPROVEMENT","IMPROVEMENT","BooleanScorer.nextDoc should also delegate to sub-scorer's bulk scoring method","BooleanScorer uses the bulk score methods of its sub scorers, asking them to score each chunk of 2048 docs.

However, its .nextDoc fails to do this, instead manually walking through the sub's docs (calling .nextDoc()), which is slower (though this'd be tiny in practice).

As far as I can tell it should delegate to the bulk scorer just like it does in its bulk scorer method."
"LUCENE-3881","RFE","RFE","Create UAX29URLEmailAnalyzer: a standard analyzer that recognizes URLs and emails","This Analyzer should contain the same components as StandardAnalyzer, except for the tokenizer, which should be UAX29URLEmailTokenizer instead of StandardTokenizer."
"LUCENE-3738","BUG","BUG","Be consistent about negative vInt/vLong","Today, write/readVInt ""allows"" a negative int, in that it will encode and decode correctly, just horribly inefficiently (5 bytes).

However, read/writeVLong fails (trips an assert).

I'd prefer that both vInt/vLong trip an assert if you ever try to write a negative number... it's badly trappy today.  But, unfortunately, we sometimes rely on this... had we had this assert in 'since the beginning' we could have avoided that.

So, if we can't add that assert in today, I think we should at least fix readVLong to handle negative longs... but then you quietly spend 9 bytes (even more trappy!)."
"LUCENE-3139","TEST","TEST","LuceneTestCase.afterClass does not print enough information if a temp-test-dir fails to delete","I've hit an exception from LTC.afterClass when _TestUtil.rmDir failed (on write.lock, as if some test did not release resources). However, I had no idea which test caused that (i.e. opened the temp directory and did not release resources).

I think we should do the following:
* Track in LTC a map from dirName -> StackTraceElement
* In afterClass if _TestUtil.rmDir fails, print the STE of that particular dir, so we know where was this directory created from
* Make tempDirs private and create accessor method, so that we control the inserts to this map (today the Set is updated by LTC, _TestUtils and TestBackwards !)"
"LUCENE-2960","RFE","IMPROVEMENT","Allow (or bring back) the ability to setRAMBufferSizeMB on an open IndexWriter","In 3.1 the ability to setRAMBufferSizeMB is deprecated, and removed in trunk. It would be great to be able to control that on a live IndexWriter. Other possible two methods that would be great to bring back are setTermIndexInterval and setReaderTermsIndexDivisor. Most of the other setters can actually be set on the MergePolicy itself, so no need for setters for those (I think)."
"LUCENE-3698","BUG","BUG","FastVectorHighlighter adds a multi value separator (space) to the end of the highlighted text","The FVH adds an additional ' ' (the multi value separator) to the end of the highlighted text."
"LUCENE-2194","IMPROVEMENT","IMPROVEMENT","improve efficiency of snowballfilter","snowball stemming currently creates 2 new strings and 1 new stringbuilder for every word.

all of this is unnecessary, so don't do it.
"
"LUCENE-1662","REFACTORING","IMPROVEMENT","consolidate FieldCache and ExtendedFieldCache instances","It's confusing and error prone having two instances of FieldCache... FieldCache .DEFAULT and ExtendedFieldCache .EXT_DEFAULT.
Accidentally use the wrong one and you silently double the memory usage for that field.  Since ExtendedFieldCache extends FieldCache, there's no reason not to share the same instance across both."
"LUCENE-3175","TEST","TEST","speed up core tests","Our core tests have gotten slower and slower, if you don't have a really fast computer its probably frustrating.

I think we should:
1. still have random parameters, but make the 'obscene' settings like SimpleText rarer... we can always make them happen more on NIGHTLY
2. tests that make a lot of documents can conditionalize on NIGHTLY so that they are still doing a reasonable test on ordinary runs e.g. numdocs = (NIGHTLY ? 10000 : 1000) * multiplier
3. refactor some of the slow huge classes with lots of tests like TestIW/TestIR, at least pull out really slow methods like TestIR.testDiskFull into its own class. this gives better parallelization.
"
"LUCENE-2638","DESIGN_DEFECT","IMPROVEMENT","Make HighFreqTerms.TermStats class public","It's not possible to use public methods in contrib/misc/... /HighFreqTerms from outside the package because the return type has package visibility. I propose to move TermStats class to a separate file and make it public."
"LUCENE-2226","REFACTORING","TASK","move contrib/snowball to contrib/analyzers","to fix bugs in some duplicate, handcoded impls of these stemmers (nl, fr, ru, etc) we should simply merge snowball and analyzers, and replace the buggy impls with the proper snowball stemfilters.
"
"LUCENE-3656","IMPROVEMENT","BUG","IndexReader's add/removeCloseListener should not use ConcurrentHashMap, just a synchronized set","The use-case for ConcurrentHashMap is when many threads are reading and less writing to the structure. Here this is just funny: The only reader is close(). Here you can just use a synchronized HashSet. The complexity of CHM is making this just a joke :-)"
"LUCENE-495","BUG","IMPROVEMENT","Suggested Patches to MultiPhraseQuery and QueryTermExtractor (for use with HighLighter)","I encountered a problem with the Highlighter, where it was not recognizing MultiPhraseQuery.
To fix this, I developed the following two patches:

=====================================================
1. Addition to org.apache.lucene.search.MultiPhraseQuery:

Add the following method:

/** Returns the set of terms in this phrase. */
public Term[] getTerms() {
  ArrayList allTerms = new ArrayList();
  Iterator iterator = termArrays.iterator();
  while (iterator.hasNext()) {
    Term[] terms = (Term[])iterator.next();
    for (int i = 0, n = terms.length; i < n; ++i) {
      allTerms.add(terms[i]);
    }
  }
  return (Term[])allTerms.toArray(new Term[0]);
}

=====================================================
2. Patch to org.apache.lucene.search.highlight.QueryTermExtractor:

a) Add the following import:
import org.apache.lucene.search.MultiPhraseQuery;

b) Add the following code to the end of the getTerms(...) method:
      else  if(query instanceof MultiPhraseQuery)
              getTermsFromMultiPhraseQuery((MultiPhraseQuery) query, terms, fieldName);
  }

c) Add the following method:
 private static final void getTermsFromMultiPhraseQuery(MultiPhraseQuery query, HashSet terms, String fieldName)
 {
   Term[] queryTerms = query.getTerms();
   int i;

   for (i = 0; i < queryTerms.length; i++)
   {
       if((fieldName==null)||(queryTerms[i].field()==fieldName))
       {
           terms.add(new WeightedTerm(query.getBoost(),queryTerms[i].text()));
       }
   }
 }


=====================================================

Can the team update the repository?

Thanks
Michael Harhen "
"LUCENE-1490","BUG","BUG","CJKTokenizer convert   HALFWIDTH_AND_FULLWIDTH_FORMS wrong","CJKTokenizer have these lines..
                if (ub == Character.UnicodeBlock.HALFWIDTH_AND_FULLWIDTH_FORMS) {
                    /** convert  HALFWIDTH_AND_FULLWIDTH_FORMS to BASIC_LATIN */
                    int i = (int) c;
                    i = i - 65248;
                    c = (char) i;
                }

This is wrong. Some character in the block (e.g. U+ff68) have no BASIC_LATIN counterparts.
Only 65281-65374 can be converted this way.

The fix is

             if (ub == Character.UnicodeBlock.HALFWIDTH_AND_FULLWIDTH_FORMS && i <= 65474 && i> 65281) {
                    /** convert  HALFWIDTH_AND_FULLWIDTH_FORMS to BASIC_LATIN */
                    int i = (int) c;
                    i = i - 65248;
                    c = (char) i;
                }"
"LUCENE-352","BUG","BUG","[PATCH] NullPointerException when using nested SpanOrQuery in SpanNotQuery","Overview description: 
I'm using the span query classes in Lucene to generate higher scores for 
search results where the search terms are closer together. In certain 
situations I want to exclude terms from the span. When I attempt to exclude 
more than one term I get an error. 
 
The example query I'm using is:  
 
'brighton AND tourism' -pier -contents 
 
I construct the query objects and the toString() version is: 
 
spanNot(spanNear([contents:brighton contents:tourism], 10, false), 
spanOr([contents:pier, contents:road])) 
  
 
Steps to reproduce: 
1. Construct a SpanNearQuery (must have at least one term, but at least two 
makes more sense) 
2. Construct a SpanOrQuery containing two or more terms 
3. Construct a SpanNotQuery to include the first query object and exclude the 
second (SpanOrQuery) 
4. Execute the search 
 
 
Actual Results: 
A null pointer exception is thrown while generating the scores within the 
search. 
 
Stack trace:  
java.lang.NullPointerException   
        at   
org.apache.lucene.search.spans.SpanOrQuery$1.doc(SpanOrQuery.java:174)   
        at   
org.apache.lucene.search.spans.SpanNotQuery$1.next(SpanNotQuery.java:75)   
        at org.apache.lucene.search.spans.SpanScorer.next(SpanScorer.java:50)   
        at org.apache.lucene.search.Scorer.score(Scorer.java:37)   
        at   
org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:92)   
        at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:64)   
        at org.apache.lucene.search.Hits.<init>(Hits.java:43)   
        at org.apache.lucene.search.Searcher.search(Searcher.java:33)   
        at org.apache.lucene.search.Searcher.search(Searcher.java:27)   
        at   
com.runtimecollective.search.LuceneSearch.search(LuceneSearch.java:362)   
 
 
Expected Resuts: 
It executes the search and results where the first search terms (near query) 
are close together but without the second terms (or query) appearing."
"LUCENE-2188","RFE","RFE","A handy utility class for tracking deprecated overridden methods","This issue provides a new handy utility class that keeps track of overridden deprecated methods in non-final sub classes. This class can be used in new deprecations.

See the javadocs for an example."
"LUCENE-1127","RFE","IMPROVEMENT","TokenSources.getTokenStream(Document...) ","Sometimes, one already has the Document, and just needs to generate a TokenStream from it, so I am going to add a convenience method to TokenSources.  Sometimes, you also already have just the string, so I will add a convenience method for that."
"LUCENE-1210","BUG","BUG","IndexWriter & ConcurrentMergeScheduler deadlock case if starting a merge hits an exception","If you're using CMS (the default) and mergeInit hits an exception (eg
OOME), we are not properly clearing IndexWriter's internal tracking of
running merges.  This causes IW.close() to hang while it incorrectly
waits for these non-started merges to finish.

"
"LUCENE-1835","REFACTORING","TASK","Signature changes in AttributeSource for better Generics support of AddAttribute/getAttribute","The last update of Attribute API using AttributeImpl as implementation oif Attributes changed the API a little bit. This change leads to the fact, that in Java 1.5 using generics we are no longer able to add Attributes without casting. addAttribute and getAttribute should return the Attribute interface because the implementation of the attribute is not interesting to the caller. By that in 1.5 using generics, one could add a TermAttribute without casting using:
{code}
TermAttribute termAtt = addAttribute(TermAttribute.class);
{code}
The signature to do this is:
{code}
public <T extends Attribute> T addAttribute(Class<T>)
{code}

The attached patch applies the mentioned change to the signature (without generic, only returning Attribute). No other code changes are needed, as current code always casts the result to the requested interface. I also added the 1.5 method signature for all these methods to the javadocs.

All tests pass."
"LUCENE-2273","IMPROVEMENT","BUG","FieldCacheImpl's getCacheEntries() is buggy as it uses WeakHashMap incorrectly and leads to ConcurrentModExceptions","The way how WeakHashMap works internally leads to the fact that it is not allowed to iterate over a WHM.keySet() and then get() the value. As each get() operation inspects the ReferenceQueue of the weak keys, they may suddenly disappear. If you use the entrySet() iterator you get key and value and no need to call get(), contains(),... that inspects the ReferenceQueue."
"LUCENE-2014","BUG","BUG","position increment bug: smartcn","If i use LUCENE_VERSION >= 2.9 with smart chinese analyzer, it will crash indexwriter with any reasonable amount of chinese text.

its especially annoying because it happens in 2.9.1 RC as well.

this is because the position increments for tokens after stopwords are bogus:

Here's an example (from test case), where the position increment should be 2, but is instead 91975314!

{code}
  public void testChineseStopWords2() throws Exception {
    Analyzer ca = new SmartChineseAnalyzer(Version.LUCENE_CURRENT); /* will load stopwords */
    String sentence = ""Title:San""; // : is a stopword
    String result[] = { ""titl"", ""san""};
    int startOffsets[] = { 0, 6 };
    int endOffsets[] = { 5, 9 };
    int posIncr[] = { 1, 2 };
    assertAnalyzesTo(ca, sentence, result, startOffsets, endOffsets, posIncr);
  }
{code}

junit.framework.AssertionFailedError: posIncrement 1 expected:<2> but was:<91975314>
	at junit.framework.Assert.fail(Assert.java:47)
	at junit.framework.Assert.failNotEquals(Assert.java:280)
	at junit.framework.Assert.assertEquals(Assert.java:64)
	at junit.framework.Assert.assertEquals(Assert.java:198)
	at org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:83)
	...




"
"LUCENE-2385","REFACTORING","IMPROVEMENT","Move NoDeletionPolicy from benchmark to core","As the subject says, but I'll also make it a singleton + add some unit tests, as well as some documentation. I'll post a patch hopefully today."
"LUCENE-906","RFE","RFE","Elision filter for simple french analyzing","If you don't wont to use stemming, StandardAnalyzer miss some french strangeness like elision.
""l'avion"" wich means ""the plane"" must be tokenized as ""avion"" (plane).
This filter could be used with other latin language if elision exists."
"LUCENE-1660","IMPROVEMENT","IMPROVEMENT","Make StopFilter.enablePositionIncrements explicit","I think the default for this should be true, ie, do not lose
information when filtering (preserve the positions of the original
tokens).

But, we can't change this without breaking back-compat.

So, as workaround, we should make the parameter explicit so one must
decide up front.
"
"LUCENE-2500","RFE","IMPROVEMENT","A Linux-specific Directory impl that bypasses the buffer cache","I've been testing how we could prevent Lucene's merges from evicting
pages from the OS's buffer cache.  I tried fadvise/madvise (via JNI)
but (frustratingly), I could not get them to work (details at
http://chbits.blogspot.com/2010/06/lucene-and-fadvisemadvise.html).

The only thing that worked was to use Linux's O_DIRECT flag, which
forces all IO to bypass the buffer cache entirely... so I created a
Linux-specific Directory impl to do this.
"
"LUCENE-3895","TEST","BUG","Not getting random-seed/reproduce-with if a test fails from another thread","See https://builds.apache.org/job/Lucene-Solr-tests-only-trunk/12822/console as an example.

This is at least affecting 4.0, maybe 3.x too"
"LUCENE-930","BUILD_SYSTEM","BUG","fail build if contrib tests fail to compile","spinoff of LUCENE-885, from Steven's comments...

Looking at the current build (r545324) it looks like the some contrib failures are getting swallowed. Things like lucli are throwing errors along the lines of

 [subant] /home/barronpark/smparkes/work/lucene/trunk/common-build.xml:366: srcdir ""/home/barronpark/smparkes/work/lucene/trunk/contrib/lucli/src/test"" does not exist!

but these don't make it back up to the top level status.

It looks like the current state will bubble up junit failures, but maybe not build failures?

...

It's ""test-compile-contrib"" (if you will) that fails and rather being contrib-crawled, that's only done as the target of ""test"" in each contrib directory, at which point, it's running in the protected contrib-crawl.

Easy enough to lift this loop into another target, e.g., build-contrib-test. And that will start surfacing errors, which I can work through.
"
"LUCENE-3035","BUG","BUG","TestIndexWriter.testCommitThreadSafety fails on realtime_search branch","Hudson failed on RT with this error - I wasn't able to reproduce yet....

{noformat}
NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testCommitThreadSafety -Dtests.seed=410261592077577885:-4099127561715488589 -Dtests.multiplier=3
NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testCommitThreadSafety -Dtests.seed=410261592077577885:-4099127561715488589 -Dtests.multiplier=3
The following exceptions were thrown by threads:
*** Thread: Thread-331 ***
java.lang.RuntimeException: java.lang.AssertionError: term=f:2_0; r=DirectoryReader(segments_6 _8(4.0):Cv7) expected:<1> but was:<0>
	at org.apache.lucene.index.TestIndexWriter$5.run(TestIndexWriter.java:2416)
Caused by: java.lang.AssertionError: term=f:2_0; r=DirectoryReader(segments_6 _8(4.0):Cv7) expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:91)
	at org.junit.Assert.failNotEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:126)
	at org.junit.Assert.assertEquals(Assert.java:470)
	at org.apache.lucene.index.TestIndexWriter$5.run(TestIndexWriter.java:2410)
NOTE: test params are: codec=RandomCodecProvider: {=SimpleText, f6=MockVariableIntBlock(baseBlockSize=91), f7=MockFixedIntBlock(blockSize=1289), f8=Standard, f9=MockRandom, f1=MockSep, f0=Pulsing(freqCutoff=15), f3=Pulsing(freqCutoff=15), f2=MockFixedIntBlock(blockSize=1289), f5=MockVariableIntBlock(baseBlockSize=91), f4=MockRandom, f=MockSep, c=MockVariableIntBlock(baseBlockSize=91), termVector=SimpleText, d9=SimpleText, d8=MockSep, d5=MockVariableIntBlock(baseBlockSize=91), d4=MockRandom, d7=Standard, d6=SimpleText, d25=Standard, d0=MockVariableIntBlock(baseBlockSize=91), c29=Standard, d24=SimpleText, d1=MockFixedIntBlock(blockSize=1289), c28=MockFixedIntBlock(blockSize=1289), d23=MockVariableIntBlock(baseBlockSize=91), d2=Standard, c27=MockVariableIntBlock(baseBlockSize=91), d22=MockRandom, d3=MockRandom, d21=MockFixedIntBlock(blockSize=1289), d20=MockVariableIntBlock(baseBlockSize=91), c22=MockVariableIntBlock(baseBlockSize=91), c21=MockRandom, c20=Pulsing(freqCutoff=15), d29=MockVariableIntBlock(baseBlockSize=91), c26=SimpleText, d28=MockRandom, c25=MockSep, d27=Pulsing(freqCutoff=15), c24=MockRandom, d26=MockFixedIntBlock(blockSize=1289), c23=Standard, e9=MockRandom, e8=MockFixedIntBlock(blockSize=1289), e7=MockVariableIntBlock(baseBlockSize=91), e6=MockSep, e5=Pulsing(freqCutoff=15), c17=Standard, e3=MockFixedIntBlock(blockSize=1289), d12=SimpleText, c16=SimpleText, e4=Pulsing(freqCutoff=15), d11=MockSep, c19=MockSep, e1=MockSep, d14=Pulsing(freqCutoff=15), c18=Pulsing(freqCutoff=15), e2=SimpleText, d13=MockFixedIntBlock(blockSize=1289), e0=Standard, d10=Standard, d19=Pulsing(freqCutoff=15), c11=SimpleText, c10=MockSep, d16=MockRandom, c13=MockSep, c12=Pulsing(freqCutoff=15), d15=Standard, d18=SimpleText, c15=MockFixedIntBlock(blockSize=1289), d17=MockSep, c14=MockVariableIntBlock(baseBlockSize=91), b3=MockRandom, b2=Standard, b5=SimpleText, b4=MockSep, b7=MockSep, b6=Pulsing(freqCutoff=15), d50=MockVariableIntBlock(baseBlockSize=91), b9=MockFixedIntBlock(blockSize=1289), b8=MockVariableIntBlock(baseBlockSize=91), d43=Pulsing(freqCutoff=15), d42=MockFixedIntBlock(blockSize=1289), d41=SimpleText, d40=MockSep, d47=MockRandom, d46=Standard, b0=SimpleText, d45=MockFixedIntBlock(blockSize=1289), b1=Standard, d44=MockVariableIntBlock(baseBlockSize=91), d49=MockSep, d48=Pulsing(freqCutoff=15), c6=MockVariableIntBlock(baseBlockSize=91), c5=MockRandom, c4=Pulsing(freqCutoff=15), c3=MockFixedIntBlock(blockSize=1289), c9=MockSep, c8=MockRandom, c7=Standard, d30=MockFixedIntBlock(blockSize=1289), d32=MockRandom, d31=Standard, c1=MockVariableIntBlock(baseBlockSize=91), d34=Standard, c2=MockFixedIntBlock(blockSize=1289), d33=SimpleText, d36=MockSep, c0=MockSep, d35=Pulsing(freqCutoff=15), d38=MockVariableIntBlock(baseBlockSize=91), d37=MockRandom, d39=SimpleText, e92=MockFixedIntBlock(blockSize=1289), e93=Pulsing(freqCutoff=15), e90=MockSep, e91=SimpleText, e89=MockVariableIntBlock(baseBlockSize=91), e88=MockSep, e87=Pulsing(freqCutoff=15), e86=SimpleText, e85=MockSep, e84=MockRandom, e83=Standard, e80=MockFixedIntBlock(blockSize=1289), e81=Standard, e82=MockRandom, e77=MockVariableIntBlock(baseBlockSize=91), e76=MockRandom, e79=Standard, e78=SimpleText, e73=MockSep, e72=Pulsing(freqCutoff=15), e75=MockFixedIntBlock(blockSize=1289), e74=MockVariableIntBlock(baseBlockSize=91), binary=MockVariableIntBlock(baseBlockSize=91), f98=Pulsing(freqCutoff=15), f97=MockFixedIntBlock(blockSize=1289), f99=MockRandom, f94=Standard, f93=SimpleText, f96=MockSep, f95=Pulsing(freqCutoff=15), e95=SimpleText, e94=MockSep, e97=Pulsing(freqCutoff=15), e96=MockFixedIntBlock(blockSize=1289), e99=MockFixedIntBlock(blockSize=1289), e98=MockVariableIntBlock(baseBlockSize=91), id=MockRandom, f34=Standard, f33=SimpleText, f32=MockVariableIntBlock(baseBlockSize=91), f31=MockRandom, f30=MockFixedIntBlock(blockSize=1289), f39=Standard, f38=MockVariableIntBlock(baseBlockSize=91), f37=MockRandom, f36=Pulsing(freqCutoff=15), f35=MockFixedIntBlock(blockSize=1289), f43=MockSep, f42=Pulsing(freqCutoff=15), f45=MockFixedIntBlock(blockSize=1289), f44=MockVariableIntBlock(baseBlockSize=91), f41=SimpleText, f40=MockSep, f47=Standard, f46=SimpleText, f49=MockSep, f48=Pulsing(freqCutoff=15), content=MockSep, e19=SimpleText, e18=MockSep, e17=Standard, f12=SimpleText, e16=SimpleText, f11=MockSep, f10=MockRandom, e15=MockVariableIntBlock(baseBlockSize=91), e14=MockRandom, f16=MockRandom, e13=MockSep, f15=Standard, e12=Pulsing(freqCutoff=15), e11=Standard, f14=MockFixedIntBlock(blockSize=1289), e10=SimpleText, f13=MockVariableIntBlock(baseBlockSize=91), f19=Pulsing(freqCutoff=15), f18=Standard, f17=SimpleText, e29=MockRandom, e26=MockSep, f21=Pulsing(freqCutoff=15), e25=Pulsing(freqCutoff=15), f20=MockFixedIntBlock(blockSize=1289), e28=MockFixedIntBlock(blockSize=1289), f23=MockVariableIntBlock(baseBlockSize=91), e27=MockVariableIntBlock(baseBlockSize=91), f22=MockRandom, f25=SimpleText, e22=MockFixedIntBlock(blockSize=1289), f24=MockSep, e21=MockVariableIntBlock(baseBlockSize=91), f27=Pulsing(freqCutoff=15), e24=MockRandom, f26=MockFixedIntBlock(blockSize=1289), e23=Standard, f29=MockFixedIntBlock(blockSize=1289), f28=MockVariableIntBlock(baseBlockSize=91), e20=Pulsing(freqCutoff=15), field=MockRandom, string=Standard, e30=MockRandom, e31=MockVariableIntBlock(baseBlockSize=91), a98=Standard, e34=MockSep, a99=MockRandom, e35=SimpleText, f79=MockSep, e32=Standard, e33=MockRandom, b97=MockRandom, f77=MockRandom, e38=Standard, b98=MockVariableIntBlock(baseBlockSize=91), f78=MockVariableIntBlock(baseBlockSize=91), e39=MockRandom, b99=SimpleText, f75=MockFixedIntBlock(blockSize=1289), e36=MockVariableIntBlock(baseBlockSize=91), f76=Pulsing(freqCutoff=15), e37=MockFixedIntBlock(blockSize=1289), f73=Pulsing(freqCutoff=15), f74=MockSep, f71=SimpleText, f72=Standard, f81=MockFixedIntBlock(blockSize=1289), f80=MockVariableIntBlock(baseBlockSize=91), e40=Standard, e41=Pulsing(freqCutoff=15), e42=MockSep, e43=MockFixedIntBlock(blockSize=1289), e44=Pulsing(freqCutoff=15), e45=MockRandom, e46=MockVariableIntBlock(baseBlockSize=91), f86=SimpleText, e47=MockSep, f87=Standard, e48=SimpleText, f88=Pulsing(freqCutoff=15), e49=MockFixedIntBlock(blockSize=1289), f89=MockSep, f82=MockVariableIntBlock(baseBlockSize=91), f83=MockFixedIntBlock(blockSize=1289), f84=Standard, f85=MockRandom, f90=MockRandom, f92=SimpleText, f91=MockSep, str=MockSep, a76=SimpleText, e56=SimpleText, f59=MockVariableIntBlock(baseBlockSize=91), a77=Standard, e57=Standard, a78=Pulsing(freqCutoff=15), e54=MockRandom, f57=Pulsing(freqCutoff=15), a79=MockSep, e55=MockVariableIntBlock(baseBlockSize=91), f58=MockSep, e52=MockVariableIntBlock(baseBlockSize=91), e53=MockFixedIntBlock(blockSize=1289), e50=Pulsing(freqCutoff=15), e51=MockSep, f51=MockFixedIntBlock(blockSize=1289), f52=Pulsing(freqCutoff=15), f50=SimpleText, f55=Standard, f56=MockRandom, f53=MockVariableIntBlock(baseBlockSize=91), e58=MockFixedIntBlock(blockSize=1289), f54=MockFixedIntBlock(blockSize=1289), e59=Pulsing(freqCutoff=15), a80=MockRandom, e60=MockRandom, a82=SimpleText, a81=MockSep, a84=MockSep, a83=Pulsing(freqCutoff=15), a86=MockFixedIntBlock(blockSize=1289), a85=MockVariableIntBlock(baseBlockSize=91), a89=Standard, f68=Standard, e65=Pulsing(freqCutoff=15), f69=MockRandom, e66=MockSep, a87=MockVariableIntBlock(baseBlockSize=91), e67=MockVariableIntBlock(baseBlockSize=91), a88=MockFixedIntBlock(blockSize=1289), e68=MockFixedIntBlock(blockSize=1289), e61=Standard, e62=MockRandom, e63=MockSep, e64=SimpleText, f60=MockRandom, f61=MockVariableIntBlock(baseBlockSize=91), f62=SimpleText, f63=Standard, e69=SimpleText, f64=MockSep, f65=SimpleText, f66=MockFixedIntBlock(blockSize=1289), f67=Pulsing(freqCutoff=15), f70=MockSep, a93=MockVariableIntBlock(baseBlockSize=91), a92=MockRandom, a91=Pulsing(freqCutoff=15), e71=Pulsing(freqCutoff=15), a90=MockFixedIntBlock(blockSize=1289), e70=MockFixedIntBlock(blockSize=1289), a97=SimpleText, a96=MockSep, a95=MockRandom, a94=Standard, c58=MockFixedIntBlock(blockSize=1289), a63=MockVariableIntBlock(baseBlockSize=91), a64=MockFixedIntBlock(blockSize=1289), c59=Pulsing(freqCutoff=15), c56=MockSep, d59=MockRandom, a61=Pulsing(freqCutoff=15), c57=SimpleText, a62=MockSep, c54=SimpleText, c55=Standard, a60=SimpleText, c52=MockRandom, c53=MockVariableIntBlock(baseBlockSize=91), d53=Standard, d54=MockRandom, d51=MockVariableIntBlock(baseBlockSize=91), d52=MockFixedIntBlock(blockSize=1289), d57=Pulsing(freqCutoff=15), b62=MockFixedIntBlock(blockSize=1289), d58=MockSep, b63=Pulsing(freqCutoff=15), d55=SimpleText, b60=MockSep, d56=Standard, b61=SimpleText, b56=SimpleText, b55=MockSep, b54=MockRandom, b53=Standard, d61=SimpleText, b59=MockVariableIntBlock(baseBlockSize=91), d60=MockSep, b58=MockSep, b57=Pulsing(freqCutoff=15), c62=MockSep, c61=Pulsing(freqCutoff=15), a59=Pulsing(freqCutoff=15), c60=Standard, a58=MockFixedIntBlock(blockSize=1289), a57=MockSep, a56=Pulsing(freqCutoff=15), a55=Standard, a54=SimpleText, a72=Standard, c67=MockRandom, a73=MockRandom, c68=MockVariableIntBlock(baseBlockSize=91), a74=MockSep, c69=SimpleText, a75=SimpleText, c63=Pulsing(freqCutoff=15), c64=MockSep, a70=MockRandom, c65=MockVariableIntBlock(baseBlockSize=91), a71=MockVariableIntBlock(baseBlockSize=91), c66=MockFixedIntBlock(blockSize=1289), d62=MockSep, d63=SimpleText, d64=MockFixedIntBlock(blockSize=1289), b70=MockFixedIntBlock(blockSize=1289), d65=Pulsing(freqCutoff=15), b71=MockRandom, d66=MockVariableIntBlock(baseBlockSize=91), b72=MockVariableIntBlock(baseBlockSize=91), d67=MockFixedIntBlock(blockSize=1289), b73=SimpleText, d68=Standard, b74=Standard, d69=MockRandom, b65=Pulsing(freqCutoff=15), b64=MockFixedIntBlock(blockSize=1289), b67=MockVariableIntBlock(baseBlockSize=91), b66=MockRandom, d70=Pulsing(freqCutoff=15), b69=MockRandom, b68=Standard, d72=MockVariableIntBlock(baseBlockSize=91), d71=MockRandom, c71=MockFixedIntBlock(blockSize=1289), c70=MockVariableIntBlock(baseBlockSize=91), a69=SimpleText, c73=MockRandom, c72=Standard, a66=MockFixedIntBlock(blockSize=1289), a65=MockVariableIntBlock(baseBlockSize=91), a68=MockRandom, a67=Standard, c32=MockSep, c33=SimpleText, c30=Standard, c31=MockRandom, c36=MockVariableIntBlock(baseBlockSize=91), a41=MockRandom, c37=MockFixedIntBlock(blockSize=1289), a42=MockVariableIntBlock(baseBlockSize=91), a0=SimpleText, c34=Pulsing(freqCutoff=15), c35=MockSep, a40=Pulsing(freqCutoff=15), b84=Pulsing(freqCutoff=15), d79=MockSep, b85=MockSep, b82=SimpleText, d77=Standard, c38=SimpleText, b83=Standard, d78=MockRandom, c39=Standard, b80=Standard, d75=MockRandom, b81=MockRandom, d76=MockVariableIntBlock(baseBlockSize=91), d73=MockFixedIntBlock(blockSize=1289), d74=Pulsing(freqCutoff=15), d83=Standard, a9=MockSep, d82=SimpleText, d81=MockVariableIntBlock(baseBlockSize=91), d80=MockRandom, b79=MockSep, b78=Standard, b77=SimpleText, b76=MockVariableIntBlock(baseBlockSize=91), b75=MockRandom, a1=SimpleText, a35=Pulsing(freqCutoff=15), a2=Standard, a34=MockFixedIntBlock(blockSize=1289), a3=Pulsing(freqCutoff=15), a33=SimpleText, a4=MockSep, a32=MockSep, a5=MockFixedIntBlock(blockSize=1289), a39=MockRandom, c40=Pulsing(freqCutoff=15), a6=Pulsing(freqCutoff=15), a38=Standard, a7=MockRandom, a37=MockFixedIntBlock(blockSize=1289), a8=MockVariableIntBlock(baseBlockSize=91), a36=MockVariableIntBlock(baseBlockSize=91), c41=MockFixedIntBlock(blockSize=1289), c42=Pulsing(freqCutoff=15), c43=MockRandom, c44=MockVariableIntBlock(baseBlockSize=91), c45=Standard, a50=SimpleText, c46=MockRandom, a51=Standard, c47=MockSep, a52=Pulsing(freqCutoff=15), c48=SimpleText, a53=MockSep, b93=MockVariableIntBlock(baseBlockSize=91), d88=MockFixedIntBlock(blockSize=1289), c49=MockVariableIntBlock(baseBlockSize=91), b94=MockFixedIntBlock(blockSize=1289), d89=Pulsing(freqCutoff=15), b95=Standard, b96=MockRandom, d84=SimpleText, b90=SimpleText, d85=Standard, b91=MockFixedIntBlock(blockSize=1289), d86=Pulsing(freqCutoff=15), b92=Pulsing(freqCutoff=15), d87=MockSep, d92=MockSep, d91=Pulsing(freqCutoff=15), d94=MockFixedIntBlock(blockSize=1289), d93=MockVariableIntBlock(baseBlockSize=91), b87=MockSep, b86=Pulsing(freqCutoff=15), d90=SimpleText, b89=MockFixedIntBlock(blockSize=1289), b88=MockVariableIntBlock(baseBlockSize=91), a44=MockVariableIntBlock(baseBlockSize=91), a43=MockRandom, a46=Standard, a45=SimpleText, a48=SimpleText, a47=MockSep, c51=Standard, a49=MockFixedIntBlock(blockSize=1289), c50=SimpleText, d98=MockFixedIntBlock(blockSize=1289), d97=MockVariableIntBlock(baseBlockSize=91), d96=MockSep, d95=Pulsing(freqCutoff=15), d99=MockRandom, a20=MockRandom, c99=MockVariableIntBlock(baseBlockSize=91), c98=MockRandom, c97=Pulsing(freqCutoff=15), c96=MockFixedIntBlock(blockSize=1289), b19=MockVariableIntBlock(baseBlockSize=91), a16=SimpleText, a17=Standard, b17=Pulsing(freqCutoff=15), a14=MockRandom, b18=MockSep, a15=MockVariableIntBlock(baseBlockSize=91), a12=MockVariableIntBlock(baseBlockSize=91), a13=MockFixedIntBlock(blockSize=1289), a10=Pulsing(freqCutoff=15), a11=MockSep, b11=MockFixedIntBlock(blockSize=1289), b12=Pulsing(freqCutoff=15), b10=SimpleText, b15=Standard, b16=MockRandom, a18=MockFixedIntBlock(blockSize=1289), b13=MockVariableIntBlock(baseBlockSize=91), a19=Pulsing(freqCutoff=15), b14=MockFixedIntBlock(blockSize=1289), b30=MockSep, a31=Pulsing(freqCutoff=15), a30=MockFixedIntBlock(blockSize=1289), b28=Standard, a25=Pulsing(freqCutoff=15), b29=MockRandom, a26=MockSep, a27=MockVariableIntBlock(baseBlockSize=91), a28=MockFixedIntBlock(blockSize=1289), a21=Standard, a22=MockRandom, a23=MockSep, a24=SimpleText, b20=MockRandom, b21=MockVariableIntBlock(baseBlockSize=91), b22=SimpleText, b23=Standard, a29=SimpleText, b24=MockSep, b25=SimpleText, b26=MockFixedIntBlock(blockSize=1289), b27=Pulsing(freqCutoff=15), b41=MockFixedIntBlock(blockSize=1289), b40=MockVariableIntBlock(baseBlockSize=91), c77=MockRandom, c76=Standard, c75=MockFixedIntBlock(blockSize=1289), c74=MockVariableIntBlock(baseBlockSize=91), c79=Standard, c78=SimpleText, c80=MockVariableIntBlock(baseBlockSize=91), c83=MockSep, c84=SimpleText, c81=Standard, b39=MockSep, c82=MockRandom, b37=MockRandom, b38=MockVariableIntBlock(baseBlockSize=91), b35=MockFixedIntBlock(blockSize=1289), b36=Pulsing(freqCutoff=15), b33=Pulsing(freqCutoff=15), b34=MockSep, b31=SimpleText, b32=Standard, str2=Pulsing(freqCutoff=15), b50=MockRandom, b52=SimpleText, str3=MockRandom, b51=MockSep, c86=SimpleText, tvtest=MockSep, c85=MockSep, c88=Pulsing(freqCutoff=15), c87=MockFixedIntBlock(blockSize=1289), c89=MockVariableIntBlock(baseBlockSize=91), c90=Pulsing(freqCutoff=15), c91=MockSep, c92=MockFixedIntBlock(blockSize=1289), c93=Pulsing(freqCutoff=15), c94=MockRandom, c95=MockVariableIntBlock(baseBlockSize=91), content1=MockSep, b46=SimpleText, b47=Standard, content3=Standard, b48=Pulsing(freqCutoff=15), content4=SimpleText, b49=MockSep, content5=MockRandom, b42=MockVariableIntBlock(baseBlockSize=91), b43=MockFixedIntBlock(blockSize=1289), b44=Standard, b45=MockRandom}, locale=lv_LV, timezone=Australia/Lindeman
NOTE: all tests run in this JVM:
[TestNumericTokenStream, TestIndexFileDeleter, TestIndexInput, TestIndexReaderCloneNorms, TestIndexReaderReopen, TestIndexWriter]
NOTE: FreeBSD 8.2-RELEASE amd64/Sun Microsystems Inc. 1.6.0 (64-bit)/cpus=16,threads=1,free=44228576,total=213778432
{noformat}"
"LUCENE-3713","BUG","BUG","TestIndexWriterOnDiskFull.testAddIndexOnDiskFull fails with java.lang.IllegalStateException: CFS has pending open files ","{noformat}
 Testsuite: org.apache.lucene.index.TestIndexWriterOnDiskFull
    [junit] Testcase: testAddIndexOnDiskFull(org.apache.lucene.index.TestIndexWriterOnDiskFull):	Caused an ERROR
    [junit] CFS has pending open files
    [junit] java.lang.IllegalStateException: CFS has pending open files
    [junit] 	at org.apache.lucene.store.CompoundFileWriter.close(CompoundFileWriter.java:162)
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.close(CompoundFileDirectory.java:206)
    [junit] 	at org.apache.lucene.index.IndexWriter.createCompoundFile(IndexWriter.java:4099)
    [junit] 	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3661)
    [junit] 	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3260)
    [junit] 	at org.apache.lucene.index.SerialMergeScheduler.merge(SerialMergeScheduler.java:37)
    [junit] 	at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1902)
    [junit] 	at org.apache.lucene.index.IndexWriter.forceMerge(IndexWriter.java:1716)
    [junit] 	at org.apache.lucene.index.IndexWriter.forceMerge(IndexWriter.java:1670)
    [junit] 	at org.apache.lucene.index.TestIndexWriterOnDiskFull.testAddIndexOnDiskFull(TestIndexWriterOnDiskFull.java:304)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:529)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 
    [junit] 
    [junit] Tests run: 4, Failures: 0, Errors: 1, Time elapsed: 31.96 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterOnDiskFull -Dtestmethod=testAddIndexOnDiskFull -Dtests.seed=-7dd066d256827211:127c018cbf5b0975:20481cd18a7d8b6e -Dtests.multiplier=3 -Dtests.nightly=true -Dargs=""-Dfile.encoding=ISO8859-1""
    [junit] NOTE: test params are: codec=SimpleText, sim=RandomSimilarityProvider(queryNorm=true,coord=false): {field=DFR GB1, id=DFR I(F)L1, content=IB SPL-D3(800.0), f=DFR G2}, locale=de_AT, timezone=America/Cambridge_Bay
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestAssertions, TestSearchForDuplicates, TestMockAnalyzer, TestDocValues, TestPerFieldPostingsFormat, TestDocument, TestAddIndexes, TestConcurrentMergeScheduler, TestCrashCausesCorruptIndex, TestDocCount, TestDocumentsWriterDeleteQueue, TestFieldInfos, TestFilterIndexReader, TestFlex, TestIndexInput, TestIndexWriter, TestIndexWriterMergePolicy, TestIndexWriterMerging, TestIndexWriterNRTIsCurrent, TestIndexWriterOnDiskFull]
    [junit] NOTE: FreeBSD 8.2-RELEASE amd64/Sun Microsystems Inc. 1.6.0 (64-bit)/cpus=16,threads=1,free=39156976,total=180748288
{noformat}"
"LUCENE-1230","BUILD_SYSTEM","BUG","Source release files missing the *.pom.template files","The source release files should contain the *.pom.template files, otherwise it is not possible to build the maven artifacts using ""ant generate-maven-artifacts"" from official release files."
"LUCENE-1022","CLEANUP","BUG","IndexWriter.setInfoStream should percolate down to mergePolicy & mergeScheduler","Right now *MergePolicy and *MergeScheduler have their own ad-hoc means
of being verbose about their actions.  We should unify these with
IndexWriter's infoStream.  Thanks to Hoss for suggesting this."
"LUCENE-3533","CLEANUP","TASK","Nuke SpanFilters and CachingSpanFilter (maybe move to sandbox)","SpanFilters are inefficient and OOM easily (they don't scale at all: Create large Lists of Objects for every match, also filtering deleted docs is a pain). Some talks with Grant on Eurocon and also the fact that caching of them is still broken in 3.x (but fixed on trunk) - I assume nobody uses them, so let's nuke them. They are also in wrong package, so standard statement: ""Die, SpanFilters, die!"""
"LUCENE-3403","BUG","BUG","Term vectors missing after addIndexes + optimize","I encountered a problem with addIndexes where term vectors disappeared following optimize(). I wrote a simple test case which demonstrates the problem. The bug appears with both addIndexes() versions, but does not appear if addDocument is called twice, committing changes in between.

I think I tracked the problem down to IndexWriter.mergeMiddle() -- it sets term vectors before merger.merge() was called. In the addDocs case, merger.fieldInfos is already populated, while in the addIndexes case it is empty, hence fieldInfos.hasVectors returns false.

will post a patch shortly."
"LUCENE-2580","BUG","BUG","MultiPhraseQuery throws AIOOBE","See thread ""MultiPhraseQuery throws ArrayIndexOutOfBounds Exception"" on dev@ by Jayendra Patil."
"LUCENE-1183","IMPROVEMENT","IMPROVEMENT","TRStringDistance uses way too much memory (with patch)","The implementation of TRStringDistance is based on version 2.1 of org.apache.commons.lang.StringUtils#getLevenshteinDistance(String, String), which uses an un-optimized implementation of the Levenshtein Distance algorithm (it uses way too much memory). Please see Bug 38911 (http://issues.apache.org/bugzilla/show_bug.cgi?id=38911) for more information.

The commons-lang implementation has been heavily optimized as of version 2.2 (3x speed-up). I have reported the new implementation to TRStringDistance."
"LUCENE-937","IMPROVEMENT","IMPROVEMENT","Make CachingTokenFilter faster","The LinkedList used by CachingTokenFilter is accessed using the get() method. Direct access on a LinkedList is slow and an Iterator should be used instead. For more than a handful of tokens, the difference in speed grows exponentially."
"LUCENE-3072","BUG","BUG","3.1 fileformats out of date","The 3.1 fileformats is missing the change from LUCENE-2811"
"LUCENE-502","IMPROVEMENT","IMPROVEMENT","TermScorer caches values unnecessarily","TermScorer aggressively caches the doc and freq of 32 documents at a time for each term scored.  When querying for a lot of terms, this causes a lot of garbage to be created that's unnecessary.  The SegmentTermDocs from which it retrieves its information doesn't have any optimizations for bulk loading, and it's unnecessary.

In addition, it has a SCORE_CACHE, that's of limited benefit.  It's caching the result of a sqrt that should be placed in DefaultSimilarity, and if you're only scoring a few documents that contain those terms, there's no need to precalculate the SQRT, especially on modern VMs.

Enclosed is a patch that replaces TermScorer with a version that does not cache the docs or feqs.  In the case of a lot of queries, that saves 196 bytes/term, the unnecessary disk IO, and extra SQRTs which adds up."
"LUCENE-1792","BUG","BUG","new QueryParser fails to set AUTO REWRITE for multi-term queries","The old QueryParser defaults to constant score rewrite for Prefix,Fuzzy,Wildcard,TermRangeQuery, but the new one seems not to."
"LUCENE-1954","BUG","BUG","InitiatedIndex: CCE on casting NumericField to Field","An unchecked cast to List<Field> throws a ClassCastException when applied to, for example, a NumericField.
Appearently, this has been fixed trunk, but for a 2.9.1 release, this could be helpful.
The patch can be applied against the 2.9.0 tag."
"LUCENE-1430","BUG","BUG","IndexReader.open(String|File) may incorrectly throw AlreadyClosedException","Spinoff from here:

    http://www.nabble.com/Runtime-exception-when-creating-IndexSearcher-to20226279.html

If you open an IndexSearcher/Reader, passing in String or File, then
closeDirectory is set to true in the reader.

If the index has a single segment, then SegmentReader.get is used to
open the index.  If an IOException is hit in there, the SegmentReader
closes itself and then closes the directory since closeDirectory is
true.

The problem is, the retry logic in SegmentInfos (to look for another
segments_N to try) kicks in and hits an AlreadyClosedException,
masking the original root cause.

Workaround is to separately get the Directory using
FSDirectory.getDirectory, and then instantiate IndexSearcher/Reader
from that.

This manifests as masking the root cause of a corrupted single-segment
index with a confusing AlreadyClosedException.  You could also hit
the false exception if the writer was in the process of committing
(ie, a retry was really needed) or if there is some transient IO
problem opening the index (eg too many open files).
"
"LUCENE-2027","CLEANUP","IMPROVEMENT","Deprecate Directory.touchFile","Lucene doesn't use this method, and, FindBugs reports that FSDirectory's impl shouldn't swallow the returned result from File.setLastModified."
"LUCENE-1150","DESIGN_DEFECT","BUG","The token types of the standard tokenizer is not accessible","The StandardTokenizerImpl not being public, these token types are not accessible :

{code:java}
public static final int ALPHANUM          = 0;
public static final int APOSTROPHE        = 1;
public static final int ACRONYM           = 2;
public static final int COMPANY           = 3;
public static final int EMAIL             = 4;
public static final int HOST              = 5;
public static final int NUM               = 6;
public static final int CJ                = 7;
/**
 * @deprecated this solves a bug where HOSTs that end with '.' are identified
 *             as ACRONYMs. It is deprecated and will be removed in the next
 *             release.
 */
public static final int ACRONYM_DEP       = 8;

public static final String [] TOKEN_TYPES = new String [] {
    ""<ALPHANUM>"",
    ""<APOSTROPHE>"",
    ""<ACRONYM>"",
    ""<COMPANY>"",
    ""<EMAIL>"",
    ""<HOST>"",
    ""<NUM>"",
    ""<CJ>"",
    ""<ACRONYM_DEP>""
};
{code}

So no custom TokenFilter can be based of the token type. Actually even the StandardFilter cannot be writen outside the org.apache.lucene.analysis.standard package.
"
"LUCENE-1986","BUG","BUG","NPE in NearSpansUnordered from PayloadNearQuery","The following query causes a NPE in NearSpansUnordered, and is reproducible with the the attached unit test. The failure occurs on the last document scored.
"
"LUCENE-738","IMPROVEMENT","IMPROVEMENT","read/write .del as d-gaps when the deleted bit vector is sufficiently sparse",".del file of a segment maintains info on deleted documents in that segment. The file exists only for segments having deleted docs, so it does not exists for newly created segments (e.g. resulted from merge). Each time closing an index reader that deleted any document, the .del file is rewritten. In fact, since the lock-less commits change a new (generation of) .del file is created in each such occasion.

For small indexes there is no real problem with current situation. But for very large indexes, each time such an index reader is closed, creating such new bit-vector seems like unnecessary overhead in cases that the bit vector is sparse (just a few docs were deleted). For instance, for an index with a segment of 1M docs, the sequence: {open reader; delete 1 doc from that segment; close reader;} would write a file of ~128KB. Repeat this sequence 8 times: 8 new files of total size of 1MB are written to disk.

Whether this is a bottleneck or not depends on the application deletes pattern, but for the case that deleted docs are sparse, writing just the d-gaps would save space and time. 

I have this (simple) change to BitVector running and currently trying some performance tests to, yet, convince myself on the worthiness of this.

"
"LUCENE-1645","BUG","BUG","Deleted documents are visible across reopened MSRs",""
"LUCENE-455","BUG","BUG","FieldsReader does not regard offset and position flags","When creating a Field the FieldsReader looks at the storeTermVector flag of the FieldInfo. If true Field.TermVector.YES is used as parameter. But it should be checked if storeOffsetWithTermVector and storePositionWithTermVector are set and Field.TermVector.WITH_OFFSETS, ...WITH_POSITIONS, or ...WITH_POSITIONS_OFFSETS should be used as appropriate."
"LUCENE-2392","RFE","IMPROVEMENT","Enable flexible scoring","This is a first step (nowhere near committable!), implementing the
design iterated to in the recent ""Baby steps towards making Lucene's
scoring more flexible"" java-dev thread.

The idea is (if you turn it on for your Field; it's off by default) to
store full stats in the index, into a new _X.sts file, per doc (X
field) in the index.

And then have FieldSimilarityProvider impls that compute doc's boost
bytes (norms) from these stats.

The patch is able to index the stats, merge them when segments are
merged, and provides an iterator-only API.  It also has starting point
for per-field Sims that use the stats iterator API to compute boost
bytes.  But it's not at all tied into actual searching!  There's still
tons left to do, eg, how does one configure via Field/FieldType which
stats one wants indexed.

All tests pass, and I added one new TestStats unit test.

The stats I record now are:

  - field's boost

  - field's unique term count (a b c a a b --> 3)

  - field's total term count (a b c a a b --> 6)

  - total term count per-term (sum of total term count for all docs
    that have this term)

Still need at least the total term count for each field.
"
"LUCENE-1445","BUILD_SYSTEM","BUG","include junit JAR in source dist","We recently added the junit JAR under ""lib"" so that we can checkout & run tests, but we fail to include it in the source dist."
"LUCENE-2339","RFE","IMPROVEMENT","Allow Directory.copy() to accept a collection of file names to be copied","Par example, I want to copy files pertaining to a certain commit, and not everything there is in a Directory."
"LUCENE-2603","RFE","IMPROVEMENT","FastVectorHighlighter: add a method to set an arbitrary char that is used when concatenating multiValued data","If the following multiValued names are in authors field:

* Michael McCandless
* Erik Hatcher
* Otis Gospodneti

Since FragmentsBuilder concatenates multiValued data with a space in BaseFragmentsBuilder.getFragmentSource():

{code}
while( buffer.length() < endOffset && index[0] < values.length ){
  if( index[0] > 0 && values[index[0]].isTokenized() && values[index[0]].stringValue().length() > 0 )
    buffer.append( ' ' );
  buffer.append( values[index[0]++].stringValue() );
}
{code}

an entire field snippet (using LUCENE-2464) will be ""Michael McCandless Erik Hatcher Otis Gospodneti"". There is a requirement an arbitrary char (e.g. '/') can be set so that client can separate the snippet easily. i.e. ""Michael McCandless/Erik Hatcher/Otis Gospodneti"""
"LUCENE-763","BUG","BUG","LuceneDictionary skips first word in enumeration","The current code for LuceneDictionary will always skip the first word of the TermEnum. The reason is that it doesn't initially retrieve TermEnum.term - its first call is to TermEnum.next, which moves it past the first term (line 76).
To see this problem cause a failure, add this test to TestSpellChecker:
similar = spellChecker.suggestSimilar(""eihgt"",2);
      assertEquals(1, similar.length);
      assertEquals(similar[0], ""eight"");

Because ""eight"" is the first word in the index, it will fail.
"
"LUCENE-3795","RFE","RFE","Replace spatial contrib module with LSP's spatial-lucene module","I propose that Lucene's spatial contrib module be replaced with the spatial-lucene module within Lucene Spatial Playground (LSP).  LSP has been in development for approximately 1 year by David Smiley, Ryan McKinley, and Chris Male and we feel it is ready.  LSP is here: http://code.google.com/p/lucene-spatial-playground/  and the spatial-lucene module is intuitively in svn/trunk/spatial-lucene/.

I'll add more comments to prevent the issue description from being too long."
"LUCENE-2387","BUG","BUG","IndexWriter retains references to Readers used in Fields (memory leak)","As described in [1] IndexWriter retains references to Reader used in Fields and that can lead to big memory leaks when using tika's ParsingReaders (as those can take 1MB per ParsingReader). 

[2] shows a screenshot of the reference chain to the Reader from the IndexWriter taken with Eclipse MAT (Memory Analysis Tool) . The chain is the following:

IndexWriter -> DocumentsWriter -> DocumentsWriterThreadState -> DocFieldProcessorPerThread  -> DocFieldProcessorPerField -> Fieldable -> Field (fieldsData) 


-------------
[1] http://markmail.org/thread/ndmcgffg2mnwjo47
[2] http://skitch.com/ecerulm/n7643/eclipse-memory-analyzer

"
"LUCENE-908","BUILD_SYSTEM","BUG","MANIFEST.MF cleanup (main jar and luci customizations)","there are several problems with the MANIFEST.MF file used in the core jar, and some inconsistencies in th luci jar:

Lucli's build.xml has an own ""jar"" target and does not use the jar target from common-build.xml. The result
is that the MANIFEST.MF file is not consistent and the META-INF dir does not contain LICENSE.TXT and NOTICE.TXT.

Is there a reason why lucli behaves different in this regard? If not I think we should fix this."
"LUCENE-1520","BUG","BUG","OOM erros with CheckIndex with indexes containg a lot of fields with norms","All index readers have a cache of the last used norms (SegmentReader, MultiReader, MultiSegmentReader,...). This cache is never cleaned up, so if you access norms of a field, the norm's byte[maxdoc()] array is not freed until you close/reopen the index.

You can see this problem, if you create an index with many fields with norms (I tested with about 4,000 fields) and many documents (half a million). If you then call CheckIndex, that calls norms() for each (!) field in the Segment and each of this calls creates a new cache entry, you get OutOfMemoryExceptions after short time (I tested with the above index: I was not able to do a CheckIndex even with ""-Xmx 16GB"" on 64bit Java).

CheckIndex opens and then tests each segment of a index with a separate SegmentReader. The big index with the OutOfMemory problem was optimized, so consisting of one segment with about half a million docs and about 4,000 fields. Each byte[] array takes about a half MiB for this index. The CheckIndex funtion created the norm for 4000 fields and the SegmentReader cached them, which is about 2 GiB RAM. So OOMs are not unusal.

In my opinion, the best would be to use a Weak- or better a SoftReference so norms.bytes gets java.lang.ref.SoftReference<byte[]> and used for caching. With proper synchronization (which is done on the norms cache in SegmentReader) you can do the best with SoftReference, as this reference is garbage collected only when an OOM may happen. If the byte[] array is freed (but it is only freed if no other references exist), a lter call to getNorms() creates a new array. When code is hard referencing the norms array, it will not be freed, so no problem. The same could be done for the other IndexReaders.

Fields without norm() do not have this problem, as all these fields share a one-time allocated dummy norm array. So the same index without norms enabled for most of the fields checked perfectly.

I will prepare a patch tomorrow.

Mike proposed another quick fix for CheckIndex:
bq. we could do something first specifically for CheckIndex (eg it could simply use the 3-arg non-caching bytes method instead) to prevent OOM errors when using it.
"
"LUCENE-1104","OTHER","TASK","Clean up old JIRA issues in component ""Analysis""","A list of all JIRA issues in component ""Analysis"" that haven't been updated in 2007:

   *	 LUCENE-760  	 Spellchecker could/should use n-gram tokenizers instead of rolling its own n-gramming   
   *	LUCENE-677 	Italian Analyzer 
   *	LUCENE-571 	StandardTokenizer parses decimal number as <HOST> 
   *	LUCENE-566 	Esperanto Analyzer 
   *	LUCENE-559 	Turkish Analyzer for Lucene 
   *	LUCENE-494 	Analyzer for preventing overload of search service by queries with common terms in large indexes 
   *	LUCENE-424 	[PATCH] Submissiom form simple Romanian Analyzer 
   *	LUCENE-417 	StandardTokenizer has problems with comma-separated values 
   *	LUCENE-400 	NGramFilter -- construct n-grams from a TokenStream 
   *	LUCENE-396 	[PATCH] Add position increment back into StopFilter 
   *	LUCENE-387 	Contrib: Main memory based SynonymMap and SynonymTokenFilter 
   *	LUCENE-321 	[PATCH] Submissiom of my Tswana Analyzer 
   *	LUCENE-233 	[PATCH] analyzer refactoring based on CVS HEAD from 6/21/2004 
   *	LUCENE-210 	[PATCH] Never write an Analyzer again 
   *	LUCENE-205 	[PATCH] Patches for RussianAnalyzer 
   *	LUCENE-185 	[PATCH] Thai Analysis Enhancement 
   *	LUCENE-152 	[PATCH] KStem for Lucene 
   *	LUCENE-82 	[PATCH] HTMLParser: IOException: Pipe closed 

"
"LUCENE-701","RFE","IMPROVEMENT","Lock-less commits","This is a patch based on discussion a while back on lucene-dev:

    http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200608.mbox/%3c44E5B16D.4010805@mikemccandless.com%3e

The approach is a small modification over the original discussion (see
Retry Logic below).  It works correctly in all my cross-machine test
case, but I want to open it up for feedback, testing by
users/developers in more diverse environments, etc.

This is a small change to how lucene stores its index that enables
elimination of the commit lock entirely.  The write lock still
remains.

Of the two, the commit lock has been more troublesome for users since
it typically serves an active role in production.  Whereas the write
lock is usually more of a design check to make sure you only have one
writer against the index at a time.

The basic idea is that filenames are never reused (""write once""),
meaning, a writer never writes to a file that a reader may be reading
(there is one exception: the segments.gen file; see ""RETRY LOGIC""
below).  Instead it writes to generational files, ie, segments_1, then
segments_2, etc.  Besides the segments file, the .del files and norm
files (.sX suffix) are also now generational.  A generation is stored
as an ""_N"" suffix before the file extension (eg, _p_4.s0 is the
separate norms file for segment ""p"", generation 4).

One important benefit of this is it avoids files contents caching
entirely (the likely cause of errors when readers open an index
mounted on NFS) since the file is always a new file.

With this patch I can reliably instantiate readers over NFS when a
writer is writing to the index.  However, with NFS, you are still forced to
refresh your reader once a writer has committed because ""point in
time"" searching doesn't work over NFS (see LUCENE-673 ).

The changes are fully backwards compatible: you can open an old index
for searching, or to add/delete docs, etc.  I've added a new unit test
to test these cases.

All units test pass, and I've added a number of additional unit tests,
some of which fail on WIN32 in the current lucene but pass with this
patch.  The ""fileformats.xml"" has been updated to describe the changes
to the files (but XXX references need to be fixed before committing).

There are some other important benefits:

  * Readers are now entirely read-only.

  * Readers no longer block one another (false contention) on
    initialization.

  * On hitting contention, we immediately retry instead of a fixed
    (default 1.0 second now) pause.

  * No file renaming is ever done.  File renaming has caused sneaky
    access denied errors on WIN32 (see LUCENE-665 ).  (Yonik, I used
    your approach here to not rename the segments_N file(try
    segments_(N-1) on hitting IOException on segments_N): the separate
    "".done"" file did not work reliably under very high stress testing
    when a directory listing was not ""point in time"").

  * On WIN32, you can now call IndexReader.setNorm() even if other
    readers have the index open (fixes a pre-existing minor bug in
    Lucene).

  * On WIN32, You can now create an IndexWriter with create=true even
    if readers have the index open (eg see
    www.gossamer-threads.com/lists/lucene/java-user/39265) .


Here's an overview of the changes:

  * Every commit writes to the next segments_(N+1).

  * Loading the segments_N file (& opening the segments) now requires
    retry logic.  I've captured this logic into a new static class:
    SegmentInfos.FindSegmentsFile.  All places that need to do
    something on the current segments file now use this class.

  * No more deletable file.  Instead, the writer computes what's
    deletable on instantiation and updates this in memory whenever
    files can be deleted (ie, when it commits).  Created a common
    class index.IndexFileDeleter shared by reader & writer, to manage
    deletes.

  * Storing more information into segments info file: whether it has
    separate deletes (and which generation), whether it has separate
    norms, per field (and which generation), whether it's compound or
    not.  This is instead of relying on IO operations (file exists
    calls).  Note that this fixes the current misleading
    FileNotFoundException users now see when an _X.cfs file is missing
    (eg http://www.nabble.com/FileNotFound-Exception-t6987.html).

  * Fixed some small things about RAMDirectory that were not
    filesystem-like (eg opening a non-existent IndexInput failed to
    raise IOException; renames were not atomic).  I added a stress
    test against a RAMDirectory (1 writer thread & 2 reader threads)
    that uncovered these.

  * Added option to not remove old files when create=true on creating
    FSDirectory; this is so the writer can do its own [more
    sophisticated because it retries on errors] removal.

  * Removed all references to commit lock, COMMIT_LOCK_TIMEOUT, etc.
    (This is an API change).

  * Extended index/IndexFileNames.java and index/IndexFileNameFilter.java
    with logic for computing generational file names.

  * Changed index/IndexFileNameFilter.java to use a HashSet to check
    file extentsions for better performance.

  * Fixed the test case TestIndexReader.testLastModified: it was
    incorrectly (I think?) comparing lastModified to version, of the
    index.  I fixed that and then added a new test case for version.


Retry Logic (in index/SegmentInfos.java)

If a reader tries to load the segments just as a writer is committing,
it may hit an IOException.  This is just normal contention.  In
current Lucene contention causes a [default] 1.0 second pause then
retry.  With lock-less the contention causes no added delay beyond the
time to retry.

When this happens, we first try segments_(N-1) if present, because it
could be segments_N is still being written.  If that fails, we
re-check to see if there is now a newer segments_M where M > N and
advance if so.  Else we retry segments_N once more (since it could be
it was in process previously but must now be complete since
segments_(N-1) did not load).

In order to find the current segments_N file, I list the directory and
take the biggest segments_N that exists.

However, under extreme stress testing (5 threads just opening &
closing readers over and over), on one platform (OS X) I found that
the directory listing can be incorrect (stale) by up to 1.0 seconds.
This means the listing will show a segments_N file but that file does
not exist (fileExists() returns false).

In order to handle this (and other such platforms), I switched to a
hybrid approach (originally proposed by Doron Cohen in the original
thread): on committing, the writer writes to a file ""segments.gen"" the
generation it just committed.  It writes 2 identical longs into this
file.  The retry logic, on detecting that the directory listing is
stale falls back to the contents of this file.  If that file is
consistent (the two longs are identical), and, the generation is
indeed newer than the dir listing, it will use that.

Finally, if this approach is also stale, we fallback to stepping
through sequential generations (up to a maximum # tries).  If all 3
methods fail, we throw the original exception we hit.

I added a static method SegmentInfos.setInfoStream() which will print
details of retry attempts.  In the patch it's set to System.out right
now (we should turn off before a real commit) so if there are problems
we can see what retry logic had done.
"
"LUCENE-2364","RFE","IMPROVEMENT","Add support for terms in BytesRef format to Term, TermQuery, TermRangeQuery & Co.","It would be good to directly allow BytesRefs in TermQuery and TermRangeQuery (as both queries convert the strings to BytesRef internally). For NumericRange support in Solr it will be needed to support numerics as ByteRef in single-term queries.

When this will be added, don't forget to change TestNumericRangeQueryXX to use the BytesRef ctor of TRQ."
"LUCENE-723","RFE","RFE","QueryParser support for MatchAllDocs","It seems like there really should be QueryParser support for MatchAllDocsQuery.
I propose *:* (brings back memories of DOS :-)
"
"LUCENE-992","BUG","BUG","IndexWriter.updateDocument is no longer atomic","Spinoff from LUCENE-847.

Ning caught that as of LUCENE-843, we lost the atomicity of the delete
+ add in IndexWriter.updateDocument.

Ning suggested a simple fix: move the buffered deletes into
DocumentsWriter and let it do the delete + add atomically.  This has a
nice side effect of also consolidating the ""time to flush"" logic in
DocumentsWriter.

"
"LUCENE-236","DOCUMENTATION","BUG","typos on FAQ","I found out the following typos on the FAQ (http://lucene.sourceforge.net/cgi-
bin/faq/faqmanager.cgi) of lucene:
in 8. Will Lucene work with my Java application ?
- felxible
- applciations"
"LUCENE-2865","RFE","IMPROVEMENT","Pass a context struct to Weight#scorer instead of naked booleans","Weight#scorer(AtomicReaderContext, boolean, boolean) is hard to extend if another boolean like ""needsScoring"" or similar flags / information need to be passed to Scorers. An immutable struct would make such an extension trivial / way easier. "
"LUCENE-2888","BUG","BUG","Several DocsEnum / DocsAndPositionsEnum return wrong docID when next() / advance(int) return NO_MORE_DOCS","During work on LUCENE-2878 I found some minor problems in PreFlex and Pulsing Codec - they are not returning NO_MORE_DOCS but the last docID instead from DocsEnum#docID() when next() or advance(int) returned NO_MORE_DOCS. The JavaDoc clearly says that it should return NO_MORE_DOCS."
"LUCENE-1089","RFE","IMPROVEMENT","Add insertWithOverflow to PriorityQueue","This feature proposes to add an insertWithOverflow to PriorityQueue so that callers can reuse the objects that are being dropped off the queue. Also, it changes heap to protected for easier extensibility of PQ"
"LUCENE-3286","REFACTORING","","Move XML QueryParser to queryparser module","The XML QueryParser will be ported across to queryparser module.

As part of this work, we'll move the QP's demo into the demo module."
"LUCENE-830","IMPROVEMENT","BUG","norms file can become unexpectedly enormous","
Spinoff from this user thread:

   http://www.gossamer-threads.com/lists/lucene/java-user/46754

Norms are not stored sparsely, so even if a doc doesn't have field X
we still use up 1 byte in the norms file (and in memory when that
field is searched) for that segment.  I think this is done for
performance at search time?

For indexes that have a large # documents where each document can have
wildly varying fields, each segment will use # documents times # fields
seen in that segment.  When optimize merges all segments, that product
grows multiplicatively so the norms file for the single segment will
require far more storage than the sum of all previous segments' norm
files.

I think it's uncommon to have a huge number of distinct fields (?) so
we would need a solution that doesn't hurt the more common case where
most documents have the same fields.  Maybe something analogous to how
bitvectors are now optionally stored sparsely?

One simple workaround is to disable norms.
"
"LUCENE-1941","BUG","BUG","MinPayloadFunction returns 0 when only one payload is present","In some experiments with payload scoring through PayloadTermQuery, I'm seeing 0 returned when using MinPayloadFunction.  I believe there is a bug there.  No time at the moment to flesh out a unit test, but wanted to report it for tracking."
"LUCENE-3741","TEST","BUG","MockCharFilter offset correction is wrong","This is a fake charfilter used in basetokenstreamtestcase.

it occasionally doubles some characters, and corrects offsets.

its used to find bugs where analysis components would fail otherwise with charfilters,
but its correctOffset is actually wrong (harmless to any tests today, but still wrong).
"
"LUCENE-1778","RFE","IMPROVEMENT","Add log.step support per task","Following LUCENE-1774, this will add support for log.step per task name, rather than a single log.step setting for all tasks. The .alg file will support:
* log.step - for all tasks.
* log.step.<Task Class Name> - for a specific task. For example, log.step.AddDoc, or log.step.DeleteDoc

I will post the patch soon"
"LUCENE-1945","REFACTORING","IMPROVEMENT","Make all classes that have a close() methods instanceof Closeable (Java 1.5)","This should be simple."
"LUCENE-3816","BUG","BUG","FilteredDocIdSet does not handle a case where the inner set iterator is null","DocIdSet#iterator is allowed to return null, when used in FilteredDocIdSet, if null is returned from the inner set, the FilteredDocIdSetIterator fails since it does not allow for nulls to be passed to it.

The fix is simple, return null in FilteredDocIdSet in the iterator method is the iterator is null."
"LUCENE-966","IMPROVEMENT","IMPROVEMENT","A faster JFlex-based replacement for StandardAnalyzer","JFlex (http://www.jflex.de/) can be used to generate a faster (up to several times) replacement for StandardAnalyzer. Will add a patch and a simple benchmark code in a while."
"LUCENE-3818","TEST","BUG","TestIndexWriterNRTIsCurrent failure","found by jenkins: 

https://builds.apache.org/job/Lucene-Solr-tests-only-trunk/12492/

make your computer busy (e.g. run tests in another checkout) then,

ant test-core -Dtests.iter=100 -Dtestcase=TestIndexWriterNRTIsCurrent -Dtestmethod=testIsCurrentWithThreads -Dtests.seed=-78f6fa16b849cf27:382126da79c1e146:-d2cdec79e86e1b3 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1""

takes a few tries till it pops...

{noformat}
junit-sequential:
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterNRTIsCurrent
    [junit] Tests run: 100, Failures: 1, Errors: 1, Time elapsed: 277.818 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] WARNING: you are using -Dtests.iter=n where n > 1, not all tests support this option.
    [junit] Some may crash or fail: this is not a bug.
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterNRTIsCurrent -Dtestmethod=testIsCurrentWithThreads -Dtests.seed=-78f6fa16b849cf27:382126da79c1e146:-d2cdec79e86e1b3 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1""
    [junit] The following exceptions were thrown by threads:
    [junit] *** Thread: Lucene Merge Thread #17 ***
    [junit] org.apache.lucene.index.MergePolicy$MergeException: java.lang.AssertionError
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:520)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:480)
    [junit] Caused by: java.lang.AssertionError
    [junit] 	at org.apache.lucene.index.IndexWriter$ReadersAndLiveDocs.initWritableLiveDocs(IndexWriter.java:580)
    [junit] 	at org.apache.lucene.index.IndexWriter.commitMergedDeletes(IndexWriter.java:3061)
    [junit] 	at org.apache.lucene.index.IndexWriter.commitMerge(IndexWriter.java:3137)
    [junit] 	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3718)
    [junit] 	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3257)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:382)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:451)
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterNRTIsCurrent -Dtestmethod=testIsCurrentWithThreads -Dtests.seed=-78f6fa16b849cf27:382126da79c1e146:-d2cdec79e86e1b3 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1""
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterNRTIsCurrent -Dtestmethod=testIsCurrentWithThreads -Dtests.seed=-78f6fa16b849cf27:382126da79c1e146:-d2cdec79e86e1b3 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1""
    [junit] NOTE: test params are: codec=Lucene40: {id=MockFixedIntBlock(blockSize=525)}, sim=DefaultSimilarity, locale=es_PY, timezone=Africa/Luanda
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestIndexWriterNRTIsCurrent]
    [junit] NOTE: Linux 3.0.0-14-generic amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=74907448,total=255787008
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testIsCurrentWithThreads(org.apache.lucene.index.TestIndexWriterNRTIsCurrent):	FAILED
    [junit] info=_qx(4.0):C1/1 isn't live
    [junit] junit.framework.AssertionFailedError: info=_qx(4.0):C1/1 isn't live
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReaderPool.infoIsLive(IndexWriter.java:663)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReaderPool.dropAll(IndexWriter.java:717)
    [junit] 	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1136)
    [junit] 	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1069)
    [junit] 	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1033)
    [junit] 	at org.apache.lucene.index.TestIndexWriterNRTIsCurrent.testIsCurrentWithThreads(TestIndexWriterNRTIsCurrent.java:68)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$SubclassSetupTeardownRule$1.evaluate(LuceneTestCase.java:707)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:606)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:511)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:569)
    [junit] 
    [junit] 
    [junit] Testcase: testIsCurrentWithThreads(org.apache.lucene.index.TestIndexWriterNRTIsCurrent):	Caused an ERROR
    [junit] java.lang.AssertionError: Some threads threw uncaught exceptions!
    [junit] java.lang.RuntimeException: java.lang.AssertionError: Some threads threw uncaught exceptions!
    [junit] 	at org.apache.lucene.util.LuceneTestCase.tearDownInternal(LuceneTestCase.java:780)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.access$1000(LuceneTestCase.java:138)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:607)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:511)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:569)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.checkUncaughtExceptionsAfter(LuceneTestCase.java:808)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.tearDownInternal(LuceneTestCase.java:752)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestIndexWriterNRTIsCurrent FAILED

BUILD FAILED
{noformat}"
"LUCENE-1359","BUG","BUG","FrenchAnalyzer's tokenStream method does not honour the contract of Analyzer","In {{Analyzer}} :
{code}
/** Creates a TokenStream which tokenizes all the text in the provided
    Reader.  Default implementation forwards to tokenStream(Reader) for 
    compatibility with older version.  Override to allow Analyzer to choose 
    strategy based on document and/or field.  Must be able to handle null
    field name for backward compatibility. */
  public abstract TokenStream tokenStream(String fieldName, Reader reader);
{code}


and in {{FrenchAnalyzer}}

{code}
public final TokenStream tokenStream(String fieldName, Reader reader) {

    if (fieldName == null) throw new IllegalArgumentException(""fieldName must not be null"");
    if (reader == null) throw new IllegalArgumentException(""reader must not be null"");
{code}"
"LUCENE-3501","RFE","BUG","random sampler is not random (and so facet SamplingWrapperTest occasionally fails)","RandomSample is not random at all:
It does not even import java.util.Random, and its behavior is deterministic.

in addition, the test testCountUsingSamping() never retries as it was supposed to (for taking care of the hoped-for randomness)."
"LUCENE-2815","RFE","BUG","MultiFields not thread safe","MultiFields looks like it has thread safety issues"
"LUCENE-1470","RFE","RFE","Add TrieRangeFilter to contrib","According to the thread in java-dev (http://www.gossamer-threads.com/lists/lucene/java-dev/67807 and http://www.gossamer-threads.com/lists/lucene/java-dev/67839), I want to include my fast numerical range query implementation into lucene contrib-queries.

I implemented (based on RangeFilter) another approach for faster
RangeQueries, based on longs stored in index in a special format.

The idea behind this is to store the longs in different precision in index
and partition the query range in such a way, that the outer boundaries are
search using terms from the highest precision, but the center of the search
Range with lower precision. The implementation stores the longs in 8
different precisions (using a class called TrieUtils). It also has support
for Doubles, using the IEEE 754 floating-point ""double format"" bit layout
with some bit mappings to make them binary sortable. The approach is used in
rather big indexes, query times are even on low performance desktop
computers <<100 ms (!) for very big ranges on indexes with 500000 docs.

I called this RangeQuery variant and format ""TrieRangeRange"" query because
the idea looks like the well-known Trie structures (but it is not identical
to real tries, but algorithms are related to it).

"
"LUCENE-3188","BUG","BUG","The class from cotrub directory org.apache.lucene.index.IndexSplitter creates a non correct index","When using the method IndexSplitter.split(File destDir, String[] segs) from the Lucene cotrib directory (contrib/misc/src/java/org/apache/lucene/index) it creates an index with segments descriptor file with wrong data. Namely wrong is the number representing the name of segment that would be created next in this index.
If some of the segments of the index already has this name this results either to impossibility to create new segment or in crating of an corrupted segment."
"LUCENE-1633","CLEANUP","BUG","Copy/Paste-Typo in toString() for SpanQueryFilter","   public String toString() {
-    return ""QueryWrapperFilter("" + query + "")"";
+    return ""SpanQueryFilter("" + query + "")"";
   }

says it all."
"LUCENE-1480","IMPROVEMENT","IMPROVEMENT","Wrap messages output with a check of InfoStream != null","I've found several places in the code where messages are output w/o first checking if infoStream != null. The result is that in most of the time, unnecessary strings are created but never output (because infoStream is not set). We should follow Java's logging best practices, where a log message is always output in the following format:
if (logger.isLoggable(leve)) {
    logger.log(level, msg);
}

Log messages are usually created w/o paying too much attention to performance (such as string concatenation using '+' instead of StringBuffer). Therefore, at runtime it is important to avoid creating those messages, if they will be discarded eventually.

I will add a method to IndexWriter messagesEnabled() and then use it wherever a call to iw.message() is made.

Patch will follow"
"LUCENE-1886","DOCUMENTATION","IMPROVEMENT","Improve Javadoc",""
"LUCENE-1707","IMPROVEMENT","IMPROVEMENT","Don't use ensureOpen() excessively in IndexReader and IndexWriter","A spin off from here: http://www.nabble.com/Excessive-use-of-ensureOpen()-td24127806.html.

We should stop calling this method when it's not necessary for any internal Lucene code. Currently, this code seems to hurt properly written apps, unnecessarily.

Will post a patch soon"
"LUCENE-3344","RFE","BUG","Add workaround for ICU bug in combination with Java7 to LuceneTestCase","There is a bug in ICU that makes it fail to load it ULocale class in Java7: http://bugs.icu-project.org/trac/ticket/8734

The problem is caused by some new locales in Java 7, that lead to a chicken-and-egg problem in the static initializer of ULocale. It initializes its default locale from the JDK locale in a static ctor. Until the default ULocale instance is created, the default is not set in ULocale. But ULocales ctor itsself needs the default locale to fetch some ressource bundles and throws NPE.

The code in LuceneTestCase that randomizes the default locale should classload ULocale before it tries to set another random locale, using a defined, safe locale (Locale.US). Patch is easy."
"LUCENE-3848","BUG","BUG","basetokenstreamtestcase should fail if tokenstream starts with posinc=0","This is meaningless for a tokenstream to start with posinc=0,

Its also caused problems and hairiness in the indexer (LUCENE-1255, LUCENE-1542),
and it makes senseless tokenstreams. We should add a check and fix any that do this.

Furthermore the same bug can exist in removing-filters if they have enablePositionIncrements=false.
I think this option is useful: but it shouldnt mean 'allow broken tokenstream', it just means we
don't add gaps. 

If you remove tokens with enablePositionIncrements=false it should not cause the TS to start with
positionincrement=0, and it shouldnt 'restructure' the tokenstream (e.g. moving synonyms on top of a different word).
It should just not add any 'holes'.
"
"LUCENE-2497","BUG","BUG","Revision 949509 (LUCENE-2480) causes IOE ""read past EOF"" when processing older format SegmentInfo data when JVM assertion processing is disabled.","At revision 949509 in org.apache.lucene.index.SegmentInfo at line 155, there is the following code:
{noformat} 
    if (format > SegmentInfos.FORMAT_4_0) {
      // pre-4.0 indexes write a byte if there is a single norms file
      assert 1 == input.readByte();
    }
{noformat} 
Note that the assert statement invokes input.readByte().
If asserts are disabled for the JVM, input.readByte() will not be invoked, causing the following readInt() to return a bogus value, and then causing an IOE during the (mistakenly entered) loop at line 165.
This can occur when processing old format (format ""-9"") index data under Tomcat (whose startup scripts by default do not turn on asserts).

Full stacktrace:
{noformat} 
SEVERE: java.lang.RuntimeException: java.io.IOException: read past EOF
	at org.apache.solr.core.SolrCore.getSearcher(SolrCore.java:1066)
	at org.apache.solr.core.SolrCore.<init>(SolrCore.java:581)
	at org.apache.solr.core.CoreContainer.create(CoreContainer.java:431)
	at org.apache.solr.core.CoreContainer.load(CoreContainer.java:286)
	at org.apache.solr.core.CoreContainer$Initializer.initialize(CoreContainer.java:125)
	at org.apache.solr.servlet.SolrDispatchFilter.init(SolrDispatchFilter.java:86)
	at org.apache.catalina.core.ApplicationFilterConfig.getFilter(ApplicationFilterConfig.java:275)
	at org.apache.catalina.core.ApplicationFilterConfig.setFilterDef(ApplicationFilterConfig.java:397)
	at org.apache.catalina.core.ApplicationFilterConfig.<init>(ApplicationFilterConfig.java:108)
	at org.apache.catalina.core.StandardContext.filterStart(StandardContext.java:3800)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4450)
	at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:791)
	at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:771)
	at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:526)
	at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:850)
	at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:724)
	at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:493)
	at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1206)
	at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:314)
	at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:119)
	at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1053)
	at org.apache.catalina.core.StandardHost.start(StandardHost.java:722)
	at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1045)
	at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:443)
	at org.apache.catalina.core.StandardService.start(StandardService.java:516)
	at org.apache.catalina.core.StandardServer.start(StandardServer.java:710)
	at org.apache.catalina.startup.Catalina.start(Catalina.java:583)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:288)
	at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:413)
Caused by: java.io.IOException: read past EOF
	at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:154)
	at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)
	at org.apache.lucene.store.ChecksumIndexInput.readByte(ChecksumIndexInput.java:40)
	at org.apache.lucene.store.DataInput.readInt(DataInput.java:76)
	at org.apache.lucene.store.DataInput.readLong(DataInput.java:99)
	at org.apache.lucene.index.SegmentInfo.<init>(SegmentInfo.java:165)
	at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:230)
	at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:91)
	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:649)
	at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:87)
	at org.apache.lucene.index.IndexReader.open(IndexReader.java:415)
	at org.apache.lucene.index.IndexReader.open(IndexReader.java:294)
	at org.apache.solr.core.StandardIndexReaderFactory.newReader(StandardIndexReaderFactory.java:38)
	at org.apache.solr.core.SolrCore.getSearcher(SolrCore.java:1055)
	... 32 more
{noformat} "
"LUCENE-3330","RFE","IMPROVEMENT","revise Scorer visitor API","Currently there is an (expert) API in scorer to allow you to take a scorer, and visit its children etc (e.g. booleanquery).

I think we should improve this, so its general to all queries.

The current issues are:
* the current api is hardcoded for booleanquery's Occurs, but we should support other types of children (e.g. disjunctionmax)
* it can be difficult to use the API, because of the amount of generics and the visitor callback API. 
* the current API enforces a DFS traversal when you might prefer BFS instead.
"
"LUCENE-2065","RFE","IMPROVEMENT","Java 5 port phase II ","LUCENE-1257 addresses the public API changes ( generics , mainly ) and other j.u.c. package changes related to the API .  The changes are frozen and closed for 3.0 . This would be a placeholder JIRA for 3.0+ version to address the pending changes ( tests for generics etc.) and any other internal API changes as necessary. "
"LUCENE-3285","REFACTORING","","Move QueryParsers from contrib/queryparser to queryparser module","Each of the QueryParsers will be ported across.

Those which use the flexible parsing framework will be placed under the package flexible.  The StandardQueryParser will be renamed to FlexibleQueryParser and surround.QueryParser will be renamed to SurroundQueryParser."
"LUCENE-1478","RFE","IMPROVEMENT","Missing possibility to supply custom FieldParser when sorting search results","When implementing the new TrieRangeQuery for contrib (LUCENE-1470), I was confronted by the problem that the special trie-encoded values (which are longs in a special encoding) cannot be sorted by Searcher.search() and SortField. The problem is: If you use SortField.LONG, you get NumberFormatExceptions. The trie encoded values may be sorted using SortField.String (as the encoding is in such a way, that they are sortable as Strings), but this is very memory ineffective.

ExtendedFieldCache gives the possibility to specify a custom LongParser when retrieving the cached values. But you cannot use this during searching, because there is no possibility to supply this custom LongParser to the SortField.

I propose a change in the sort classes:
Include a pointer to the parser instance to be used in SortField (if not given use the default). My idea is to create a SortField using a new constructor
{code}SortField(String field, int type, Object parser, boolean reverse){code}

The parser is ""object"" because all current parsers have no super-interface. The ideal solution would be to have:

{code}SortField(String field, int type, FieldCache.Parser parser, boolean reverse){code}

and FieldCache.Parser is a super-interface (just empty, more like a marker-interface) of all other parsers (like LongParser...). The sort implementation then must be changed to respect the given parser (if not NULL), else use the default FieldCache.getXXXX without parser."
"LUCENE-1737","IMPROVEMENT","IMPROVEMENT","Always use bulk-copy when merging stored fields and term vectors","Lucene has nice optimizations in place during merging of stored fields
(LUCENE-1043) and term vectors (LUCENE-1120) whereby the bytes are
bulk copied to the new segmetn.  This is much faster than decoding &
rewriting one document at a time.

However the optimization is rather brittle: it relies on the mapping
of field name to number to be the same (""congruent"") for the segment
being merged.

Unfortunately, the field mapping will be congruent only if the app
adds the same fields in precisely the same order to each document.

I think we should fix IndexWriter to assign the same field number for
a given field that has been assigned in the past.  Ie, when writing a
new segment, we pre-seed the field numbers based on past segments.
All other aspects of FieldInfo would remain fully dynamic."
"LUCENE-3408","IMPROVEMENT","IMPROVEMENT","Remove unnecessary memory barriers in DWPT","Currently DWPT still uses AtomicLong to count the bytesUsed. Each write access issues an implicite memory barrier which is totally unnecessary since we doing everything single threaded on that level. This might be very minor but we shouldn't issue unnecessary memory barriers causing processors to lock their instruction pipeline for no reason."
"LUCENE-301","REFACTORING","IMPROVEMENT","Index Writer constructor flags unclear - and annoying in certain cases","Wouldn't it make more sense if the constructor for the IndexWriter always
created an index if it doesn't exist - and the boolean parameter should be
""clear"" (instead of ""create"")

So instead of this (from javadoc):

IndexWriter

public IndexWriter(Directory d,
                   Analyzer a,
                   boolean create)
            throws IOException

    Constructs an IndexWriter for the index in d. Text will be analyzed with a.
If create is true, then a new, empty index will be created in d, replacing the
index already there, if any.

Parameters:
    d - the index directory
    a - the analyzer to use
    create - true to create the index or overwrite the existing one; false to
append to the existing index 
Throws:
    IOException - if the directory cannot be read/written to, or if it does not
exist, and create is false


We would have this:

IndexWriter

public IndexWriter(Directory d,
                   Analyzer a,
                   boolean clear)
            throws IOException

    Constructs an IndexWriter for the index in d. Text will be analyzed with a.
If clear is true, and a index exists at location d, then it will be erased, and
a new, empty index will be created in d.

Parameters:
    d - the index directory
    a - the analyzer to use
    clear - true to overwrite the existing one; false to append to the existing
index 
Throws:
    IOException - if the directory cannot be read/written to, or if it does not
exist.



Its current behavior is kind of annoying, because I have an app that should
never clear an existing index, it should always append.  So I want create set to
false.  But when I am starting a brand new index, then I have to change the
create flag to keep it from throwing an exception...  I guess for now I will
have to write code to check if a index actually has content yet, and if it
doesn't, change the flag on the fly."
"LUCENE-745","IMPROVEMENT","IMPROVEMENT","Make inspection of BooleanQuery more efficient","Just attempting to inspect a BooleanQuery allocates two new arrays.  This could be cheaper."
"LUCENE-309","BUG","BUG","[PATCH] IndexSearcher.search(query,filter,nDocs) accepts zero nDocs","This caused an npe from the ht.top().score lateron. 
The root cause was a bug in a test case, which took 
more time to track down than would have been necessary 
with the attached patch. 
The patch throws an IllegalArgumentException for non positive nDocs. 
All current tests pass with the patch applied. 
 
Regards, 
Paul"
"LUCENE-3529","BUG","BUG","creating empty field + empty term leads to invalid index","Spinoff from LUCENE-3526.

* if you create new Field("""", """"), you get IllegalArgumentException from Field's ctor: ""name and value cannot both be empty""
* But there are tons of other ways to index an empty term for the empty field (for example initially make it ""garbage"" then .setValue(""""), or via tokenstream).
* If you do this, and you have assertions enabled, you will trip an assert (the assert is fixed in trunk, in LUCENE-3526)
* But If you don't have assertions enabled, you will create a corrupt index: test: terms, freq, prox...ERROR [term : docFreq=1 != num docs seen 0 + num docs deleted 0]
"
"LUCENE-1968","CLEANUP","TASK","Remove deprecated methods in PriorityQueue",""
"LUCENE-2208","BUG","BUG","Token div exceeds length of provided text sized 4114","I have a doc which contains html codes. I want to strip html tags and make the test clear after then apply highlighter on the clear text . But highlighter throws an exceptions if I strip out the html characters  , if i don't strip out , it works fine. It just confuses me at the moment 

I copy paste 3 thing here from the console as it may contain special characters which might cause the problem.


1 -) Here is the html text 

          <h2>Starter</h2>
          <div id=""tab1-content"" class=""tabContent selected"">
            <div class=""head""></div>
            <div class=""body"">
             <div class=""subject-header"">Learning path: History</div>
              <h3>Key question</h3>
              <p>Did transport fuel the industrial revolution?</p>
              <h3>Learning Objective</h3>
	      <ul>
              <li>To categorise points as for or against an argument</li>
              </ul>
	      <p>
              <h3>What to do?</h3>
              <ul>
                <li>Watch the clip: <em>Transport fuelled the industrial revolution.</em></li>
              </ul>
              <p>The clips claims that transport fuelled the industrial revolution. Some historians argue that the industrial revolution only happened because of developments in transport.</p>
			  <ul>
			  	<li>Read the statements below and decide which points are <em>for</em> and which points are <em>against</em> the argument that industry expanded in the 18th and 19th centuries because of developments in transport.</li>
			</ul>
			
			<ol type=""a"">
				<li>Industry expanded because of inventions and the discovery of steam power.</li>
				<li>Improvements in transport allowed goods to be sold all over the country and all over the world so there were more customers to develop industry for.</li>
				<li>Developments in transport allowed resources, such as coal from mines and cotton from America to come together to manufacture products.</li>
				<li>Transport only developed because industry needed it. It was slow to develop as money was spent on improving roads, then building canals and the replacing them with railways in order to keep up with industry.</li>
			</ol>
			
			<p>Now try to think of 2 more statements of your own.</p>
			
            </div>
            <div class=""foot""></div>
          </div>
          <h2>Main activity</h2>
          <div id=""tab2-content"" class=""tabContent"">
            <div class=""head""></div>
            <div class=""body""><div class=""subject-header"">Learning path: History</div>
              <h3>Learning Objective</h3>
              <ul>
                <li>To select evidence to support points</li>
              </ul>
              <h3>What to do?</h3>
              <!--<ul>
                <li>Watch the clip: <em>Windmill and water mill</em></li>
              </ul>-->
              <ul><li>Choose the 4 points that you think are most important - try to be balanced by having two <strong>for</strong> and two <strong>against</strong>.</li>
			  <li>Write one in each of the point boxes of the paragraphs on the sheet <a href=""lp_history_industry_transport_ws1.html"" class=""link-internal"">Constructing a balanced argument</a>.</li></ul> <p>You might like to re write the points in your own words and use connectives to link the paragraphs.</p>
              
			  <p>In history and in any argument, you need evidence to support your points.</p>
			  <ul><li>Find evidence from these sources and from your own knowledge to support each of your points:</li></ul>
			  <ol>
                <li><a href=""../servlet/link?template=vid&macro=setResource&resourceID=2044"" class=""link-internal"">At a toll gate</a></li>
                <li><a href=""../servlet/link?macro=setResource&template=vid&resourceID=2046"" class=""link-internal"">Canals</a></li>
                <li><a href=""../servlet/link?macro=setResource&template=vid&resourceID=2043"" class=""link-internal"">Growing cities: traffic</a></li>
				<li><a href=""../servlet/link?macro=setResource&template=vid&resourceID=2047"" class=""link-internal"">Impact of the railway</a> </li>
				<li><a href=""../servlet/link?macro=setResource&template=vid&resourceID=2048"" class=""link-internal"">Sailing ships</a> </li>
				<li><a href=""../servlet/link?macro=setResource&template=vid&resourceID=2050"" class=""link-internal"">Liverpool: Capital of Culture</a> </li>
              </ol>
			  <p>Try to be specific in your evidence - use named examples of places or people. Use dates if you can.</p>
            </div>
            <div class=""foot""></div>
          </div>
          <h2>Plenary</h2>
          <div id=""tab3-content"" class=""tabContent"">
            <div class=""head""></div>
            <div class=""body""><div class=""subject-header"">Learning path: History</div>
              <h3>Learning Objective</h3>
              <ul>
                <li>To judge which of the arguments is most valid</li>
              </ul>
              <h3>What to do?</h3>
<!--              <ul>
                <li>Watch the clip: <em>Food of the rich</em></li>
              </ul>-->
              <p>In order to be a good historian, and get good marks in exams, you need to show your evaluation skills and make a judgement. Having been through the evidence which point do you think is most important? Why? Is there more evidence? Is the evidence more convincing?</p>
			  <ul><li>In the final box on your worksheet write a conclusion explaining whether on balance the evidence is enough to convince you that transport fuelled the industrial revolution.</li></ul>
            </div>
            <div class=""foot""></div>
          </div>
          <h2>Extension</h2>
          <div id=""tab4-content"" class=""tabContent"">
            <div class=""head""></div>
            <div class=""body""><div class=""subject-header"">Learning path: History</div>
              <h3>What to do?</h3>
              <p>Watch the clip <em>Stress in a ski resort</em></p>
			  <p>New industries, such as tourism, can now be said to be fuelled by transport improvements.</p>
              <ul><li>Search Clipbank, using the Related clip lists as well as the search function, to find examples from around the world of how transport has helped industry.</li></ul>              
            </div>
            <div class=""foot""></div>
          </div>
          
          
2-) here is the text after stripped html tags  out 

           Starter 
           
              
             
              Learning path: History 
               Key question 
               Did transport fuel the industrial revolution? 
               Learning Objective 
	       
               To categorise points as for or against an argument 
               
	       
               What to do? 
               
                 Watch the clip:  Transport fuelled the industrial revolution.  
               
               The clips claims that transport fuelled the industrial revolution. Some historians argue that the industrial revolution only happened because of developments in transport. 
			   
			  	 Read the statements below and decide which points are  for  and which points are  against  the argument that industry expanded in the 18th and 19th centuries because of developments in transport. 
			 
			
			 
				 Industry expanded because of inventions and the discovery of steam power. 
				 Improvements in transport allowed goods to be sold all over the country and all over the world so there were more customers to develop industry for. 
				 Developments in transport allowed resources, such as coal from mines and cotton from America to come together to manufacture products. 
				 Transport only developed because industry needed it. It was slow to develop as money was spent on improving roads, then building canals and the replacing them with railways in order to keep up with industry. 
			 
			
			 Now try to think of 2 more statements of your own. 
			
             
              
           
           Main activity 
           
              
              Learning path: History 
               Learning Objective 
               
                 To select evidence to support points 
               
               What to do? 
               
                Choose the 4 points that you think are most important - try to be balanced by having two  for  and two  against . 
			   Write one in each of the point boxes of the paragraphs on the sheet  Constructing a balanced argument .    You might like to re write the points in your own words and use connectives to link the paragraphs. 
              
			   In history and in any argument, you need evidence to support your points. 
			    Find evidence from these sources and from your own knowledge to support each of your points:  
			   
                  At a toll gate  
                  Canals  
                  Growing cities: traffic  
				  Impact of the railway   
				  Sailing ships   
				  Liverpool: Capital of Culture   
               
			   Try to be specific in your evidence - use named examples of places or people. Use dates if you can. 
             
              
           
           Plenary 
           
              
              Learning path: History 
               Learning Objective 
               
                 To judge which of the arguments is most valid 
               
               What to do? 
 
               In order to be a good historian, and get good marks in exams, you need to show your evaluation skills and make a judgement. Having been through the evidence which point do you think is most important? Why? Is there more evidence? Is the evidence more convincing? 
			    In the final box on your worksheet write a conclusion explaining whether on balance the evidence is enough to convince you that transport fuelled the industrial revolution.  
             
              
           
           Extension 
           
              
              Learning path: History 
               What to do? 
               Watch the clip  Stress in a ski resort  
			   New industries, such as tourism, can now be said to be fuelled by transport improvements. 
                Search Clipbank, using the Related clip lists as well as the search function, to find examples from around the world of how transport has helped industry.                
             
              
           
          
         3-) here is the exception I get

org.apache.lucene.search.highlight.InvalidTokenOffsetsException: Token div exceeds length of provided text sized 4114
	at org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:228)
	at org.apache.lucene.search.highlight.Highlighter.getBestFragments(Highlighter.java:158)
	at org.apache.lucene.search.highlight.Highlighter.getBestFragments(Highlighter.java:462)

"
"LUCENE-1803","DOCUMENTATION","BUG","Wrong javadoc on LowerCaseTokenizer.normalize","The javadoc on LowerCaseTokenizer.normalize seems to be copy/paste from LetterTokenizer.isTokenChar."
"LUCENE-1629","RFE","IMPROVEMENT","contrib intelligent Analyzer for Chinese","I wrote a Analyzer for apache lucene for analyzing sentences in Chinese language. it's called ""imdict-chinese-analyzer"", the project on google code is here: http://code.google.com/p/imdict-chinese-analyzer/

In Chinese, """"(I am Chinese), should be tokenized as """"(I)   """"(am)   """"(Chinese), not """" """" """". So the analyzer must handle each sentence properly, or there will be mis-understandings everywhere in the index constructed by Lucene, and the accuracy of the search engine will be affected seriously!

Although there are two analyzer packages in apache repository which can handle Chinese: ChineseAnalyzer and CJKAnalyzer, they take each character or every two adjoining characters as a single word, this is obviously not true in reality, also this strategy will increase the index size and hurt the performance baddly.

The algorithm of imdict-chinese-analyzer is based on Hidden Markov Model (HMM), so it can tokenize chinese sentence in a really intelligent way. Tokenizaion accuracy of this model is above 90% according to the paper ""HHMM-based Chinese Lexical analyzer ICTCLAL"" while other analyzer's is about 60%.

As imdict-chinese-analyzer is a really fast and intelligent. I want to contribute it to the apache lucene repository."
"LUCENE-620","TEST","BUG","GData Server - TestCase Deadlock  StorageModifier","Solfed the racecondition deadlock while closing the StorageController.
This occured the first time hossman tried to run the test cases. 

Concurrent Modification Exception while iteration over a collection in a sepereate thread -- ModifiedEntryFilter replaced list with array.

@Hossman if you can get to it, could you try the testcases again.

@all If you guys do have time you could run the testcases on different environment, that would help to resolve bugs in the test cases and the server.

simon"
"LUCENE-2928","DOCUMENTATION","IMPROVEMENT","Improve LuceneTestCase javadocs","Now that the Lucene test-framework javadocs will be published, they should get some attention."
"LUCENE-1998","RFE","IMPROVEMENT","Use Java 5 enums","Replace the use of o.a.l.util.Parameter with Java 5 enums, deprecating Parameter.

Replace other custom enum patterns with Java 5 enums."
"LUCENE-2597","REFACTORING","IMPROVEMENT","Query scorers should not use MultiFields","Lucene does all searching/filtering per-segment, today, but there are a number of tests that directly invoke Scorer.scorer or Filter.getDocIdSet on a composite reader."
"LUCENE-1807","RFE","IMPROVEMENT","Add convenient constructor to PerFieldAnalyzerWrapper for Dependency Injection","It would be good if PerFieldAnalyzerWrapper had a constructor which took in an analyzer map, rather than having to repeatedly call addAnalyzer -- this would make it much easier/cleaner to use this class in e.g. Spring XML configurations.

Relatively trivial change, patch to be attached."
"LUCENE-1950","CLEANUP","IMPROVEMENT","Remove autoCommit from IndexWriter","IndexWriter's autoCommit is deprecated; in 3.0 it will be hardwired to false."
"LUCENE-377","BUILD_SYSTEM","BUG","GCJ build fails with JDK 1.5","The build.xml doesn't specify a target VM version. Using JDK 1.5, this means the compiled .class files 
are automatically made for 1.5, with java.lang.StringBuilder insidiously used for string concatenation. 
GCJ doesn't seem to include this class yet, so when it gets to the gcj build it dies trying to read the class 
files.

Steps to reproduce:
1. Install Sun JDK 1.5 for a Java compiler
2. Check out Lucene from svn
3. 'ant gcj'

Expected behavior:
Should build Lucene to .class files and .jar with the JDK compiler and then compile an .a with GCJ.

Actual behavior:
The GCJ build fails, complaining of being unable to find java.lang.StringBuilder.

Suggested fix:
Adding source=""1.3"" target=""1.3"" to the <javac> tasks seems to take care of this. Patch to be attached.

Additional notes:
Using Lucene from SVN and GCJ pulled from GCC CVS circa 2005-04-19. Ant 1.6.2."
"LUCENE-3033","BUG","BUG","TestAddIndexes#testAddIndexesWithThreads fails on Realtime","Selckin reported two failures on LUCENE-3023 which I can unfortunately not reproduce at all. here are the traces

{noformat}
  [junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Testcase: testAddIndexesWithThreads(org.apache.lucene.index.TestAddIndexes):	FAILED
    [junit] expected:<3160> but was:<3060>
    [junit] junit.framework.AssertionFailedError: expected:<3160> but was:<3060>
    [junit] 	at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithThreads(TestAddIndexes.java:783)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1226)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1154)
    [junit] 
    [junit] 
    [junit] Tests run: 18, Failures: 1, Errors: 0, Time elapsed: 14.272 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithThreads -Dtests.seed=6128854208955988865:2552774338676281184
    [junit] NOTE: test params are: codec=PreFlex, locale=no_NO_NY, timezone=America/Edmonton
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestToken, TestDateTools, Test2BTerms, TestAddIndexes]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=84731792,total=258080768
    [junit] ------------- ---------------- ---------------
{noformat}
and 
{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Testcase: testAddIndexesWithThreads(org.apache.lucene.index.TestAddIndexes):	FAILED
    [junit] expected:<3160> but was:<3060>
    [junit] junit.framework.AssertionFailedError: expected:<3160> but was:<3060>
    [junit] 	at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithThreads(TestAddIndexes.java:783)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1226)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1154)
    [junit] 
    [junit] 
    [junit] Tests run: 18, Failures: 1, Errors: 0, Time elapsed: 14.841 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithThreads -Dtests.seed=4502815121171887759:-6764285049309266272
    [junit] NOTE: test params are: codec=PreFlex, locale=tr_TR, timezone=Mexico/BajaNorte
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestToken, TestDateTools, Test2BTerms, TestAddIndexes]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=163663416,total=243335168
    [junit] ------------- ---------------- ---------------
{noformat}"
"LUCENE-3637","IMPROVEMENT","IMPROVEMENT","Make IndexReader.decRef() call refCount.decrementAndGet instead of getAndDecrement","IndexReader.decRef() has this code:

{code}
    final int rc = refCount.getAndDecrement();
    if (rc == 1) {
{code}

I think it will be clearer if it was written like this:

{code}
    final int rc = refCount.decrementAndGet();
    if (rc == 0) {
{code}

It's a very simple change, which makes reading the code (at least IMO) easier. Will post a patch shortly."
"LUCENE-3219","REFACTORING","IMPROVEMENT","Change SortField types to an Enum","When updating my SOLR-2533 patch, one issue was that the int value I had given my new type had been used by another change in the mean time.  Since we don't use these fields in a bitset kind of way, we can convert them to an enum."
"LUCENE-2676","TEST","TEST","TestIndexWriter failes for SimpleTextCodec","I just ran into this failure since SimpleText obviously takes a lot of disk space though.

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
    [junit] Testcase: testCommitOnCloseDiskUsage(org.apache.lucene.index.TestIndexWriter):	FAILED
    [junit] writer used too much space while adding documents: mid=608162 start=5293 end=634214
    [junit] junit.framework.AssertionFailedError: writer used too much space while adding documents: mid=608162 start=5293 end=634214
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit] 	at org.apache.lucene.index.TestIndexWriter.testCommitOnCloseDiskUsage(TestIndexWriter.java:1047)
    [junit] 
    [junit] 
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 3.281 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testCommitOnCloseDiskUsage -Dtests.seed=-7526585723238322940:-1609544650150801239
    [junit] NOTE: test params are: codec=SimpleText, locale=th_TH, timezone=UCT
    [junit] ------------- ---------------- ---------------
    [junit] Test org.apache.lucene.index.TestIndexWriter FAILED
{noformat}

I did not look into SimpleText but I guess we need either change the threshold for this test or exclude SimpleText from it.

any ideas?"
"LUCENE-3284","REFACTORING","","Move contribs/modules away from QueryParser dependency","Some contribs and modules depend on the core QueryParser just for simplicity in their tests.  We should apply the same process as I did to the core tests, and move them away from using the QueryParser where possible."
"LUCENE-1402","BACKPORT","BUG","CheckIndex API changed without backwards compaitibility","The API of CheckIndex changed. The Check function returns a CheckIndexStatus and not boolean. And JavaDocs notes the boolean return value.

I am not sure if it works, but it would be good to have the check method that returns boolean available @Deprecated, i.e.
@Deprecated public static CheckIndexStatus check(Directory dir, boolean doFix) throws IOException {
 final CheckIndexStatus stat=this.check(dir,doFix);
 return stat.clean;
}

I am not sure, if it can be done with the same method name, but it prevents drop-in-replacements of Lucene to work."
"LUCENE-708","DOCUMENTATION","IMPROVEMENT","Setup nightly build website links and docs","Per discussion on mailing list, we are going to setup a Nightly Build link on the website linking to the docs (and javadocs) generated by the nightly build process.  The build process may need to be modified to complete this task.

Going forward, the main website will, for the most part, only be updated per releases (I imagine exceptions will be made for News items and per committer's discretion).  The Javadocs linked to from the main website will always be for the latest release."
"LUCENE-2721","TEST","TEST","Random Failure TestSizeBoundedOptimize#testFirstSegmentTooLarge","I am seeing this on trunk  

{noformat}

[junit] Testsuite: org.apache.lucene.index.TestSizeBoundedOptimize
    [junit] Testcase: testFirstSegmentTooLarge(org.apache.lucene.index.TestSizeBoundedOptimize):	FAILED
    [junit] expected:<2> but was:<1>
    [junit] junit.framework.AssertionFailedError: expected:<2> but was:<1>
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:882)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:848)
    [junit] 	at org.apache.lucene.index.TestSizeBoundedOptimize.testFirstSegmentTooLarge(TestSizeBoundedOptimize.java:160)
    [junit] 
    [junit] 
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.658 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestSizeBoundedOptimize -Dtestmethod=testFirstSegmentTooLarge -Dtests.seed=7354441978302993522:-457602792543755447 -Dtests.multiplier=3
    [junit] NOTE: test params are: codec=Standard, locale=sv_SE, timezone=Mexico/BajaNorte
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestSizeBoundedOptimize]
    [junit] ------------- ---------------- ---------------
    [junit] Test org.apache.lucene.index.TestSizeBoundedOptimize FAILED
{noformat}

when running with this seed
ant test -Dtestcase=TestSizeBoundedOptimize -Dtestmethod=testFirstSegmentTooLarge -Dtests.seed=7354441978302993522:-457602792543755447 -Dtests.multiplier=3"
"LUCENE-2768","TEST","IMPROVEMENT","add infrastructure for longer running nightly test cases","I'm spinning this out of LUCENE-2762...

The patch there adds initial infrastructure for tests to pull documents from a line file, and adds a longish running test case using that line file to test NRT.

I'd like to see some tests run on more substantial indices based on real data... so this is just a start."
"LUCENE-1924","RFE","IMPROVEMENT","BalancedSegmentMergePolicy, contributed from the Zoie project for realtime indexing","Written by Yasuhiro Matsuda for Zoie realtime indexing system used to handle high update rates to avoid large segment merges.
Detailed write-up is at:

http://code.google.com/p/zoie/wiki/ZoieMergePolicy
"
"LUCENE-1115","IMPROVEMENT","BUG","Some small fixes to contrib/benchmark","I've fixed a few small issues I've hit in contrib/benchmark.

First, this alg was only doing work on the first round.  All
subsequent rounds immediately finished:

{code}
analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker
work.dir = /lucene/work
docs.file=work/reuters.lines.txt
doc.maker.forever=false
directory=FSDirectory
doc.add.log.step=3000

{ ""Rounds""
  ResetSystemErase
  CreateIndex
  { ""AddDocs"" AddDoc > : *
  CloseIndex
  NewRound
} : 3
{code}

I think this is because we are failing to reset ""exhausted"" to false
in PerfTask.doLogic(), so I added that.  Plus I had to re-open the
file in LineDocMaker.

Second, I made a small optimization to not call updateExhausted unless
any of the child tasks are TaskSequence or ResetInputsTask (which I
compute up-front).

Finally, we were not allowing flushing by RAM and doc count, so I
fixed the logic in Create/OpenIndexTask to set both RAMBufferSizeMB
and MaxBufferedDocs.
"
"LUCENE-615","DOCUMENTATION","IMPROVEMENT","[patch] javadoc and comment updates for BooleanClause.","Javadoc and comment updates for BooleanClause, one minor code simplification."
"LUCENE-1824","IMPROVEMENT","IMPROVEMENT","FastVectorHighlighter truncates words at beginning and end of fragments","FastVectorHighlighter does not take word boundaries into consideration when building fragments, so that in most cases the first and last word of a fragment are truncated.  This makes the highlights less legible than they should be.  I will attach a patch to BaseFragmentBuilder that resolves this by expanding the start and end boundaries of the fragment to the first whitespace character on either side of the fragment, or the beginning or end of the source text, whichever comes first.  This significantly improves legibility, at the cost of returning a slightly larger number of characters than specified for the fragment size."
"LUCENE-1366","REFACTORING","IMPROVEMENT","Rename Field.Index.UN_TOKENIZED/TOKENIZED/NO_NORMS","There is confusion about these current Field options and I think we
should rename them, deprecating the old names in 2.4/2.9 and removing
them in 3.0.  How about this:

{code}
TOKENIZED --> ANALYZED
UN_TOKENIZED --> NOT_ANALYZED
NO_NORMS --> NOT_ANALYZED_NO_NORMS
{code}

Should we also add ANALYZED_NO_NORMS?

Spinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200808.mbox/%3C48a3076a.2679420a.1c53.ffffa5c4%40mx.google.com%3E
    "
"LUCENE-2996","BUG","BUG","addIndexes(IndexReader) incorrectly applies existing deletes","If you perform these operations:
# deleteDocuments(Term) for all the new documents
# addIndexes(IndexReader)
# commit

Then addIndexes applies the previous deletes on the incoming indexes as well, which is incorrect. If you call addIndexes(Directory) instead, the deletes are applied beforehand, as they should. The solution, as Mike indicated here: http://osdir.com/ml/general/2011-03/msg20876.html is to add *flush(false,true)* to addIndexes(IndexReader).

I will create a patch with a matching unit test shortly."
"LUCENE-2464","RFE","IMPROVEMENT","FastVectorHighlighter: add a FragmentBuilder to return entire field contents","In Highlightrer, there is a Nullfragmenter. There is a requirement its counterpart in FastVectorhighlighter."
"LUCENE-2074","RFE","BUG","Use a separate JFlex generated Unicode 4 by Java 5 compatible StandardTokenizer","The current trunk version of StandardTokenizerImpl was generated by Java 1.4 (according to the warning). In Java 3.0 we switch to Java 1.5, so we should regenerate the file.

After regeneration the Tokenizer behaves different for some characters. Because of that we should only use the new TokenizerImpl when Version.LUCENE_30 or LUCENE_31 is used as matchVersion."
"LUCENE-1508","RFE","IMPROVEMENT","CartesianTierPlotter fieldPrefix should be configurable","CartesianTierPlotter field prefix is currrently hardcoded to ""_localTier"" -- this should be configurable"
"LUCENE-1568","BUG","BUG","Fix for NPE's in Spatial Lucene for searching bounding box only","NPE occurs when using DistanceQueryBuilder for minimal bounding box search without the distance filter."
"LUCENE-2687","IMPROVEMENT","IMPROVEMENT","Remove Priority-Queue size trap in MultiTermQuery.TopTermsBooleanQueryRewrite","These APIs are new in 3.x, so we can do this with no backwards-compatibility issue:

Before 3.1, FuzzyQuery had its own internal rewrite method.
We exposed this in 3.x as TopTermsBooleanQueryRewrite, and then as subclasses for Scoring and Boost-only variants.

The problem I have is that the PQ has a default (large) size of Integer.MAX_VALUE... of course its later limited by
the value of BooleanQuery's maxClauseCount, but I think this is a trap.

Instead its better to simply remove these defaults and force the user to provide a default (reasonable) size.
"
"LUCENE-217","RFE","IMPROVEMENT","[PATCH] new method: Document.remove()","Here's a patch that adds a remove() method to the Document class (+test case). This 
is very useful if you have converter classes that return a Lucene Document object but 
you need to make changes to that object. 
 
In my case, I wanted to index PDF files that were saved as BLOBs in a database. The 
files need to be saved to a temporary file and that file name is given to the PDF 
converter class. The PDF converter then saves the name of the temporary file name 
as the file name, which doesn't make sense. So my code needs to remove the 
'filename' field and re-add it, this time with the columns primary ID. This is only possible 
with the attached patch."
"LUCENE-1681","BUG","BUG","DocValues infinite loop caused by - a call to getMinValue | getMaxValue | getAverageValue","org.apache.lucene.search.function.DocValues offers 3 public (optional) methods to access value statistics like min, max and average values of the internal values. A call to one of the methods will result in an infinite loop. The internal counter is not incremented. 
I added a testcase, javadoc and a slightly different implementation to it. I guess this is not breaking any back compat. as a call to those methodes would have caused an infinite loop anyway.
I changed the return value of all of those methods to Float.NaN if the DocValues implementation does not contain any values.

It might be considerable to fix this in 2.4.2 and 2.3.3

"
"LUCENE-2560","TEST","TEST","random analyzer tests","we have been finding+fixing lots of bugs by randomizing lucene tests.
in r966878 I added a variant of random unicode string that gives you a random string within the same unicode block (for other purposes)

I think we should use this to test the analyzers better, for example we should pound tons of random greek strings against the greek analyzer and at least make sure there aren't exceptions.
"
"LUCENE-3279","RFE","IMPROVEMENT","Allow CFS be empty","since we changed CFS semantics slightly closing a CFS directory on an error can lead to an exception. Yet, an empty CFS is still a valid CFS so for consistency we should allow CFS to be empty.
here is an example:

{noformat}
1 tests failed.
REGRESSION:  org.apache.lucene.index.TestIndexWriterOnDiskFull.testAddDocumentOnDiskFull

Error Message:
CFS has no entries

Stack Trace:
java.lang.IllegalStateException: CFS has no entries
       at org.apache.lucene.store.CompoundFileWriter.close(CompoundFileWriter.java:139)
       at org.apache.lucene.store.CompoundFileDirectory.close(CompoundFileDirectory.java:181)
       at org.apache.lucene.store.DefaultCompoundFileDirectory.close(DefaultCompoundFileDirectory.java:58)
       at org.apache.lucene.index.SegmentMerger.createCompoundFile(SegmentMerger.java:139)
       at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4252)
       at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3863)
       at org.apache.lucene.index.SerialMergeScheduler.merge(SerialMergeScheduler.java:37)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:2715)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:2710)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:2706)
       at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3513)
       at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2064)
       at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2031)
       at org.apache.lucene.index.TestIndexWriterOnDiskFull.addDoc(TestIndexWriterOnDiskFull.java:539)
       at org.apache.lucene.index.TestIndexWriterOnDiskFull.testAddDocumentOnDiskFull(TestIndexWriterOnDiskFull.java:74)
       at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1277)
       at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1195)
{noformat}"
"LUCENE-1407","REFACTORING","IMPROVEMENT","Refactor Searchable to not have RMI Remote dependency","Per http://lucene.markmail.org/message/fu34tuomnqejchfj?q=RemoteSearchable

We should refactor Searchable slightly so that it doesn't extend the java.rmi.Remote marker interface.  I believe the same could be achieved by just marking the RemoteSearchable and refactoring the RMI implementation out of core and into a contrib.

If we do this, we should deprecate/denote it for 2.9 and then move it for 3.0"
"LUCENE-1687","REFACTORING","IMPROVEMENT","Remove ExtendedFieldCache by rolling functionality into FieldCache","It is silly that we have ExtendedFieldCache.  It is a workaround to our supposed back compatibility problem.  This patch will merge the ExtendedFieldCache interface into FieldCache, thereby breaking back compatibility, but creating a much simpler API for FieldCache."
"LUCENE-1421","RFE","RFE","Ability to group search results by field","It would be awesome to group search results by specified field. Some functionality was provided for Apache Solr but I think it should be done in Core Lucene. There could be some useful information like total hits about collapsed data like total count and so on.

Thanks,
Artyom"
"LUCENE-957","BUG","BUG","Lucene RAM Directory doesn't work for Index Size > 8 GB","from user list - http://www.gossamer-threads.com/lists/lucene/java-user/50982

Problem seems to be casting issues in RAMInputStream.

Line 90:
      bufferStart = BUFFER_SIZE * currentBufferIndex;
both rhs are ints while lhs is long.
so a very large product would first overflow MAX_INT, become negative, and only then (auto) casted to long, but this is too late. 

Line 91: 
     bufferLength = (int) (length - bufferStart);
both rhs are longs while lhs is int.
so the (int) cast result may turn negative and the logic that follows would be wrong.
"
"LUCENE-1590","IMPROVEMENT","BUG","Stored-only fields automatically enable norms and tf when added to document","During updating my internal components to the new TrieAPI, I have seen the following:

I index a lot of numeric fields with trie encoding omitting norms and term frequency. This works great. Luke shows that both is omitted.

As I sometimes also want to have the components of the field stored and want to use the same field name for it. So I add additionally the field again to the document, but stored only (as the Field c'tor using a TokenStream cannot additionally store the field). As it is stored only, I thought, that I can left out explicit setting of omitNorms and omitTermFreqAndPositions. After adding the stored-only-without-omits field, Luke shows all fields with norms enabled. I am not sure, if the norms/tf were really added to the index, but Luke shows a value for the norms and FieldInfo has it enabled.

In my opinion, this is not intuitive, o.a.l.document.Field  should switch both omit* options on when storing fields only (and also disable other indexing-only options). Alternatively the internal FieldInfo.update(boolean isIndexed, boolean storeTermVector, boolean storePositionWithTermVector, boolean storeOffsetWithTermVector, boolean omitNorms, boolean storePayloads, boolean omitTermFreqAndPositions) should only change the omit* and other options, if the isIndexed parameter (not this.isIndexed) is also true, elsewhere leave it as it is.

In principle, when adding a stored-only field, any indexing-specific options should not be changed in FieldInfo. If the field was indexed with norms before, norms should stay enabled (but this would be the default as it is)."
"LUCENE-2656","TEST","TEST","If tests fail, don't report about unclosed resources","LuceneTestCase ensures in afterClass() if you closed all your directories, which in turn will check if you have closed any open files.

This is good, as a test will fail if we have resource leaks.

But if a test truly fails, this is just confusing, because its usually not going to make it to the part of its code where it would call .close()

So, if any tests fail, I think we should omit this check in afterClass()"
"LUCENE-1780","DOCUMENTATION","IMPROVEMENT","deprecate Scorer.explain","Spinoff from LUCENE-1749.

We already have QueryWeight.explain, which is directly invoked by IndexSearcher.explain.  Some queries in turn will defer to their Scorer impl's explain, but many do not (and their Scorer.explain simply throw UOE).  So we should deprecate & remove Scorer.explain, leaving it up to individual queries to define that method if they need it."
"LUCENE-1586","RFE","IMPROVEMENT","add IndexReader.getUniqueTermCount","Simple API to return number of unique terms (across all fields).  Spinoff from here:

http://www.lucidimagination.com/search/document/536b22e017be3e27/term_limit"
"LUCENE-1285","BUG","BUG","WeightedSpanTermExtractor incorrectly treats the same terms occurring in different query types","Given a BooleanQuery with multiple clauses, if a term occurs both in a Span / Phrase query, and in a TermQuery, the results of term extraction are unpredictable and depend on the order of clauses. Concequently, the result of highlighting are incorrect.

Example text: t1 t2 t3 t4 t2
Example query: t2 t3 ""t1 t2""
Current highlighting: [t1 t2] [t3] t4 t2
Correct highlighting: [t1 t2] [t3] t4 [t2]

The problem comes from the fact that we keep a Map<termText, WeightedSpanTerm>, and if the same term occurs in a Phrase or Span query the resulting WeightedSpanTerm will have a positionSensitive=true, whereas terms added from TermQuery have positionSensitive=false. The end result for this particular term will depend on the order in which the clauses are processed.

My fix is to use a subclass of Map, which on put() always sets the result to the most lax setting, i.e. if we already have a term with positionSensitive=true, and we try to put() a term with positionSensitive=false, we set the result positionSensitive=false, as it will match both cases."
"LUCENE-440","DESIGN_DEFECT","BUG","FilteredQuery should have getFilter()","Unless you are in the same package, you can't access the filter in a FilteredQuery.
A getFilter() method should be added."
"LUCENE-1688","REFACTORING","IMPROVEMENT","Deprecating StopAnalyzer ENGLISH_STOP_WORDS - General replacement with an immutable Set","StopAnalyzer and StandartAnalyzer are using the static final array ENGLISH_STOP_WORDS by default in various places. Internally this array is converted into a mutable set which looks kind of weird to me. 
I think the way to go is to deprecate all use of the static final array and replace it with an immutable implementation of CharArraySet. Inside an analyzer it does not make sense to have a mutable set anyway and we could prevent set creation each time an analyzer is created. In the case of an immutable set we won't have multithreading issues either. 
in essence we get rid of a fair bit of ""converting string array to set"" code, do not have a PUBLIC static reference to an array (which is mutable) and reduce the overhead of analyzer creation.

let me know what you think and I create a patch for it.

simon"
"LUCENE-1730","BUG","BUG","TrecContentSource should use a fixed encoding, rather than system dependent","TrecContentSource opens InputStreamReader w/o a fixed encoding. On Windows, this means CP1252 (at least on my machine) which is ok. However, when I opened it on a Linux machine w/ a default of UTF-8, it failed to read the files. The patch changes it to use ISO-8859-1, which seems to be the right one (and http://mg4j.dsi.unimi.it/man/manual/ch01s04.html mentions this encoding in its example of a script which reads the data).

Patch to follow shortly."
"LUCENE-2344","BUG","BUG","PostingsConsumer#merge does not call finishDoc","We discovered that the current merge function in PostingsConsumer is not calling the #finishDoc method. This does not have consequences for the standard codec (since the lastPosition is set to 0 in #startDoc, and its #finishDoc method is empty), but for the SepCodec, this results in position file corruption (the lastPosition is set to 0 in #finishDoc for the SepCodec)."
"LUCENE-1759","RFE","TASK","Implement TokenStream.end() in contrib TokenStreams","See LUCENE-1448. Mike's patch there already contains the necessary fixes.

I'll attach a patch here as soon as LUCENE-1460 is committed."
"LUCENE-955","BUG","BUG","Bug in SegmentTermPositions if used for first term in the dictionary","When a SegmentTermPositions object is reset via seek() it does not move
the proxStream to the correct position in case the term is the first one
in the dictionary.

The reason for this behavior is that the skipStream is only moved if
lazySkipPointer is != 0. But 0 is a valid value for the posting list of
the very first term. The fix is easy: We simply have to set lazySkipPointer
to -1 in case no lazy skip has to be performed and then we only move the
skipStream if lazySkipPointer!=-1."
"LUCENE-2995","REFACTORING","TASK","factor out a shared spellchecking module","In lucene's contrib we have spellchecking support (index-based spellchecker, directspellchecker, etc). 
we also have some things like pluggable comparators.

In solr we have auto-suggest support (with two implementations it looks like), some good utilities like HighFrequencyDictionary, etc.

I think spellchecking is really important... google has upped the ante to what users expect.
So I propose we combine all this stuff into a shared modules/spellchecker, which will make it easier
to refactor and improve the quality.
"
"LUCENE-1725","BUG","BUG","Benchmark package uses new TopFieldCollector but also still uses AUTO without resolving it - result is, our sort algorithms won't run","AUTO does not work with TopFieldCollector. If you want to use AUTO with TopFieldCollector, we have a convienence method called detectType on SortField, but it is package protected and so cannot be used here as a stop gap or by users if they wanted to mix AUTO with TopFieldCollector. Lucene does still handle this for back compat internally. Solr got bit here when it was switched to use TopFieldCollector - no auto resolution was added (detectType help couldn't have been used due to visibility), and the result was that plugin code that used to be able to use AUTO would now blow up. You shouldn't use AUTO in Solr anyway though.

The Benchmark package got bit as well  when it moved to TopFieldCollector. Sort algorithms allowed auto if you specified it, or if you left off the type. Now our sort algs fail because they didn't specify a type.

I'll change to require the type to be specified to get the algs working again. I was thinking of just putting auto resolution in as a stop gap till 3.0 (when auto is removed), but since detectFieldType is package protected and I don't want to repeat it, disallowing auto seems the best way to go."
"LUCENE-1250","BUG","BUG","Some equals methods do not check for null argument","The equals methods in the following classes do not check for a null argument and thus would incorrectly fail with a null pointer exception if passed null:

- org.apache.lucene.index.SegmentInfo
- org.apache.lucene.search.function.CustomScoreQuery
- org.apache.lucene.search.function.OrdFieldSource
- org.apache.lucene.search.function.ReverseOrdFieldSource
- org.apache.lucene.search.function.ValueSourceQuery

If a null parameter is passed to equals() then false should be returned."
"LUCENE-2559","RFE","IMPROVEMENT","reopen support for SegmentReader","Reopen for SegmentReader can be supported simply as the following:

  @Override
  public synchronized IndexReader reopen() throws CorruptIndexException,
		IOException {
	return reopenSegment(this.si,false,readOnly);
  }

  @Override
  public synchronized IndexReader reopen(boolean openReadOnly)
		throws CorruptIndexException, IOException {
	return reopenSegment(this.si,false,openReadOnly);
  }
"
"LUCENE-621","DESIGN_DEFECT","BUG","Default lock timeouts should have static setter/getters","
We recently stopped using Java system properties to derive defaults for things like the write/commit lock timeout, and switched to getter/setter's across all classes.  See here:

    http://www.gossamer-threads.com/lists/lucene/java-dev/27447

But, in the case at least of the write lock timeout, because it's marked ""public final static"", a consumer of this API can no longer change this value before instantiating the IndexWriter.  This is because the getter/setter for this is not static, which generally makes sense so you can change the timeout for each instance of IndexWriter.  But because IndexWriter on construction uses the timeout value, some uses cases need to change the value before getting an instance of IndexWriter.

This was actually a regression, in that Lucene users lost functionality they previously had, on upgrading.

I would propose that that we add getter/setter for the default value of this timeout, which would be static.  I'll attach a patch file.

See this thread for context that led to this issue:

   http://www.gossamer-threads.com/lists/lucene/java-dev/37421"
"LUCENE-690","BUG","BUG","LazyField use of IndexInput not thread safe","Hypothetical problem: IndexInput.clone() of an active IndexInput could result in a corrupt copy.
LazyField clones the FieldsReader.fieldsStream, which could be in use via IndexReader.document()"
"LUCENE-1579","IMPROVEMENT","BUG","Cloned SegmentReaders fail to share FieldCache entries","I just hit this on LUCENE-1516, which returns a cloned readOnly
readers from IndexWriter.

The problem is, when cloning, we create a new [thin] cloned
SegmentReader for each segment.  FieldCache keys directly off this
object, so if you clone the reader and do a search that requires the
FieldCache (eg, sorting) then that first search is always very slow
because every single segment is reloading the FieldCache.

This is of course a complete showstopper for LUCENE-1516.

With LUCENE-831 we'll switch to a new FieldCache API; we should ensure
this bug is not present there.  We should also fix the bug in the
current FieldCache API since for 2.9, users may hit this.
"
"LUCENE-1422","RFE","RFE","New TokenStream API","This is a very early version of the new TokenStream API that 
we started to discuss here:

http://www.gossamer-threads.com/lists/lucene/java-dev/66227

This implementation is a bit different from what I initially
proposed in the thread above. I introduced a new class called
AttributedToken, which contains the same termBuffer logic 
from Token. In addition it has a lazily-initialized map of
Class<? extends Attribute> -> Attribute. Attribute is also a
new class in a new package, plus several implementations like
PositionIncrementAttribute, PayloadAttribute, etc.

Similar to my initial proposal is the prototypeToken() method
which the consumer (e. g. DocumentsWriter) needs to call.
The token is created by the tokenizer at the end of the chain
and pushed through all filters to the end consumer. The 
tokenizer and also all filters can add Attributes to the 
token and can keep references to the actual types of the
attributes that they need to read of modify. This way, when
boolean nextToken() is called, no casting is necessary.

I added a class called TestNewTokenStreamAPI which is not 
really a test case yet, but has a static demo() method, which
demonstrates how to use the new API.

The reason to not merge Token and TokenStream into one class 
is that we might have caching (or tee/sink) filters in the 
chain that might want to store cloned copies of the tokens
in a cache. I added a new class NewCachingTokenStream that
shows how such a class could work. I also implemented a deep
clone method in AttributedToken and a 
copyFrom(AttributedToken) method, which is needed for the 
caching. Both methods have to iterate over the list of 
attributes. The Attribute subclasses itself also have a
copyFrom(Attribute) method, which unfortunately has to down-
cast to the actual type. I first thought that might be very
inefficient, but it's not so bad. Well, if you add all
Attributes to the AttributedToken that our old Token class
had (like offsets, payload, posIncr), then the performance
of the caching is somewhat slower (~40%). However, if you 
add less attributes, because not all might be needed, then
the performance is even slightly faster than with the old API.
Also the new API is flexible enough so that someone could
implement a custom caching filter that knows all attributes
the token can have, then the caching should be just as 
fast as with the old API.


This patch is not nearly ready, there are lot's of things 
missing:

- unit tests
- change DocumentsWriter to use new API 
  (in backwards-compatible fashion)
- patch is currently java 1.5; need to change before 
  commiting to 2.9
- all TokenStreams and -Filters should be changed to use 
  new API
- javadocs incorrect or missing
- hashcode and equals methods missing in Attributes and 
  AttributedToken
  
I wanted to submit it already for brave people to give me 
early feedback before I spend more time working on this."
"LUCENE-2754","RFE","RFE","add spanquery support for all multitermqueries","I set fix version: 4.0, but possibly we could do this for 3.x too

Currently, we have a special SpanRegexQuery in contrib, and issues like LUCENE-522 open for SpanFuzzyQuery.
The SpanRegexQuery in contrib is a little messy additionally.

For any arbitrary MultiTermQueries to work as a SpanQuery, there are only 3 requirements:
# The un-rewritten query must extend SpanQuery so it can be included in Span clauses
# The rewritten query should be SpanOrQuery instead of BooleanQuery
# The rewritten term clauses should be SpanTermQueries.

Instead of having logic like this for each query, i suggest adding two rewrite methods:
* ScoringSpanBoolean rewrite
* TopTermsSpanBoolean rewrite

as a start i wrote these up, and added a SpanMultiTermQueryWrapper that can be used to wrap any multitermquery this way.
there are a few kinks, but I think the MTQ policeman can probably help get through them.
"
"LUCENE-1356","RFE","IMPROVEMENT","Allow easy extensions of TopDocCollector","TopDocCollector's members and constructor are declared either private or package visible. It makes it hard to extend it as if you want to extend it you can reuse its *hq* and *totatlHits* members, but need to define your own. It also forces you to override getTotalHits() and topDocs().
By changing its members and constructor (the one that accepts a PQ) to protected, we allow users to extend it in order to get a different view of 'top docs' (like TopFieldCollector does), but still enjoy its getTotalHits() and topDocs() method implementations."
"LUCENE-1255","BUG","BUG","CheckIndex should allow term position = -1","
Spinoff from this discussion:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200803.mbox/%3CPine.LNX.4.62.0803292323350.16762@radix.cryptio.net%3E

Right now CheckIndex claims the index is corrupt if you index a Token with -1 position, which happens if your first token has positionIncrementGap set to 0.

But, as far as I can tell, Lucene doesn't ""mind"" when this happens.

So I plan to fix CheckIndex to allow this case.  I'll backport to 2.3.2 as well.

LUCENE-1253 is one example where Lucene's core analyzers could do this."
"LUCENE-806","IMPROVEMENT","IMPROVEMENT","Synchronization bottleneck in FieldSortedHitQueue with many concurrent readers","The below is from a post by (my colleague) Paul Smith to the java-users list:

---

Hi ho peoples.

We have an application that is internationalized, and stores data from many languages (each project has it's own index, mostly aligned with a single language, maybe 2).

Anyway, I've noticed during some thread dumps diagnosing some performance issues, that there appears to be a _potential_ synchronization bottleneck using Locale-based sorting of Strings.  I don't think this problem is the root cause of our performance problem, but I thought I'd mention it here.  Here's the stack dump of a thread waiting:

""http-1001-Processor245"" daemon prio=1 tid=0x31434da0 nid=0x3744 waiting for monitor entry [0x2cd44000..0x2cd45f30]
        at java.text.RuleBasedCollator.compare(RuleBasedCollator.java)
        - waiting to lock <0x6b1e8c68> (a java.text.RuleBasedCollator)
        at org.apache.lucene.search.FieldSortedHitQueue$4.compare(FieldSortedHitQueue.java:320)
        at org.apache.lucene.search.FieldSortedHitQueue.lessThan(FieldSortedHitQueue.java:114)
        at org.apache.lucene.util.PriorityQueue.upHeap(PriorityQueue.java:120)
        at org.apache.lucene.util.PriorityQueue.put(PriorityQueue.java:47)
        at org.apache.lucene.util.PriorityQueue.insert(PriorityQueue.java:58)
        at org.apache.lucene.search.FieldSortedHitQueue.insert(FieldSortedHitQueue.java:90)
        at org.apache.lucene.search.FieldSortedHitQueue.insert(FieldSortedHitQueue.java:97)
        at org.apache.lucene.search.TopFieldDocCollector.collect(TopFieldDocCollector.java:47)
        at org.apache.lucene.search.BooleanScorer2.score(BooleanScorer2.java:291)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:132)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:110)
        at com.aconex.index.search.FastLocaleSortIndexSearcher.search(FastLocaleSortIndexSearcher.java:90)
.....

In our case we had 12 threads waiting like this, while one thread had the lock on the RuleBasedCollator.  Turns out RuleBasedCollator's.compare(...) method is synchronized.  I wonder if a ThreadLocal based collator would be better here... ?  There doesn't appear to be a reason for other threads searching the same index to wait on this sort.  Be just as easy to use their own.  (Is RuleBasedCollator a ""heavy"" object memory wise?  Wouldn't have thought so, per thread)

Thoughts?

---

I've investigated this somewhat, and agree that this is a potential problem with a series of possible workarounds. Further discussion (including proof-of-concept patch) to follow."
"LUCENE-433","OTHER","BUG","Issue LUCENE-352 was closed, but the patch there is not applied in the current trunk","See here:
http://issues.apache.org/jira/browse/LUCENE-352

And thanks for making JIRA easier, I noticed the Lucene Java project
was preselected for me.

Regards,
Paul Elschot"
"LUCENE-3065","IMPROVEMENT","IMPROVEMENT","NumericField should be stored in binary format in index (matching Solr's format)","(Spinoff of LUCENE-3001)

Today when writing stored fields we don't record that the field was a NumericField, and so at IndexReader time you get back an ""ordinary"" Field and your number has turned into a string.  See https://issues.apache.org/jira/browse/LUCENE-1701?focusedCommentId=12721972&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12721972

We have spare bits already in stored fields, so, we should use one to record that the field is numeric, and then encode the numeric field in Solr's more-compact binary format.

A nice side-effect is we fix the long standing issue that you don't get a NumericField back when loading your document."
"LUCENE-1295","REFACTORING","IMPROVEMENT","Make retrieveTerms(int docNum) public in MoreLikeThis","It would be useful if 
{code}
private PriorityQueue retrieveTerms(int docNum) throws IOException {
{code}

were public, since it is similar in use to 
{code}
public PriorityQueue retrieveTerms(Reader r) throws IOException {
{code}

It also seems useful to add 
{code}
public String [] retrieveInterestingTerms(int docNum) throws IOException{
{code}
to mirror the one that works on Reader.

"
"LUCENE-2994","TEST","TASK","When 3.1 is released, update backwards tests in 3.x branch","When we have released the official artifacts of Lucene 3.1 (the final ones!!!), we need to do the following:

- svn rm backwards/src/test
- svn cp https://svn.apache.org/repos/asf/lucene/dev/branches/lucene_solr_3_1/lucene/src/test backwards/src/test
- Copy the lucene-core-3.1.0.jar from the last release tarball to lucene/backwards/lib and delete old one.
- Check that everything is correct: The backwards folder should contain a src/ folder that now contains ""test"". The files should be the ones from the branch.
- Run ""ant test-backwards""

Uwe will take care of this!"
"LUCENE-346","IMPROVEMENT","BUG","[PATCH] disable coord for generated BooleanQueries","Here's a patch that disables Similiarty.coord() in the scoring of most
automatically generated boolean queries.  The coord() score factor is
appropriate when clauses are independently specified by a user, but is usually
not appropriate when clauses are generated automatically, e.g., by a fuzzy,
wildcard or range query.  Matches on such automatically generated queries are
currently penalized for not matching all terms."
"LUCENE-654","DOCUMENTATION","","GData-Server - Website sandbox part","Added GData-Server to the sandbox part of the website -- xdocs/sandbox/

Build of website is fine."
"LUCENE-329","BUG","BUG","Fuzzy query scoring issues","Queries which automatically produce multiple terms (wildcard, range, prefix, 
fuzzy etc)currently suffer from two problems:

1) Scores for matching documents are significantly smaller than term queries 
because of the volume of terms introduced (A match on query Foo~ is 0.1 
whereas a match on query Foo is 1).
2) The rarer forms of expanded terms are favoured over those of more common 
forms because of the IDF. When using Fuzzy queries for example, rare mis-
spellings typically appear in results before the more common correct spellings.


I will attach a patch that corrects the issues identified above by 
1) Overriding Similarity.coord to counteract the downplaying of scores 
introduced by expanding terms.
2) Taking the IDF factor of the most common form of expanded terms as the 
basis of scoring all other expanded terms."
"LUCENE-3404","BUG","BUG","testIWondiskfull checkindex failure","looks like charlie cron created a corrupt index on disk full.. can't reproduce with the seed on this machine, i can try on that VM with the same environment and see if i have better luck."
"LUCENE-3234","RFE","IMPROVEMENT","Provide limit on phrase analysis in FastVectorHighlighter","With larger documents, FVH can spend a lot of time trying to find the best-scoring snippet as it examines every possible phrase formed from matching terms in the document.  If one is willing to accept
less-than-perfect scoring by limiting the number of phrases that are examined, substantial speedups are possible.  This is analogous to the Highlighter limit on the number of characters to analyze.

The patch includes an artifical test case that shows > 1000x speedup.  In a more normal test environment, with English documents and random queries, I am seeing speedups of around 3-10x when setting phraseLimit=1, which has the effect of selecting the first possible snippet in the document.  Most of our sites operate in this way (just show the first snippet), so this would be a big win for us.

With phraseLimit = -1, you get the existing FVH behavior. At larger values of phraseLimit, you may not get substantial speedup in the normal case, but you do get the benefit of protection against blow-up in pathological cases.
"
"LUCENE-1322","IMPROVEMENT","IMPROVEMENT","Remove synchronization in CompoundFileReader","Currently there is what seems to be unnecessary synchronization in CompoundFileReader.  This is solved by cloning the base IndexInput.  Synchronization in low level IO classes creates lock contention on highly multi threaded Lucene installations, so much so that in many cases the CPU utilization never reaches the maximum without using something like ParallelMultiSearcher."
"LUCENE-238","CLEANUP","BUG","[PATCH] import cleanup","This patch just removes useless imports so you get less warnings in Eclipse."
"LUCENE-515","BUG","BUG","Using ConstantScoreQuery on a RemoteSearchable throws java.io.NotSerializableException","Using a ConstantScoreQuery through a MultiSearcher on a Searchable obtained through RMI (RemoteSearchable) will throw a java.io.NotSerializableException

The problem seems to be the fact that the ConstantScoreQuery.ConstantWeight has a Searcher member variable which is not serializable. Keeping a reference to the Searcher does not seem to be required: the fix seems trivial.

I've created the TestCase to reproduce the issue and the patch to fix it."
"LUCENE-2537","BUG","BUG","FSDirectory.copy() impl is unsafe","There are a couple of issues with it:

# FileChannel.transferFrom documents that it may not copy the number of bytes requested, however we don't check the return value. So need to fix the code to read in a loop until all bytes were copied..
# When calling addIndexes() w/ very large segments (few hundred MBs in size), I ran into the following exception (Java 1.6 -- Java 1.5's exception was cryptic):
{code}
Exception in thread ""main"" java.io.IOException: Map failed
    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:770)
    at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:450)
    at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:523)
    at org.apache.lucene.store.FSDirectory.copy(FSDirectory.java:450)
    at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:3019)
Caused by: java.lang.OutOfMemoryError: Map failed
    at sun.nio.ch.FileChannelImpl.map0(Native Method)
    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:767)
    ... 7 more
{code}

I changed the impl to something like this:
{code}
long numWritten = 0;
long numToWrite = input.size();
long bufSize = 1 << 26;
while (numWritten < numToWrite) {
  numWritten += output.transferFrom(input, numWritten, bufSize);
}
{code}

And the code successfully adds the indexes. This code uses chunks of 64MB, however that might be too large for some applications, so we definitely need a smaller one. The question is how small so that performance won't be affected, and it'd be great if we can let it be configurable, however since that API is called by other API, such as addIndexes, not sure it's easily controllable.

Also, I read somewhere (can't remember now where) that on Linux the native impl is better and does copy in chunks. So perhaps we should make a Linux specific impl?"
"LUCENE-1832","BUG","BUG","minor/nitpick TermInfoReader bug ?","Some code flagged by a bytecode static analyzer - I guess a nitpick, but we should just drop the null check in the if? If its null it will fall to the below code and then throw a NullPointer exception anyway. Keeping the nullpointer check implies we expect its possible - but then we don't handle it correctly.

{code}
  /** Returns the nth term in the set. */
  final Term get(int position) throws IOException {
    if (size == 0) return null;

    SegmentTermEnum enumerator = getThreadResources().termEnum;
    if (enumerator != null && enumerator.term() != null &&
        position >= enumerator.position &&
	position < (enumerator.position + totalIndexInterval))
      return scanEnum(enumerator, position);      // can avoid seek

    seekEnum(enumerator, position/totalIndexInterval); // must seek
    return scanEnum(enumerator, position);
  }

{code}"
"LUCENE-1303","BUG","BUG","BoostingTermQuery's explanation should be marked as Match even if the payload part negated or zero'ed it","Since BTQ multiplies the payload on the score it might return a negative score.
The explanation should be marked as ""Match"" otherwise it is not added to container explanations,
See also in LUCENE-1302."
"LUCENE-212","BUG","BUG","FileInputStream never closed in HTMLParser","HTMLParser.java contains this code: 
 
  public HTMLParser(File file) throws FileNotFoundException { 
    this(new FileInputStream(file)); 
  } 
 
This FileInputStream should be closed with the close() method, as there's no 
guarantee that the garbage collection will run and do this for you. I don't 
know how to fix this without changing the API to take a FileInputStream 
instead of a File, as the call to this() must be the first thing in the 
constructor, i.e. you cannot create the stream, call this(...), and then close 
the stream."
"LUCENE-3492","TEST","IMPROVEMENT","Extract a generic framework for running randomized tests.","{color:red}The work on this issue is temporarily at github{color} (lots of experiments and tweaking):
https://github.com/carrotsearch/randomizedtesting
Or directly: git clone git://github.com/carrotsearch/randomizedtesting.git
{color}
----

RandomizedRunner is a JUnit runner, so it is capable of running @Test-annotated test cases. It
respects regular lifecycle hooks such as @Before, @After, @BeforeClass or @AfterClass, but it
also adds the following:

Randomized, but repeatable execution and infrastructure for dealing with randomness:

- uses pseudo-randomness (so that a given run can be repeated if given the same starting seed)
  for many things called ""random"" below,
- randomly shuffles test methods to ensure they don't depend on each other,
- randomly shuffles hooks (within a given class) to ensure they don't depend on each other,
- base class RandomizedTest provides a number of methods for generating random numbers, strings
  and picking random objects from collections (again, this is fully repeatable given the
  initial seed if there are no race conditions),
- the runner provides infrastructure to augment stack traces with information about the initial
  seeds used for running the test, so that it can be repeated (or it can be determined that
  the test is not repeatable -- this indicates a problem with the test case itself).

Thread control:

- any threads created as part of a test case are assigned the same initial random seed 
  (repeatability),
- tracks and attempts to terminate any Threads that are created and not terminated inside 
  a test case (not cleaning up causes a test failure),
- tracks and attempts to terminate test cases that run for too long (default timeout: 60 seconds,
  adjustable using global property or annotations),

Improved validation and lifecycle support:

- RandomizedRunner uses relaxed contracts of hook methods' accessibility (hook methods _can_ be
  private). This helps in avoiding problems with method shadowing (static hooks) or overrides
  that require tedious super.() chaining). Private hooks are always executed and don't affect
  subclasses in any way, period.
- @Listeners annotation on a test class allows it to hook into the execution progress and listen
  to events.
- @Validators annotation allows a test class to provide custom validation strategies 
  (project-specific). For example a base class can request specific test case naming strategy
  (or reject JUnit3-like methods, for instance).
- RandomizedRunner does not ""chain"" or ""suppress"" exceptions happening during execution of 
  a test case (including hooks). All exceptions are reported as soon as they happened and multiple
  failure reports can occur. Most environments we know of then display these failures sequentially
  allowing a clearer understanding of what actually happened first.
"
"LUCENE-1845","BUILD_SYSTEM","IMPROVEMENT","if the build fails to download JARs for contrib/db, just skip its tests","Every so often our nightly build fails because contrib/db is unable to download the necessary BDB JARs from http://downloads.osafoundation.org.  I think in such cases we should simply skip contrib/db's tests, if it's the nightly build that's running, since it's a false positive failure."
"LUCENE-877","DOCUMENTATION","BUG","2.1 Locking documentation in ""Apache Lucene - Index File Formats"" section ""6.2 Lock File"" out dated","I am in the process to migrate from Lucene 2.0 to Lucene 2.1.

From reading the Changes document I understand that the write locks are now written into the index folder instead of the java.io.tmpdir. 

In the ""Apache Lucene - Index File Formats"" document in section ""6.2 Lock File"" I read that there is a write lock used to indicate that another process is writing into the index and that this file is stored in the java.io.tempdir.

This is confusing to me.  I had the impression all lock files go into the index folder now.  And using the the java.io.tempdir is only local and does not support access to shared index folders.

Do I miss something here or is the documentation not updated?
"
"LUCENE-1002","BUILD_SYSTEM","BUG","Nightly Builds","Nightly builds for Lucene are HUGE due to the inclusion of the contrib/benchmark temp and work directories.  These directories should be excluded."
"LUCENE-1019","RFE","IMPROVEMENT","CustomScoreQuery should support multiple ValueSourceQueries","CustomScoreQuery's constructor currently accepts a subQuery, and a ValueSourceQuery.  I would like it to accept multiple ValueSourceQueries.  The workaround of nested CustomScoreQueries works for simple cases, but it quickly becomes either cumbersome to manage, or impossible to implement the desired function.

This patch implements CustomMultiScoreQuery with my desired functionality, and refactors CustomScoreQuery to implement the special case of a CustomMultiScoreQuery with 0 or 1 ValueSourceQueries.  This keeps the CustomScoreQuery API intact.

This patch includes basic tests, more or less taken from the original implementation, and customized a bit to cover the new cases."
"LUCENE-493","DOCUMENTATION","BUG","Nightly build archives do not contain Java source code.","Under the Lucene News section of the Overview page, this item's link:

26 January 2006 - Nightly builds available
http://cvs.apache.org/dist/lucene/java/nightly/

goes to a directory with several 1.9M files, none of which have the src/java tree in them."
"LUCENE-3574","RFE","RFE","Add some more constants for newer Java versions to Constants.class, remove outdated ones.","Preparation for LUCENE-3235:
This adds constants to quickly detect Java6 and Java7 to Constants.java. It also deprecated and removes the outdated historical Java versions."
"LUCENE-1546","RFE","IMPROVEMENT","Add IndexReader.flush(commitUserData)","IndexWriter offers a commit(String commitUserData) method.
IndexReader can commit as well using the flush/close methods and so
needs an analogous method that accepts commitUserData."
"LUCENE-2419","TEST","IMPROVEMENT","Improve parallel tests","As mentioned on the dev@ mailing list here: http://www.lucidimagination.com/search/document/93432a677917b9bd/lucenejunitresultformatter_sometimes_fails_to_lock

It would be useful to not create a lockfactory for each test suite (As they are run sequentially in the same separate JVM).
Additionally, we create a lot of JVMs (26) for each batch, because we have to run one for each letter.
Instead, we use a technique here to divide up the tests with a custom selector: http://blog.code-cop.org/2009/09/parallel-junit.html
(I emailed the blog author and received permission to use this code)

This gives a nice boost to the speed of overall tests, especially Solr tests, as many start with an ""S"", but this is no longer a problem.

"
"LUCENE-2282","DESIGN_DEFECT","IMPROVEMENT","Expose IndexFileNames as public, and make use of its methods in the code","IndexFileNames is useful for applications that extend Lucene, an in particular those who extend Directory or IndexWriter. It provides useful constants and methods to query whether a certain file is a core Lucene file or not. In addition, IndexFileNames should be used by Lucene's code to generate segment file names, or query whether a certain file matches a certain extension.

I'll post the patch shortly."
"LUCENE-832","BUG","BUG","NPE when calling isCurrent() on a ParallellReader","As demonstrated by the test case below, if you call isCurrent() on a ParallelReader it causes an NPE. Fix appears to be to add an isCurrent() to ParallelReader which calls it on the underlying indexes but I'm not sure what other problems may be lurking here. Do methods such as getVersion(), lastModified(), isOptimized() also have to be rewritten or is this a use case where ParallelReader will never mimic IndexReader perfectly? At the very least this behavior should be documented so others know what to expect.


    [junit] Testcase: testIsCurrent(org.apache.lucene.index.TestParallelReader):        Caused an ERROR
    [junit] null
    [junit] java.lang.NullPointerException
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:502)
    [junit]     at org.apache.lucene.index.SegmentInfos.readCurrentVersion(SegmentInfos.java:336)
    [junit]     at org.apache.lucene.index.IndexReader.isCurrent(IndexReader.java:316)
    [junit]     at org.apache.lucene.index.TestParallelReader.testIsCurrent(TestParallelReader.java:146)



Index: src/test/org/apache/lucene/index/TestParallelReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestParallelReader.java    (revision 518122)
+++ src/test/org/apache/lucene/index/TestParallelReader.java    (working copy)
@@ -135,6 +135,15 @@
       assertEquals(docParallel.get(""f4""), docSingle.get(""f4""));
     }
   }
+  
+  public void testIsCurrent() throws IOException {
+    Directory dir1 = getDir1();
+    Directory dir2 = getDir2();
+    ParallelReader pr = new ParallelReader();
+    pr.add(IndexReader.open(dir1));
+    pr.add(IndexReader.open(dir2));
+    assertTrue(pr.isCurrent());
+  }
 
   // Fiels 1-4 indexed together:
   private Searcher single() throws IOException {
"
"LUCENE-546","BUG","BUG","Index corruption when using RAMDirectory( Directory) constructor","LUCENE-475 introduced a bug in creating RAMDirectories for large indexes. It truncates the length of the file to an int, from its original long value. Any files that are larger than an int are truncated. Patch to fix is attached."
"LUCENE-1882","REFACTORING","BUG","move SmartChineseAnalyzer into the smartcn package","an offshoot of LUCENE-1862, org.apache.lucene.analysis.cn.SmartChineseAnalyzer should become org.apache.lucene.analysis.cn.smartcn.SmartChineseAnalyzer"
"LUCENE-1775","REFACTORING","TASK","Change remaining contrib streams/filters to use new TokenStream API","All other contrib streams/filters have already been converted with LUCENE-1460.

The two shingle filters are the last ones we need to convert."
"LUCENE-1243","RFE","IMPROVEMENT","A few new benchmark tasks","Some tasks that would be helpful to see added. Might want some expansion, but here are some basic ones I have been using:

CommitIndexTask
ReopenReaderTask
SearchWithSortTask

I do the sort in a similar way that the highlighting was done, but another method may be better. Just would be great to have sorting.
Also, since there is no great field for sorting (reuters date always appears to be the same) I changed the id field from doc+id to just id. Again maybe not the best solution, but here I am to get the ball rolling :)"
"LUCENE-2080","DOCUMENTATION","TASK","Improve the documentation of Version","In my opinion, we should elaborate more on the effects of changing the Version parameter.
Particularly, changing this value, even if you recompile your code, likely involves reindexing your data.
I do not think this is adequately clear from the current javadocs.
"
"LUCENE-646","DOCUMENTATION","IMPROVEMENT","[PATCH] fix various small issues with the ""getting started"" demo pages","
This patch contains numerous small fixes for the ""getting started""
pages on the Lucene Java web site.  Here are the rough fixes:

  * To results.jsp:

    - changed StopAnalyzer -> StandardAnalyzer

    - changed references of ""url"" to ""path"" (field ""url"" is never set
      and was therefore always null)

    - remove prefix of ""../webapps"" from path so clicking through works

  * Fixed typos, grammar and other cosmetic things.

  * Modernized some things that have changed with time (names of JAR
    files, which languages have analyzers, etc.)

  * Added outbound links to Javadocs, Wiki, Lucene static web site,
    external sites, when appropriate.

  * Removed exact version of Tomcat for the demo web app (I think all
    recent versions of Tomcat will work as described)

  * Other small changes...

Net/net I think this is an improved version of what's available on the
site today."
"LUCENE-1323","BUG","BUG","MultiReader should make a private copy of the subReaders array","Spinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200806.mbox/%3C88F3F6A4-FBFB-43DF-890D-DB5F0D9A2461@gmail.com%3E

Because MultiReader just holds a reference to the array that was passed in, it's possible to hit scary exceptions (that look like index corruption) if that array is later altered eg by reopening some of the readers.

The fix is trivial: just make a private copy."
"LUCENE-2809","BUG","BUG","IndexWriter.numDocs doesn't take into account applied but not flushed deletes","The javadoc states that buffered deletes are not taken into account and so you must call commit first.

But, if you do that, and you're using CMS, and you're unlucky enough to have a background merge commit just after you call commit but before you call .numDocs, you can still get a wrong count back.

The fix is trivial -- numDocs should also consult any pooled readers for their current del count.

This is causing an intermittent failure in the new TestNRTThreads.
"
"LUCENE-2911","REFACTORING","","synchronize grammar/token types across StandardTokenizer, UAX29EmailURLTokenizer, ICUTokenizer, add CJK types.","I'd like to do LUCENE-2906 (better cjk support for these tokenizers) for a future target such as 3.2

But, in 3.1 I would like to do a little cleanup first, and synchronize all these token types, etc.
"
"LUCENE-1544","BUG","BUG","Deadlock: IndexWriter.addIndexes(IndexReader[])","A deadlock issue occurs under the following circumstances
- IndexWriter.autoCommit == true
- IndexWriter.directory contains multiple segments
- IndexWriter.AddIndex(IndexReader[]) is invoked

I put together a JUnit test that recreates the deadlock, which I've attached.  It is the first test method, 'testAddIndexByIndexReader()'.

In a nutshell, here is what happens:

        // 1) AddIndexes(IndexReader[]) acquires the write lock,
        // then begins optimization of destination index (this is
        // prior to adding any external segments).
        //
        // 2) Main thread starts a ConcurrentMergeScheduler.MergeThread
        // to merge the 2 segments.
        //
        // 3) Merging thread tries to acquire the read lock at
        // IndexWriter.blockAddIndexes(boolean) in
        // IndexWriter.StartCommit(), but cannot as...
        //
        // 4) Main thread still holds the write lock, and is
        // waiting for the IndexWriter.runningMerges data structure
        // to be devoid of merges with their optimize flag
        // set (IndexWriter.optimizeMergesPending()).
"
"LUCENE-1673","RFE","RFE","Move TrieRange to core","TrieRange was iterated many times and seems stable now (LUCENE-1470, LUCENE-1582, LUCENE-1602). There is lots of user interest, Solr added it to its default FieldTypes (SOLR-940) and if possible I want to move it to core before release of 2.9.
Before this can be done, there are some things to think about:
# There are now classes called LongTrieRangeQuery, IntTrieRangeQuery, how should they be called in core? I would suggest to leave it as it is. On the other hand, if this keeps our only numeric query implementation, we could call it LongRangeQuery, IntRangeQuery or NumericRangeQuery (see below, here are problems). Same for the TokenStreams and Filters.
# Maybe the pairs of classes for indexing and searching should be moved into one class: NumericTokenStream, NumericRangeQuery, NumericRangeFilter. The problem here: ctors must be able to pass int, long, double, float as range parameters. For the end user, mixing these 4 types in one class is hard to handle. If somebody forgets to add a L to a long, it suddenly instantiates a int version of range query, hitting no results and so on. Same with other types. Maybe accept java.lang.Number as parameter (because nullable for half-open bounds) and one enum for the type.
# TrieUtils move into o.a.l.util? or document or?
# Move TokenStreams into o.a.l.analysis, ShiftAttribute into o.a.l.analysis.tokenattributes? Somewhere else?
# If we rename the classes, should Solr stay with Trie (because there are different impls)?
# Maybe add a subclass of AbstractField, that automatically creates these TokenStreams and omits norms/tf per default for easier addition to Document instances?"
"LUCENE-3390","BUG","BUG","Incorrect sort by Numeric values for documents missing the sorting field","While sorting results over a numeric field, documents which do not contain a value for the sorting field seem to get 0 (ZERO) value in the sort. (Tested against Double, Float, Int & Long numeric fields ascending and descending order).
This behavior is unexpected, as zero is ""comparable"" to the rest of the values. A better solution would either be allowing the user to define such a ""non-value"" default, or always bring those document results as the last ones.

Example scenario:
Adding 3 documents, 1st with value 3.5d, 2nd with -10d, and 3rd without any value.
Searching with MatchAllDocsQuery, with sort over that field in descending order yields the docid results of 0, 2, 1.

Asking for the top 2 documents brings the document without any value as the 2nd result - which seems as a bug?"
"LUCENE-2079","IMPROVEMENT","IMPROVEMENT","Further improvements to contrib/benchmark for testing NRT","Some small changes:

  * Allow specifying a priority for BG threads, after the ""&""
    character; priority increment is + or - int that's added to main
    thread's priority to set child thread's.  For my NRT tests I make
    the reopen thread +2, the indexing threads +1, and leave searching
    threads at their default.

  * Added test case

  * NearRealTimeReopenTask now reports @ the end the full array of
    msec of each reopen latency

  * Added optional breakout of counts by time steps.  If you set
    log.time.step.msec to eg 1000 then reported counts for serial task
    sequence is broken out by 1 second windows.  EG you can use this
    to measure slowdown over time.
"
"LUCENE-2943","BUG","BUG","ICU collator thread-safety issues","The ICU Collators (unlike the JDK ones) aren't thread safe: http://userguide.icu-project.org/collation/architecture , a little non-obvious since its not mentioned
in the javadocs, and its not clear if the docs apply to only the C code, but i looked
at the source and there is all kinds of internal state.

So in my opinion, we should clone the icu collators (which are passed in from the outside) 
when creating a new TokenStream/AttributeImpl to prevent problems. This shouldn't be a big
deal since everything uses reusableTokenStream anyway.
"
"LUCENE-3260","BUG","BUG","need a test that uses termsenum.seekExact() (which returns true), then calls next()","i tried to do some seekExact (where the result must exist) then next()ing in the faceting module,
and it seems like there could be a bug here.

I think we should add a test that mixes seekExact/seekCeil/next like this, to ensure that
if seekExact returns true, that the enum is properly positioned."
"LUCENE-2633","RFE","BUG","PackedInts does not support structures above 256MB","The PackedInts Packed32 and Packed64 fails when the internal structure exceeds 256MB. This is due to a missing cast that results in the bit position calculation being limited by Integer.MAX_VALUE (256MB * 8 = 2GB)."
"LUCENE-1531","IMPROVEMENT","IMPROVEMENT","contrib/xml-query-parser, BoostingTermQuery support","I'm not 100% on this patch. 

BooleanTermQuery is a part of the spans family, but I generally use that class as a replacement for TermQuery.  Thus in the DTD I have stated that it can be a part of the root queries as well as a part of a span. 

However, SpanFooQueries xml elements are named <SpanFoo/> rather than <SpanFooQuery/>, I have however chosen to call it <BoostingTermQuery/>. It would be possible to set it up so it would be parsed as <SpanBoostingTerm/> when inside of a <SpanSomething>, but I just find that confusing.
"
"LUCENE-1123","BUILD_SYSTEM","TASK","Allow overriding the specification version in MANIFEST.MF","The specification version in MANIFEST.MF should only consist of
digits. When we e. g. build a release candidate with a version like
2.3-rc1 then we have to specify a different specification version.

See related discussion:
http://www.gossamer-threads.com/lists/lucene/java-dev/56611

"
"LUCENE-1219","RFE","IMPROVEMENT","support array/offset/ length setters for Field with binary data","currently Field/Fieldable interface supports only compact, zero based byte arrays. This forces end users to create and copy content of new objects before passing them to Lucene as such fields are often of variable size. Depending on use case, this can bring far from negligible  performance  improvement. 

this approach extends Fieldable interface with 3 new methods   
getOffset(); gettLenght(); and getBinaryValue() (this only returns reference to the array)

   "
"LUCENE-2814","REFACTORING","IMPROVEMENT","stop writing shared doc stores across segments","Shared doc stores enables the files for stored fields and term vectors to be shared across multiple segments.  We've had this optimization since 2.1 I think.

It works best against a new index, where you open an IW, add lots of docs, and then close it.  In that case all of the written segments will reference slices a single shared doc store segment.

This was a good optimization because it means we never need to merge these files.  But, when you open another IW on that index, it writes a new set of doc stores, and then whenever merges take place across doc stores, they must now be merged.

However, since we switched to shared doc stores, there have been two optimizations for merging the stores.  First, we now bulk-copy the bytes in these files if the field name/number assignment is ""congruent"".  Second, we now force congruent field name/number mapping in IndexWriter.  This means this optimization is much less potent than it used to be.

Furthermore, the optimization adds *a lot* of hair to IndexWriter/DocumentsWriter; this has been the source of sneaky bugs over time, and causes odd behavior like a merge possibly forcing a flush when it starts.  Finally, with DWPT (LUCENE-2324), which gets us truly concurrent flushing, we can no longer share doc stores.

So, I think we should turn off the write-side of shared doc stores to pave the path for DWPT to land on trunk and simplify IW/DW.  We still must support reading them (until 5.0), but the read side is far less hairy."
"LUCENE-2767","BUG","BUG","Missing sync in IndexWriter.addIndexes(IndexReader[])","The 3.x build just hit this:

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Testcase: testAddIndexesWithThreads(org.apache.lucene.index.TestAddIndexes):	FAILED
    [junit] expected:<3160> but was:<2701>
    [junit] junit.framework.AssertionFailedError: expected:<3160> but was:<2701>
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:779)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:745)
    [junit] 	at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithThreads(TestAddIndexes.java:708)
    [junit] 
    [junit] 
    [junit] Tests run: 15, Failures: 1, Errors: 0, Time elapsed: 9.28 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] java.lang.AssertionError: RefCount is 0 pre-decrement for file ""_8a.tvf""
    [junit] 	at org.apache.lucene.index.IndexFileDeleter$RefCount.DecRef(IndexFileDeleter.java:608)
    [junit] 	at org.apache.lucene.index.IndexFileDeleter.decRef(IndexFileDeleter.java:505)
    [junit] 	at org.apache.lucene.index.IndexFileDeleter.decRef(IndexFileDeleter.java:496)
    [junit] 	at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:2972)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes.doBody(TestAddIndexes.java:681)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:624)
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithThreads -Dtests.seed=-6912763261803132408:-5575674032550262483 -Dtests.multiplier=3
    [junit] NOTE: test params are: locale=en_AU, timezone=America/Atka
{noformat}

It looks like it's caused by a long-standing missing sync (since at least 2.9.x).  I think likely we hit it just now thanks to adding random Thread.yield()'s in MockDirWrapper!"
"LUCENE-811","DESIGN_DEFECT","BUG","Public API inconsistency","org.apache.lucene.index.SegmentInfos is public, and contains public methods (which is good for expert-level index manipulation tools such as Luke). However, SegmentInfo class has package visibility. This leads to a strange result that it's possible to read SegmentInfos, but it's not possible to access its details (SegmentInfos.info(int)) from a user application.

The solution is to make SegmentInfo class public."
"LUCENE-2251","TEST","","Change contrib tests to use the special LuceneTestCase(J4) constant for the current version used a matchVersion parameter","Sub issue for contrib changes"
"LUCENE-3734","DESIGN_DEFECT","","Allow customizing/subclassing of DirectoryReader","DirectoryReader is final and has only static factory methods. It is not possible to subclass it in any way.

The problem is mainly Solr, as Solr accesses directory(), IndexCommits,... and therefore cannot work on abstract IndexReader anymore. This should be changed, by e.g. handling reopening in the IRFactory, also versions, commits,... Currently its not possible to implement any other IRFactory that returns something else.

On the other hand, it should be possible to ""wrap"" a DirectoryReader / CompositeReader to handle filtering of collection based information (subreaders, reopening hooks,...). This can be done by making DirectoryReader abstract and let DirectoryReader.open return a internal hidden class ""StandardDirectoryReader"". This is similar to the relatinship between IndexReader and hidden DirectoryReader in the past.

DirectoryReader will have final implementations of most methods like getting document stored fields, global docFreq and other statistics, but allows hooking into doOpenIfChanged. Also it should not be limited to SegmentReaders as childs - any AtomicReader is fine. This allows users to create e.g. a Directory-based ParallelReader (see LUCENE-3736) that supports reopen and (partially commits)."
"LUCENE-3507","IMPROVEMENT","IMPROVEMENT","Improve Memory Consumption for merging DocValues SortedBytes variants","Currently SortedBytes are loaded into memory during merge which could be a potential trap. Instead of loading them into Heap memory we can merge those sorted values with much smaller memory and without loading all values into ram."
"LUCENE-3419","TEST","IMPROVEMENT","Resolve JUnit assert deprecations","Many tests use assertEquals methods which have been deprecated.  The culprits are assertEquals(float, float), assertEquals(double, double) and assertEquals(Object[], Object[]).  Although not a big issue, they annoy me every time I see them so I'm going to fix them."
"LUCENE-1749","RFE","IMPROVEMENT","FieldCache introspection API","FieldCache should expose an Expert level API for runtime introspection of the FieldCache to provide info about what is in the FieldCache at any given moment.  We should also provide utility methods for sanity checking that the FieldCache doesn't contain anything ""odd""...
   * entries for the same reader/field with different types/parsers
   * entries for the same field/type/parser in a reader and it's subreader(s)
   * etc...


"
"LUCENE-3062","BUG","BUG","TestBytesRefHash#testCompact is broken","TestBytesRefHash#testCompact fails when run with ant test -Dtestcase=TestBytesRefHash -Dtestmethod=testCompact -Dtests.seed=-7961072421643387492:5612141247152835360
{noformat}

    [junit] Testsuite: org.apache.lucene.util.TestBytesRefHash
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.454 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestBytesRefHash -Dtestmethod=testCompact -Dtests.seed=-7961072421643387492:5612141247152835360
    [junit] NOTE: test params are: codec=PreFlex, locale=et, timezone=Pacific/Tahiti
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestBytesRefHash]
    [junit] NOTE: Linux 2.6.35-28-generic amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=12,threads=1,free=363421800,total=379322368
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testCompact(org.apache.lucene.util.TestBytesRefHash):	Caused an ERROR
    [junit] bitIndex < 0: -27
    [junit] java.lang.IndexOutOfBoundsException: bitIndex < 0: -27
    [junit] 	at java.util.BitSet.set(BitSet.java:262)
    [junit] 	at org.apache.lucene.util.TestBytesRefHash.testCompact(TestBytesRefHash.java:146)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1260)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1189)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.util.TestBytesRefHash FAILED
{noformat}

the test expects that _TestUtil.randomRealisticUnicodeString(random, 1000); will never return the same string.

I will upload a patch soon."
"LUCENE-2524","BUG","BUG","when many query clases are specified in boolean or dismax query, highlighted terms are always ""yellow"" if multi-colored feature is used","The problem is the following snippet:

{code}
protected String getPreTag( int num ){
  return preTags.length > num ? preTags[num] : preTags[0];
}
{code}

it should be:

{code}
protected String getPreTag( int num ){
  int n = num % preTags.length;
  return  preTags[n];
}
{code}
"
"LUCENE-3879","BUG","BUG","fix more position corrumptions in 4.0 codecs","Spinoff of LUCENE-3876.

Some codecs have invalid asserts, wrong shift operators etc.

If a position exceeds Integer.MAX_VALUE/2 and then also has a payload,
it will produce corrumpt indexes or other strange errors.

Easiest way to trigger the bugs is to sometimes add a payload to the test from LUCENE-3876."
"LUCENE-2108","BUG","BUG","SpellChecker file descriptor leak - no way to close the IndexSearcher used by SpellChecker internally","I can't find any way to close the IndexSearcher (and IndexReader) that
is being used by SpellChecker internally.

I've worked around this issue by keeping a single SpellChecker open
for each index, but I'd really like to be able to close it and
reopen it on demand without leaking file descriptors.

Could we add a close() method to SpellChecker that will close the
IndexSearcher and null the reference to it? And perhaps add some code
that reopens the searcher if the reference to it is null? Or would
that break thread safety of SpellChecker?

The attached patch adds a close method but leaves it to the user to
call setSpellIndex to reopen the searcher if desired."
"LUCENE-1804","RFE","BUG","Can't specify AttributeSource for Tokenizer","One can't currently specify the attribute source for a Tokenizer like one can with any other TokenStream."
"LUCENE-881","BUG","BUG","QueryParser escaping/parsin issue with strings starting/ending with ||","There is a problem with query parser when search string starts/ends with ||.  When string contains || in the middle like 'something || something' everything runs without a problem.

Part of code: 
  searchText = QueryParser.escape(searchText);
  QueryParser parser = null;
  parser = new QueryParser(fieldName, new CustomAnalyser());
  parser.parse(searchText);

CustomAnalyser class extends Analyser. Here is the only redefined method: 

    @Override
    public TokenStream tokenStream(String fieldName, Reader reader) {
      return new PorterStemFilter( (new StopAnalyzer()).tokenStream(fieldName, reader));
    }

I have tested this on Lucene 2.1 and latest source I have checked-out from SVN (Revision 538867) and in both cases parsing exception was thrown.

Part of Stack Trace (Lucene - SVN checkout - Revision 538867):
Cannot parse 'someting ||': Encountered ""<EOF>"" at line 1, column 11.
Was expecting one of:
    <NOT> ...
    ""+"" ...
    ""-"" ...
    ""("" ...
    ""*"" ...
    <QUOTED> ...
    <TERM> ...
    <PREFIXTERM> ...
    <WILDTERM> ...
    ""["" ...
    ""{"" ...
    <NUMBER> ...
    
 org.apache.lucene.queryParser.ParseException: Cannot parse 'someting ||': Encountered ""<EOF>"" at line 1, column 11.
Was expecting one of:
    <NOT> ...
    ""+"" ...
    ""-"" ...
    ""("" ...
    ""*"" ...
    <QUOTED> ...
    <TERM> ...
    <PREFIXTERM> ...
    <WILDTERM> ...
    ""["" ...
    ""{"" ...
    <NUMBER> ...
    
        at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:150)


Part of Stack Trace (Lucene 2.1):
Cannot parse 'something ||': Encountered ""<EOF>"" at line 1, column 12.
Was expecting one of:
    <NOT> ...
    ""+"" ...
    ""-"" ...
    ""("" ...
    ""*"" ...
    <QUOTED> ...
    <TERM> ...
    <PREFIXTERM> ...
    <WILDTERM> ...
    ""["" ...
    ""{"" ...
    <NUMBER> ...
    
 org.apache.lucene.queryParser.ParseException: Cannot parse 'something ||': Encountered ""<EOF>"" at line 1, column 12.
Was expecting one of:
    <NOT> ...
    ""+"" ...
    ""-"" ...
    ""("" ...
    ""*"" ...
    <QUOTED> ...
    <TERM> ...
    <PREFIXTERM> ...
    <WILDTERM> ...
    ""["" ...
    ""{"" ...
    <NUMBER> ...
    
        at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:149)


"
"LUCENE-1627","DESIGN_DEFECT","BUG","SpellChecker has no ""close"" method","SpellChecker has no close method ... which means there is no way to force it to close the IndexSearcher it maintains when you are done using the SpellChecker.  (a quick skim of IndexSearcher doesn't even suggest there is a finalizer self closing in the event of GC)

http://www.nabble.com/SpellChecker-locks-folder-to23171980.html#a23171980

A hackish work around for people who want to force SpellChecker to close an IndexSearcher opened against a directory they care about doing something with... 
{code}yourSpellChecker.setSpellIndex(new RamDirecotry()){code}"
"LUCENE-398","BUG","BUG","ParallelReader crashes when trying to merge into a new index","ParallelReader causes a NullPointerException in
org.apache.lucene.index.ParallelReader$ParallelTermPositions.seek(ParallelReader.java:318)
when trying to merge into a new index.

See test case and sample output:

$ svn diff
Index: src/test/org/apache/lucene/index/TestParallelReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestParallelReader.java    (revision 179785)
+++ src/test/org/apache/lucene/index/TestParallelReader.java    (working copy)
@@ -57,6 +57,13 @@
 
   }
  
+  public void testMerge() throws Exception {
+    Directory dir = new RAMDirectory();
+    IndexWriter w = new IndexWriter(dir, new StandardAnalyzer(), true);
+    w.addIndexes(new IndexReader[] { ((IndexSearcher)
parallel).getIndexReader() });
+    w.close();
+  }
+
   private void queryTest(Query query) throws IOException {
     Hits parallelHits = parallel.search(query);
     Hits singleHits = single.search(query);
$ ant -Dtestcase=TestParallelReader test
Buildfile: build.xml
[...]
test:
    [mkdir] Created dir:
/Users/skirsch/text/lectures/da/thirdparty/lucene-trunk/build/test
    [junit] Testsuite: org.apache.lucene.index.TestParallelReader
    [junit] Tests run: 2, Failures: 0, Errors: 1, Time elapsed: 1.993 sec

    [junit] Testcase: testMerge(org.apache.lucene.index.TestParallelReader):  
Caused an ERROR
    [junit] null
    [junit] java.lang.NullPointerException
    [junit]     at
org.apache.lucene.index.ParallelReader$ParallelTermPositions.seek(ParallelReader.java:318)
    [junit]     at
org.apache.lucene.index.ParallelReader$ParallelTermDocs.seek(ParallelReader.java:294)
    [junit]     at
org.apache.lucene.index.SegmentMerger.appendPostings(SegmentMerger.java:325)
    [junit]     at
org.apache.lucene.index.SegmentMerger.mergeTermInfo(SegmentMerger.java:296)
    [junit]     at
org.apache.lucene.index.SegmentMerger.mergeTermInfos(SegmentMerger.java:270)
    [junit]     at
org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:234)
    [junit]     at
org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:96)
    [junit]     at
org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:596)
    [junit]     at
org.apache.lucene.index.TestParallelReader.testMerge(TestParallelReader.java:63)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)


    [junit] Test org.apache.lucene.index.TestParallelReader FAILED

BUILD FAILED
/Users/skirsch/text/lectures/da/thirdparty/lucene-trunk/common-build.xml:188:
Tests failed!

Total time: 16 seconds
$"
"LUCENE-1901","BUG","BUG","TermAttributeImpl.equals() does not check termLength","If you look at the code from equals(), I think it misses this check :

other.termLength==this.termLength

This check must be before the comparison of the arrays."
"LUCENE-3087","BUG","BUG","highlighting exact phrase with overlapping tokens fails.","Fields with overlapping token are not highlighted in search results when searching exact phrases, when using TermVector.WITH_OFFSET.

The document builded in MemoryIndex for highlight does not preserve positions of tokens in this case. Overlapping tokens get ""flattened"" (position increment always set to 1), the spanquery used for searching relevant fragment will fail to identify the correct token sequence because the position shift.

I corrected this by adding a position increment calculation in sub class StoredTokenStream. I added junit test covering this case.

I used the eclipse codestyle from trunk, but style add quite a few format differences between repository and working copy files. I tried to reduce them, but some linewrapping rules still doesn't match.

Correction patch joined"
"LUCENE-1755","BUG","BUG","WriteLineDocTask should keep docs w/ just title and no body","WriteLineDocTask throws away a document if it does not have a body element. However, if the document has a title, then it should be kept. Some documents, such as emails, may not have a body which is legitimate. I'll post a patch + a test case."
"LUCENE-976","BUG","BUG","MMapDirectory is missing newly added openInput method to FSDirectory","This issue was caused by the optimizations in LUCENE-888.  The new
openInput(String name, int bufferSize) added to FSDirectory was not
also overridden by MMapDirectory.
"
"LUCENE-1833","REFACTORING","IMPROVEMENT","When we move to java 1.5 in 3.0 we should replace all Interger, Long, etc construction with .valueOf","-128 to 128 are guaranteed to be cached and using valueOf in that case is 3.5 times faster than using contructor"
"LUCENE-3548","BUG","BUG","CharsRef#append broken on trunk & 3.x","Current impl. for append on CharsRef is broken - it overrides the actual content rather than append. its used in many places especially in solr so we might have some broken "
"LUCENE-807","DOCUMENTATION","IMPROVEMENT","Minor improvement to JavaDoc for ScoreDocComparator","About to attach a very small patch for ScoreDocComparator which broadens the contract of compare(ScoreDoc, ScoreDoc) to follow the same semantics as java.util.Comparator.compare() -- allow any integer to be returned, rather than specifically -1/0/-1.

Note that this behaviour must already be acceptable; the anonymous ScoreDocComparators returned by FieldSortedHitQueue.comparatorStringLocale() already return the result of Collator.compare(), which is not tied to this -1/0/1 restriction."
"LUCENE-849","IMPROVEMENT","IMPROVEMENT","contrib/benchmark:  configurable HTML Parser, external classes to path, exhaustive doc maker","""doc making"" enhancements:

1. Allow configurable html parser, with a new html.parser property.
Currently TrecDocMaker is using the Demo html parser. With this new property this can be overridden.

2. allow to add external class path, so the benchmark can be used with modified makers/parsers without having to add code to Lucene.
Run benchmark with e.g. ""ant run-task -Dbenchmark.ext.classpath=/myproj/myclasses""

3. allow to crawl a doc maker until exhausting all its files/docs once, without having to know in advance how many docs it can make. 
This can be useful for instance if the input data is in zip files."
"LUCENE-1929","BUG","BUG","Highlighter doesn't support NumericRangeQuery or deprecated RangeQuery","Sucks. Will throw a NullPointer exception. 

Only NumericRangeQuery will throw the exception.
RangeQuery just won't highlight."
"LUCENE-1666","BUG","BUG","Constants causing NullPointerException when fetching metadata ""Implementation Version"" in MANIFEST","If the MANIFEST.MF file does not contain the metadata IMPLEMENTATION_VERSION, a null value is returned, causing NullPointerException during commit:

Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.lucene.store.IndexOutput.writeString(IndexOutput.java:109)
	at org.apache.lucene.store.IndexOutput.writeStringStringMap(IndexOutput.java:229)
	at org.apache.lucene.index.SegmentInfo.write(SegmentInfo.java:558)
	at org.apache.lucene.index.SegmentInfos.write(SegmentInfos.java:337)
	at org.apache.lucene.index.SegmentInfos.prepareCommit(SegmentInfos.java:808)
	at org.apache.lucene.index.IndexWriter.startCommit(IndexWriter.java:5319)
	at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:3895)
	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:3956)

This happened after having build a jar assembly using Maven, the original MANIFEST.MF of lucene jar has been overwritten, and didn't contain anynore the implementation version metadata.

Path attached."
"LUCENE-693","IMPROVEMENT","BUG","ConjunctionScorer - more tuneup","(See also: #LUCENE-443)
I did some profile testing with the new ConjuctionScorer in 2.1 and discovered a new bottleneck in ConjunctionScorer.sortScorers. The java.utils.Arrays.sort method is cloning the Scorers array on every sort, which is quite expensive on large indexes because of the size of the 'norms' array within, and isn't necessary. 

Here is one possible solution:

  private void sortScorers() {
// squeeze the array down for the sort
//    if (length != scorers.length) {
//      Scorer[] temps = new Scorer[length];
//      System.arraycopy(scorers, 0, temps, 0, length);
//      scorers = temps;
//    }
    insertionSort( scorers,length );
    // note that this comparator is not consistent with equals!
//    Arrays.sort(scorers, new Comparator() {         // sort the array
//        public int compare(Object o1, Object o2) {
//          return ((Scorer)o1).doc() - ((Scorer)o2).doc();
//        }
//      });
  
    first = 0;
    last = length - 1;
  }
  private void insertionSort( Scorer[] scores, int len)
  {
      for (int i=0; i<len; i++) {
          for (int j=i; j>0 && scores[j-1].doc() > scores[j].doc();j-- ) {
              swap (scores, j, j-1);
          }
      }
      return;
  }
  private void swap(Object[] x, int a, int b) {
    Object t = x[a];
    x[a] = x[b];
    x[b] = t;
  }
 
The squeezing of the array is no longer needed. 
We also initialized the Scorers array to 8 (instead of 2) to avoid having to grow the array for common queries, although this probably has less performance impact.

This change added about 3% to query throughput in my testing.

Peter
"
"LUCENE-861","BUG","BUG","Contrib queries package Query implementations do not override equals()","Query implementations should override equals() so that Query instances can be cached and that Filters can know if a Query has been used before.  See the discussion in this thread.

http://www.mail-archive.com/java-user@lucene.apache.org/msg13061.html

Following 3 contrib Query implementations do no override equals()

org.apache.lucene.search.BoostingQuery;
org.apache.lucene.search.FuzzyLikeThisQuery;
org.apache.lucene.search.similar.MoreLikeThisQuery;

Test cases below show the problem.

package com.teamware.office.lucene.search;

import static org.junit.Assert.*;

import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.index.Term;
import org.apache.lucene.search.BoostingQuery;
import org.apache.lucene.search.FuzzyLikeThisQuery;
import org.apache.lucene.search.TermQuery;
import org.apache.lucene.search.similar.MoreLikeThisQuery;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;
public class ContribQueriesEqualsTest
{
    /**
     * @throws java.lang.Exception
     */
    @Before
    public void setUp() throws Exception
    {
    }

    /**
     * @throws java.lang.Exception
     */
    @After
    public void tearDown() throws Exception
    {
    }
    
    /**
     *  Show that the BoostingQuery in the queries contrib package 
     *  does not implement equals() correctly.
     */
    @Test
    public void testBoostingQueryEquals()
    {
        TermQuery q1 = new TermQuery(new Term(""subject:"", ""java""));
        TermQuery q2 = new TermQuery(new Term(""subject:"", ""java""));
        assertEquals(""Two TermQueries with same attributes should be equal"", q1, q2);
        BoostingQuery bq1 = new BoostingQuery(q1, q2, 0.1f);
        BoostingQuery bq2 = new BoostingQuery(q1, q2, 0.1f);
        assertEquals(""BoostingQuery with same attributes is not equal"", bq1, bq2);
    }

    /**
     *  Show that the MoreLikeThisQuery in the queries contrib package 
     *  does not implement equals() correctly.
     */
    @Test
    public void testMoreLikeThisQueryEquals()
    {
        String moreLikeFields[] = new String[] {""subject"", ""body""};
        
        MoreLikeThisQuery mltq1 = new MoreLikeThisQuery(""java"", moreLikeFields, new StandardAnalyzer());
        MoreLikeThisQuery mltq2 = new MoreLikeThisQuery(""java"", moreLikeFields, new StandardAnalyzer());
        assertEquals(""MoreLikeThisQuery with same attributes is not equal"", mltq1, mltq2);
    }
    /**
     *  Show that the FuzzyLikeThisQuery in the queries contrib package 
     *  does not implement equals() correctly.
     */
    @Test
    public void testFuzzyLikeThisQueryEquals()
    {
        FuzzyLikeThisQuery fltq1 = new FuzzyLikeThisQuery(10, new StandardAnalyzer());
        fltq1.addTerms(""javi"", ""subject"", 0.5f, 2);
        FuzzyLikeThisQuery fltq2 = new FuzzyLikeThisQuery(10, new StandardAnalyzer());
        fltq2.addTerms(""javi"", ""subject"", 0.5f, 2);
        assertEquals(""FuzzyLikeThisQuery with same attributes is not equal"", fltq1, fltq2);
    }
}
"
"LUCENE-2516","TEST","TEST","More clarification, improvements and correct behaviour of backwards tests","Backwards tests are used since 2.9 to assert, that the new Lucene version supports drop-in-replacement over the previous version. For this all tests from the previous version are compiled against the old version but then run against the new JAR file.

At the beginning the test suite was checking out another branch and doing this, but this was replaced in 3.1 by directly embedding the previous source tree and the previous tests into the backwards/ subdirectory of the SVN source. The whole idea has several problems:

- Tests not only check *public* APIs, they also check internals and sometimes even fields or package-private methods. This is allowed to change in later versions, so we must be able to change the tests, to support this behaviour. This can be done by modifying the backwards tests to pass, but still use the public API unchanged. Sometimes we simply comment out tests, that test internals and not public APIs. For those tests, I would like to propose a Java Annotation for trunk tests like @LuceneInternalTest - so we can tell the tests runner for backwards (when this test is moved as backwards layer, e.g in 4.1, that it runs all tests *but* not this marked one. This can be done easily with Junit3/4 in LuceneTestCase(J4). This is not part of this issue, but a good idea.
- Sometimes we break backwards compatibility. Currently we do our best to change the tests to reflect this, but this is unneeded and stupi, as it brings two problems. The backwards tests should be compiled against the old version of Lucene. If we change this old Version in the backwards folder, its suddenly becomes nonsense. At least the JAR artifacts of the previous version should stay *unchanged* in all cases! If we break backwards, the correct way to do this, is to simply disable coresponding tests! There is no need to make them work again, as we broke backwards, wy test plugin? The trunk tests already check the functionality, backwards tests only check API. If we fix the break in backwards, we do the contra of what they are for.

So I propose the following and have implemented in a patch for 3.x branch:

- Only include the *tests* and nothing else into the backwards branch, no source files of previous Lucene Core.
- Add the latest released JAR artifact of lucene-core.jar into backwards/lib, optimally with checksum (md5/sh1). This enforces that it is not changed and exactly do what they are for: To compile the previous tests against. This is the only reason for this JAR file.
- If we break backwards, simply *disable* the tests by commenting out, ideally with a short note and the JIRA issue that shows the break.
- If we change inner behaviour of classes, that are not public, dont fix, disable tests. Its simple: backwards tests are only for API compatibility testsing of public APIs. If a test uses internals it should not be run. For that we should use a new annotation in trunk (see above).

This has several good things:

- we can package backwards tests in src ZIP. Its not a full distrib, only the core tests and the JAR file. This enables people that doenloaded the src ZIP files to also run backwrads tests
- Your SVN checkout is not so big and backwards tests run faster!

There are some problems, with one example in the attached patch:

- If we have mock classes in the tests (e.g. MockRAMDirectory) that extend Lucene classes and have access to their internal APIs, a change in these APIs will make them fail to work unchanged. The above example (MockRAMDir) is used in lots of tests and uses a internal RAMDir field that changed type in 3.1. But we cannot disable all tests using this dir (no tests will survive). As we cannot change the previous versions JAR to reflect this, you have to use some trick in this interbal test class. In this case I removed static linking of this field and replaced by reflection. This enables compilation against old JAR, but supports running in new version. This is really a special case, but works good here.

Any comments?"
"LUCENE-3084","IMPROVEMENT","IMPROVEMENT","MergePolicy.OneMerge.segments should be List<SegmentInfo> not SegmentInfos, Remove Vector<SI> subclassing from SegmentInfos & more refactoring","SegmentInfos carries a bunch of fields beyond the list of SI, but for merging purposes these fields are unused.

We should cutover to List<SI> instead.

Also SegmentInfos subclasses Vector<SI>, this should be removed and the collections be hidden inside the class. We can add unmodifiable views on it (asList(), asSet())."
"LUCENE-490","BUILD_SYSTEM","BUG","JavaCC 4.0 fails to generate QueryParser.java","When generating the Java source for QueryParser via the ant task 'javacc-QueryParser' against Subversion trunk (updated Jan. 25, 2006), JavaCC 4.0 gives the following error:

javacc-QueryParser:
   [javacc] Java Compiler Compiler Version 4.0 (Parser Generator)
   [javacc] (type ""javacc"" with no arguments for help)
   [javacc] Reading from file [...]/src/java/org/apache/lucene/queryParser/QueryParser.jj . . .
   [javacc] org.javacc.parser.ParseException: Encountered ""<<"" at line 754, column 3.
   [javacc] Was expecting one of:
   [javacc]     <STRING_LITERAL> ...
   [javacc]     ""<"" ...
   [javacc]     
   [javacc] Detected 1 errors and 0 warnings.

BUILD FAILED
"
"LUCENE-3860","BUG","BUG","3.x indexes have the wrong normType set in fieldinfos","3.x codec claims the single byte norms are BYTES_VAR_STRAIGHT in FieldInfos,
but the norms implementation itself then has the type as FIXED_INTS_8."
"LUCENE-1198","BUG","BUG","Exception in DocumentsWriter.ThreadState.init leads to corruption","If an exception is hit in the init method, DocumentsWriter incorrectly
increments numDocsInRAM when in fact the document is not added.

Spinoff of this thread:

  http://markmail.org/message/e76hgkgldxhakuaa

The root cause that led to the exception in init was actually due to
incorrect use of Lucene's APIs (one thread still modifying the
Document while IndexWriter.addDocument is adding it) but still we
should protect against any exceptions coming out of init.

"
"LUCENE-1050","BUG","BUG","SimpleFSLockFactory ignores error on deleting the lock file","Spinoff from here:

    http://www.gossamer-threads.com/lists/lucene/java-user/54438

The Lock.release for SimpleFSLockFactory ignores the return value of lockFile.delete().  I plan to throw a new LockReleaseFailedException, subclassing from IOException, when this returns false.  This is a very minor change to backwards compatibility because all methods in Lucene that release a lock already throw IOException."
"LUCENE-2213","IMPROVEMENT","IMPROVEMENT","Small improvements to ArrayUtil.getNextSize","Spinoff from java-dev thread ""Dynamic array reallocation algorithms"" started on Jan 12, 2010.

Here's what I did:

  * Keep the +3 for small sizes

  * Added 2nd arg = number of bytes per element.

  * Round up to 4 or 8 byte boundary (if it's 32 or 64 bit JRE respectively)

  * Still grow by 1/8th

  * If 0 is passed in, return 0 back

I also had to remove some asserts in tests that were checking the actual values returned by this method -- I don't think we should test that (it's an impl. detail)."
"LUCENE-3171","RFE","IMPROVEMENT","BlockJoinQuery/Collector","I created a single-pass Query + Collector to implement nested docs.
The approach is similar to LUCENE-2454, in that the app must index
documents in ""join order"", as a block (IW.add/updateDocuments), with
the parent doc at the end of the block, except that this impl is one
pass.

Once you join at indexing time, you can take any query that matches
child docs and join it up to the parent docID space, using
BlockJoinQuery.  You then use BlockJoinCollector, which sorts parent
docs by provided Sort, to gather results, grouped by parent; this
collector finds any BlockJoinQuerys (using Scorer.visitScorers) and
retains the child docs corresponding to each collected parent doc.

After searching is done, you retrieve the TopGroups from a provided
BlockJoinQuery.

Like LUCENE-2454, this is less general than the arbitrary joins in
Solr (SOLR-2272) or parent/child from ElasticSearch
(https://github.com/elasticsearch/elasticsearch/issues/553), since you
must do the join at indexing time as a doc block, but it should be
able to handle nested joins as well as joins to multiple tables,
though I don't yet have test cases for these.

I put this in a new Join module (modules/join); I think as we
refactor join impls we should put them here.
"
"LUCENE-1618","RFE","IMPROVEMENT","Allow setting the IndexWriter docstore to be a different directory","Add an IndexWriter.setDocStoreDirectory method that allows doc
stores to be placed in a different directory than the IW default
dir."
"LUCENE-755","RFE","RFE","Payloads","This patch adds the possibility to store arbitrary metadata (payloads) together with each position of a term in its posting lists. A while ago this was discussed on the dev mailing list, where I proposed an initial design. This patch has a much improved design with modifications, that make this new feature easier to use and more efficient.

A payload is an array of bytes that can be stored inline in the ProxFile (.prx). Therefore this patch provides low-level APIs to simply store and retrieve byte arrays in the posting lists in an efficient way. 

API and Usage
------------------------------   
The new class index.Payload is basically just a wrapper around a byte[] array together with int variables for offset and length. So a user does not have to create a byte array for every payload, but can rather allocate one array for all payloads of a document and provide offset and length information. This reduces object allocations on the application side.

In order to store payloads in the posting lists one has to provide a TokenStream or TokenFilter that produces Tokens with payloads. I added the following two methods to the Token class:
  /** Sets this Token's payload. */
  public void setPayload(Payload payload);
  
  /** Returns this Token's payload. */
  public Payload getPayload();

In order to retrieve the data from the index the interface TermPositions now offers two new methods:
  /** Returns the payload length of the current term position.
   *  This is invalid until {@link #nextPosition()} is called for
   *  the first time.
   * 
   * @return length of the current payload in number of bytes
   */
  int getPayloadLength();
  
  /** Returns the payload data of the current term position.
   * This is invalid until {@link #nextPosition()} is called for
   * the first time.
   * This method must not be called more than once after each call
   * of {@link #nextPosition()}. However, payloads are loaded lazily,
   * so if the payload data for the current position is not needed,
   * this method may not be called at all for performance reasons.
   * 
   * @param data the array into which the data of this payload is to be
   *             stored, if it is big enough; otherwise, a new byte[] array
   *             is allocated for this purpose. 
   * @param offset the offset in the array into which the data of this payload
   *               is to be stored.
   * @return a byte[] array containing the data of this payload
   * @throws IOException
   */
  byte[] getPayload(byte[] data, int offset) throws IOException;

Furthermore, this patch indroduces the new method IndexOutput.writeBytes(byte[] b, int offset, int length). So far there was only a writeBytes()-method without an offset argument. 

Implementation details
------------------------------
- One field bit in FieldInfos is used to indicate if payloads are enabled for a field. The user does not have to enable payloads for a field, this is done automatically:
   * The DocumentWriter enables payloads for a field, if one ore more Tokens carry payloads.
   * The SegmentMerger enables payloads for a field during a merge, if payloads are enabled for that field in one or more segments.
- Backwards compatible: If payloads are not used, then the formats of the ProxFile and FreqFile don't change
- Payloads are stored inline in the posting list of a term in the ProxFile. A payload of a term occurrence is stored right after its PositionDelta.
- Same-length compression: If payloads are enabled for a field, then the PositionDelta is shifted one bit. The lowest bit is used to indicate whether the length of the following payload is stored explicitly. If not, i. e. the bit is false, then the payload has the same length as the payload of the previous term occurrence.
- In order to support skipping on the ProxFile the length of the payload at every skip point has to be known. Therefore the payload length is also stored in the skip list located in the FreqFile. Here the same-length compression is also used: The lowest bit of DocSkip is used to indicate if the payload length is stored for a SkipDatum or if the length is the same as in the last SkipDatum.
- Payloads are loaded lazily. When a user calls TermPositions.nextPosition() then only the position and the payload length is loaded from the ProxFile. If the user calls getPayload() then the payload is actually loaded. If getPayload() is not called before nextPosition() is called again, then the payload data is just skipped.
  
Changes of file formats
------------------------------
- FieldInfos (.fnm)
The format of the .fnm file does not change. The only change is the use of the sixth lowest-order bit (0x20) of the FieldBits. If this bit is set, then payloads are enabled for the corresponding field. 

- ProxFile (.prx)
ProxFile (.prx) -->  <TermPositions>^TermCount
TermPositions   --> <Positions>^DocFreq
Positions       --> <PositionDelta, Payload?>^Freq
Payload         --> <PayloadLength?, PayloadData>
PositionDelta   --> VInt
PayloadLength   --> VInt 
PayloadData     --> byte^PayloadLength

For payloads disabled (unchanged):
PositionDelta is the difference between the position of the current occurrence in the document and the previous occurrence (or zero, if this is the first   occurrence in this document).
  
For Payloads enabled:
PositionDelta/2 is the difference between the position of the current occurrence in the document and the previous occurrence. If PositionDelta is odd, then PayloadLength is stored. If PositionDelta is even, then the length of the current payload equals the length of the previous payload and thus PayloadLength is omitted.

- FreqFile (.frq)

SkipDatum     --> DocSkip, PayloadLength?, FreqSkip, ProxSkip
PayloadLength --> VInt

For payloads disabled (unchanged):
DocSkip records the document number before every SkipInterval th document in TermFreqs. Document numbers are represented as differences from the previous value in the sequence.

For payloads enabled:
DocSkip/2 records the document number before every SkipInterval th  document in TermFreqs. If DocSkip is odd, then PayloadLength follows. If DocSkip is even, then the length of the payload at the current skip point equals the length of the payload at the last skip point and thus PayloadLength is omitted.


This encoding is space efficient for different use cases:
   * If only some fields of an index have payloads, then there's no space overhead for the fields with payloads disabled.
   * If the payloads of consecutive term positions have the same length, then the length only has to be stored once for every term. This should be a common case, because users probably use the same format for all payloads.
   * If only a few terms of a field have payloads, then we don't waste much space because we benefit again from the same-length-compression since we only have to store the length zero for the empty payloads once per term.

All unit tests pass."
"LUCENE-1419","RFE","RFE","Expert API to specify indexing chain","It would be nice to add an expert API to specify an indexing chain, so that
we can make use of Mike's nice LUCENE-1301 feature.

This patch simply adds a package-protected expert API to IndexWriter and 
DocumentsWriter. It adds a inner, abstract class to DocumentsWriter called 
IndexingChain, and a default implementation that is the currently used one.

This might not be the final solution, but a nice way to play with different
modules in the indexing chain.

Could you take a look at the patch, Mike? "
"LUCENE-1591","RFE","IMPROVEMENT","Enable bzip compression in benchmark","bzip compression can aid the benchmark package by not requiring extracting bzip files (such as enwiki) in order to index them. The plan is to add a config parameter bzip.compression=true/false and in the relevant tasks either decompress the input file or compress the output file using the bzip streams.
It will add a dependency on ant.jar which contains two classes similar to GZIPOutputStream and GZIPInputStream which compress/decompress files using the bzip algorithm.

bzip is known to be superior in its compression performance to the gzip algorithm (~20% better compression), although it does the compression/decompression a bit slower.

I wil post a patch which adds this parameter and implement it in LineDocMaker, EnwikiDocMaker and WriteLineDoc task. Maybe even add the capability to DocMaker or some of the super classes, so it can be inherited by all sub-classes."
"LUCENE-1753","DESIGN_DEFECT","TASK","Make not yet final core/contrib TokenStream/Filter implementations final","Lucene's analysis package is designed in a way, that you can plug different *implementations* of analysis in chains of TokenStreams and TokenFilters. An analyzer is build of several TokenStreams/Filters that do the tokenization of text. If you want to modify the behaviour of tokenization, you implement a new subclass of TokenStream/-Filter/Tokenizer.

Most classes in the core are correctly implemented like that. They are itsself final or their implementation methods are final (CharTokenizer).

A lot of problems with backwards-compatibility of LUCENE-1693 are some classes in Lucene's core/contrib not yet final:
- KeywordTokenizer should be declared final or its implementation methods should be final
- StandardTokenizer should be declared final or its implementation methods should be final
- ISOLatin1Filter is deprecated, so it will be removed in 3.0, nothing to do.

CharTokenizer is the abstract base class of several other classes. The design is correct: Child classes cannot override the implementation, they can only change the behaviour of this final implementation.

Contrib should be checked, that all implementation classes are at least final or they are designed in the same way like CharTokenizer."
"LUCENE-780","RFE","IMPROVEMENT","Generalize directory copy operation","The copy operation in RAMDirectory(Directory) constructor can be used more generally to copy one directory to another. Why bound it only to RAMDirectory?. For example, I build index in RAMDirectory but I need it to persist in FSDirectory. I created a patch to solve it."
"LUCENE-3293","RFE","TASK","Use IOContext.READONCE in VarGapTermsIndexReader to load FST","VarGapTermsIndexReader should pass READONCE context down when it
opens/reads the FST. Yet, it should just replace the ctx passed in, ie if we are merging vs reading we want to differentiate.
"
"LUCENE-1321","BUG","BUG","Highlight fragment does not extend to maxDocCharsToAnalyze","The current highlighter code checks whether the total length of the text to highlight is strictly smaller than maxDocCharsToAnalyze before adding any text remaining after the last token to the fragment. This means that if maxDocCharsToAnalyse is set to exactly the length of the text and the last token of the text is the term to highlight and is followed by non-token text, this non-token text will not be highlighted.

For example, consider the phrase ""this is a text with searchterm in it"". ""In"" and ""it"" are not tokenized because they're stopwords. Setting maxDocCharsToAnalyze to 36 (the length of the sentence) and searching for ""searchterm"" gives a fragment ending in ""searchterm"". The expected behaviour is to have ""in it"" at the end of the fragment, since maxDocCharsToAnalyse explicitely states that the whole phrase should be considered."
"LUCENE-1006","BUG","BUG","QueryParser doesn't accept empty string","foo:"""" currently throws a parse exception
foo: bar is also parsed as foo:bar (not serious since it's arguably illegal syntax)"
"LUCENE-3376","REFACTORING","IMPROVEMENT","Move ReusableAnalyzerBase into core","In LUCENE-2309 it was suggested that we should make Analyzer reusability compulsory.  ReusableAnalyzerBase is a fantastic way to drive reusability so lets move it into core (so that we can then change all impls over to using it)."
"LUCENE-2645","BUG","BUG","False assertion of >0 position delta in StandardPostingsWriterImpl","StandardPostingsWriterImpl line 159 is:
{code:java}
    assert delta > 0 || position == 0 || position == -1: ""position="" + position + "" lastPosition="" + lastPosition;            // not quite right (if pos=0 is repeated twice we don't catch it)
{code}

I enable assertions when I run my unit tests and I've found this assertion to fail when delta is 0 which occurs when the same position value is sent in twice in arrow.  Once I added RemoveDuplicatesTokenFilter, this problem went away.  Should I really be forced to add this filter?  I think delta >= 0 would be a better assertion."
"LUCENE-709","RFE","IMPROVEMENT","[PATCH] Enable application-level management of IndexWriter.ramDirectory size","IndexWriter currently only supports bounding of in the in-memory index cache using maxBufferedDocs, which limits it to a fixed number of documents.  When document sizes vary substantially, especially when documents cannot be truncated, this leads either to inefficiencies from a too-small value or OutOfMemoryErrors from a too large value.

This simple patch exposes IndexWriter.flushRamSegments(), and provides access to size information about IndexWriter.ramDirectory so that an application can manage this based on total number of bytes consumed by the in-memory cache, thereby allow a larger number of smaller documents or a smaller number of larger documents.  This can lead to much better performance while elimianting the possibility of OutOfMemoryErrors.

The actual job of managing to a size constraint, or any other constraint, is left up the applicatation.

The addition of synchronized to flushRamSegments() is only for safety of an external call.  It has no significant effect on internal calls since they all come from a sychronized caller.
"
"LUCENE-3064","TEST","TEST","add checks to MockTokenizer to enforce proper consumption","we can enforce things like consumer properly iterates through tokenstream lifeycle
via MockTokenizer. this could catch bugs in consumers that don't call reset(), etc."
"LUCENE-1459","BUG","BUG","CachingWrapperFilter crashes if you call both bits() and getDocIdSet()","CachingWrapperFilter uses only a single cache, so calling bits() after calling getDocIdSet() will result in a type error. Additionally, more code than is necessary is wrapped in the @synchronized blocks."
"LUCENE-1932","REFACTORING","TASK","Convert PrecedenceQueryParser to new TokenStream API","Adriano Crestani provided a patch, that updates the PQP to use the new TokenStream API...all tests still pass. 
I hope this helps to keep the PQP 
"
"LUCENE-1326","BUG","BUG","Inflater.end() method not always called in FieldsReader","
We've just found an insidious memory leak in our own application as we did not always call Deflater.end() and Inflater.end(). As documented here;

http://bugs.sun.com/view_bug.do?bug_id=4797189

The non-heap memory that the native zlib code uses is not freed in a timely manner.

FieldsWriter appears safe as no exception can be thrown between the Deflater's creation and end() as it uses a ByteArrayOutputStream

FieldsReader, however, is not safe. In the event of a DataFormatException the call to end() will not occur."
"LUCENE-504","BUG","BUG","FuzzyQuery produces a ""java.lang.NegativeArraySizeException"" in PriorityQueue.initialize if I use Integer.MAX_VALUE as BooleanQuery.MaxClauseCount","PriorityQueue creates an ""java.lang.NegativeArraySizeException"" when initialized with Integer.MAX_VALUE, because Integer overflows. I think this could be a general problem with PriorityQueue. The Error occured when I set BooleanQuery.MaxClauseCount to Integer.MAX_VALUE and user a FuzzyQuery for searching."
"LUCENE-1829","BUILD_SYSTEM","IMPROVEMENT","'ant javacc' in root project should also properly create contrib/queryparser Java files","'ant javacc' in the project root doesn't run javacc in contrib/queryparser
'ant javacc' in contrib/queryparser does not properly create the Java files. What still needs to be done by hand is (partly!) described in contrib/queryparser/README.javacc. I think this process should be automated. Patch provided."
"LUCENE-401","BUILD_SYSTEM","BUG","[PATCH] fixes for gcj target.","I've modified the Makefile so that it compiles with GCJ-4.0.

This involved fixing the CORE_OBJ macro to match the generated jar file as well
as excluding FieldCacheImpl from being used from its .java source (GCJ has
problems with anonymous inner classes, I guess).

Also, I changed the behaviour of FieldInfos.fieldInfo(int). It depended on
catching IndexOutOfBoundsException exception. I've modified it to test the
bounds first, returning -1 in that case. This helps with gcj since we build with
-fno-bounds-check.

I compiled with;

GCJ=gcj-4.0 GCJH=gcjh-4.0 GPLUSPLUS=g++-4.0 ant clean gcj

patch to follow."
"LUCENE-2589","RFE","IMPROVEMENT","Add a variable-sized int block codec","We already have support for fixed block size int block codecs, making it very simple to create a codec from a int encoder algorithms like FOR/PFOR.

But algorithms like Simple9/16 are not fixed -- they encode a variable number of adjacent ints at once, depending on the specific values of those ints."
"LUCENE-3264","TEST","TEST","crank up faceting module tests","The faceting module has a large set of good tests.

lets switch them over to use all of our test infra (randomindexwriter, random iwconfig, mockanalyzer, newDirectory, ...)
I don't want to address multipliers and atLeast() etc on this issue, I think we should follow up with that on a separate issue, that also looks at speed and making sure the nightly build is exhaustive.

for now, lets just get the coverage in, it will be good to do before any refactoring.
"
"LUCENE-2615","BUG","BUG","DirectIOLinuxDirectory hardwires buffer size and creates files with invalid permissions","TestDemo fails if I use the DirectIOLinuxDirectory (using Robert's new -Dtests.directory=XXX), because when it O_CREATs a file, it fails to specify the mode, so [depending on C stack!] you can get permission denied.

Also, we currently hardwire the buffer size to 1 MB (Mark found this)... I plan to add a ""forcedBufferSize"" to the DirectIOLinuxDir's ctor, to optionally override lucene's default buffer sizes (which are way too small for direct IO to get barely OK performance).  If you pass 0 for this then you get Lucene's default buffer sizes..."
"LUCENE-3135","BACKPORT","RFE","backport suggest module to branch 3.x","It would be nice to develop a plan to expose the autosuggest functionality to Lucene users in 3.x

There are some complications, such as seeing if we can backport the FST-based functionality,
which might require a good bit of work. But I think this would be well-worth it.
"
"LUCENE-202","DOCUMENTATION","BUG","Document.fields() only returns stored fields","Document.fields() only returns stored fields, not those which are indexed but not 
stored. This is confusing, as there's a isStored() Method which doesn't make much 
sense then. 
 
Actually fields() returns all fields only just after Document.add(new Field(...)), even the 
ones which are not stored. Sounds confusing? :-) I'll attach a small program that 
demonstrates this. 
 
This should either be fixed so that all fields are always returned or it should be 
documented."
"LUCENE-3621","IMPROVEMENT","IMPROVEMENT","switch appendingcodec to use appending blocktree","currently it still uses block terms + fixed gap index"
"LUCENE-3342","IMPROVEMENT","IMPROVEMENT","make frozenbuffereddeletes more efficient for terms","when looking at LUCENE-3340, I thought its also ridiculous how much ram we use for delete by term.

so we can save a lot of memory, especially object overhead by being a little more efficient."
"LUCENE-1136","RFE","IMPROVEMENT","add ability to not count sub-task doLogic increment to contri/benchmark","Sometimes, you want to run a sub-task like CloseIndex, and include the time it takes to run, but not include the count that it returns when reporting rec/s.

We could adopt this approach: if a task is preceded by a ""-"" character, then, do not count the value returned by doLogic.

See discussion leading to this here:

  http://www.gossamer-threads.com/lists/lucene/java-dev/57081"
"LUCENE-3426","IMPROVEMENT","IMPROVEMENT","optimizer for n-gram PhraseQuery","If 2-gram is used and the length of query string is 4, for example q=""ABCD"", QueryParser generates (when autoGeneratePhraseQueries is true) PhraseQuery(""AB BC CD"") with slop 0. But it can be optimized PhraseQuery(""AB CD"") with appropriate positions.

The idea came from the Japanese paper ""N.M-gram: Implementation of Inverted Index Using N-gram with Hash Values"" by Mikio Hirabayashi, et al. (The main theme of the paper is different from the idea that I'm using here, though)"
"LUCENE-2126","REFACTORING","IMPROVEMENT","Split up IndexInput and IndexOutput into DataInput and DataOutput","I'd like to introduce the two new classes DataInput and DataOutput
that contain all methods from IndexInput and IndexOutput that actually
decode or encode data, such as readByte()/writeByte(),
readVInt()/writeVInt().

Methods like getFilePointer(), seek(), close(), etc., which are not
related to data encoding, but to files as input/output source stay in
IndexInput/IndexOutput.

This patch also changes ByteSliceReader/ByteSliceWriter to extend
DataInput/DataOutput. Previously ByteSliceReader implemented the
methods that stay in IndexInput by throwing RuntimeExceptions.

See also LUCENE-2125.

All tests pass."
"LUCENE-3642","BUG","BUG","EdgeNgrams creates invalid offsets","A user reported this because it was causing his highlighting to throw an error."
"LUCENE-2241","RFE","TASK","Core Tests should call Version based ctors instead of deprecated default ctors","LUCENE-2183 introduced new ctors for all CharTokenizer subclasses. Core - tests should use those ctors with Version.LUCENE_CURRENT instead of the the deprecated ctors. Yet, LUCENE-2240 introduces more Version ctors For WhitespaceAnalyzer and SimpleAnalyzer. Test should also use their Version ctors instead the default ones."
"LUCENE-3908","TEST","BUG","when tests fail, sometimes the testmethod in 'reproduce with' is null","an example is the recent fail: https://builds.apache.org/job/Lucene-3.x/680/

it would be better to not populate -Dtestmethod with anything here..."
"LUCENE-1163","BUG","BUG","CharArraySet.contains(char[] text, int off, int len) does not work","I try to use the CharArraySet for a filter I am writing. I heavily use char-arrays in my code to speed up things. I stumbled upon a bug in CharArraySet while doing that.

The method _public boolean contains(char[] text, int off, int len)_ seems not to work.

When I do 

{code}
if (set.contains(buffer,offset,length) {
  ...
}
{code}

my code fails.

But when I do

{code}
if (set.contains(new String(buffer,offset,length)) {
   ...
}
{code}

everything works as expected.

Both variants should behave the same. I attach a small piece of code to show the problem."
"LUCENE-3216","IMPROVEMENT","IMPROVEMENT","Store DocValues per segment instead of per field","currently we are storing docvalues per field which results in at least one file per field that uses docvalues (or at most two per field per segment depending on the impl.). Yet, we should try to by default pack docvalues into a single file if possible. To enable this we need to hold all docvalues in memory during indexing and write them to disk once we flush a segment. "
"LUCENE-1885","BUG","BUG","NativeFSLockFactory.makeLock(...).isLocked() does not work","IndexWriter.isLocked() or IndexReader.isLocked() do not work with NativeFSLockFactory.

The problem is, that the method NativeFSLock.isLocked() just checks if the same lock instance was locked before (lock != null). If the LockFactory created a new lock instance, this always returns false, even if its locked."
"LUCENE-759","RFE","IMPROVEMENT","Add n-gram tokenizers to contrib/analyzers","It would be nice to have some n-gram-capable tokenizers in contrib/analyzers.  Patch coming shortly."
"LUCENE-2847","RFE","BUG","Support all of unicode in StandardTokenizer","StandardTokenizer currently only supports the BMP.

If it encounters characters outside of the BMP, it just discards them... 
it should instead implement fully implement UAX#29 across all of unicode."
"LUCENE-710","RFE","IMPROVEMENT","Implement ""point in time"" searching without relying on filesystem semantics","This was touched on in recent discussion on dev list:

  http://www.gossamer-threads.com/lists/lucene/java-dev/41700#41700

and then more recently on the user list:

  http://www.gossamer-threads.com/lists/lucene/java-user/42088

Lucene's ""point in time"" searching currently relies on how the
underlying storage handles deletion files that are held open for
reading.

This is highly variable across filesystems.  For example, UNIX-like
filesystems usually do ""close on last delete"", and Windows filesystem
typically refuses to delete a file open for reading (so Lucene retries
later).  But NFS just removes the file out from under the reader, and
for that reason ""point in time"" searching doesn't work on NFS
(see LUCENE-673 ).

With the lockless commits changes (LUCENE-701 ), it's quite simple to
re-implement ""point in time searching"" so as to not rely on filesystem
semantics: we can just keep more than the last segments_N file (as
well as all files they reference).

This is also in keeping with the design goal of ""rely on as little as
possible from the filesystem"".  EG with lockless we no longer re-use
filenames (don't rely on filesystem cache being coherent) and we no
longer use file renaming (because on Windows it can fails).  This
would be another step of not relying on semantics of ""deleting open
files"".  The less we require from filesystem the more portable Lucene
will be!

Where it gets interesting is what ""policy"" we would then use for
removing segments_N files.  The policy now is ""remove all but the last
one"".  I think we would keep this policy as the default.  Then you
could imagine other policies:

  * Keep past N day's worth

  * Keep the last N

  * Keep only those in active use by a reader somewhere (note: tricky
    how to reliably figure this out when readers have crashed, etc.)

  * Keep those ""marked"" as rollback points by some transaction, or
    marked explicitly as a ""snaphshot"".

  * Or, roll your own: the ""policy"" would be an interface or abstract
    class and you could make your own implementation.

I think for this issue we could just create the framework
(interface/abstract class for ""policy"" and invoke it from
IndexFileDeleter) and then implement the current policy (delete all
but most recent segments_N) as the default policy.

In separate issue(s) we could then create the above more interesting
policies.

I think there are some important advantages to doing this:

  * ""Point in time"" searching would work on NFS (it doesn't now
    because NFS doesn't do ""delete on last close""; see LUCENE-673 )
    and any other Directory implementations that don't work
    currently.

  * Transactional semantics become a possibility: you can set a
    snapshot, do a bunch of stuff to your index, and then rollback to
    the snapshot at a later time.

  * If a reader crashes or machine gets rebooted, etc, it could choose
    to re-open the snapshot it had previously been using, whereas now
    the reader must always switch to the last commit point.

  * Searchers could search the same snapshot for follow-on actions.
    Meaning, user does search, then next page, drill down (Solr),
    drill up, etc.  These are each separate trips to the server and if
    searcher has been re-opened, user can get inconsistent results (=
    lost trust).  But with, one series of search interactions could
    explicitly stay on the snapshot it had started with.

"
"LUCENE-541","RFE","BUG","The DisjunctionMaxQuery lacks an implementation of extractTerms().","The DisjunctionMaxQuery lacks an implementation of extractTerms(). "
"LUCENE-1552","BUG","BUG","IndexWriter.addIndexes(IndexReader[] readers) doesn't correctly handle exception success flag.","After this bit of code in addIndexes(IndexReader[] readers)

 try {
        flush(true, false, true);
        optimize();					  // start with zero or 1 seg
        success = true;
      } finally {
        // Take care to release the write lock if we hit an
        // exception before starting the transaction
        if (!success)
          releaseWrite();
      }

The success flag should be reset to ""false"" because it's used again in another try/catch/finally block.  

TestIndexWriter.testAddIndexOnDiskFull() sometimes will hit this bug; but it's infrequent.


"
"LUCENE-1769","BUILD_SYSTEM","BUG","Fix wrong clover analysis because of backwards-tests, upgrade clover to 2.6.3 or better","This is a followup for [http://www.lucidimagination.com/search/document/6248d6eafbe10ef4/build_failed_in_hudson_lucene_trunk_902]

The problem with clover running on hudson is, that it does not instrument all tests ran. The autodetection of clover 1.x is not able to find out which files are the correct tests and only instruments the backwards test. Because of this, the current coverage report is only from the backwards tests running against the current Lucene JAR.

You can see this, if you install clover and start the tests. During test-core no clover data is added to the db, only when backwards-tests begin, new files are created in the clover db folder.

Clover 2.x supports a new ant task, <testsources> that can be used to specify the files, that are the tests. It works here locally with clover 2.4.3 and produces a really nice coverage report, also linking with test files work, it tells which tests failed and so on.

I will attach a patch, that changes common-build.xml to the new clover version (other initialization resource) and tells clover where to find the tests (using the test folder include/exclude properties).

One problem with the current patch: It does *not* instrument the backwards branch, so you see only coverage of the core/contrib tests. Getting the coverage also from the backwards tests is not easy possible because of two things:
- the tag test dir is not easy to find out and add to <testsources> element (there may be only one of them)
- the test names in BW branch are identical to the trunk tests. This completely corrupts the linkage between tests and code in the coverage report.

In principle the best would be to generate a second coverage report for the backwards branch with a separate clover DB. The attached patch does not instrument the bw branch, it only does trunk tests."
"LUCENE-2578","TEST","IMPROVEMENT","cutover FunctionQuery tests to use RandomIndexWriter, for better testing",""
"LUCENE-1238","TEST","BUG","intermittent failures of  TestTimeLimitedCollector.testTimeoutMultiThreaded in nightly tests","Occasionly TestTimeLimitedCollector.testTimeoutMultiThreaded fails. e.g. with this output:

{noformat}
   [junit] ------------- Standard Error -----------------
   [junit] Exception in thread ""Thread-97"" junit.framework.AssertionFailedError: no hits found!
   [junit]     at junit.framework.Assert.fail(Assert.java:47)
   [junit]     at junit.framework.Assert.assertTrue(Assert.java:20)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestTimeout(TestTimeLimitedCollector.java:152)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.access$100(TestTimeLimitedCollector.java:38)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector$1.run(TestTimeLimitedCollector.java:231)
   [junit] Exception in thread ""Thread-85"" junit.framework.AssertionFailedError: no hits found!
   [junit]     at junit.framework.Assert.fail(Assert.java:47)
   [junit]     at junit.framework.Assert.assertTrue(Assert.java:20)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestTimeout(TestTimeLimitedCollector.java:152)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.access$100(TestTimeLimitedCollector.java:38)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector$1.run(TestTimeLimitedCollector.java:231)
   [junit] ------------- ---------------- ---------------
   [junit] Testcase: testTimeoutMultiThreaded(org.apache.lucene.search.TestTimeLimitedCollector):      FAILED
   [junit] some threads failed! expected:<50> but was:<48>
   [junit] junit.framework.AssertionFailedError: some threads failed! expected:<50> but was:<48>
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestMultiThreads(TestTimeLimitedCollector.java:255)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.testTimeoutMultiThreaded(TestTimeLimitedCollector.java:220)
   [junit]
{noformat}

Problem either in test or in TimeLimitedCollector."
"LUCENE-589","IMPROVEMENT","IMPROVEMENT","Demo HTML parser doesn't work for international documents","Javacc assumes ASCII so it won't work with, say, japanese documents. Ideally it would read the charset from the HTML markup, but that can by tricky. For now assuming unicode would do the trick:

Add the following line marked with a + to HTMLParser.jj:

options {
  STATIC = false;
  OPTIMIZE_TOKEN_MANAGER = true;
  //DEBUG_LOOKAHEAD = true;
  //DEBUG_TOKEN_MANAGER = true;
+  UNICODE_INPUT = true;
}
"
"LUCENE-2236","RFE","IMPROVEMENT","Similarity can only be set per index, but I may want to adjust scoring behaviour at a field level","Similarity can only be set per index, but I may want to adjust scoring behaviour at a field level, to faciliate this could we pass make field name available to all score methods.
Currently it is only passed to some such as lengthNorm() but not others such as tf()"
"LUCENE-2147","IMPROVEMENT","IMPROVEMENT","Improve Spatial Utility like classes","- DistanceUnits can be improved by giving functionality to the enum, such as being able to convert between different units, and adding tests.  

- GeoHashUtils can be improved through some code tidying, documentation, and tests.

- SpatialConstants allows us to move all constants, such as the radii and circumferences of Earth, to a single consistent location that we can then use throughout the contrib.  This also allows us to improve the transparency of calculations done in the contrib, as users of the contrib can easily see the values being used.  Currently this issues does not migrate classes to use these constants, that will happen in issues related to the appropriate classes."
"LUCENE-2391","IMPROVEMENT","IMPROVEMENT","Spellchecker uses default IW mergefactor/ramMB settings of 300/10","These settings seem odd - I'd like to investigate what makes most sense here."
"LUCENE-2937","BUG","BUG","small float underflow detection bug","Underflow detection in small floats has a bug, and can incorrectly result in a byte value of 0 for a non-zero float."
"LUCENE-3563","BUG","BUG","TestPagedBytes failure","ant test -Dtestcase=TestPagedBytes -Dtestmethod=testDataInputOutput -Dtests.seed=268db1f3329b70d:3125365bc9c56c90:116e02aa4a70ec2f -Dtests.multiplier=5"
"LUCENE-2158","BUG","BUG","NRT can temporarily lose deletions at high indexing rates","OK, I found a sneaky case where NRT can temporarily lose deletions.
The deletions aren't permanently lost - they are seen on the next
opened NRT reader.

It happens like this (in IW.getReader):

  * First flush() is called, to flush added docs & materialize the
    deletes.

  * The very next statement enters a sync'd block to open the reader,
    but, if indexing rate is very high, and threads get scheduled
    ""appropriately"", a ""natural"" flush (due to RAM buffer being full
    or flush doc count being reached) could be hit before the sync
    block is entered, in which case that 2nd flush won't materialize
    the deletes associated with it, and the returned NRT reader will
    only see its adds until it's next reopened.

The fix is simple -- we should materialize deletes inside the sync
block, not during the flush.
"
"LUCENE-341","BUG","BUG","The deprecated constructor of BooleanClause does not set new state","Nick Burch reported this on lucene-user. 
 
Patch will follow. 
 
Regards, 
Paul Elschot"
"LUCENE-2586","REFACTORING","IMPROVEMENT","move intblock/sep codecs into test","The intblock and sep codecs in core exist to make it easy for people to try different low-level algos for encoding ints.

Sep breaks docs, freqs, pos, skip data, payloads into 5 separate files (vs 2 files that standard codec uses).

Intblock further enables the docs, freqs, pos files to encode fixed-sized blocks of ints at a time.

So an app can easily ""subclass"" these codecs, using their own int encoder.

But these codecs are now concrete, and they use dummy low-level block int encoder (eg encoding 128 ints as separate vints).

I'd like to change these to be abstract, and move these dummy codecs into test.

The tests would still test these dummy codecs, by rotating them in randomly for all tests.

I'd also like to rename IntBlock -> FixedIntBlock, because I'm trying to get a VariableIntBlock working well (for int encoders like Simple9, Simple16, whose block size varies depending on the particular values).
"
"LUCENE-2199","BUG","BUG","ShingleFilter skips over trie-shingles if outputUnigram is set to false","Spinoff from http://lucene.markmail.org/message/uq4xdjk26yduvnpa

{quote}
I noticed that if I set outputUnigrams to false it gives me the same output for
maxShingleSize=2 and maxShingleSize=3.

please divide divide this this sentence

when i set maxShingleSize to 4 output is:

please divide please divide this sentence divide this this sentence

I was expecting the output as follows with maxShingleSize=3 and
outputUnigrams=false :

please divide this divide this sentence 
{quote}


"
"LUCENE-683","BUG","BUG","Lazy Field Loading has edge case bug causing read past EOF","While trying to run some benchmarking of Lazy filed loading, i discovered there seems to be an edge case when accessing the last field of the last doc of an index.

the problem seems to only happen when the doc has been accessed after at least one other doc.

i have not tried to dig into the code to find the root cause, testcase to follow..."
"LUCENE-1967","IMPROVEMENT","IMPROVEMENT","make it easier to access default stopwords for language analyzers","DM Smith made the following comment: (sometimes it is hard to dig out the stop set from the analyzers)

Looking around, some of these analyzers have very different ways of storing the default list.
One idea is to consider generalizing something like what Simon did with LUCENE-1965, LUCENE-1962,
and having all stopwords lists stored as .txt files in resources folder.

{code}
  /**
   * Returns an unmodifiable instance of the default stop-words set.
   * @return an unmodifiable instance of the default stop-words set.
   */
  public static Set<String> getDefaultStopSet()
{code}
"
"LUCENE-730","IMPROVEMENT","IMPROVEMENT","Restore top level disjunction performance","This patch restores the performance of top level disjunctions. 
The introduction of BooleanScorer2 had impacted this as reported
on java-user on 21 Nov 2006 by Stanislav Jordanov.
"
"LUCENE-1503","REFACTORING","IMPROVEMENT","refactor spatial contrib ""Filter"" ""Query"" classes","From erik's comments in LUCENE-1387

    * DistanceQuery is awkwardly named. It's not an (extends) Query.... it's a POJO with helpers. Maybe DistanceQueryFactory? (but it creates a Filter also)

    * CartesianPolyFilter is not a Filter (but CartesianShapeFilter is)
"
"LUCENE-2922","IMPROVEMENT","IMPROVEMENT","Optimize BlockTermsReader.seek","When we seek, we first consult the terms index to find the right block
of 32 (default) terms that may hold the target term.  Then, we scan
that block looking for an exact match.

The scanning just uses next() and then compares the full term, but
this is actually rather wasteful.  First off, since all terms in the
block share a common prefix, we should compare the target against that
common prefix once, and then only compare the new suffix of each
term.  Second, since the term suffixes have already been read up front
into a byte[], we should do a no-copy comparison (vs today, where we
first read a copy into the local BytesRef and then compare).

With this opto, I removed the ability for BlockTermsWriter/Reader to
support arbitrary term sort order -- it's now hardwired to
BytesRef.utf8SortedAsUnicode.
"
"LUCENE-1781","BUG","BUG","Large distances in Spatial go beyond Prime MEridian","http://amidev.kaango.com/solr/core0/select?fl=*&json.nl=map&wt=json&radius=5000&rows=20&lat=39.5500507&q=honda&qt=geo&long=-105.7820674

Get an error when using Solr when distance is calculated for the boundary box past 90 degrees.


Aug 4, 2009 1:54:00 PM org.apache.solr.common.SolrException log
SEVERE: java.lang.IllegalArgumentException: Illegal lattitude value 93.1558669413734
        at org.apache.lucene.spatial.geometry.FloatLatLng.<init>(FloatLatLng.java:26)
        at org.apache.lucene.spatial.geometry.shape.LLRect.createBox(LLRect.java:93)
        at org.apache.lucene.spatial.tier.DistanceUtils.getBoundary(DistanceUtils.java:50)
        at org.apache.lucene.spatial.tier.CartesianPolyFilterBuilder.getBoxShape(CartesianPolyFilterBuilder.java:47)
        at org.apache.lucene.spatial.tier.CartesianPolyFilterBuilder.getBoundingArea(CartesianPolyFilterBuilder.java:109)
        at org.apache.lucene.spatial.tier.DistanceQueryBuilder.<init>(DistanceQueryBuilder.java:61)
        at com.pjaol.search.solr.component.LocalSolrQueryComponent.prepare(LocalSolrQueryComponent.java:151)
        at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:174)
        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)
        at org.apache.solr.core.SolrCore.execute(SolrCore.java:1328)
        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:341)
        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:244)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:286)
        at org.apache.coyote.http11.Http11AprProcessor.process(Http11AprProcessor.java:857)
        at org.apache.coyote.http11.Http11AprProtocol$Http11ConnectionHandler.process(Http11AprProtocol.java:565)
        at org.apache.tomcat.util.net.AprEndpoint$Worker.run(AprEndpoint.java:1509)
        at java.lang.Thread.run(Thread.java:619)


"
"LUCENE-913","BUG","BUG","Two consecutive score() calls return different scores for Boolean Queries.","Two consecutive calls to score() return different scores (no next() or skipTo() calls in between). 
Background in LUCENE-912 .
"
"LUCENE-1040","IMPROVEMENT","BUG","Can't quickly create StopFilter","Due to the use of CharArraySet by StopFilter, one can no longer efficiently pre-create a Set for use by future StopFilter instances."
"LUCENE-3220","RFE","","Implement various ranking models as Similarities","With [LUCENE-3174|https://issues.apache.org/jira/browse/LUCENE-3174] done, we can finally work on implementing the standard ranking models. Currently DFR, BM25 and LM are on the menu.

Done:
 * {{EasyStats}}: contains all statistics that might be relevant for a ranking algorithm
 * {{EasySimilarity}}: the ancestor of all the other similarities. Hides the DocScorers and as much implementation detail as possible
 * _BM25_: the current ""mock"" implementation might be OK
 * _LM_
 * _DFR_
 * The so-called _Information-Based Models_

"
"LUCENE-1204","TEST","BUG","IndexWriter.deleteDocuments bug","IndexWriter.deleteDocuments() fails random testing"
"LUCENE-3338","RFE","BUG","Flexible query parser does not support open ranges and range queries with mixed inclusive and exclusive ranges","Flexible query parser does not support open ranges and range queries with mixed inclusive and exclusive ranges.

These two problems were found while developing LUCENE-1768."
"LUCENE-225","BUG","BUG","ClassCastException MultiReader","(See original message below)
Sure.  'Bugzilla it', please.

Otis
P.S.
That line 274 should be line 273 in the CVS HEAD as of now.

--- Rasik Pandey <rasik.pandey@ajlsm.com> wrote:
> Howdy,
> 
> This exception was thrown with 1.4rc3. Do you need a test case for 
> this one?
> 
> java.lang.ClassCastException
>         at
> org.apache.lucene.index.MultiTermEnum.<init>(MultiReader.java:274)
>         at
> org.apache.lucene.index.MultiReader.terms(MultiReader.java:187)
> 
> 
> Regards,
> RBP
> 
> 
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: lucene-dev-unsubscribe@jakarta.apache.org
> For additional commands, e-mail: lucene-dev-help@jakarta.apache.org
> 


---------------------------------------------------------------------
To unsubscribe, e-mail: lucene-dev-unsubscribe@jakarta.apache.org
For additional commands, e-mail: lucene-dev-help@jakarta.apache.org"
"LUCENE-3576","BACKPORT","TASK","TestBackwardsCompatibility needs terms with U+E000 to U+FFFF","we changed sort order in 4.0, and have sophisticated backwards compatibility (e.g. surrogates dance),
but we don't test this at all in TestBackwardsCompatibility.

for example, nothing handles this case for term vectors..."
"LUCENE-1928","DOCUMENTATION","BUG","PayloadTermQuery refers to a deprecated documentation for redirection ","When PayloadTermQuery refers to the function for scoring Similarity - it refers to override the deprecated method - 

Similarity#scorePayload(String, byte[],int,int) . 

That method has been deprecated by  Similarity#scorePayload(int, String, int, int, byte[],int,int) . 


This javadoc patch addresses the class level javadoc for the class to provide the right signature in Similarity to be overridden. "
"LUCENE-2075","IMPROVEMENT","IMPROVEMENT","Share the Term -> TermInfo cache across threads","Right now each thread creates its own (thread private) SimpleLRUCache,
holding up to 1024 terms.

This is rather wasteful, since if there are a high number of threads
that come through Lucene, you're multiplying the RAM usage.  You're
also cutting way back on likelihood of a cache hit (except the known
multiple times we lookup a term within-query, which uses one thread).
In NRT search we open new SegmentReaders (on tiny segments) often
which each thread must then spend CPU/RAM creating & populating.

Now that we are on 1.5 we can use java.util.concurrent.*, eg
ConcurrentHashMap.  One simple approach could be a double-barrel LRU
cache, using 2 maps (primary, secondary).  You check the cache by
first checking primary; if that's a miss, you check secondary and if
you get a hit you promote it to primary.  Once primary is full you
clear secondary and swap them.

Or... any other suggested approach?
"
"LUCENE-3794","BUG","BUG","DirectoryTaxonomyWriter can lose the INDEX_CREATE_TIME property, causing DirTaxoReader.refresh() to falsely succeed (or fail)","DirTaxoWriter sets createTime to null after it put it in the commit data once. But that's wrong because if one calls commit(Map<>) twice, the second time doesn't record the creation time. Also, in the ctor, if an index exists and OpenMode is not CREATE, the creation time property is not read.

I wrote a couple of unit tests that assert this, and modified DirTaxoWriter to always record the creation time (in every commit) -- that's the only safe way.

Will upload a patch shortly."
"LUCENE-2626","IMPROVEMENT","IMPROVEMENT","FastVectorHighlighter: enable FragListBuilder and FragmentsBuilder to be set per-field override",""
"LUCENE-1937","RFE","IMPROVEMENT","Add more methods to manipulate QueryNodeProcessorPipeline elements","QueryNodeProcessorPipeline allows the user to define a list of processors to process a query tree. However, it's not very flexible when the user wants to extend/modify an already created pipeline, because it only provides an add method, which only allows the user to append a new processor to the pipeline.

So, I propose to add new methods to manipulate the processor in a pipeline. I think the methods should not consider an index position when modifying the pipeline, hence the index position in a pipeline does not mean anything, a processor has a meaning when it's after or before another processor. Therefore, I suggest the methods should always consider another processor when inserting/modifying the pipeline. For example, insertAfter(processor, newProcessor), which will insert the ""newProcessor"" after the ""processor""."
"LUCENE-3465","BUG","BUG","IndexSearcher fails to pass docBase to Collector when using ExecutorService","This bug is causing the failure in TestSearchAfter.

We are now always passing docBase 0 to Collector when you use ExecutorService with IndexSearcher.

This doesn't affect trunk (AtomicReaderContext carries the right docBase); only 3.x.
"
"LUCENE-3725","RFE","IMPROVEMENT","Add optional packing to FST building","The FSTs produced by Builder can be further shrunk if you are willing
to spend highish transient RAM to do so... our Builder today tries
hard not to use much RAM (and has options to tweak down the RAM usage,
in exchange for somewhat lager FST), even when building immense FSTs.

But for apps that can afford highish transient RAM to get a smaller
net FST, I think we should offer packing.
"
"LUCENE-3697","BUG","BUG","FastVectorHighlighter SimpleBoundaryScanner does not work well when highlighting at the beginning of the text ","The SimpleBoundaryScanner still breaks text not based on characters provided when highlighting text that end up scanning to the beginning of the text to highlight. In this case, just use the start of the text as the offset."
"LUCENE-514","RFE","IMPROVEMENT","MultiPhraseQuery should allow access to terms",""
"LUCENE-2756","BUG","BUG","MultiSearcher.rewrite() incorrectly rewrites queries","This was reported on the userlist, in the context of range queries.

Its also easy to make our existing tests fail with my patch on LUCENE-2751:
{noformat}
ant test-core -Dtestcase=TestBoolean2 -Dtestmethod=testRandomQueries -Dtests.seed=7679849347282878725:-903778383189134045
{noformat}

The fundamental problem is that MultiSearcher first rewrites against individual subs, 
then uses Query.combine() which simply OR's these sub-clauses.

This is incorrect for expanded MUST_NOT queries (e.g. from wildcard), as it violates demorgan's law.
"
"LUCENE-2902","TEST","TEST","tests should run checkIndex on indexes they create","I think we should add a boolean checkIndexesOnClose (default=true) to MockDirectoryWrapper.

Only a very few tests need to disable this.
"
"LUCENE-1839","IMPROVEMENT","IMPROVEMENT","Scorer.explain is deprecated but abstract, should have impl that throws UnsupportedOperationException","Suggest having Scorer implement explain to throw UnsupportedOperationException

right now, i have to implement this method (because its abstract), and javac yells at me for overriding a deprecated method

if the following implementation is in Scorer, i can remove my ""empty"" implementations of explain from my Scorers
{code}
  /** Returns an explanation of the score for a document.
   * <br>When this method is used, the {@link #next()}, {@link #skipTo(int)} and
   * {@link #score(HitCollector)} methods should not be used.
   * @param doc The document number for the explanation.
   *
   * @deprecated Please use {@link IndexSearcher#explain}
   * or {@link Weight#explain} instead.
   */
  public Explanation explain(int doc) throws IOException {
    throw new UnsupportedOperationException();
  }
{code}

best i figure, this shouldn't break back compat (people already have to recompile anyway) (2.9 definitely not binary compatible with 2.4)
"
"LUCENE-2555","CLEANUP","IMPROVEMENT","Remove shared doc stores","With per-thread DocumentsWriters sharing doc stores across segments doesn't make much sense anymore.

See also LUCENE-2324."
"LUCENE-978","BUG","BUG","GC resources in TermInfosReader when exception occurs in its constructor","I replaced IndexModifier with IndexWriter in test case TestStressIndexing and noticed the test failed from time to time because some .tis file is still open when MockRAMDirectory.close() is called. It turns out it is because .tis file is not closed if an exception occurs in TermInfosReader's constructor."
"LUCENE-2343","RFE","IMPROVEMENT","Add support for benchmarking Collectors","As the title says."
"LUCENE-840","TEST","TEST","contrib/benchmark unit tests","The need came up in this thread: 
http://www.mail-archive.com/java-dev@lucene.apache.org/msg09260.html

: We might want to start thinking about Unit Tests...  :-)  Seems kind
: of weird to have tests for tests, but this is becoming sufficiently
: complex that it should have some tests.
"
"LUCENE-3093","CLEANUP","BUG","Build failed in the flexscoring branch because of Javadoc warnings","Ant build log:
  [javadoc] Standard Doclet version 1.6.0_24
  [javadoc] Building tree for all the packages and classes...
  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/Similarity.java:93: warning - Tag @link: can't find tf(float) in org.apache.lucene.search.Similarity
  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/TFIDFSimilarity.java:588: warning - @param argument ""term"" is not a parameter name.
  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/TFIDFSimilarity.java:588: warning - @param argument ""docFreq"" is not a parameter name.
  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/TFIDFSimilarity.java:618: warning - @param argument ""terms"" is not a parameter name.
  [javadoc] Generating /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated//package-summary.html...
  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/classdiagram.png to directory /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated/doc-files...
  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/HitCollectionBench.jpg to directory /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated/doc-files...
  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/classdiagram.uxf to directory /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated/doc-files...
  [javadoc] Generating /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/serialized-form.html...
  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/prettify/stylesheet+prettify.css to file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/stylesheet+prettify.css...
  [javadoc] Building index for all the packages and classes...
  [javadoc] Building index for all classes...
  [javadoc] Generating /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/help-doc.html...
  [javadoc] 4 warnings
"
"LUCENE-598","RFE","RFE","GData Server MileStone 1 Revision","Some Improvements to the GData Server.
CRUD actions for Entries implemented / tested 
StorageComponent storing entries / feeds / users
Dynamic Feed elements like links added.
Decoupled all server components (storage / ReqeustHandler etc) using lookup service

Added some JavaDoc "
"LUCENE-243","RFE","IMPROVEMENT","[PATCH] setIndexInterval() in IndexWriter","Following a discussion with Doug (see
http://article.gmane.org/gmane.comp.jakarta.lucene.devel/5804) here is a patch
that add a setIndexInterval() in IndexWriter.

This patch adds also a getDirectory method to IndexWriter and modifies 
SegmentMerger, IndexWriter and TermInfosWriter.

This patch passes all tests.

Any comments/criticisms welcome.

Julien"
"LUCENE-2642","TEST","TEST","merge LuceneTestCase and LuceneTestCaseJ4","We added Junit4 support, but as a separate test class.

So unfortunately, we have two separate base classes to maintain: LuceneTestCase and LuceneTestCaseJ4.
This creates a mess and is difficult to manage.

Instead, I propose a single base test class that works both junit3 and junit4 style.

I modified our LuceneTestCaseJ4 in the following way:
* the methods to run are not limited to the ones annotated with @Test, but also any void no-arg methods that start with ""test"", like junit3. this means you dont have to sprinkle @Test everywhere.
* of course, @Ignore works as expected everywhere.
* LuceneTestCaseJ4 extends TestCase so you dont have to import static Assert.* to get all the asserts.

for most tests, no changes are required. but a few very minor things had to be changed:
* setUp() and tearDown() must be public, not protected.
* useless ctors must be removed, such as TestFoo(String name) { super(name); }
* LocalizedTestCase is gone, instead of
{code}
public class TestQueryParser extends LocalizedTestCase {
{code}
it is now
{code}
@RunWith(LuceneTestCase.LocalizedTestCaseRunner.class)
public class TestQueryParser extends LuceneTestCase {
{code}
* Same with MultiCodecTestCase: (LuceneTestCase.MultiCodecTestCaseRunner.class}

I only did the core tests in the patch as a start, and i just made an empty LuceneTestCase extends LuceneTestCaseJ4.

I'd like to do contrib and solr and rename this LuceneTestCaseJ4 to only a single class: LuceneTestCase.
"
"LUCENE-2716","IMPROVEMENT","IMPROVEMENT","Improve automaton's MinimizeOperations.minimizeHopcroft() to not create so many objects","MinimizeOperations.minimizeHopcroft() creates a lot of objects because of strange arrays and useless ArrayLists with fixed length. E.g. it created List<List<List<>>>. This patch minimizes this and makes the whole method much more GC friendler by using simple arrays or avoiding empty LinkedLists at all (inside reverse array). 

minimize() is called very very often, especially in tests (MockAnalyzer).

A test for the method is prepared by Robert, we found a bug somewhere else in automaton, so this is pending until his issue and fix arrives."
"LUCENE-285","IMPROVEMENT","IMPROVEMENT","David Spencer Spell Checker improved","hy,
i developed a SpellChecker based on the David Spencer code (DSc) but more flexible.
the structure of the index is inspired of the DSc (for a 3-4 gram):
word:
gram3:
gram4:
 
3start:
4start:
..
3end:
4end:
..
transposition:
 
This index is a dictonary so there isn't the ""freq"" field like with DSc version.
it's independant of the user index. So we can add words becoming to several
fields of several index for example or, why not, to a file with a list of words.
The suggestSimilar method return a list of suggests word sorted by the
Levenshtein distance and optionaly to the popularity of the word for a specific
field in a user index. More of that, this list can be restricted only to words
present in a specific field of a user index.
 
See the test case.
 
i hope this code will be put in the lucene sandbox. 
 
Nicolas Maisonneuve"
"LUCENE-2696","TEST","BUG","TestIndexWriterDelete makes broken segments with payloads on","This could just be a SimpleText problem.... but just in case

Grant added payloads to MockAnalyzer in LUCENE-2692

I wondered what would happen if i turned on his payload filter by default for all tests.

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterDelete
    [junit] Testcase: testDeletesOnDiskFull(org.apache.lucene.index.TestIndexWriterDelete):     Caused an ERROR
    [junit] CheckIndex failed
    [junit] java.lang.RuntimeException: CheckIndex failed
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:80)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5
38)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testDeletesOnDiskFull(TestIndexWriterDelete.java:401)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit]
    [junit]
    [junit] Testcase: testUpdatesOnDiskFull(org.apache.lucene.index.TestIndexWriterDelete):     Caused an ERROR
    [junit] CheckIndex failed
    [junit] java.lang.RuntimeException: CheckIndex failed
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:80)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5
38)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testUpdatesOnDiskFull(TestIndexWriterDelete.java:405)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit]
    [junit]
    [junit] Tests run: 14, Failures: 0, Errors: 2, Time elapsed: 0.322 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] CheckIndex failed
    [junit] Segments file=segments_2 numSegments=1 version=FORMAT_4_0 [Lucene 4.0]
    [junit]   1 of 1: name=_0 docCount=157
    [junit]     codec=MockFixedIntBlock
    [junit]     compound=true
    [junit]     hasProx=true
    [junit]     numFiles=2
    [junit]     size (MB)=0,017
    [junit]     diagnostics = {os.version=6.0, os=Windows Vista, lucene.version=4.0-SNAPSHOT, source=flush, os.arch=x86,
 java.version=1.6.0_21, java.vendor=Sun Microsystems Inc.}
    [junit]     has deletions [delFileName=_0_1.del]
    [junit]     test: open reader.........OK [13 deleted docs]
    [junit]     test: fields..............OK [2 fields]
    [junit]     test: field norms.........OK [2 fields]
    [junit]     test: terms, freq, prox...ERROR [read past EOF]
    [junit] java.io.IOException: read past EOF
    [junit]     at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:154)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:119)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:94)
    [junit]     at org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl$SepDocsAndPositionsEnum.getPayload(SepPostin
gsReaderImpl.java:689)
    [junit]     at org.apache.lucene.index.CheckIndex.testTermIndex(CheckIndex.java:693)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:489)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:290)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:286)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:76)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5
38)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testDeletesOnDiskFull(TestIndexWriterDelete.java:401)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
    [junit]     test: stored fields.......OK [223 total field count; avg 1,549 fields per doc]
    [junit]     test: term vectors........OK [242 total vector count; avg 1,681 term/freq vector fields per doc]
    [junit] FAILED
    [junit]     WARNING: fixIndex() would remove reference to this segment; full exception:
    [junit] java.lang.RuntimeException: Term Index test failed
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:502)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:290)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:286)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:76)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5
38)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testDeletesOnDiskFull(TestIndexWriterDelete.java:401)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
    [junit]
    [junit] WARNING: 1 broken segments (containing 144 documents) detected
    [junit]
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterDelete -Dtestmethod=testDeletesOnDiskFull -Dtests.s
eed=-8128829179004133416:-7192468460505114475
    [junit] CheckIndex failed
    [junit] Segments file=segments_1 numSegments=1 version=FORMAT_4_0 [Lucene 4.0]
    [junit]   1 of 1: name=_0 docCount=157
    [junit]     codec=MockFixedIntBlock
    [junit]     compound=false
    [junit]     hasProx=true
    [junit]     numFiles=14
    [junit]     size (MB)=0,017
    [junit]     diagnostics = {os.version=6.0, os=Windows Vista, lucene.version=4.0-SNAPSHOT, source=flush, os.arch=x86,
 java.version=1.6.0_21, java.vendor=Sun Microsystems Inc.}
    [junit]     no deletions
    [junit]     test: open reader.........OK
    [junit]     test: fields..............OK [2 fields]
    [junit]     test: field norms.........OK [2 fields]
    [junit]     test: terms, freq, prox...ERROR [Read past EOF]
    [junit] java.io.IOException: Read past EOF
    [junit]     at org.apache.lucene.store.RAMInputStream.switchCurrentBuffer(RAMInputStream.java:89)
    [junit]     at org.apache.lucene.store.RAMInputStream.readBytes(RAMInputStream.java:73)
    [junit]     at org.apache.lucene.store.MockIndexInputWrapper.readBytes(MockIndexInputWrapper.java:109)
    [junit]     at org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl$SepDocsAndPositionsEnum.getPayload(SepPostin
gsReaderImpl.java:689)
    [junit]     at org.apache.lucene.index.CheckIndex.testTermIndex(CheckIndex.java:693)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:489)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:290)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:286)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:76)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5
38)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testUpdatesOnDiskFull(TestIndexWriterDelete.java:405)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
    [junit]     test: stored fields.......OK [237 total field count; avg 1,51 fields per doc]
    [junit]     test: term vectors........OK [254 total vector count; avg 1,618 term/freq vector fields per doc]
    [junit] FAILED
    [junit]     WARNING: fixIndex() would remove reference to this segment; full exception:
    [junit] java.lang.RuntimeException: Term Index test failed
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:502)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:290)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:286)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:76)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5
38)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testUpdatesOnDiskFull(TestIndexWriterDelete.java:405)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
    [junit]
    [junit] WARNING: 1 broken segments (containing 157 documents) detected
    [junit]
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterDelete -Dtestmethod=testUpdatesOnDiskFull -Dtests.s
eed=-8128829179004133416:-5617162412680232634
    [junit] NOTE: test params are: codec=MockFixedIntBlock(blockSize=266), locale=tr_TR, timezone=Africa/Maseru
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.index.TestIndexWriterDelete FAILED
    [junit] Testsuite: org.apache.lucene.index.TestLazyProxSkipping
    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0.034 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestLazyProxSkipping -Dtestmethod=testLazySkipping -Dtests.seed=-7
119541877291237950:-8499388603775233752
    [junit] NOTE: test params are: codec=SimpleText, locale=da_DK, timezone=Pacific/Guam
    [junit] ------------- ---------------- ---------------
{noformat}"
"LUCENE-1311","RFE","RFE","Add ability to open prior commits to IndexReader","If you use a customized DeletionPolicy, which keeps multiple commits
around (instead of the default which is to only preserve the most
recent commit), it's useful to be able to list all such commits and
then open a reader against one of these commits.

I've added this API to list commits:

  public static Collection IndexReader.listCommits(Directory)

and these two new open methods to IndexReader to open a specific commit:

  public static IndexReader open(IndexCommit)
  public static IndexReader open(IndexCommit, IndexDeletionPolicy)

Spinoff from here:

  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200806.mbox/%3c85d3c3b60806161735o207a3238sa2e6c415171a8019@mail.gmail.com%3e

"
"LUCENE-1977","CLEANUP","TASK","Remove MultiTermQuery.getTerm()","Removes the field and methods in MTQ that return the pattern term."
"LUCENE-3595","RFE","TASK","Refactor FieldCacheRangeFilter.FieldCacheDocIdSet to be separate class and fix the dangerous matchDoc() throws AIOOBE requirement","Followup from LUCENE-3593:
The FieldCacheRangeFilter.FieldCacheDocIdSet class has a strange requirement on the abstract matchDoc(): It should throw AIOOBE if the docId is > maxDoc. This check should be done by caller as especially on trunk, e.g. FieldCacheTermsFilter does not seem to always throw this exception correctly (getOrd() is a method and no array in TermsIndex cache).

Also in 3.x the Filter does not correctly respect deletions when a FieldCache based on a reopened reader is used.

This issue will refactor this and fix the bugs and moves the docId check up to the iterator."
"LUCENE-3040","IMPROVEMENT","IMPROVEMENT","analysis consumers should use reusable tokenstreams","Some analysis consumers (highlighter, more like this, memory index, contrib queryparser, ...) are using Analyzer.tokenStream but should be using Analyzer.reusableTokenStream instead for better performance."
"LUCENE-335","TEST","BUG","Testcase for StandardAnalyzer","As per our discussion on lucene-user, I'm attaching a unit test for 
StandardAnalyzer.  I wrote most of the tests from reading the comments in the 
StandardTokenizer.jj grammar.  Someone familiar with the grammar (and its 
intent) should review the tests."
"LUCENE-1540","IMPROVEMENT","IMPROVEMENT","Improvements to contrib.benchmark for TREC collections","The benchmarking utilities for  TREC test collections (http://trec.nist.gov) are quite limited and do not support some of the variations in format of older TREC collections.  

I have been doing some benchmarking work with Lucene and have had to modify the package to support:
* Older TREC document formats, which the current parser fails on due to missing document headers.
* Variations in query format - newlines after <title> tag causing the query parser to get confused.
* Ability to detect and read in uncompressed text collections
* Storage of document numbers by default without storing full text.

I can submit a patch if there is interest, although I will probably want to write unit tests for the new functionality first.

"
"LUCENE-3121","IMPROVEMENT","IMPROVEMENT","FST should offer lookup-by-output API when output strictly increases","Spinoff from ""FST and FieldCache"" java-dev thread http://lucene.markmail.org/thread/swoawlv3fq4dntvl

FST is able to associate arbitrary outputs with the sorted input keys, but in the special (and, common) case where the function is strictly monotonic (each output only ""increases"" vs prior outputs), such as mapping to term ords or mapping to file offsets in the terms dict, we should offer a lookup-by-output API that efficiently walks the FST and locates input key (exact or floor or ceil) matching that output.
"
"LUCENE-756","IMPROVEMENT","IMPROVEMENT","Maintain norms in a single file .nrm","Non-compound indexes are ~10% faster at indexing, and perform 50% IO activity comparing to compound indexes. But their file descriptors foot print is much higher. 

By maintaining all field norms in a single .nrm file, we can bound the number of files used by non compound indexes, and possibly allow more applications to use this format.

More details on the motivation for this in: http://www.nabble.com/potential-indexing-perormance-improvement-for-compound-index---cut-IO---have-more-files-though-tf2826909.html (in particular http://www.nabble.com/Re%3A-potential-indexing-perormance-improvement-for-compound-index---cut-IO---have-more-files-though-p7910403.html).
"
"LUCENE-761","IMPROVEMENT","IMPROVEMENT","Clone proxStream lazily in SegmentTermPositions","In SegmentTermPositions the proxStream should be cloned lazily, i. e. at the first time nextPosition() is called. Then the initialization costs of TermPositions are not higher anymore compared to TermDocs and thus there is no reason anymore for Scorers to use TermDocs instead of TermPositions. In fact, all Scorers should use TermPositions, because custom subclasses of existing scorers might want to access payloads, which is only possible via TermPositions. We could further merge SegmentTermDocs and SegmentTermPositions into one class and deprecate the interface TermDocs.

I'm going to attach a patch once the payloads feature (LUCENE-755) is committed."
"LUCENE-222","DOCUMENTATION","IMPROVEMENT","[PATCH] Better ""lock obtain timed out"" error message","The attached patch prints the complete path and name of the lock file. This 
should simplify debugging (it's actually a wish from the Wiki)."
"LUCENE-2195","IMPROVEMENT","IMPROVEMENT","Speedup CharArraySet if set is empty","CharArraySet#contains(...) always creates a HashCode of the String, Char[] or CharSequence even if the set is empty. 
contains should return false if set it empty"
"LUCENE-1097","IMPROVEMENT","BUG","IndexWriter.close(false) does not actually stop background merge threads","Right now when you close(false), IndexWriter marks any running merges
as aborted but then does not wait for these merges to finish.  This
can cause problems because those threads still hold files open, so,
someone might think they can call close(false) and then (say) delete
all files from that directory, which would fail on Windows.

Instead, close(false) should notify each running merge that it has
been aborted, and not return until all running merges are done.  Then,
SegmentMerger should periodically check whether it has been aborted
and stop if so.
"
"LUCENE-3609","BUG","BUG","BooleanFilter changed behavior in 3.5, no longer acts as if ""minimum should match"" set to 1","The change LUCENE-3446 causes a change in behavior in BooleanFilter. It used to work as if minimum should match clauses is 1 (compared to BQ lingo), but now, if no should clauses match, then the should clauses are ignored, and for example, if there is a must clause, only that one will be used and returned.

For example, a single must clause and should clause, with the should clause not matching anything, should not match anything, but, it will match whatever the must clause matches.

The fix is simple, after iterating over the should clauses, if the aggregated bitset is null, return null."
"LUCENE-411","IMPROVEMENT","IMPROVEMENT","[PATCH] BitSetQuery, FastPrefixQuery, FastWildcardQuery and FastQueryParser","FastPrefixQuery and FastWildcardQuery rewrites to BitSetQuery instead of OR'ed
BooleanQuery's.  A BitSetQuery contains a BitSet that desginates which document
should be included in the search result.  BitSetQuery cannot be used by itself
with MultiSearcher as of now."
"LUCENE-3577","REFACTORING","TASK","rename expungeDeletes","Similar to optimize(), expungeDeletes() has a misleading name.

We already had problems with this on the user list because TieredMergePolicy
didn't 'expunge' all their deletes.

Also I think expunge is the wrong word, because expunge makes it seem
like you just wrangle up the deletes and kick them out of the party and
that it should be fast.



"
"LUCENE-2883","REFACTORING","TASK","Consolidate Solr  & Lucene FunctionQuery into modules","Spin-off from the [dev list | http://www.mail-archive.com/dev@lucene.apache.org/msg13261.html]  "
"LUCENE-3527","RFE","IMPROVEMENT","Implement getDistance() on DirectSpellChecker.INTERNAL_LEVENSHTEIN","DirectSpellChecker.INTERNAL_LEVENSHTEIN is currently not a full-fledged implementation of StringDistance.  But an full implementation is needed for Solr's SpellCheckComponent.finishStage(), and also would be helpful for those trying to take the advice given in LIA 2nd ed section sect8.5.3."
"LUCENE-2705","TEST","BUG","TestThreadSafety.testLazyLoadThreadSafety test failure","TestThreadSafety.testLazyLoadThreadSafety failed with this error:

unable to create new native thread

Maybe because of SimpleText

Here is the stacktrace:
{noformat}
 [junit] Testsuite: org.apache.lucene.search.TestThreadSafe
    [junit] Testcase: testLazyLoadThreadSafety(org.apache.lucene.search.TestThreadSafe):	Caused an ERROR
    [junit] unable to create new native thread
    [junit] java.lang.OutOfMemoryError: unable to create new native thread
    [junit] 	at java.lang.Thread.start0(Native Method)
    [junit] 	at java.lang.Thread.start(Thread.java:614)
    [junit] 	at org.apache.lucene.search.TestThreadSafe.doTest(TestThreadSafe.java:129)
    [junit] 	at org.apache.lucene.search.TestThreadSafe.testLazyLoadThreadSafety(TestThreadSafe.java:148)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit] 
    [junit] 
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 6.051 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestThreadSafe -Dtestmethod=testLazyLoadThreadSafety -Dtests.seed=-277698010445513699:-89599297372877779
    [junit] NOTE: test params are: codec=SimpleText, locale=zh_SG, timezone=Pacific/Tongatapu
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.search.TestThreadSafe FAILED
{noformat}"
"LUCENE-2050","RFE","IMPROVEMENT","Improve contrib/benchmark for testing near-real-time search performance","It's not easy to test NRT performance right now w/ contrib/benchmark.
I've made some initial fixes to improve this:

  * Added new '&', that can follow any task within a serial sequence,
    to ""background"" the task (just like a shell).  The test runs in
    the BG, and then at the end of all serial tasks, any still running
    BG tasks are stopped & joined.

  * Added WaitTask that simply waits; useful for controlling how long
    the BG'd tasks get to run.

  * Added RollbackIndex task, which is real handy for using a given
    index for an NRT test, doing a bunch of updates, then reverting it
    all so your next run uses the same starting index

  * Fixed the existing NearRealTimeReaderTask to simply periodically
    open the new reader (previously it was also running a fixed
    search), and removed its own threading (since & can do that
    now). It periodically wakes up, opens the new reader, and swaps it
    into the PerfRunData, at the schedule you specify.  I switched all
    usage of PerfRunData's get/setIndexReader APIs to use ref
    counting.

With these changes you can now make some very simple but powerful
algs, eg:

{code}
OpenIndex
{
  NearRealtimeReader(0.5) &
  # Warm
  Search
  { ""Index1"" AddDoc > : * : 100/sec &
  [ { ""Search"" Search > : * ] : 4 &
  Wait(30.0)
}
CloseReader
RollbackIndex
RepSumByName
{code}

This alg first opens the IndexWriter, then spawns the BG thread to
reopen the NRT reader twice per second, does one warming Search (in
the FG), spans a new thread to index documents at the rate of 100 per
second, then spawns 4 search threads that do as many searches as they
can.  We then wait for 30 seconds, then stop all the threads, revert
the index, and report.

The patch is a work in progress -- it generally works, but there're a
few nocommits, and, we may want to improve reporting (though I think
that's a separate issue).
"
"LUCENE-981","RFE","RFE","NewAnalyzerTask","NewAnalyzerTask (patch to follow) allows a contrib/benchmark algorithm to change Analyzers during a run.  This is useful when comparing Analyzers

{""NewAnalyzer"" NewAnalyzer(WhitespaceAnalyzer, SimpleAnalyzer, StopAnalyzer, standard.StandardAnalyzer) >

is a sample declaration in an algorithm file."
"LUCENE-2010","IMPROVEMENT","IMPROVEMENT","Remove segments with all documents deleted in commit/flush/close of IndexWriter instead of waiting until a merge occurs.","I do not know if this is a bug in 2.9.0, but it seems that segments with all documents deleted are not automatically removed:

{noformat}
4 of 14: name=_dlo docCount=5
  compound=true
  hasProx=true
  numFiles=2
  size (MB)=0.059
  diagnostics = {java.version=1.5.0_21, lucene.version=2.9.0 817268P - 2009-09-21 10:25:09, os=SunOS,
     os.arch=amd64, java.vendor=Sun Microsystems Inc., os.version=5.10, source=flush}
  has deletions [delFileName=_dlo_1.del]
  test: open reader.........OK [5 deleted docs]
  test: fields..............OK [136 fields]
  test: field norms.........OK [136 fields]
  test: terms, freq, prox...OK [1698 terms; 4236 terms/docs pairs; 0 tokens]
  test: stored fields.......OK [0 total field count; avg ? fields per doc]
  test: term vectors........OK [0 total vector count; avg ? term/freq vector fields per doc]
{noformat}

Shouldn't such segments not be removed automatically during the next commit/close of IndexWriter?

*Mike McCandless:*
Lucene doesn't actually short-circuit this case, ie, if every single doc in a given segment has been deleted, it will still merge it [away] like normal, rather than simply dropping it immediately from the index, which I agree would be a simple optimization. Can you open a new issue? I would think IW can drop such a segment immediately (ie not wait for a merge or optimize) on flushing new deletes.
"
"LUCENE-2658","BUG","BUG","TestIndexWriterExceptions random failure: AIOOBE in ByteBlockPool.allocSlice","TestIndexWriterExceptions threw this today, and its reproducable"
"LUCENE-2272","RFE","BUG","PayloadNearQuery has hardwired explanation for 'AveragePayloadFunction'","The 'explain' method in PayloadNearSpanScorer assumes the AveragePayloadFunction was used. This patch adds the 'explain' method to the 'PayloadFunction' interface, where the Scorer can call it. Added unit tests for 'explain' and for {Min,Max}PayloadFunction."
"LUCENE-2573","IMPROVEMENT","IMPROVEMENT","Tiered flushing of DWPTs by RAM with low/high water marks","Now that we have DocumentsWriterPerThreads we need to track total consumed RAM across all DWPTs.

A flushing strategy idea that was discussed in LUCENE-2324 was to use a tiered approach:  
- Flush the first DWPT at a low water mark (e.g. at 90% of allowed RAM)
- Flush all DWPTs at a high water mark (e.g. at 110%)
- Use linear steps in between high and low watermark:  E.g. when 5 DWPTs are used, flush at 90%, 95%, 100%, 105% and 110%.

Should we allow the user to configure the low and high water mark values explicitly using total values (e.g. low water mark at 120MB, high water mark at 140MB)?  Or shall we keep for simplicity the single setRAMBufferSizeMB() config method and use something like 90% and 110% for the water marks?"
"LUCENE-2309","REFACTORING","IMPROVEMENT","Fully decouple IndexWriter from analyzers","IndexWriter only needs an AttributeSource to do indexing.

Yet, today, it interacts with Field instances, holds a private
analyzers, invokes analyzer.reusableTokenStream, has to deal with a
wide variety (it's not analyzed; it is analyzed but it's a Reader,
String; it's pre-analyzed).

I'd like to have IW only interact with attr sources that already
arrived with the fields.  This would be a powerful decoupling -- it
means others are free to make their own attr sources.

They need not even use any of Lucene's analysis impls; eg they can
integrate to other things like [OpenPipeline|http://www.openpipeline.org].
Or make something completely custom.

LUCENE-2302 is already a big step towards this: it makes IW agnostic
about which attr is ""the term"", and only requires that it provide a
BytesRef (for flex).

Then I think LUCENE-2308 would get us most of the remaining way -- ie, if the
FieldType knows the analyzer to use, then we could simply create a
getAttrSource() method (say) on it and move all the logic IW has today
onto there.  (We'd still need existing IW code for back-compat).
"
"LUCENE-2060","IMPROVEMENT","IMPROVEMENT","CMS should default its maxThreadCount to 1 (not 3)","From rough experience, I think the current default of 3 is too large.  I think we get the most bang for the buck going from 0 to 1.

I think this will especially impact optimize on an index with many segments -- in this case the MergePolicy happily exposes concurrency (multiple pending merges), and CMS will happily launch 3 threads to carry that out."
"LUCENE-1607","IMPROVEMENT","IMPROVEMENT","String.intern() faster alternative","By using our own interned string pool on top of default, String.intern() can be greatly optimized.

On my setup (java 6) this alternative runs ~15.8x faster for already interned strings, and ~2.2x faster for 'new String(interned)'
For java 5 and 4 speedup is lower, but still considerable."
"LUCENE-2568","BUG","BUG","TestUTF32ToUTF8 fails on IBM's JRE","This is because AutomatonTestUtil.RandomAcceptedString is returning an invalid UTF32 int[] -- it has an unpaired surrogate, and IBM's JRE handles this differently than Oracle's."
"LUCENE-2703","BUG","BUG","multitermquery scoring differences between 3x and trunk","try this patch with a test, that applies clean to both 3x and trunk, but fails on trunk.

if you modify the test-data-generator to use TopTerms*BoostOnly* rewrite, then it acts like TestFuzzyQuery2, and passes.

So the problem is in TopTermsScoringBooleanRewrite, or BooleanQuery, or somewhere else.
"
"LUCENE-3863","REFACTORING","IMPROVEMENT","DocValues.type() -> DocValues.getType()","This makes the method easier to find and more clear that it has no side effects... on a
few occasions I've looked for this getter and missed it because of the name.

"
"LUCENE-2682","TEST","IMPROVEMENT","create test case to verify we support > 2.1B terms","I created a test case for this... I'm leaving it as @Ignore because it takes more than four hours on a faaast machine (beast) to run.  I think we should run this before each release."
"LUCENE-3841","BUG","BUG","CloseableThreadLocal does not work well with Tomcat thread pooling","We tracked down a large memory leak (effectively a leak anyway) caused
by how Analyzer users CloseableThreadLocal.
CloseableThreadLocal.hardRefs holds references to Thread objects as
keys.  The problem is that it only frees these references in the set()
method, and SnowballAnalyzer will only call set() when it is used by a
NEW thread.

The problem scenario is as follows:

The server experiences a spike in usage (say by robots or whatever)
and many threads are created and referenced by
CloseableThreadLocal.hardRefs.  The server quiesces and lets many of
these threads expire normally.  Now we have a smaller, but adequate
thread pool.  So CloseableThreadLocal.set() may not be called by
SnowBallAnalyzer (via Analyzer) for a _long_ time.  The purge code is
never called, and these threads along with their thread local storage
(lucene related or not) is never cleaned up.

I think calling the purge code in both get() and set() would have
avoided this problem, but is potentially expensive.  Perhaps using 
WeakHashMap instead of HashMap may also have helped.  WeakHashMap 
purges on get() and set().  So this might be an efficient way to
clean up threads in get(), while set() might do the more expensive
Map.keySet() iteration.

Our current work around is to not share SnowBallAnalyzer instances
among HTTP searcher threads.  We open and close one on every request.

Thanks,
Matt"
"LUCENE-3361","RFE","BUG","port url+email tokenizer to standardtokenizerinterface (or similar)","We should do this so that we can fix the LUCENE-3358 bug there, and preserve backwards.
We also want this mechanism anyway, for upgrading to new unicode versions in the future.

We can regenerate the new TLD list for 3.4 but, we should ensure the existing one is used for the urlemail33 or whatever,
so that its exactly the same."
"LUCENE-3117","REFACTORING","TASK","yank SegmentReader.norm out of SegmentReader.java","While working on flex scoring branch and LUCENE-3012, I noticed it was difficult to navigate 
the norms handling in SegmentReader's code.

I think we should yank this inner class out into a separate file as a start."
"LUCENE-3197","BUG","BUG","Optimize runs forever if you keep deleting docs at the same time","Because we ""cascade"" merges for an optimize... if you also delete documents while the merges are running, then the merge policy will see the resulting single segment as still not optimized (since it has pending deletes) and do a single-segment merge, and will repeat indefinitely (as long as your app keeps deleting docs)."
"LUCENE-1898","DOCUMENTATION","IMPROVEMENT","Decide if we should remove lines numbers from latest Changes","As Lucene dev has grown, a new issue has arisen - many times, new changes invalidate old changes. A proper changes file should just list the changes from the last version, not document the dev life of the issues. Keeping changes in proper order now requires a lot of renumbering sometimes. The numbers have no real meaning and could be added to more rich versions (such as the html version) automatically if desired.

I think an * makes a good replacement myself. The issues already have ids that are stable, rather than the current, decorational numbers which are subject to change over a dev cycle.

I think we should replace the numbers with an asterix for the 2.9 section and going forward (ie 4. becomes *).

If we don't get consensus very quickly, this issue won't block."
"LUCENE-478","BUG","BUG","CJK char list","Seems the character list in the CJK section of the StandardTokenizer.jj is not quite complete. Following is a more complete list:

< CJK:                                          // non-alphabets
      [
	   ""\u1100""-""\u11ff"",
       ""\u3040""-""\u30ff"",
       ""\u3130""-""\u318f"",
       ""\u31f0""-""\u31ff"",
       ""\u3300""-""\u337f"",
       ""\u3400""-""\u4dbf"",
       ""\u4e00""-""\u9fff"",
       ""\uac00""-""\ud7a3"",
       ""\uf900""-""\ufaff"",
       ""\uff65""-""\uffdc""       
      ]
  >

"
"LUCENE-698","BUG","BUG","FilteredQuery ignores boost","Filtered query ignores it's own boost."
"LUCENE-871","IMPROVEMENT","BUG","ISOLatin1AccentFilter a bit slow","The ISOLatin1AccentFilter is a bit slow giving 300+ ms responses when used in a highligher for output responses.

Patch to follow"
"LUCENE-2302","IMPROVEMENT","IMPROVEMENT","Replacement for TermAttribute+Impl with extended capabilities (byte[] support, CharSequence, Appendable)","For flexible indexing terms can be simple byte[] arrays, while the current TermAttribute only supports char[]. This is fine for plain text, but e.g NumericTokenStream should directly work on the byte[] array.
Also TermAttribute lacks of some interfaces that would make it simplier for users to work with them: Appendable and CharSequence

I propose to create a new interface ""CharTermAttribute"" with a clean new API that concentrates on CharSequence and Appendable.
The implementation class will simply support the old and new interface working on the same term buffer. DEFAULT_ATTRIBUTE_FACTORY will take care of this. So if somebody adds a TermAttribute, he will get an implementation class that can be also used as CharTermAttribute. As both attributes create the same impl instance both calls to addAttribute are equal. So a TokenFilter that adds CharTermAttribute to the source will work with the same instance as the Tokenizer that requested the (deprecated) TermAttribute.

To also support byte[] only terms like Collation or NumericField needs, a separate getter-only interface will be added, that returns a reusable BytesRef, e.g. BytesRefGetterAttribute. The default implementation class will also support this interface. For backwards compatibility with old self-made-TermAttribute implementations, the indexer will check with hasAttribute(), if the BytesRef getter interface is there and if not will wrap a old-style TermAttribute (a deprecated wrapper class will be provided): new BytesRefGetterAttributeWrapper(TermAttribute), that is used by the indexer then."
"LUCENE-3334","RFE","IMPROVEMENT","IOUtils.closeSafely should log suppressed Exceptions in stack trace of original Exception (a new feature of Java 7)","I was always against Java 6 support, as it brings no really helpful new features into Lucene. But there are several things that make life easier in coming Java 7 (hopefully on July 28th, 2011). One of those is simplier Exception handling and suppression on Closeable, called ""Try-With-Resources"" (see http://docs.google.com/View?id=ddv8ts74_3fs7483dp, by the way all Lucene classes support these semantics in Java 7 automatically, the cool try-code below would work e.g. for IndexWriter, TokenStreams,...).

We already have this functionality in Lucene since adding the IOUtils.closeSafely() utility (which can be removed when Java 7 is the minimum requirement of Lucene - maybe in 10 years):

{code:java}
try (Closeable a = new ...; Closeable b = new ...) {
  ... use Closeables ...
} catch (Exception e) {
  dosomething;
  throw e;
}
{code}

This code will close a and b in an autogenerated finally block and supress any exception. This is identical to our IOUtils.closeSafely:

{code:java}
Exception priorException = null;
Closeable a,b;
try (Closeable a = new ...; Closeable b = new ...) {
  a = new ...;
  b = new ...
  ... use Closeables ...
} catch (Exception e) {
  priorException = e;
  dosomething;
} finally {
  IOUtils.closeSafely(priorException, a, b);
}
{code}

So this means we have the same functionality without Java 7, but there is one thing that makes logging/debugging much nicer:
The above Java 7 code also adds maybe suppressed Exceptions in those Closeables to the priorException, so when you print the stacktrace, it not only shows the stacktrace of the original Exception, it also prints all Exceptions that were suppressed to throw this Exception (all Closeable.close() failures):

{noformat}
org.apache.lucene.util.TestIOUtils$TestException: BASE-EXCEPTION
    at org.apache.lucene.util.TestIOUtils.testSuppressedExceptions(TestIOUtils.java:61)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:601)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1486)
    at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1404)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)
    Suppressed: java.io.IOException: TEST-IO-EXCEPTION-1
            at org.apache.lucene.util.TestIOUtils$BrokenCloseable.close(TestIOUtils.java:36)
            at org.apache.lucene.util.IOUtils.closeSafely(IOUtils.java:58)
            at org.apache.lucene.util.TestIOUtils.testSuppressedExceptions(TestIOUtils.java:62)
            ... 26 more
    Suppressed: java.io.IOException: TEST-IO-EXCEPTION-2
            at org.apache.lucene.util.TestIOUtils$BrokenCloseable.close(TestIOUtils.java:36)
            at org.apache.lucene.util.IOUtils.closeSafely(IOUtils.java:58)
            at org.apache.lucene.util.TestIOUtils.testSuppressedExceptions(TestIOUtils.java:62)
            ... 26 more
{noformat}

For this in Java 7 a new method was added to Throwable, that allows logging such suppressed Exceptions (it is called automatically by the synthetic bytecode emitted by javac). This patch simply adds this functionality conditionally to IOUtils, so it ""registers"" all suppressed Exceptions, if running on Java 7. This is done by reflection: once it looks for this method in Throwable.class and if found, it invokes it in closeSafely, so the exceptions thrown on Closeable.close() don't get lost.

This makes debugging much easier and logs all problems that may occur.

This patch does *not* change functionality or behaviour, it just adds more nformation to the stack trace in a Java-7-way (similar to the way how Java 1.4 added causes). It works here locally on Java 6 and Java 7, but only Java 7 gets the additional stack traces. For Java 6 nothing changes. Same for Java 5 (if we backport to 3.x).

This would be our first Java 7 improvement (a minor one). Next would be NIO2... - but thats not easy to do with reflection only, so we have to wait 10 years :-)"
"LUCENE-2693","BUG","BUG","Add delete term and query need to more precisely record the bytes used","DocumentsWriter's add delete query and add delete term add to the number of bytes used regardless of the query or term already existing in the respective map."
"LUCENE-1899","IMPROVEMENT","BUG","Inefficient growth of OpenBitSet","Hi, I found a potentially serious efficiency problem with OpenBitSet.

One typical (I think) way to build a bit set is to set() the bits one by one -
e.g., have a HitCollector set() the bit for each matching document.
The underlying array of longs needs to grow as more as more bits are set, of
course.

But looking at the code, it appears to me that the array grows very
ineefficiently - in the worst case (when doc ids are sorted, as they would
normally be in the HitCollector case for example), copying the array again
and again for every added bit... The relevant code in OpenBitSet.java is:

  public void set(long index) {
    int wordNum = expandingWordNum(index);
    ...
  }

  protected int expandingWordNum(long index) {
    int wordNum = (int)(index >> 6);
    if (wordNum>=wlen) {
      ensureCapacity(index+1);
    ...
  }
  public void ensureCapacityWords(int numWords) {
    if (bits.length < numWords) {
      long[] newBits = new long[numWords];
      System.arraycopy(bits,0,newBits,0,wlen);
      bits = newBits;
    }
  }

As you can see, if the bits array is not long enough, a new one is
allocated at exactly the right size - and in the worst case it can grow
just one word every time...

Shouldn't the growth be more exponential in nature, e.g., grow to the maximum
of index+1 and twice the existing size?

Alternatively, if the growth is so inefficient, this should be documented,
and it should be recommended to use the variant of the constructor with the
correct initial size (e.g., in the HitCollector case, the number of documents
in the index). and the fastSet() method instead of set().

Thanks,
Nadav.
"
"LUCENE-3094","IMPROVEMENT","IMPROVEMENT","optimize lev automata construction","in our lev automata algorithm, we compute an upperbound of the maximum possible states (not the true number), and
create some ""useless"" unconnected states ""floating around"".

this isn't harmful, in the original impl we did the Automaton is simply a pointer to the initial state, and all algorithms
traverse this list, so effectively the useless states were dropped immediately. But recently we changed automaton to
cache its numberedStates, and we set them here, so these useless states are being kept around.

it has no impact on performance, but can be really confusing if you are debugging (e.g. toString). Thanks to Dawid Weiss
for noticing this. 

at the same time, forcing an extra traversal is a bit scary, so i did some benchmarking with really long strings and found
that actually its helpful to reduce() the number of transitions (typically cuts them in half) for these long strings, as it
speeds up some later algorithms. 

won't see any speedup for short terms, but I think its easier to work with these simpler automata anyway, and it eliminates
the confusion of seeing the redundant states without slowing anything down.
"
"LUCENE-1005","BUG","BUG","GData TestDateFormater (sic) fails when the Date returned is: Sun, 23 Sep 2007 01:29:06 GMT+00:00","TestDateFormater.testFormatDate fails when the Date returned is Sun, 23 Sep 2007 01:29:06 GMT+00:00

The issue lies with the +00:00 at the end of the string.  

The question is, though, is that a valid date for GData?

This is marked as major b/c it is causing nightly builds to fail."
"LUCENE-1296","RFE","RFE","Allow use of compact DocIdSet in CachingWrapperFilter","Extends CachingWrapperFilter with a protected method to determine the DocIdSet to be cached."
"LUCENE-2985","IMPROVEMENT","IMPROVEMENT","Build SegmentCodecs incrementally for consistent codecIDs during indexing","currently we build the SegementCodecs during flush which is fine as long as no codec needs to know which fields it should handle. This will change with DocValues or when we expose StoredFields / TermVectors via Codec (see LUCENE-2621 or LUCENE-2935). The other downside it that we don't have a consistent view of which codec belongs to which field during indexing and all FieldInfo instances are unassigned (set to -1). Instead we should build the SegmentCodecs incrementally as fields come in so no matter when a codec needs to be selected to process a document / field we have the right codec ID assigned.

"
"LUCENE-2459","BUG","BUG","FilterIndexReader doesn't work correctly with post-flex SegmentMerger","IndexWriter.addIndexes(IndexReader...) internally uses SegmentMerger to add data from input index readers. However, SegmentMerger uses the new post-flex API to do this, which bypasses the pre-flex TermEnum/TermPositions API that FilterIndexReader implements. As a result, filtering is not applied."
"LUCENE-3744","RFE","IMPROVEMENT","Add support for type whitelist in TypeTokenFilter","A usual use case for TypeTokenFilter is allowing only a set of token types. That is, listing allowed types, instead of filtered ones. I'm attaching a patch to add a useWhitelist option for that."
"LUCENE-276","BUG","BUG","Memory leak when sorting","This is the same post I sended two days before to the Lucene user's list. This 
bug seems to have something in common with bug no. 30628 but that bug is closed 
as invalid.

I'm sending test code that everyone can try. The code is singular, don't say 
there is no sense in reopening the same index. I can only show, that reopening 
leaks memory. The index is filled by pseudo-real data, they aren't significant 
and the process of index creation as well. 

The problem must be in field caching code used by sort.

Affected versions of Lucene:
1.4.1
CVS 1.5-rc1-dev

This code survives only few first iterations if you run java with -Xmx5m. With 
Lucene 1.4-final ends regulary.

import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.document.Field;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.index.Term;
import org.apache.lucene.search.Hits;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.Searcher;
import org.apache.lucene.search.Sort;
import org.apache.lucene.search.SortField;
import org.apache.lucene.search.TermQuery;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.RAMDirectory;

import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.Calendar;
import java.util.Date;

/**
 * Run this test with Lucene 1.4.1 and -Xmx5m
 */
public class ReopenTest
{
    private static long mem_last = 0;

    public static void main(String[] args) throws IOException
    {
        Directory directory = create_index();

        for (int i = 1; i < 100; i++) {
            System.err.println(""loop "" + i + "", index version: "" + IndexReader.
getCurrentVersion(directory));
            search_index(directory);
            add_to_index(directory, i);
        }
    }

    private static void add_to_index(Directory directory, int i) throws 
IOException
    {
        IndexWriter writer = new IndexWriter(directory, new StandardAnalyzer(), 
false);

        SimpleDateFormat df = new SimpleDateFormat(""yyyy-MM-dd"");
        Document doc = new Document();

        doc.add(Field.Keyword(""date"", 
          df.format(new Date(System.currentTimeMillis()))));
        doc.add(Field.Keyword(""id"", ""CD"" + String.valueOf(i)));
        doc.add(Field.Text(""text"", ""Tohle neni text "" + i));
        writer.addDocument(doc);

        System.err.println(""index size: "" + writer.docCount());
        writer.close();
    }

    private static void search_index(Directory directory) throws IOException
    {
        IndexReader reader = IndexReader.open(directory);
        Searcher searcher = new IndexSearcher(reader);

        print_mem(""search 1"");
        SortField[] fields = new SortField[2];
        fields[0] = new SortField(""date"", SortField.STRING, true);
        fields[1] = new SortField(""id"", SortField.STRING, false);
        Sort sort = new Sort(fields);
        TermQuery query = new TermQuery(new Term(""text"", ""\""text 5\""""));

        print_mem(""search 2"");
        Hits hits = searcher.search(query, sort);
        print_mem(""search 3"");

        for (int i = 0; i < hits.length(); i++) {
            Document doc = hits.doc(i);
            System.out.println(""doc "" + i + "": "" + doc.toString());
        }
        print_mem(""search 4"");
        searcher.close();
        reader.close();
    }

    private static void print_mem(String log)
    {
        long mem_free = Runtime.getRuntime().freeMemory();
        long mem_total = Runtime.getRuntime().totalMemory();
        long mem_max = Runtime.getRuntime().maxMemory();

        long delta = (mem_last - mem_free) * -1;

        System.out.println(log + ""= delta: "" + delta + "", free: "" + mem_free + 
"", used: "" + (mem_total-mem_free) + "", total: "" + mem_total + "", max: "" + 
mem_max);

        mem_last = mem_free;
    }

    private static Directory create_index() throws IOException
    {
        print_mem(""create 1"");
        Directory directory = new RAMDirectory();

        Calendar c = Calendar.getInstance();
        SimpleDateFormat df = new SimpleDateFormat(""yyyy-MM-dd"");
        IndexWriter writer = new IndexWriter(directory, new StandardAnalyzer(), 
true);
        for (int i = 0; i < 365 * 15; i++) {
            Document doc = new Document();

            doc.add(Field.Keyword(""date"", 
               df.format(new Date(c.getTimeInMillis()))));
            doc.add(Field.Keyword(""id"", ""AB"" + String.valueOf(i)));
            doc.add(Field.Text(""text"", ""Tohle je text "" + i));
            writer.addDocument(doc);

            doc = new Document();

            doc.add(Field.Keyword(""date"", 
               df.format(new Date(c.getTimeInMillis()))));
            doc.add(Field.Keyword(""id"", ""ef"" + String.valueOf(i)));
            doc.add(Field.Text(""text"", ""Je tohle text "" + i));
            writer.addDocument(doc);

            c.add(Calendar.DAY_OF_YEAR, 1);
        }
        writer.optimize();
        System.err.println(""index size: "" + writer.docCount());
        writer.close();

        print_mem(""create 2"");
        return directory;
    }
}"
"LUCENE-2393","RFE","RFE","Utility to output total term frequency and df from a lucene index","This is a pair of command line utilities that provide information on the total number of occurrences of a term in a Lucene index.  The first  takes a field name, term, and index directory and outputs the document frequency for the term and the total number of occurrences of the term in the index (i.e. the sum of the tf of the term for each document).   The second reads the index to determine the top N most frequent terms (by document frequency) and then outputs a list of those terms along with  the document frequency and the total number of occurrences of the term. Both utilities are useful for estimating the size of the term's entry in the *prx files and consequent Disk I/O demands. "
"LUCENE-2319","DOCUMENTATION","IMPROVEMENT","IndexReader # doCommit - typo nit about v3.0 in trunk","Trunk is already in 3.0.1+ . But the documentation says -  ""In 3.0, this will become ... "".  Since it is already in 3.0, it might as well be removed. 
"
"LUCENE-316","DOCUMENTATION","BUG","[PATCH] Mention RangeFilter in javadoc for maxClauseCount"," "
"LUCENE-1502","BUG","BUG","CharArraySet behaves inconsistently in add(Object) and contains(Object)","CharArraySet's add(Object) method looks like this:
    if (o instanceof char[]) {
      return add((char[])o);
    } else if (o instanceof String) {
      return add((String)o);
    } else if (o instanceof CharSequence) {
      return add((CharSequence)o);
    } else {
      return add(o.toString());
    }
You'll notice that in the case of an Object (for example, Integer), the o.toString() is added. However, its contains(Object) method looks like this:
    if (o instanceof char[]) {
      char[] text = (char[])o;
      return contains(text, 0, text.length);
    } else if (o instanceof CharSequence) {
      return contains((CharSequence)o);
    }
    return false;
In case of contains(Integer), it always returns false. I've added a simple test to TestCharArraySet, which reproduces the problem:
  public void testObjectContains() {
    CharArraySet set = new CharArraySet(10, true);
    Integer val = new Integer(1);
    set.add(val);
    assertTrue(set.contains(val));
    assertTrue(set.contains(new Integer(1)));
  }
Changing contains(Object) to this, solves the problem:
    if (o instanceof char[]) {
      char[] text = (char[])o;
      return contains(text, 0, text.length);
    } 
    return contains(o.toString());

The patch also includes few minor improvements (which were discussed on the mailing list) such as the removal of the following dead code from getHashCode(CharSequence):
      if (false && text instanceof String) {
        code = text.hashCode();
and simplifying add(Object):
    if (o instanceof char[]) {
      return add((char[])o);
    }
    return add(o.toString());
(which also aligns with the equivalent contains() method).

One thing that's still left open is whether we can avoid the calls to Character.toLowerCase calls in all the char[] array methods, by first converting the char[] to lowercase, and then passing it through the equals() and getHashCode() methods. It works for add(), but fails for contains(char[]) since it modifies the input array."
"LUCENE-2704","TEST","BUG","TestIndexWriter.testOptimizeTempSpaceUsage fails w/ SimpleText codec","{noformat}
   [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
   [junit] Testcase: testOptimizeTempSpaceUsage(org.apache.lucene.index.TestIndexWriter):      FAILED
   [junit] optimized used too much temporary space: starting usage was 117867 bytes; max temp usage was 363474 but should have been 353601 (= 3X starting usage)
   [junit] junit.framework.AssertionFailedError: optimized used too much temporary space: starting usage was 117867 bytes; max temp usage was 363474 but should have been 353601 (= 3X starting usage)
   [junit]     at org.apache.lucene.index.TestIndexWriter.testOptimizeTempSpaceUsage(TestIndexWriter.java:662)
   [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
   [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
   [junit]
   [junit]
   [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 5.284 sec
   [junit]
   [junit] ------------- Standard Output ---------------
   [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testOptimizeTempSpaceUsage -Dtests.seed=-3299990090561349208:2824386407253661541
   [junit] NOTE: test params are: codec=SimpleText, locale=el_GR, timezone=Africa/Dar_es_Salaam
{noformat}

It's not just SimpleText (because -Dtests.codec=SimpleText, alone, sometimes passes)... there must be something else about the RIWC settings.
"
"LUCENE-2842","RFE","RFE","add Galician analyzer","Adds analyzer for Galician, based upon [""Regras do lematizador para o galego""|http://bvg.udc.es/recursos_lingua/stemming.jsp] , and a set of stopwords created in the usual fashion.

This is really just an adaptation of the Portuguese [RSLP|http://www.inf.ufrgs.br/~viviane/rslp/index.htm], so I added that too, and modified our existing hand-coded RSLP-S (RSLP's plural-only step) to just be a plural-only flow of RSLP.
"
"LUCENE-2621","RFE","IMPROVEMENT","Extend Codec to handle also stored fields and term vectors","Currently Codec API handles only writing/reading of term-related data, while stored fields data and term frequency vector data writing/reading is handled elsewhere.

I propose to extend the Codec API to handle this data as well."
"LUCENE-1102","IMPROVEMENT","IMPROVEMENT","EnwikiDocMaker id field","The EnwikiDocMaker is fairly usable outside of the benchmarking class, but it would benefit from indexing the ID field of the docs.

Patch to follow that adds an ID field."
"LUCENE-3789","DESIGN_DEFECT","IMPROVEMENT","Expose FilteredTermsEnum from MTQ ","MTQ#getEnum() is protected and in order to access it you need to be in the o.a.l.search package. 

here is a relevant snipped from the mailing list discussion

{noformat}
getEnum() is protected so it is intended to be called *only* by subclasses (that's the idea behind protected methods). They are also accessible by other classes from the same package, but that's more a Java bug than a feature. The problem with MTQ is that RewriteMethod is a separate *class* and *not a subclass* of MTQ, so the method cannot be called (it can because of the ""java bug"" called from same package). So theoretically it has to be public otherwise you cannot call getEnum().

Another cleaner fix would be to add a protected final method to RewriteMethod that calls this method from MTQ. So anything subclassing RewriteMethod can get the enum from inside the RewriteMethod class which is the ""correct"" way to handle it. Delegating to MTQ is then ""internal"".
{noformat}"
"LUCENE-2310","CLEANUP","","Reduce Fieldable, AbstractField and Field complexity","In order to move field type like functionality into its own class, we really need to try to tackle the hierarchy of Fieldable, AbstractField and Field.  Currently AbstractField depends on Field, and does not provide much more functionality that storing fields, most of which are being moved over to FieldType.  Therefore it seems ideal to try to deprecate AbstractField (and possible Fieldable), moving much of the functionality into Field and FieldType."
"LUCENE-2758","BUG","BUG","TestPerFieldCodecSupport intermittent fail","{noformat}

    [junit] Testsuite: org.apache.lucene.index.TestPerFieldCodecSupport
    [junit] Testcase: testChangeCodecAndMerge(org.apache.lucene.index.TestPerFieldCodecSupport):	FAILED
    [junit] expected:<4> but was:<3>
    [junit] junit.framework.AssertionFailedError: expected:<4> but was:<3>
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:881)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:847)
    [junit] 	at org.apache.lucene.index.TestPerFieldCodecSupport.assertHybridCodecPerField(TestPerFieldCodecSupport.java:189)
    [junit] 	at org.apache.lucene.index.TestPerFieldCodecSupport.testChangeCodecAndMerge(TestPerFieldCodecSupport.java:145)
    [junit] 
    [junit] 
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.416 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestPerFieldCodecSupport -Dtestmethod=testChangeCodecAndMerge -Dtests.seed=1508266713336297966:-102145263724760840
    [junit] NOTE: test params are: codec=SimpleText, locale=ms, timezone=America/Winnipeg
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestPerFieldCodecSupport]
    [junit] ------------- ---------------- ---------------
    [junit] Test org.apache.lucene.index.TestPerFieldCodecSupport FAILED
{noformat}

I haven't tried to figure it out yet..."
"LUCENE-3815","CLEANUP","BUG","Correct copy-paste victim Comment","Correct the doc-comment of FieldsProducer (being a copy-paste victim of FieldsConsumer).
""consumes"" replaced with ""produces"".

One word change to avoid confusion: safe to commit.
"
"LUCENE-3612","DESIGN_DEFECT","TASK","remove _X.fnx","Currently we store a global (not per-segment) field number->name mapping in _X.fnx

However, it doesn't actually save us any performance e.g on IndexWriter's init because
since LUCENE-2984 we are to loading the fieldinfos anyway to compute files() for IFD, etc, 
as thats where hasProx/hasVectors is.

Additionally in the past global files like shared doc stores have caused us problems,
(recently we just fixed a bug related to this file in LUCENE-3601).

Finally this is trouble for backwards compatibility as its difficult to handle a global
file with the codecs mechanism."
"LUCENE-960","RFE","IMPROVEMENT","SpanQueryFilter addition","Similar to the QueryFilter (or whatever it is called now) the SpanQueryFilter is a regular Lucene Filter, but it also can return Spans-like information.  This is useful if you not only want to filter based on a Query, but you then want to be able to compare how a given match from a new query compared to the positions of the filtered SpanQuery.  Patch to come shortly also contains a caching mechanism for the SpanQueryFilter"
"LUCENE-2978","IMPROVEMENT","BUG","Upgrade benchmark from commons-compress-1.0 to commons-compress-1.1 for 15 times faster gzip decompression","In LUCENE-1540 TrecContentSource moved from Java's GZipInputStream to common-compress 1.0. 
This slowed down gzip decompression by a factor of 15. 
Upgrading to 1.1 solves this problem.
I verified that the problem is only in GZIP, not in BZIP.
On the way, as 1.1 introduced constants for the compression methods, the code can be made a bit nicer."
"LUCENE-1153","BUILD_SYSTEM","BUG","Lucene needs to ship the JUnit jar for testing","In order for Hudson builds, etc. to work properly, Lucene needs to ship the JUnit jar and have it made available in the testing classpath.  Our system reqs say 3.8.1, but I have 3.8.2 laying around, so I will update the system requirements, too."
"LUCENE-3397","TEST","","Cleanup Test TokenStreams so they are reusable","Many TokenStreams created in tests are not reusable.  Some do some really messy things which prevent their reuse so we may have to change the tests themselves.

We'll target back porting this to 3x."
"LUCENE-2119","BUG","BUG","If you pass Integer.MAX_VALUE as 2nd param to search(Query, int) you hit unexpected NegativeArraySizeException","Note that this is a nonsense value to pass in, since our PQ impl allocates the array up front.

It's because PQ takes 1+ this value (which wraps to -1), and attempts to allocate that.  We should bounds check it, and drop PQ size by one in this case.

Better, maybe: in IndexSearcher, if that n is ever > maxDoc(), set it to maxDoc().

This trips users up fairly often because they assume our PQ doesn't statically pre-allocate (a reasonable assumption...)."
"LUCENE-1776","RFE","BUG","NearSpansOrdered does not lazy load payloads as the PayloadSpans javadoc implies","Best would be to lazy load, but I don't see how with the current algorithm. Short that, we should add an option to ignore payloads - otherwise, if you are doing non payload searching, but the payloads are present, they will be needlessly loaded.

Already added this to LUCENE-1748, but spinning from that issue to this - patch to follow when LUCENE-1748 is committed."
"LUCENE-2805","IMPROVEMENT","BUG","SegmentInfos shouldn't blindly increment version on commit","SegmentInfos currently increments version on the assumption that there are always changes.

But, both DirReader and IW are more careful about tracking whether there are changes.  DirReader has hasChanges and IW has changeCount.  I think these classes should notify the SIS when there are in fact changes; this will fix the case Simon hit on fixing LUCENE-2082 when the NRT reader thought there were changes, but in fact there weren't because IW simply committed the exact SIS it already had.
"
"LUCENE-657","DESIGN_DEFECT","IMPROVEMENT","FuzzyQuery should not be final","I am trying to extend the FuzzyQuery to further filter the TermEnum. (I am indexing stem forms and original forms, but I only want to match original forms with a fuzzy term, otherwise I get to much noise). However, FuzzyQuery is a final class and I cannot extend it.  

As discussed in the mailing list (http://www.gossamer-threads.com/lists/lucene/java-dev/38756), we want to make the private variables and inner classes protected.

I am attaching a patch for FuzzyQuery.java that implements this. I ran all unit tests and they passed without errors.

Andreas."
"LUCENE-744","TEST","BUG","TestFieldsReader - TestLazyPerformance problems w/ permissions in temp dir in multiuser environment","Was trying to setup some enhancements to the nightly builds and the testLazyPerformance test failed in TestFieldsReader since it couldn't delete the lazyDir directory from someone else's running of the test.  Will change it to append user.name System property to the directory name."
"LUCENE-2235","RFE","BUG","implement PerFieldAnalyzerWrapper.getOffsetGap","PerFieldAnalyzerWrapper does not delegates calls to getOffsetGap(Fieldable), instead it returns the default values from the implementation of Analyzer. (Similar to LUCENE-659 ""PerFieldAnalyzerWrapper fails to implement getPositionIncrementGap"")"
"LUCENE-1003","BUG","BUG","[PATCH] RussianAnalyzer's tokenizer skips numbers from input text,","RussianAnalyzer's tokenizer skips numbers from input text, so that resulting token stream miss numbers. Problem can be solved by adding numbers to RussianCharsets.UnicodeRussian. See test case below  for details.

{code:title=TestRussianAnalyzer.java|borderStyle=solid}

public class TestRussianAnalyzer extends TestCase {

  Reader reader = new StringReader(""text 1000"");

  // test FAILS
  public void testStemmer() {
    testAnalyzer(new RussianAnalyzer());
  }

  // test PASSES
  public void testFixedRussianAnalyzer() {
    testAnalyzer(new RussianAnalyzer(getRussianCharSet()));
  }

  private void testAnalyzer(RussianAnalyzer analyzer) {
    try {
      TokenStream stream = analyzer.tokenStream(""text"", reader);
      assertEquals(""text"", stream.next().termText());
      assertNotNull(stream.next());
    } catch (IOException e) {
      fail(e.getMessage());
    }
  }

  private char[] getRussianCharSet() {
    int length = RussianCharsets.UnicodeRussian.length;
    final char[] russianChars = new char[length + 10];

    System
        .arraycopy(RussianCharsets.UnicodeRussian, 0, russianChars, 0, length);
    russianChars[length++] = '0';
    russianChars[length++] = '1';
    russianChars[length++] = '2';
    russianChars[length++] = '3';
    russianChars[length++] = '4';
    russianChars[length++] = '5';
    russianChars[length++] = '6';
    russianChars[length++] = '7';
    russianChars[length++] = '8';
    russianChars[length] = '9';
    return russianChars;
  }
}

{code} "
"LUCENE-3112","RFE","IMPROVEMENT","Add IW.add/updateDocuments to support nested documents","I think nested documents (LUCENE-2454) is a very compelling addition
to Lucene.  It's also a popular (many votes) issue.

Beyond supporting nested document querying, which is already an
incredible addition since it preserves the relational model on
indexing normalized content (eg, DB tables, XML docs), LUCENE-2454
should also enable speedups in grouping implementation when you group
by a nested field.

For the same reason, it can also enable very fast post-group facet
counting impl (LUCENE-3097) when you what to
count(distinct(nestedField)), instead of unique documents, as your
""identifier"".  I expect many apps that use faceting need this ability
(to count(distinct(nestedField)) not distinct(docID)).

To support these use cases, I believe the only core change needed is
the ability to atomically add or update multiple documents, which you
cannot do today since in between add/updateDocument calls a flush (eg
due to commit or getReader()) could occur.

This new API (addDocuments(Iterable<Document>), updateDocuments(Term
delTerm, Iterable<Document>) would also further guarantee that the
documents are assigned sequential docIDs in the order the iterator
provided them, and that the docIDs all reside in one segment.

Segment merging never splits segments apart, so this invariant would
hold even as merges/optimizes take place.
"
"LUCENE-2053","DESIGN_DEFECT","IMPROVEMENT","When thread is interrupted we should throw a clear exception","This is the 3.0 followon from LUCENE-1573.  We should throw a dedicated exception, not just RuntimeException.

Recent discussion from java-dev ""Thread.interrupt()"" subject: http://www.lucidimagination.com/search/document/8423f9f0b085034e/thread_interrupt"
"LUCENE-3455","IMPROVEMENT","","All Analysis Consumers should use reusableTokenStream","With Analyzer now using TokenStreamComponents, theres no reason for Analysis consumers to use tokenStream() (it just gives bad performance).  Consequently all consumers will be moved over to using reusableTokenStream().  The only challenge here is that reusableTokenStream throws an IOException which many consumers are not rigged to deal with.

Once all consumers have been moved, we can rename reusableTokenStream() back to tokenStream()."
"LUCENE-1334","RFE","IMPROVEMENT","Term improvement","Term is designed for reuse of the supplied filter, to minimize intern().

One of the common use patterns is to create a Term with the txt field being an empty string.

To simplify this pattern and to document it's usefulness, I suggest adding a constructor:
public Term(String fld)
with the obvious implementation
and use it throughout core and contrib as a replacement.

"
"LUCENE-1676","RFE","RFE","New Token filter for adding payloads ""in-stream""","This TokenFilter is able to split a token based on a delimiter and use one part as the token and the other part as a payload.  This allows someone to include payloads inline with tokens (presumably setup by a pipeline ahead of time).  An example is apropos.  Given a | delimiter, we could have a stream that looks like:
{quote}The quick|JJ red|JJ fox|NN jumped|VB over the lazy|JJ brown|JJ dogs|NN{quote}

In this case, this would produce tokens and payloads (assuming whitespace tokenization):
Token: the
Payload: null

Token: quick
Payload: JJ

Token: red
Pay: JJ.

and so on.

This patch will also support pluggable encoders for the payloads, so it can convert from the character array to byte arrays as appropriate."
"LUCENE-905","BUILD_SYSTEM","BUG","left nav of docs/index.html in dist artifacts links to hudson for javadocs","When building the zip or tgz release artifacts, the docs/index.html file contained in that release (the starter point for people to read documentation) links ""API Docs"" to 
http://lucene.zones.apache.org:8080/hudson/job/Lucene-Nightly/javadoc/ instead of to ./api/index.html (the local copy of the javadocs)

this relates to the initial migration to hudson for the nightly builds and a plan to copy the javadocs back to lucene.apache.org that wasn't considered urgent since it was just for transient nightly docs, but a side affect is that the release documentation also links to hudson.

even if we don't modify the nightly build process before the 2.2 release, we should update the link in the left nav in the 2.2 release branch before building the final release."
"LUCENE-3473","IMPROVEMENT","IMPROVEMENT","CheckIndex should verify numUniqueTerms == recomputedNumUniqueTerms","Just glancing at the code it seems to sorta do this check, but only in the hasOrd==true case maybe (which seems to be testing something else)?

It would be nice to verify this also for terms dicts that dont support ord.

we should add explicit checks per-field in 4.x, and for-all-fields in 3.x and preflex"
"LUCENE-2260","BUILD_SYSTEM","BUG","AttributeSource holds strong reference to class instances and prevents unloading e.g. in Solr if webapplication reload and custom attributes in separate classloaders are used (e.g. in the Solr plugins classloader)","When working on the dynmaic proxy classes using cglib/javaassist i recognized a problem in the caching code inside AttributeSource:
- AttributeSource has a static (!) cache map that holds implementation classes for attributes to be faster on creating new attributes (reflection cost)
- AttributeSource has a static (!) cache map that holds a list of all interfaces implemented by a specific AttributeImpl

Also:
- VirtualMethod in 3.1 hold a map of implementation distances keyed by subclasses of the deprecated API

Both have the problem that this strong reference is inside Lucene's classloader and so persists as long as lucene lives. The classes referenced can never be unloaded therefore, which would be fine if all live in the same classloader. As soon as the Attribute or implementation class or the subclass of the deprecated API are loaded by a different classloder (e.g. Lucene lives in bootclasspath of tomcat, but lucene-consumer with custom attributes lives in a webapp), they can never be unloaded, because a reference exists.

Libs like CGLIB or JavaAssist or JDK's reflect.Proxy have a similar cache for generated class files. They also manage this by a WeakHashMap. The cache will always work perfect and no class will be evicted without reason, as classes are only unloaded when the classloader goes and this will only happen on request (e.g. by Tomcat)."
"LUCENE-2629","BUG","BUG","In modules/analysys/icu, ant gennorm2 does not work","
Command to run gennorm2 does not work at present.  Also, icupkg needs to be called to convert the binary file to big-endian.

I will attach a patch."
"LUCENE-2156","IMPROVEMENT","IMPROVEMENT","use AtomicInteger/Boolean to track IR.refCount and IW.closed","Less costly than synchronized methods we have now..."
"LUCENE-3558","REFACTORING","IMPROVEMENT","SearcherManager and NRTManager should be in the same package","I didnt even know NRTManager was still around, because its in the .index package, whereas SearcherManager is in the .search package.

Separately, I don't like that this stuff is so 'hard' with core lucene... would it be so bad if this stuff was added to core?

I suspect a lot of people have issues with this stuff (see http://www.lucidimagination.com/search/document/37964e5f0e5d733b) for example.

Worst case is just that, combine mistakes with trying to manage this stuff with MMap unmapping and total lack of error detection
for searching closed readers (LUCENE-3439) and its a mess.
"
"LUCENE-275","RFE","BUG","Occur incompletely implemented for remote use.","Occur does not implement readResolve() creating problems for
ParallelMultiSearcher y."
"LUCENE-3179","RFE","IMPROVEMENT","OpenBitSet.prevSetBit()","Find a previous set bit in an OpenBitSet.
Useful for parent testing in nested document query execution LUCENE-2454 ."
"LUCENE-3553","IMPROVEMENT","IMPROVEMENT","tweak AppendingCodec to write segments_N compatible with 'normal' Lucene","Just an easy improvement from LUCENE-3490:

Currently AppendingCodec writes a different segments_N format (it writes no checksum at all in commit())
If you don't configure your codecprovider correctly in IndexReader, you will get read past EOF.
(we have some proposed fixes for this stuff in LUCENE-3490 branch)

But besides this, all it really needs to do is no-op prepareCommit(), it can still write the 'final' checksum
which is a good thing."
"LUCENE-785","DESIGN_DEFECT","BUG","RAMDirectory not Serializable","The current implementation of RAMDirectory throws a NotSerializableException when trying to serialize, due to the inner class KeySet of HashMap not being serializable (god knows why)

java.io.NotSerializableException: java.util.HashMap$KeySet
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1081)

Caused by line 43:

private Set fileNames = fileMap.keySet();

EDIT:

while we're at it: same goes for inner class Values 

java.io.NotSerializableException: java.util.HashMap$Values
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1081)

Collection files = fileMap.values();
"
"LUCENE-1939","BUG","BUG","IndexOutOfBoundsException at ShingleMatrixFilter's Iterator#hasNext method","I tried to use the ShingleMatrixFilter within Solr. To test the functionality etc., I first used the built-in field analysis view.The filter was configured to be used only at query time analysis with ""_"" as spacer character and a min. and max. shingle size of 2. The generation of the shingles for query strings with this filter seems to work at this view, but by turn on the highlighting of indexed terms that will match the query terms, the exception was thrown. Also, each time I tried to query the index the exception was immediately thrown.

Stacktrace:
{code}
java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
	at java.util.ArrayList.RangeCheck(Unknown Source)
	at java.util.ArrayList.get(Unknown Source)
	at org.apache.lucene.analysis.shingle.ShingleMatrixFilter$Matrix$1.hasNext(ShingleMatrixFilter.java:729)
	at org.apache.lucene.analysis.shingle.ShingleMatrixFilter.next(ShingleMatrixFilter.java:380)
	at org.apache.lucene.analysis.StopFilter.next(StopFilter.java:120)
	at org.apache.lucene.analysis.TokenStream.next(TokenStream.java:47)
	...
{code}

Within the hasNext method, there is the {{s-1}}-th Column from the ArrayList {{columns}} requested, but there isn't this entry within columns.

I created a patch that checks, if {{columns}} contains enough entries."
"LUCENE-829","BUG","BUG","StandardBenchmarker#makeDocument does not explicitly close opened files","StandardBenchmarker#makeDocument(File in, String[] tags, boolean stored, boolean tokenized, boolean tfv)

        BufferedReader reader = new BufferedReader(new FileReader(in));

Above reader is not closed until GC hits it. Can cause problems in cases where ulimit is set too low.

I did this:

        while ((line = reader.readLine()) != null)
        {
            body.append(line).append(' ');
        }
+        reader.close();"
"LUCENE-1175","BUG","BUG","occasional MergeException while indexing","TestStressIndexing2.testMultiConfig occasionally hits merge exceptions"
"LUCENE-1411","RFE","IMPROVEMENT","Enable IndexWriter to open an arbitrary commit point","With a 2-phase commit involving multiple resources, each resource
first does its prepareCommit and then if all are successful they each
commit.  If an exception or timeout/power loss is hit in any of the
resources during prepareCommit or commit, all of the resources must
then rollback.

But, because IndexWriter always opens the most recent commit, getting
Lucene to rollback after commit() has been called is not easy, unless
you make Lucene the last resource to commit.  A simple workaround is
to simply remove the segments_N files of the newer commits but that's
sort of a hassle.

To fix this, we just need to add a ctor to IndexWriter that takes an
IndexCommit.  We recently added this for IndexReader (LUCENE-1311) as
well.  This ctor is definitely an ""expert"" method, and only makes
sense if you have a custom DeletionPolicy that preserves more than
just the most recent commit.
"
"LUCENE-3292","IMPROVEMENT","TASK","IOContext should be part of the SegmentReader cache key ","Once IOContext (LUCENE-2793) is landed the IOContext should be part of the key used to cache that reader in the pool"
"LUCENE-2360","IMPROVEMENT","IMPROVEMENT","speedup recycling of per-doc RAM","Robert found one source of slowness when indexing tiny docs, where we use List.toArray to recycle the byte[] buffers used by per-doc doc store state (stored field, term vectors).  This was added in LUCENE-2283, so not yet released."
"LUCENE-3526","BUG","BUG","preflex codec returns wrong terms if you use an empty field name","spinoff from LUCENE-3473.

I have a standalone test for this... the termsenum is returning a bogus extra empty-term (I assume it has no postings, i didnt try).

This causes the checkindex test in LUCENE-3473 to fail, because there are 4 terms instead of 3. 

"
"LUCENE-3857","TEST","TASK","exceptions from other threads in beforeclass/etc do not fail the test","Lots of tests create indexes in beforeClass methods, but if an exception is thrown from another thread
it won't fail the test... e.g. this test passes:
{code}
public class TestExc extends LuceneTestCase {
  @BeforeClass
  public static void beforeClass() {
    new Thread() {
      public void run() {
        throw new RuntimeException(""boo!"");
      }  
    }.start();
  }
  
  public void test() { }
}
{code}

this is because the uncaught exception handler is in setup/teardown"
"LUCENE-3540","CLEANUP","BUG","In 3.x branch (starting with 3.4) the IndexFormatTooOldException was backported, but the error message was not modified for 3.x","In 3.x branch (starting with 3.4) the IndexFormatTooOldException was backported, but the error message was not modified for 3.x:

bq. This version of Lucene only supports indexes created with release 3.0 and later.

In 3.x it must be:

bq. This version of Lucene only supports indexes created with release 1.9 and later.

Indexes before 1.9 will throw this exception on reading SegmentInfos (LUCENE-3255)."
"LUCENE-2386","IMPROVEMENT","BUG","IndexWriter commits unnecessarily on fresh Directory","I've noticed IndexWriter's ctor commits a first commit (empty one) if a fresh Directory is passed, w/ OpenMode.CREATE or CREATE_OR_APPEND. This seems unnecessarily, and kind of brings back an autoCommit mode, in a strange way ... why do we need that commit? Do we really expect people to open an IndexReader on an empty Directory which they just passed to an IW w/ create=true? If they want, they can simply call commit() right away on the IW they created.

I ran into this when writing a test which committed N times, then compared the number of commits (via IndexReader.listCommits) and was surprised to see N+1 commits.

Tried to change doCommit to false in IW ctor, but it got IndexFileDeleter jumping on me .. so the change might not be that simple. But I think it's manageable, so I'll try to attack it (and IFD specifically !) back :)."
"LUCENE-2333","BUILD_SYSTEM","BUG","Failures during contrib builds, when classes in core were changed without ant clean","From java-dev by Shai Erera:

{quote}
I've noticed that sometimes, after I run test-core and test-contrib, and then change core code, test-contrib fail on NoSuchMethodError and stuff like that. I've noticed that core.jar exists under build, and I assumed it's used by test-contrib, and probably is not recreated after core code has changed.

I verified it when looking in contrib-build.xml, which defines a property lucene.jar.present which is set to true if the jar is ... well, present. Which I believe is the reason for these failures. I've been thinking how to resolve that, and I can think of two ways:

(1) have test-core always delete that file, but that has two issues:
(1.1) It's redundant if the code hasn't changed.
(1.2) It forces you to either jar-core or test-core before you test-contrib, if you want to make sure you run w/ the latest jar.

or

(2) have test-contrib always call jar-core, which will first delete the file and then re-create it by compiling first. Compiling should not do anything if the code hasn't changed. So the only waste would be to create the .jar, but I think that's quite fast?

Does anyone, with more Ant skills than me, know of a better way to detect from test-contrib that core code has changed and only then rebuild the jar?
{quote}"
"LUCENE-583","BUG","BUG","ISOLatin1AccentFilter discards position increments of filtered terms","Not sure if this is a bug, but looks like one to me..."
"LUCENE-1706","RFE","RFE","Site search powered by Lucene/Solr","For a number of years now, the Lucene community has been criticized for not eating our own ""dog food"" when it comes to search. My company has built and hosts a site search (http://www.lucidimagination.com/search) that is powered by Apache Solr and Lucene and we'd like to donate it's use to the Lucene community. Additionally, it allows one to search all of the Lucene content from a single place, including web, wiki, JIRA and mail archives. See also http://www.lucidimagination.com/search/document/bf22a570bf9385c7/search_on_lucene_apache_org

You can see it live on Mahout, Tika and Solr

Lucid has a fault tolerant setup with replication and fail over as well as monitoring services in place. We are committed to maintaining and expanding the search capabilities on the site.

The following patch adds a skin to the Forrest site that enables the Lucene site to search Lucene only content using Lucene/Solr. When a search is submitted, it automatically selects the Lucene facet such that only Lucene content is searched. From there, users can then narrow/broaden their search criteria.


I plan on committing in a 3 or 4 days."
"LUCENE-1318","BUG","BUG","InstantiatedIndexReader.norms called from MultiReader bug","Small bug in InstantiatedIndexReader.norms(String field, byte[] bytes, int offset) where the offset is not applied properly in the System.arraycopy"
"LUCENE-3407","BUG","BUG","wrong stats/scoring from MemoryCodec","I hit some random failures in the flexscoring branch: wierd because its not a random test.

I noticed the test always failed with memorycodec, and wrote a specific test for it.

I haven't traced thru it yet, but I think its likely the issue that memorycodec is somehow returning wrong stats here?"
"LUCENE-2479","RFE","IMPROVEMENT","need the ability to also sort SpellCheck results by freq, instead of just by Edit Distance+freq","This issue was first noticed and reported in this Solr thread; http://lucene.472066.n3.nabble.com/spellcheck-issues-td489776.html#a489788

Basically, there are situations where it would be useful to sort by freq first, instead of the current ""sort by edit distance, and then subsort by freq if edit distance is equal""

The author of the thread suggested ""What I think would work even better than allowing a custom compareTo function would be to incorporate the frequency directly into the distance function.  This would allow for greater control over the trade-off between frequency and edit distance""

However, custom compareTo functions are not always be possible (ie if a certain version of Lucene must be used, because it was release with Solr) and incorporating freq directly into the distance function may be overkill (ie depending on the implementation)

it is suggested that we have a simple modification of the existing compareTo function in Lucene to allow users to specify if they want the existing sort method or if they want to sort by freq.

"
"LUCENE-1891","IMPROVEMENT","IMPROVEMENT","Spatial uses java util logging that causes needless minor work (multiple string concat, a method call) due to not checking log level","Not sure there should be logging here - just used in two spots and looks more for debug - but if its going to be there, should check for isFineEnabled."
"LUCENE-1184","RFE","IMPROVEMENT","Allow SnapshotDeletionPolicy to be reused across writer close/open","If you re-use the same instance of SnapshotDeletionPolicy across a
close/open of your writer, and you had a snapshot open, it can still
be removed when the 2nd writer is opened.  This is because SDP is
comparing IndexCommitPoint instances.

The fix is to instead compare segments file names.

I've also changed the inner class IndexFileDeleter.CommitPoint to be
static so an instance of SnapshotDeletionPolicy does not hold
references to IndexFileDeleter, DocumentsWriter, etc.

Spinoff from here:

  http://markmail.org/message/bojgqfgyxkkv4fyb
"
"LUCENE-3391","REFACTORING","","Make EasySimilarityProvider a full-fledged class ","The {{EasySimilarityProvider}} in {{TestEasySimilarity}} would be a good candidate for a full-fledged class. Both {{DefaultSimilarity}} and {{BM25Similarity}} have their own providers, which are effectively the same,so I don't see why we couldn't add one generic provider for convenience."
"LUCENE-3705","BUG","BUG","deadlock in TestIndexWriterExceptions","    [junit] 2012-01-18 18:18:16
    [junit] Full thread dump Java HotSpot(TM) 64-Bit Server VM (19.1-b02 mixed mode):
    [junit] 
    [junit] ""Indexer 3"" prio=10 tid=0x0000000041b9b800 nid=0x6291 waiting for monitor entry [0x00007f7e8868f000]
    [junit]    java.lang.Thread.State: BLOCKED (on object monitor)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.innerPurge(DocumentsWriterFlushQueue.java:118)
    [junit] 	- waiting to lock <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.tryPurge(DocumentsWriterFlushQueue.java:141)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:439)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] 
    [junit] ""Indexer 2"" prio=10 tid=0x0000000041b9b000 nid=0x6290 waiting on condition [0x00007f7e8838c000]
    [junit]    java.lang.Thread.State: WAITING (parking)
    [junit] 	at sun.misc.Unsafe.park(Native Method)
    [junit] 	- parking to wait for  <0x00000000e4103100> (a org.apache.lucene.index.DocumentsWriterStallControl$Sync)
    [junit] 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(AbstractQueuedSynchronizer.java:941)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(AbstractQueuedSynchronizer.java:1261)
    [junit] 	at org.apache.lucene.index.DocumentsWriterStallControl.waitIfStalled(DocumentsWriterStallControl.java:115)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushControl.waitIfStalled(DocumentsWriterFlushControl.java:591)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.preUpdate(DocumentsWriter.java:302)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:362)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] 
    [junit] ""Indexer 1"" prio=10 tid=0x0000000042500000 nid=0x628f waiting for monitor entry [0x00007f7e8858e000]
    [junit]    java.lang.Thread.State: BLOCKED (on object monitor)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.addSegment(DocumentsWriterFlushQueue.java:84)
    [junit] 	- waiting to lock <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] 
    [junit] ""Indexer 0"" prio=10 tid=0x0000000041508000 nid=0x628d waiting on condition [0x00007f7e8848d000]
    [junit]    java.lang.Thread.State: WAITING (parking)
    [junit] 	at sun.misc.Unsafe.park(Native Method)
    [junit] 	- parking to wait for  <0x00000000e414b408> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    [junit] 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178)
    [junit] 	at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
    [junit] 	at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.forcePurge(DocumentsWriterFlushQueue.java:130)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.addDeletesAndPurge(DocumentsWriterFlushQueue.java:50)
    [junit] 	- locked <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.applyAllDeletes(DocumentsWriter.java:179)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:460)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] 
    [junit] ""Low Memory Detector"" daemon prio=10 tid=0x00007f7e84025800 nid=0x6003 runnable [0x0000000000000000]
    [junit]    java.lang.Thread.State: RUNNABLE
    [junit] 
    [junit] ""CompilerThread1"" daemon prio=10 tid=0x00007f7e84022800 nid=0x6002 waiting on condition [0x0000000000000000]
    [junit]    java.lang.Thread.State: RUNNABLE
    [junit] 
    [junit] ""CompilerThread0"" daemon prio=10 tid=0x00007f7e8401f800 nid=0x6001 waiting on condition [0x0000000000000000]
    [junit]    java.lang.Thread.State: RUNNABLE
    [junit] 
    [junit] ""Signal Dispatcher"" daemon prio=10 tid=0x00007f7e8401d800 nid=0x6000 waiting on condition [0x0000000000000000]
    [junit]    java.lang.Thread.State: RUNNABLE
    [junit] 
    [junit] ""Finalizer"" daemon prio=10 tid=0x00007f7e84001000 nid=0x5ffa in Object.wait() [0x00007f7e8961b000]
    [junit]    java.lang.Thread.State: WAITING (on object monitor)
    [junit] 	at java.lang.Object.wait(Native Method)
    [junit] 	- waiting on <0x00000000e2ece380> (a java.lang.ref.ReferenceQueue$Lock)
    [junit] 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)
    [junit] 	- locked <0x00000000e2ece380> (a java.lang.ref.ReferenceQueue$Lock)
    [junit] 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:134)
    [junit] 	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)
    [junit] 
    [junit] ""Reference Handler"" daemon prio=10 tid=0x000000004101e000 nid=0x5ff9 in Object.wait() [0x00007f7e8971c000]
    [junit]    java.lang.Thread.State: WAITING (on object monitor)
    [junit] 	at java.lang.Object.wait(Native Method)
    [junit] 	- waiting on <0x00000000e2ece318> (a java.lang.ref.Reference$Lock)
    [junit] 	at java.lang.Object.wait(Object.java:485)
    [junit] 	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
    [junit] 	- locked <0x00000000e2ece318> (a java.lang.ref.Reference$Lock)
    [junit] 
    [junit] ""main"" prio=10 tid=0x0000000040fb2000 nid=0x5fe2 in Object.wait() [0x00007f7e8ecc1000]
    [junit]    java.lang.Thread.State: WAITING (on object monitor)
    [junit] 	at java.lang.Object.wait(Native Method)
    [junit] 	- waiting on <0x00000000e4105080> (a org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread)
    [junit] 	at java.lang.Thread.join(Thread.java:1186)
    [junit] 	- locked <0x00000000e4105080> (a org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread)
    [junit] 	at java.lang.Thread.join(Thread.java:1239)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:286)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit] 	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:528)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit] 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit] 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
    [junit] 
    [junit] ""VM Thread"" prio=10 tid=0x0000000041017800 nid=0x5fef runnable 
    [junit] 
    [junit] ""GC task thread#0 (ParallelGC)"" prio=10 tid=0x0000000040fc5000 nid=0x5fe3 runnable 
    [junit] 
    [junit] ""GC task thread#1 (ParallelGC)"" prio=10 tid=0x0000000040fc7000 nid=0x5fe4 runnable 
    [junit] 
    [junit] ""GC task thread#2 (ParallelGC)"" prio=10 tid=0x0000000040fc9000 nid=0x5fe5 runnable 
    [junit] 
    [junit] ""GC task thread#3 (ParallelGC)"" prio=10 tid=0x0000000040fca800 nid=0x5fe6 runnable 
    [junit] 
    [junit] ""GC task thread#4 (ParallelGC)"" prio=10 tid=0x0000000040fcc800 nid=0x5fe7 runnable 
    [junit] 
    [junit] ""GC task thread#5 (ParallelGC)"" prio=10 tid=0x0000000040fce800 nid=0x5fe8 runnable 
    [junit] 
    [junit] ""GC task thread#6 (ParallelGC)"" prio=10 tid=0x0000000040fd0000 nid=0x5fe9 runnable 
    [junit] 
    [junit] ""GC task thread#7 (ParallelGC)"" prio=10 tid=0x0000000040fd2000 nid=0x5fea runnable 
    [junit] 
    [junit] ""VM Periodic Task Thread"" prio=10 tid=0x00007f7e84030000 nid=0x6004 waiting on condition 
    [junit] 
    [junit] JNI global references: 1578
    [junit] 
    [junit] 
    [junit] Found one Java-level deadlock:
    [junit] =============================
    [junit] ""Indexer 3"":
    [junit]   waiting to lock monitor 0x0000000041477498 (object 0x00000000e40ff2a8, a org.apache.lucene.index.DocumentsWriterFlushQueue),
    [junit]   which is held by ""Indexer 0""
    [junit] ""Indexer 0"":
    [junit]   waiting for ownable synchronizer 0x00000000e414b408, (a java.util.concurrent.locks.ReentrantLock$NonfairSync),
    [junit]   which is held by ""Indexer 3""
    [junit] 
    [junit] Java stack information for the threads listed above:
    [junit] ===================================================
    [junit] ""Indexer 3"":
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.innerPurge(DocumentsWriterFlushQueue.java:118)
    [junit] 	- waiting to lock <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.tryPurge(DocumentsWriterFlushQueue.java:141)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:439)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] ""Indexer 0"":
    [junit] 	at sun.misc.Unsafe.park(Native Method)
    [junit] 	- parking to wait for  <0x00000000e414b408> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    [junit] 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178)
    [junit] 	at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
    [junit] 	at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.forcePurge(DocumentsWriterFlushQueue.java:130)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.addDeletesAndPurge(DocumentsWriterFlushQueue.java:50)
    [junit] 	- locked <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.applyAllDeletes(DocumentsWriter.java:179)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:460)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] 
    [junit] Found 1 deadlock.
    [junit] 
    [junit] Heap
    [junit]  PSYoungGen      total 67136K, used 4647K [0x00000000f5560000, 0x00000000fbc60000, 0x0000000100000000)
    [junit]   eden space 65792K, 5% used [0x00000000f5560000,0x00000000f58a5e10,0x00000000f95a0000)
    [junit]   from space 1344K, 96% used [0x00000000f9740000,0x00000000f98840a0,0x00000000f9890000)
    [junit]   to   space 19840K, 0% used [0x00000000fa900000,0x00000000fa900000,0x00000000fbc60000)
    [junit]  PSOldGen        total 171392K, used 66868K [0x00000000e0000000, 0x00000000ea760000, 0x00000000f5560000)
    [junit]   object space 171392K, 39% used [0x00000000e0000000,0x00000000e414d080,0x00000000ea760000)
    [junit]  PSPermGen       total 21248K, used 14733K [0x00000000dae00000, 0x00000000dc2c0000, 0x00000000e0000000)
    [junit]   object space 21248K, 69% used [0x00000000dae00000,0x00000000dbc635a8,0x00000000dc2c0000)
    [junit] 
"
"LUCENE-3693","TEST","TEST","Add a testing implementation for DocumentsWriterPerThreadPool","currently we only have one impl for DocumentsWriterPerThreadPool. We should add some more to make sure the interface is sufficient and to beef up tests. For testing I'm working on a randomized impl. selecting and locking states randomly."
"LUCENE-1847","DESIGN_DEFECT","BUG","PhraseQuery/TermQuery/SpanQuery use IndexReader specific stats in their explains","PhraseQuery uses IndexReader in explainfor top level stats - as mentioned by Mike McCandless in LUCENE-1837.
TermQuery uses IndexReader in explain for top level stats

Always been a bug with MultiSearcher, but per segment search makes it worse.

"
"LUCENE-2831","RFE","IMPROVEMENT","Revise Weight#scorer & Filter#getDocIdSet API to pass Readers context","Spinoff from LUCENE-2694 - instead of passing a reader into Weight#scorer(IR, boolean, boolean) we should / could revise the API and pass in a struct that has parent reader, sub reader, ord of that sub. The ord mapping plus the context with its parent would make several issues way easier. See LUCENE-2694, LUCENE-2348 and LUCENE-2829 to name some.

"
"LUCENE-1892","OTHER","BUG","Demo HTMLParser compares StringBuffer to an empty String with .equals",""
"LUCENE-436","BUG","IMPROVEMENT","[PATCH] TermInfosReader, SegmentTermEnum Out Of Memory Exception","We've been experiencing terrible memory problems on our production search server, running lucene (1.4.3).

Our live app regularly opens new indexes and, in doing so, releases old IndexReaders for garbage collection.

But...there appears to be a memory leak in org.apache.lucene.index.TermInfosReader.java.
Under certain conditions (possibly related to JVM version, although I've personally observed it under both linux JVM 1.4.2_06, and 1.5.0_03, and SUNOS JVM 1.4.1) the ThreadLocal member variable, ""enumerators"" doesn't get garbage-collected when the TermInfosReader object is gc-ed.

Looking at the code in TermInfosReader.java, there's no reason why it _shouldn't_ be gc-ed, so I can only presume (and I've seen this suggested elsewhere) that there could be a bug in the garbage collector of some JVMs.

I've seen this problem briefly discussed; in particular at the following URL:
  http://java2.5341.com/msg/85821.html
The patch that Doug recommended, which is included in lucene-1.4.3 doesn't work in our particular circumstances. Doug's patch only clears the ThreadLocal variable for the thread running the finalizer (my knowledge of java breaks down here - I'm not sure which thread actually runs the finalizer). In our situation, the TermInfosReader is (potentially) used by more than one thread, meaning that Doug's patch _doesn't_ allow the affected JVMs to correctly collect garbage.

So...I've devised a simple patch which, from my observations on linux JVMs 1.4.2_06, and 1.5.0_03, fixes this problem.

Kieran
PS Thanks to daniel naber for pointing me to jira/lucene

@@ -19,6 +19,7 @@
 import java.io.IOException;

 import org.apache.lucene.store.Directory;
+import java.util.Hashtable;

 /** This stores a monotonically increasing set of <Term, TermInfo> pairs in a
  * Directory.  Pairs are accessed either by Term or by ordinal position the
@@ -29,7 +30,7 @@
   private String segment;
   private FieldInfos fieldInfos;

-  private ThreadLocal enumerators = new ThreadLocal();
+  private final Hashtable enumeratorsByThread = new Hashtable();
   private SegmentTermEnum origEnum;
   private long size;

@@ -60,10 +61,10 @@
   }

   private SegmentTermEnum getEnum() {
-    SegmentTermEnum termEnum = (SegmentTermEnum)enumerators.get();
+    SegmentTermEnum termEnum = (SegmentTermEnum)enumeratorsByThread.get(Thread.currentThread());
     if (termEnum == null) {
       termEnum = terms();
-      enumerators.set(termEnum);
+      enumeratorsByThread.put(Thread.currentThread(), termEnum);
     }
     return termEnum;
   }
@@ -195,5 +196,15 @@
   public SegmentTermEnum terms(Term term) throws IOException {
     get(term);
     return (SegmentTermEnum)getEnum().clone();
+  }
+
+  /* some jvms might have trouble gc-ing enumeratorsByThread */
+  protected void finalize() throws Throwable {
+    try {
+        // make sure gc can clear up.
+        enumeratorsByThread.clear();
+    } finally {
+        super.finalize();
+    }
   }
 }



TermInfosReader.java, full source:
======================================
package org.apache.lucene.index;

/**
 * Copyright 2004 The Apache Software Foundation
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;

import org.apache.lucene.store.Directory;
import java.util.Hashtable;

/** This stores a monotonically increasing set of <Term, TermInfo> pairs in a
 * Directory.  Pairs are accessed either by Term or by ordinal position the
 * set.  */

final class TermInfosReader {
  private Directory directory;
  private String segment;
  private FieldInfos fieldInfos;

  private final Hashtable enumeratorsByThread = new Hashtable();
  private SegmentTermEnum origEnum;
  private long size;

  TermInfosReader(Directory dir, String seg, FieldInfos fis)
       throws IOException {
    directory = dir;
    segment = seg;
    fieldInfos = fis;

    origEnum = new SegmentTermEnum(directory.openFile(segment + "".tis""),
                                   fieldInfos, false);
    size = origEnum.size;
    readIndex();
  }

  public int getSkipInterval() {
    return origEnum.skipInterval;
  }

  final void close() throws IOException {
    if (origEnum != null)
      origEnum.close();
  }

  /** Returns the number of term/value pairs in the set. */
  final long size() {
    return size;
  }

  private SegmentTermEnum getEnum() {
    SegmentTermEnum termEnum = (SegmentTermEnum)enumeratorsByThread.get(Thread.currentThread());
    if (termEnum == null) {
      termEnum = terms();
      enumeratorsByThread.put(Thread.currentThread(), termEnum);
    }
    return termEnum;
  }

  Term[] indexTerms = null;
  TermInfo[] indexInfos;
  long[] indexPointers;

  private final void readIndex() throws IOException {
    SegmentTermEnum indexEnum =
      new SegmentTermEnum(directory.openFile(segment + "".tii""),
			  fieldInfos, true);
    try {
      int indexSize = (int)indexEnum.size;

      indexTerms = new Term[indexSize];
      indexInfos = new TermInfo[indexSize];
      indexPointers = new long[indexSize];

      for (int i = 0; indexEnum.next(); i++) {
	indexTerms[i] = indexEnum.term();
	indexInfos[i] = indexEnum.termInfo();
	indexPointers[i] = indexEnum.indexPointer;
      }
    } finally {
      indexEnum.close();
    }
  }

  /** Returns the offset of the greatest index entry which is less than or equal to term.*/
  private final int getIndexOffset(Term term) throws IOException {
    int lo = 0;					  // binary search indexTerms[]
    int hi = indexTerms.length - 1;

    while (hi >= lo) {
      int mid = (lo + hi) >> 1;
      int delta = term.compareTo(indexTerms[mid]);
      if (delta < 0)
	hi = mid - 1;
      else if (delta > 0)
	lo = mid + 1;
      else
	return mid;
    }
    return hi;
  }

  private final void seekEnum(int indexOffset) throws IOException {
    getEnum().seek(indexPointers[indexOffset],
	      (indexOffset * getEnum().indexInterval) - 1,
	      indexTerms[indexOffset], indexInfos[indexOffset]);
  }

  /** Returns the TermInfo for a Term in the set, or null. */
  TermInfo get(Term term) throws IOException {
    if (size == 0) return null;

    // optimize sequential access: first try scanning cached enum w/o seeking
    SegmentTermEnum enumerator = getEnum();
    if (enumerator.term() != null                 // term is at or past current
	&& ((enumerator.prev != null && term.compareTo(enumerator.prev) > 0)
	    || term.compareTo(enumerator.term()) >= 0)) {
      int enumOffset = (int)(enumerator.position/enumerator.indexInterval)+1;
      if (indexTerms.length == enumOffset	  // but before end of block
	  || term.compareTo(indexTerms[enumOffset]) < 0)
	return scanEnum(term);			  // no need to seek
    }

    // random-access: must seek
    seekEnum(getIndexOffset(term));
    return scanEnum(term);
  }

  /** Scans within block for matching term. */
  private final TermInfo scanEnum(Term term) throws IOException {
    SegmentTermEnum enumerator = getEnum();
    while (term.compareTo(enumerator.term()) > 0 && enumerator.next()) {}
    if (enumerator.term() != null && term.compareTo(enumerator.term()) == 0)
      return enumerator.termInfo();
    else
      return null;
  }

  /** Returns the nth term in the set. */
  final Term get(int position) throws IOException {
    if (size == 0) return null;

    SegmentTermEnum enumerator = getEnum();
    if (enumerator != null && enumerator.term() != null &&
        position >= enumerator.position &&
	position < (enumerator.position + enumerator.indexInterval))
      return scanEnum(position);		  // can avoid seek

    seekEnum(position / enumerator.indexInterval); // must seek
    return scanEnum(position);
  }

  private final Term scanEnum(int position) throws IOException {
    SegmentTermEnum enumerator = getEnum();
    while(enumerator.position < position)
      if (!enumerator.next())
	return null;

    return enumerator.term();
  }

  /** Returns the position of a Term in the set or -1. */
  final long getPosition(Term term) throws IOException {
    if (size == 0) return -1;

    int indexOffset = getIndexOffset(term);
    seekEnum(indexOffset);

    SegmentTermEnum enumerator = getEnum();
    while(term.compareTo(enumerator.term()) > 0 && enumerator.next()) {}

    if (term.compareTo(enumerator.term()) == 0)
      return enumerator.position;
    else
      return -1;
  }

  /** Returns an enumeration of all the Terms and TermInfos in the set. */
  public SegmentTermEnum terms() {
    return (SegmentTermEnum)origEnum.clone();
  }

  /** Returns an enumeration of terms starting at or after the named term. */
  public SegmentTermEnum terms(Term term) throws IOException {
    get(term);
    return (SegmentTermEnum)getEnum().clone();
  }

  /* some jvms might have trouble gc-ing enumeratorsByThread */ 
  protected void finalize() throws Throwable {
    try {
        // make sure gc can clear up.
        enumeratorsByThread.clear();
    } finally {
        super.finalize();
    }
  }
}
"
"LUCENE-2215","RFE","RFE","paging collector","http://issues.apache.org/jira/browse/LUCENE-2127?focusedCommentId=12796898&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12796898

Somebody assign this to Aaron McCurry and we'll see if we can get enough votes on this issue to convince him to upload his patch.  :)"
"LUCENE-209","BUILD_SYSTEM","BUG","Lucene requires ant 1.6?","The latest version in CVS as of April 3rd only builds with ant 1.6.   If this is intentional, BUILD.txt should 
be updated.

Here's the error I get with ant 1.5:

BUILD FAILED
file:/Users/skybrian/remote-cvs/jakarta-lucene/build.xml:11: Unexpected element ""tstamp"""
"LUCENE-2220","BUG","BUG","Stackoverflow when calling deprecated CharArraySet.copy()","Calling CharArraySet#copy(set) without the version argument (deprecated) with an instance of CharArraySet results in a stack overflow as this method checks if the given set is a CharArraySet and then calls itself again. This was accidentially introduced due to an overloaded alternative method during LUCENE-2169 which was not used in the final patch."
"LUCENE-1865","DOCUMENTATION","TASK","Add a ton of missing license headers throughout test/demo/contrib",""
"LUCENE-1018","BUG","BUG","intermittant exceptions in TestConcurrentMergeScheduler","
The TestConcurrentMergeScheduler throws intermittant exceptions that
do not result in a test failure.

The exception happens in the ""testNoWaitClose()"" test, which repeated
tests closing an IndexWriter with ""false"", meaning abort any
still-running merges.  When a merge is aborted it can hit various
exceptions because the files it is reading and/or writing have been
deleted, so we ignore these exceptions.

The bug was just that we were failing to properly check whether the
running merge was actually aborted because of a scoping issue of the
""merge"" variable in ConcurrentMergeScheduler.  So the exceptions are
actually ""harmless"".  Thanks to Ning for spotting it!

"
"LUCENE-2672","IMPROVEMENT","IMPROVEMENT","speed up automaton seeking in nextString","While testing, i found there are some queries (e.g. wildcard ?????????) that do quite a lot of backtracking.

nextString doesn't handle this particularly well, when it walks the DFA, if it hits a dead-end and needs to backtrack, it increments the bytes, and starts over completely.

alternatively it could save the path information in an int[], and backtrack() could return a position to restart from, instead of just a boolean.
"
"LUCENE-2116","RFE","RFE","Add link to irc channel #lucene on the website","We should add a link to #lucene IRC channel on chat.freenode.org. "
"LUCENE-659","BUG","BUG","[PATCH] PerFieldAnalyzerWrapper fails to implement getPositionIncrementGap()","The attached patch causes PerFieldAnalyzerWrapper to delegate calls to getPositionIncrementGap() to the analyzer that is appropriate for the field in question.  The current behavior without this patch is to always use the default value from Analyzer, which is a bug because PerFieldAnalyzerWrapper should behave just as if it was the analyzer for the selected field.
"
"LUCENE-630","DOCUMENTATION","BUG","results.jsp in luceneweb.war uses unknown parse-Method","results.jsp in luceneweb.war demo throws JasperException:

org.apache.jasper.JasperException: Unable to compile class for JSP

An error occurred at line: 60 in the jsp file: /results.jsp
Generated servlet error:
The method parse(String) in the type QueryParser is not applicable for the arguments (String, String, Analyzer)

I think, the code in line 81 of results.jsp should maybe look like the following ?

QueryParser qp = new QueryParser(""contents"", analyzer);
query = qp.parse(queryString);"
"LUCENE-1456","BUG","BUG","FieldInfo omitTerms bug","Around line 95 you have:

    if (this.omitTf != omitTf) {
      this.omitTf = true;                // if one require omitTf at least once, it remains off for life
    }

Both references of the omitTf booleans in the if statement refer to the same field. I am guessing its meant to be other.omitTf like the norms code above it."
"LUCENE-2550","TEST","BUG","3.x backwards tests are using Version.LUCENE_CURRENT: aren't testing backwards!","The 3.x backwards tests are mostly all using Version.LUCENE_CURRENT, therefore they don't always test the behavior as they should.

I added TEST_VERSION_CURRENT = 3.0 to the backwards/LuceneTestCase, and I think we should fix all backwards tests to use TEST_VERSION_CURRENT instead."
"LUCENE-3191","RFE","IMPROVEMENT","Add TopDocs.merge to merge multiple TopDocs","It's not easy today to merge TopDocs, eg produced by multiple shards,
supporting arbitrary Sort.
"
"LUCENE-1493","RFE","IMPROVEMENT","Enable setting hits queue size in Search*Task in contrib/benchmark","In testing for LUCENE-1483, I'd like to try different collector queue
sizes during benchmarking.  But currently contrib/benchmark uses
deprecated Hits with hardwired ""top 100"" queue size.  I'll switch it to
the TopDocs APIs."
"LUCENE-460","IMPROVEMENT","IMPROVEMENT","hashCode improvements","It would be nice for all Query classes to implement hashCode and equals to enable them to be used as keys when caching.
"
"LUCENE-1182","DESIGN_DEFECT","BUG","SimilarityDelegator is missing a delegating scorePayload() method","The handy SimilarityDelegator method is missing a scoreDelegator() delegating method.
The fix is trivial, add the code below at the end of the class:

  public float scorePayload(String fieldName, byte [] payload, int offset, int length)
  {
      return delegee.scorePayload(fieldName, payload, offset, length);
  }
"
"LUCENE-1080","OTHER","IMPROVEMENT","Make Token.DEFAULT_TYPE public","Make Token.DEFAULT_TYPE public so that TokenFilters using the reusable Token model have a way of setting the type back to the default.

No patch necessary.  I will commit soon."
"LUCENE-926","DOCUMENTATION","","Document Package level javadocs need improving","The document package package level javadocs could use some improving, such as:
1. Info on what a Document is, as well as Field and Fieldable
2. Examples of FieldSelectors and how to implement
3. Samples of using DateTools and NumberTools"
"LUCENE-578","RFE","RFE","Summer of Code GDATA Server  --Project structure and simple version to start with--","This is the initial issue for the GDATA - Server project (Google Summer of Code). 
The purpose of the issue is to create the project structure in the svn repository to kick off the project. The source e.g. the project will be located at URL: http://svn.apache.org/repos/asf/lucene/java/trunk/contrib
The attachment includes the diff text file and the jar files included in the lib directory as a tar.gz file.
To get some information about the project see http://wiki.apache.org/general/SimonWillnauer/SummerOfCode2006"
"LUCENE-1157","DOCUMENTATION","IMPROVEMENT","Formatable changes log  (CHANGES.txt is easy to edit but not so friendly to read by Lucene users)","Background in http://www.nabble.com/formatable-changes-log-tt15078749.html"
"LUCENE-242","IMPROVEMENT","BUG","Lucene Search has poor cpu utilization on a 4-CPU machine","I noticed that the class org.apache.lucene.index.FieldInfos uses private class
members Vector byNumber and Hashtable byName, both of which are synchronized
objects, thus resulting in unessesary locks. 

By changing the Vector byNumber to ArrayList byNumber, and Hashtable byName to
HashMap byName, both are not synchronized objects, I was able to get 110%
improvement in performance (number of searches per second).


Here is a sample of blocked thread
""Thread-32"" daemon prio=1 tid=0x082334c0 nid=0xa66 waiting for monitor entry
[4f385000..4f38687c]
        at java.util.Vector.elementAt(Vector.java:430)
        - waiting to lock <0x452b93a8> (a java.util.Vector)
        at org.apache.lucene.index.FieldInfos.fieldInfo(FieldInfos.java:155)
        at org.apache.lucene.index.FieldInfos.fieldName(FieldInfos.java:151)
        at
org.apache.lucene.index.SegmentTermEnum.readTerm(SegmentTermEnum.java:149)
        at org.apache.lucene.index.SegmentTermEnum.next(SegmentTermEnum.java:115)
        at
org.apache.lucene.index.TermInfosReader.scanEnum(TermInfosReader.java:143)
        at org.apache.lucene.index.TermInfosReader.get(TermInfosReader.java:137)
        at org.apache.lucene.index.SegmentTermDocs.seek(SegmentTermDocs.java:51)
        at org.apache.lucene.index.IndexReader.termDocs(IndexReader.java:364)
        at org.apache.lucene.search.TermQuery$TermWeight.scorer(TermQuery.java:59)
        at
org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:165)
        at
org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:165)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:154)
        at
gov.gsa.search.SearcherByPageAndSortedField.search(SearcherByPageAndSortedField.java:317)
        at
gov.gsa.search.SearcherByPageAndSortedField.search(SearcherByPageAndSortedField.java:203)
        at
gov.gsa.search.grants.SearchGrants.searchByPageAndSortedField(SearchGrants.java:308)
        at
gov.gsa.search.grants.SearchServlet.searchByIndex(SearchServlet.java:1541)
        at gov.gsa.search.grants.SearchServlet.getResults(SearchServlet.java:1325)
        at gov.gsa.search.grants.SearchServlet.doGet(SearchServlet.java:500)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:740)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:853)
        at
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:247)
        at
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:193)
        at
org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:256)
        at
org.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:643)
        at
org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:480)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:995)
        at
org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)
        at
org.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:643)
        at
org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:480)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:995)
        at
org.apache.catalina.core.StandardContext.invoke(StandardContext.java:2415)
        at
org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:180)
        at
org.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:643)
        at
org.apache.catalina.valves.ErrorDispatcherValve.invoke(ErrorDispatcherValve.java:171)
        at
org.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:641)
        at
org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:172)
        at
org.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:641)
        at
org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:480)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:995)
        at
org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:174)
        at
org.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:643)
        at
org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:480)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:995)
        at org.apache.coyote.tomcat4.CoyoteAdapter.service(CoyoteAdapter.java:223)
        at org.apache.jk.server.JkCoyoteHandler.invoke(JkCoyoteHandler.java:261)
        at org.apache.jk.common.HandlerRequest.invoke(HandlerRequest.java:360)
        at org.apache.jk.common.ChannelSocket.invoke(ChannelSocket.java:604)
        at
org.apache.jk.common.ChannelSocket.processConnection(ChannelSocket.java:562)
        at org.apache.jk.common.SocketConnection.runIt(ChannelSocket.java:679)
        at
org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:619)
        at java.lang.Thread.run(Thread.java:534)"
"LUCENE-3052","BUG","BUG","PerFieldCodecWrapper.loadTermsIndex concurrency problem","Selckin's while(1) testing on RT branch hit another error:
{noformat}
    [junit] Testsuite: org.apache.lucene.TestExternalCodecs
    [junit] Testcase: testPerFieldCodec(org.apache.lucene.TestExternalCodecs):	Caused an ERROR
    [junit] (null)
    [junit] java.lang.NullPointerException
    [junit] 	at org.apache.lucene.index.PerFieldCodecWrapper$FieldsReader.loadTermsIndex(PerFieldCodecWrapper.java:202)
    [junit] 	at org.apache.lucene.index.SegmentReader.loadTermsIndex(SegmentReader.java:1005)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:652)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:609)
    [junit] 	at org.apache.lucene.index.BufferedDeletesStream.applyDeletes(BufferedDeletesStream.java:276)
    [junit] 	at org.apache.lucene.index.IndexWriter.applyAllDeletes(IndexWriter.java:2660)
    [junit] 	at org.apache.lucene.index.IndexWriter.maybeApplyDeletes(IndexWriter.java:2651)
    [junit] 	at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:381)
    [junit] 	at org.apache.lucene.index.IndexReader.open(IndexReader.java:316)
    [junit] 	at org.apache.lucene.TestExternalCodecs.testPerFieldCodec(TestExternalCodecs.java:541)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1246)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1175)
    [junit] 
    [junit] 
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.909 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestExternalCodecs -Dtestmethod=testPerFieldCodec -Dtests.seed=-7296204858082494534:5010909751437000758
    [junit] WARNING: test method: 'testPerFieldCodec' left thread running: merge thread: _i(4.0):Cv130 _m(4.0):Cv30 _n(4.0):cv10 into _o
    [junit] RESOURCE LEAK: test method: 'testPerFieldCodec' left 1 thread(s) running
    [junit] NOTE: test params are: codec=PreFlex, locale=zh_TW, timezone=America/Santo_Domingo
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDemo, TestExternalCodecs]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=2,free=104153512,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.TestExternalCodecs FAILED
    [junit] Exception in thread ""Lucene Merge Thread #5"" org.apache.lucene.util.ThreadInterruptedException: java.lang.InterruptedException: sleep interrupted
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:505)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:472)
    [junit] Caused by: java.lang.InterruptedException: sleep interrupted
    [junit] 	at java.lang.Thread.sleep(Native Method)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:503)
    [junit] 	... 1 more
{noformat}

I suspect this is also a trunk issue, but I can't reproduce it yet.

I think this is happening because the codecs HashMap is changing (via another thread), while .loadTermsIndex is called."
"LUCENE-944","CLEANUP","IMPROVEMENT","Remove deprecated methods in BooleanQuery","Remove deprecated methods setUseScorer14 and getUseScorer14 in BooleanQuery, and adapt javadocs."
"LUCENE-1870","BUILD_SYSTEM","BUG","dists include analyzer contrib in src dist but not binary dist","dists include analyzer contrib in src dist but not binary dist"
"LUCENE-3627","BUG","BUG","CorruptIndexException on indexing after a failure occurs after segments file creation but before any bytes are written","FSDirectory.createOutput(..) uses a RandomAccessFile to do its work.  On my system the default FSDirectory.open(..) creates an NIOFSDirectory.  If createOutput is called on a segments_* file and a crash occurs between RandomAccessFile creation (file system shows a segments_* file exists but has zero bytes) but before any bytes are written to the file, subsequent IndexWriters cannot proceed.  The difficulty is that it does not know how to clear the empty segments_* file.  None of the file deletions will happen on such a segment file because the opening bytes cannot not be read to determine format and version.

An initial proposed patch file is attached below.

"
"LUCENE-1009","IMPROVEMENT","BUG","LogByteSizeMergePolicy over-merges with autoCommit=false and documents with term vectors and/or stored fields","Mark Miller noticed this slowdown (see details in LUCENE-994) in his
app.

This happens because in SegmentInfo.sizeInBytes(), we just run through
all files associated with that segment, summing up their byte sizes.

But in the case of shared doc stores (which happens when
autoCommit=false), this is not quite correct because those files are
shared across multiple segments.

I plan to fix sizeInBytes() to not include the size of the doc stores
when they are shared.
"
"LUCENE-1766","DOCUMENTATION","IMPROVEMENT","Add Thread-Safety note to IndexWriter JavaDoc","IndexWriter Javadocs should contain a note about thread-safety. This is already mentioned on the wiki FAQ page but such an essential information should be part of the module documentation too.
"
"LUCENE-1948","DOCUMENTATION","TASK","Deprecating InstantiatedIndexWriter","http://markmail.org/message/j6ip266fpzuaibf7

I suppose that should have been suggested before 2.9 rather than  
after...

There are at least three reasons to why I want to do this:

The code is based on the behaviour or the Directory IndexWriter as of  
2.3 and I have not been touching it since then. If there will be  
changes in the future one will have to keep IIW in sync, something  
that's easy to forget.
There is no locking which will cause concurrent modification  
exceptions when accessing the index via searcher/reader while  
committing.
It use the old token stream API so it has to be upgraded in case it  
should stay.

The java- and package level docs have since it was committed been  
suggesting that one should consider using II as if it was immutable  
due to the locklessness. My suggestion is that we make it immutable  
for real.

Since II is ment for small corpora there is very little time lost by  
using the constructor that builts the index from an IndexReader. I.e.  
rather than using InstantiatedIndexWriter one would have to use a  
Directory and an IndexWriter and then pass an IndexReader to a new  
InstantiatedIndex.

Any objections?"
"LUCENE-1094","BUG","BUG","Exception in DocumentsWriter.addDocument can corrupt stored fields file (fdt)","DocumentsWriter writes the number of stored fields, up front, into the
fdtLocal buffer.  Then, as each field is processed, it writes each
stored field into this buffer.  When the document is done, in a
finally clause, it flushes the buffer to the real fdt file in the
Directory.

The problem is, if an exception is hit, that number of stored fields
can be too high, which corrupts the fdt file.

The solution is to not write it up front, and instead write only the
number of fields we actually saw."
"LUCENE-2964","RFE","IMPROVEMENT","Allow benchmark tasks from alternative packages","Relax current limitation of all tasks in same package - that of PerfTask.
Add a property ""alt.tasks.packages"" - its value are comma separated full package names.
If the task class is not found in the default package, an attempt is made to load it from the alternate packages specified in that property."
"LUCENE-1301","REFACTORING","IMPROVEMENT","Refactor DocumentsWriter","I've been working on refactoring DocumentsWriter to make it more
modular, so that adding new indexing functionality (like column-stride
stored fields, LUCENE-1231) is just a matter of adding a plugin into
the indexing chain.

This is an initial step towards flexible indexing (but there is still
alot more to do!).

And it's very much still a work in progress -- there are intemittant
thread safety issues, I need to add tests cases and test/iterate on
performance, many ""nocommits"", etc.  This is a snapshot of my current
state...

The approach introduces ""consumers"" (abstract classes defining the
interface) at different levels during indexing.  EG DocConsumer
consumes the whole document.  DocFieldConsumer consumes separate
fields, one at a time.  InvertedDocConsumer consumes tokens produced
by running each field through the analyzer.  TermsHashConsumer writes
its own bytes into in-memory posting lists stored in byte slices,
indexed by term, etc.

DocumentsWriter*.java is then much simpler: it only interacts with a
DocConsumer and has no idea what that consumer is doing.  Under that
DocConsumer there is a whole ""indexing chain"" that does the real work:

  * NormsWriter holds norms in memory and then flushes them to _X.nrm.

  * FreqProxTermsWriter holds postings data in memory and then flushes
    to _X.frq/prx.

  * StoredFieldsWriter flushes immediately to _X.fdx/fdt

  * TermVectorsTermsWriter flushes immediately to _X.tvx/tvf/tvd

DocumentsWriter still manages things like flushing a segment, closing
doc stores, buffering & applying deletes, freeing memory, aborting
when necesary, etc.

In this first step, everything is package-private, and, the indexing
chain is hardwired (instantiated in DocumentsWriter) to the chain
currently matching Lucene trunk.  Over time we can open this up.

There are no changes to the index file format.

For the most part this is just a [large] refactoring, except for these
two small actual changes:

  * Improved concurrency with mixed large/small docs: previously the
    thread state would be tied up when docs finished indexing
    out-of-order.  Now, it's not: instead I use a separate class to
    hold any pending state to flush to the doc stores, and immediately
    free up the thread state to index other docs.

  * Buffered norms in memory now remain sparse, until flushed to the
    _X.nrm file.  Previously we would ""fill holes"" in norms in memory,
    as we go, which could easily use way too much memory.  Really this
    isn't a solution to the problem of sparse norms (LUCENE-830); it
    just delays that issue from causing memory blowup during indexing;
    memory use will still blowup during searching.

I expect performance (indexing throughput) will be worse with this
change.  I'll profile & iterate to minimize this, but I think we can
accept some loss.  I also plan to measure benefit of manually
re-cycling RawPostingList instances from our own pool, vs letting GC
recycle them.

"
"LUCENE-1641","DOCUMENTATION","IMPROVEMENT","Correct spatial and trie documentation links in JavaDocs and site","After updating myself in the site docs, I have some changes to the site and javadocs of Lucene 2.9:
- Add spatial contrib to javadocs
- Add trie package to the contrib/queries package
Both changes prevent these pacakges from a apearing in core's pacakge list on the javadocs/all homepage. I also adjusted the documentation page to reflect the changes.

I will commit the attached patch, if nobody objects."
"LUCENE-3126","RFE","IMPROVEMENT","IndexWriter.addIndexes can make any incoming segment into CFS if it isn't already","Today, IW.addIndexes(Directory) does not modify the CFS-mode of the incoming segments. However, if IndexWriter's MP wants to create CFS (in general), there's no reason why not turn the incoming non-CFS segments into CFS. We anyway copy them, and if MP is not against CFS, we should create a CFS out of them.

Will need to use CFW, not sure it's ready for that w/ current API (I'll need to check), but luckily we're allowed to change it (@lucene.internal).

This should be done, IMO, even if the incoming segment is large (i.e., passes MP.noCFSRatio) b/c like I wrote above, we anyway copy it. However, if you think otherwise, speak up :).

I'll take a look at this in the next few days."
"LUCENE-2322","TEST","","Remove verbosity from tests and make configureable","The parent issue added the functionality to LuceneTestCase(J4), this patch applies it to most tests."
"LUCENE-2185","DOCUMENTATION","TASK","add @Deprecated annotations","as discussed on LUCENE-2084, I think we should be consistent about use of @Deprecated annotations if we are to use it.

This patch adds the missing annotations... unfortunately i cannot commit this for some time, because my internet connection does not support heavy committing (it is difficult to even upload a large patch).

So if someone wants to take it, have fun, otherwise in a week or so I will commit it if nobody objects.
"
"LUCENE-2412","DOCUMENTATION","TASK","Architecture Diagrams needed for Lucene, Solr and Nutch",""
"LUCENE-1138","BUG","BUG","SpellChecker.clearIndex calls unlock inappropriately","As noted in LUCENE-1050, fixing a bug in SimpleLockFactory related to not reporting success/filure of lock file deletion has surfaced bad behavior in SpellChecker.clearIndex...

Grant...
{quote}
It seems the SpellChecker is telling the IndexReader to delete the lockFile, but the lockFile doesn't exist.
  ...
I don't know much about the locking mechanism, but it seems like this should check to see if the lockFile exists before trying to delete it.
{quote}

Hoss...
{quote}
Grant: my take on this is that SpellChecker.clearIndex is in the wrong. it shouldn't be calling unlock unless it has reason to think there is a ""stale lock"" that needs to be closed - ie: this is a bug in SpellChecker that you have only discovered because this bug LUCENE-1050 was fixed.

I would suggest a new issue for tracking, and a patch in which SpellChecker.clearIndex doesn't call unlock unless isLocked returns true. Even then, it might make sense to catch and ignore LockReleaseFailedException and let whatever resulting exception may originate from ""new IndexWriter"" be returned.
{quote}

marking for 2.3 since it seems like a fairly trivial fix, and if we don't deal with it now it will be a bug introduced in 2.3.

"
"LUCENE-2616","BUG","BUG","FastVectorHighlighter: out of alignment when the first value is empty in multiValued field",""
"LUCENE-2161","IMPROVEMENT","IMPROVEMENT","Some concurrency improvements for NRT","Some concurrency improvements for NRT

I found & fixed some silly thread bottlenecks that affect NRT:

  * Multi/DirectoryReader.numDocs is synchronized, I think so only 1
    thread computes numDocs if it's -1.  I removed this sync, and made
    numDocs volatile, instead.  Yes, multiple threads may compute the
    numDocs for the first time, but I think that's harmless?

  * Fixed BitVector's ctor to set count to 0 on creating a new BV, and
    clone to copy the count over; this saves CPU computing the count
    unecessarily.

  * Also strengthened assertions done in SR, testing the delete docs
    count.

I also found an annoying thread bottleneck that happens, due to CMS.
Whenever CMS hits the max running merges (default changed from 3 to 1
recently), and the merge policy now wants to launch another merge, it
forces the incoming thread to wait until one of the BG threads
finishes.

This is a basic crude throttling mechanism -- you force the mutators
(whoever is causing new segments to appear) to stop, so that merging
can catch up.

Unfortunately, when stressing NRT, that thread is the one that's
opening a new NRT reader.

So, the first serious problem happens when you call .reopen() on your
NRT reader -- this call simply forwards to IW.getReader if the reader
was an NRT reader.  But, because DirectoryReader.doReopen is
synchronized, this had the horrible effect of holding the monitor lock
on your main IR.  In my test, this blocked all searches (since each
search uses incRef/decRef, still sync'd until LUCENE-2156, at least).
I fixed this by making doReopen only sync'd on this if it's not simply
forwarding to getWriter.  So that's a good step forward.

This prevents searches from being blocked while trying to reopen to a
new NRT.

However... it doesn't fix the problem that when an immense merge is
off and running, opening an NRT reader could hit a tremendous delay
because CMS blocks it.  The BalancedSegmentMergePolicy should help
here... by avoiding such immense merges.

But, I think we should also pursue an improvement to CMS.  EG, if it
has 2 merges running, where one is huge and one is tiny, it ought to
increase thread priority of the tiny one.  I think with such a change
we could increase the max thread count again, to prevent this
starvation.  I'll open a separate issue....
"
"LUCENE-2493","BUILD_SYSTEM","TASK","Rename lucene/solr dev jar files to -SNAPSHOT.jar","Currently the lucene dev jar files end with '-dev.jar' this is all fine, but it makes people using maven jump through a few hoops to get the -SNAPSHOT naming convention required by maven.  If we want to publish snapshot builds with hudson, we would need to either write some crazy scripts or run the build twice.

I suggest we switch to -SNAPSHOT.jar.  Hopefully for the 3.x branch and for the /trunk (4.x) branch"
"LUCENE-413","BUG","BUG","[PATCH] BooleanScorer2 ArrayIndexOutOfBoundsException + alternative NearSpans","From Erik's post at java-dev: 
 
>   [java] Caused by: java.lang.ArrayIndexOutOfBoundsException: 4 
>   [java]   at org.apache.lucene.search.BooleanScorer2  
> $Coordinator.coordFactor(BooleanScorer2.java:54) 
>   [java]   at org.apache.lucene.search.BooleanScorer2.score  
> (BooleanScorer2.java:292) 
... 
 
and my answer: 
 
Probably nrMatchers is increased too often in score() by calling score() 
more than once."
"LUCENE-3189","BUG","BUG","TestIndexWriter.testThreadInterruptDeadlock failed (can't reproduce)","trunk: r1134163 

ran it a few times with tests.iter=200 and couldn't reproduce, but i believe you like an issue anyway.

{code}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
    [junit] Testcase: testThreadInterruptDeadlock(org.apache.lucene.index.TestIndexWriter):     FAILED
    [junit]
    [junit] junit.framework.AssertionFailedError:
    [junit]     at org.apache.lucene.index.TestIndexWriter.testThreadInterruptDeadlock(TestIndexWriter.java:1203)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1403)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1321)
    [junit]
    [junit]
    [junit] Tests run: 40, Failures: 1, Errors: 0, Time elapsed: 23.79 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] CheckIndex failed
    [junit] ERROR: could not read any segments file in directory
    [junit] java.io.FileNotFoundException: segments_2w
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:407)
    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.openInput(DefaultSegmentInfosReader.java:112)
    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.read(DefaultSegmentInfosReader.java:45)
    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:257)
    [junit]     at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:287)
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:698)
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:533)
    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:283)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:311)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:154)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)
    [junit]     at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:1154)
    [junit]
    [junit] CheckIndex FAILED: unexpected exception
    [junit] java.lang.RuntimeException: CheckIndex failed
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:158)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)
    [junit]     at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:1154)
    [junit] IndexReader.open FAILED: unexpected exception
    [junit] java.io.FileNotFoundException: segments_2w
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:407)
    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.openInput(DefaultSegmentInfosReader.java:112)
    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.read(DefaultSegmentInfosReader.java:45)
    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:257)
    [junit]     at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:88)
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:698)
    [junit]     at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:84)
    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:500)
    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:293)
    [junit]     at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:1161)

    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testThreadInterruptDeadlock -Dtests.seed=6733070832417768606:3130345095020099096
    [junit] NOTE: test params are: codec=RandomCodecProvider: {=MockRandom, f6=SimpleText, f7=MockRandom, f8=MockSep, f9=Standard, f1=SimpleText, f0=Standard, f3=Standard, f2=MockSep, f5=Pulsing(freqCutoff=12),
 f4=MockFixedIntBlock(blockSize=552), c=MockVariableIntBlock(baseBlockSize=43), d9=MockVariableIntBlock(baseBlockSize=43), d8=MockRandom, d5=MockSep, d4=Pulsing(freqCutoff=12), d7=MockFixedIntBlock(blockSize=55
2), d6=MockVariableIntBlock(baseBlockSize=43), d25=MockSep, d0=MockVariableIntBlock(baseBlockSize=43), c29=MockFixedIntBlock(blockSize=552), d24=Pulsing(freqCutoff=12), d1=MockFixedIntBlock(blockSize=552), c28=
Standard, d23=SimpleText, d2=SimpleText, c27=MockSep, d22=Standard, d3=MockRandom, d21=MockRandom, d20=SimpleText, c22=MockSep, c21=Pulsing(freqCutoff=12), c20=SimpleText, d29=SimpleText, c26=MockVariableIntBlo
ck(baseBlockSize=43), d28=Standard, c25=MockRandom, d27=MockVariableIntBlock(baseBlockSize=43), c24=Pulsing(freqCutoff=12), d26=MockRandom, c23=MockFixedIntBlock(blockSize=552), e9=Pulsing(freqCutoff=12), e8=St
andard, e7=MockSep, e6=MockRandom, e5=SimpleText, c17=MockFixedIntBlock(blockSize=552), e3=MockFixedIntBlock(blockSize=552), d12=Pulsing(freqCutoff=12), c16=MockVariableIntBlock(baseBlockSize=43), e4=Pulsing(fr
eqCutoff=12), d11=MockFixedIntBlock(blockSize=552), c19=MockRandom, e1=MockSep, d14=MockVariableIntBlock(baseBlockSize=43), c18=SimpleText, e2=Standard, d13=MockRandom, e0=SimpleText, d10=MockSep, d19=MockVaria
bleIntBlock(baseBlockSize=43), c11=MockVariableIntBlock(baseBlockSize=43), c10=MockRandom, d16=Standard, c13=MockRandom, c12=SimpleText, d15=MockSep, d18=Pulsing(freqCutoff=12), c15=Standard, d17=MockFixedIntBl
ock(blockSize=552), c14=MockSep, b3=Standard, b2=MockSep, b5=Pulsing(freqCutoff=12), b4=MockFixedIntBlock(blockSize=552), b7=MockFixedIntBlock(blockSize=552), b6=MockVariableIntBlock(baseBlockSize=43), d50=Mock
Random, b9=MockRandom, b8=SimpleText, d43=SimpleText, d42=Standard, d41=MockVariableIntBlock(baseBlockSize=43), d40=MockRandom, d47=Pulsing(freqCutoff=12), d46=MockFixedIntBlock(blockSize=552), b0=MockRandom, d
45=Standard, b1=MockVariableIntBlock(baseBlockSize=43), d44=MockSep, d49=MockRandom, d48=SimpleText, c6=SimpleText, c5=Standard, c4=MockVariableIntBlock(baseBlockSize=43), c3=MockRandom, c9=MockFixedIntBlock(bl
ockSize=552), c8=Standard, c7=MockSep, d30=Standard, d32=Pulsing(freqCutoff=12), d31=MockFixedIntBlock(blockSize=552), c1=Pulsing(freqCutoff=12), d34=MockFixedIntBlock(blockSize=552), c2=MockSep, d33=MockVariab
leIntBlock(baseBlockSize=43), d36=MockRandom, c0=SimpleText, d35=SimpleText, d38=MockSep, d37=Pulsing(freqCutoff=12), d39=MockVariableIntBlock(baseBlockSize=43), e92=MockFixedIntBlock(blockSize=552), e93=Pulsin
g(freqCutoff=12), e90=MockSep, e91=Standard, e89=Standard, e88=MockVariableIntBlock(baseBlockSize=43), e87=MockRandom, e86=MockFixedIntBlock(blockSize=552), e85=MockVariableIntBlock(baseBlockSize=43), e84=MockS
ep, e83=Pulsing(freqCutoff=12), e80=MockFixedIntBlock(blockSize=552), e81=SimpleText, e82=MockRandom, e77=Standard, e76=MockSep, e79=Pulsing(freqCutoff=12), e78=MockFixedIntBlock(blockSize=552), e73=MockVariabl
eIntBlock(baseBlockSize=43), e72=MockRandom, e75=SimpleText, e74=Standard, binary=MockSep, f98=MockRandom, f97=SimpleText, f99=MockSep, f94=Pulsing(freqCutoff=12), f93=MockFixedIntBlock(blockSize=552), f96=Mock
VariableIntBlock(baseBlockSize=43), f95=MockRandom, e95=MockRandom, e94=SimpleText, e97=Standard, e96=MockSep, e99=MockSep, e98=Pulsing(freqCutoff=12), id=Standard, f34=SimpleText, f33=Standard, f32=MockVariabl
eIntBlock(baseBlockSize=43), f31=MockRandom, f30=MockFixedIntBlock(blockSize=552), f39=SimpleText, f38=MockVariableIntBlock(baseBlockSize=43), f37=MockRandom, f36=Pulsing(freqCutoff=12), f35=MockFixedIntBlock(b
lockSize=552), f43=MockSep, f42=Pulsing(freqCutoff=12), f45=MockFixedIntBlock(blockSize=552), f44=MockVariableIntBlock(baseBlockSize=43), f41=Standard, f40=MockSep, f47=SimpleText, f46=Standard, f49=MockSep, f4
8=Pulsing(freqCutoff=12), content=Standard, e19=Standard, e18=MockSep, e17=SimpleText, f12=MockRandom, e16=Standard, f11=SimpleText, f10=MockFixedIntBlock(blockSize=552), e15=MockVariableIntBlock(baseBlockSize=
43), e14=MockRandom, f16=MockFixedIntBlock(blockSize=552), e13=MockSep, e12=Pulsing(freqCutoff=12), f15=MockVariableIntBlock(baseBlockSize=43), e11=SimpleText, f14=MockSep, e10=Standard, f13=Pulsing(freqCutoff=
12), f19=Standard, f18=MockVariableIntBlock(baseBlockSize=43), f17=MockRandom, e29=MockRandom, e26=MockSep, f21=Standard, e25=Pulsing(freqCutoff=12), f20=MockSep, e28=MockFixedIntBlock(blockSize=552), f23=Pulsi
ng(freqCutoff=12), e27=MockVariableIntBlock(baseBlockSize=43), f22=MockFixedIntBlock(blockSize=552), f25=MockRandom, e22=MockFixedIntBlock(blockSize=552), f24=SimpleText, e21=MockVariableIntBlock(baseBlockSize=
43), f27=Standard, e24=MockRandom, f26=MockSep, e23=SimpleText, f29=MockSep, f28=Pulsing(freqCutoff=12), e20=Pulsing(freqCutoff=12), field=MockSep, string=MockVariableIntBlock(baseBlockSize=43), e30=MockFixedIn
tBlock(blockSize=552), e31=Pulsing(freqCutoff=12), a98=MockSep, e34=SimpleText, a99=Standard, e35=MockRandom, f79=MockSep, e32=MockVariableIntBlock(baseBlockSize=43), e33=MockFixedIntBlock(blockSize=552), b97=M
ockRandom, f77=MockRandom, e38=MockVariableIntBlock(baseBlockSize=43), b98=MockVariableIntBlock(baseBlockSize=43), f78=MockVariableIntBlock(baseBlockSize=43), e39=MockFixedIntBlock(blockSize=552), b99=Standard,
 f75=MockFixedIntBlock(blockSize=552), e36=Pulsing(freqCutoff=12), f76=Pulsing(freqCutoff=12), e37=MockSep, f73=Pulsing(freqCutoff=12), f74=MockSep, f71=Standard, f72=SimpleText, f81=Standard, f80=MockSep, e40=
MockVariableIntBlock(baseBlockSize=43), e41=Standard, e42=SimpleText, e43=MockSep, e44=Standard, e45=MockFixedIntBlock(blockSize=552), e46=Pulsing(freqCutoff=12), f86=Standard, e47=SimpleText, f87=SimpleText, e
48=MockRandom, f88=Pulsing(freqCutoff=12), e49=MockSep, f89=MockSep, f82=MockVariableIntBlock(baseBlockSize=43), f83=MockFixedIntBlock(blockSize=552), f84=SimpleText, f85=MockRandom, f90=Pulsing(freqCutoff=12),
 f92=MockVariableIntBlock(baseBlockSize=43), f91=MockRandom, str=MockRandom, a76=Standard, e56=Standard, f59=Pulsing(freqCutoff=12), a77=SimpleText, e57=SimpleText, a78=Pulsing(freqCutoff=12), e54=MockRandom, f
57=Standard, a79=MockSep, e55=MockVariableIntBlock(baseBlockSize=43), f58=SimpleText, e52=MockVariableIntBlock(baseBlockSize=43), e53=MockFixedIntBlock(blockSize=552), e50=Pulsing(freqCutoff=12), e51=MockSep, f
51=MockSep, f52=Standard, f50=MockRandom, f55=MockVariableIntBlock(baseBlockSize=43), f56=MockFixedIntBlock(blockSize=552), f53=Pulsing(freqCutoff=12), e58=MockFixedIntBlock(blockSize=552), f54=MockSep, e59=Pul
sing(freqCutoff=12), a80=Pulsing(freqCutoff=12), e60=Pulsing(freqCutoff=12), a82=MockVariableIntBlock(baseBlockSize=43), a81=MockRandom, a84=MockRandom, a83=SimpleText, a86=Standard, a85=MockSep, a89=SimpleText
, f68=MockVariableIntBlock(baseBlockSize=43), e65=Pulsing(freqCutoff=12), f69=MockFixedIntBlock(blockSize=552), e66=MockSep, a87=MockVariableIntBlock(baseBlockSize=43), e67=MockVariableIntBlock(baseBlockSize=43
), a88=MockFixedIntBlock(blockSize=552), e68=MockFixedIntBlock(blockSize=552), e61=SimpleText, e62=MockRandom, e63=MockSep, e64=Standard, f60=MockFixedIntBlock(blockSize=552), f61=Pulsing(freq

Cutoff=12), f62=MockRandom, f63=MockVariableIntBlock(baseBlockSize=43), e69=Standard, f64=SimpleText, f65=MockRandom, f66=MockSep, f67=Standard, f70=MockFixedIntBlock(blockSize=552), a93=MockSep, a92=Pulsing(freqCutoff=12), a91=SimpleText, e71=SimpleText, a90=Standard, e70=Standard, a97=MockVariableIntBlock(baseBlockSize=43), a96=MockRandom, a95=Pulsing(freqCutoff=12), a94=MockFixedIntBlock(blockSize=552), c58=MockRandom, a63=MockFixedIntBlock(blockSize=552), a64=Pulsing(freqCutoff=12), c59=MockVariableIntBlock(baseBlockSize=43), c56=MockFixedIntBlock(blockSize=552), d59=MockRandom, a61=MockSep, c57=Pulsing(freqCutoff=12), a62=Standard, c54=Pulsing(freqCutoff=12), c55=MockSep, a60=SimpleText, c52=Standard, c53=SimpleText, d53=SimpleText, d54=MockRandom, d51=MockVariableIntBlock(baseBlockSize=43), d52=MockFixedIntBlock(blockSize=552), d57=Pulsing(freqCutoff=12), b62=Standard, d58=MockSep, b63=SimpleText, d55=Standard, b60=MockRandom, d56=SimpleText, b61=MockVariableIntBlock(baseBlockSize=43), b56=Standard, b55=MockSep, b54=MockRandom, b53=SimpleText, d61=MockVariableIntBlock(baseBlockSize=43), b59=MockVariableIntBlock(baseBlockSize=43), d60=MockRandom, b58=MockSep, b57=Pulsing(freqCutoff=12), c62=Standard, c61=MockSep, a59=MockVariableIntBlock(baseBlockSize=43), c60=MockRandom, a58=MockRandom, a57=MockFixedIntBlock(blockSize=552), a56=MockVariableIntBlock(baseBlockSize=43), a55=MockSep, a54=Pulsing(freqCutoff=12), a72=MockRandom, c67=Standard, a73=MockVariableIntBlock(baseBlockSize=43), c68=SimpleText, a74=Standard, c69=Pulsing(freqCutoff=12), a75=SimpleText, c63=MockVariableIntBlock(baseBlockSize=43), c64=MockFixedIntBlock(blockSize=552), a70=MockVariableIntBlock(baseBlockSize=43), c65=SimpleText, a71=MockFixedIntBlock(blockSize=552), c66=MockRandom, d62=MockSep, d63=Standard, d64=MockFixedIntBlock(blockSize=552), b70=Standard, d65=Pulsing(freqCutoff=12), b71=Pulsing(freqCutoff=12), d66=MockVariableIntBlock(baseBlockSize=43), b72=MockSep, d67=MockFixedIntBlock(blockSize=552), b73=MockVariableIntBlock(baseBlockSize=43), d68=SimpleText, b74=MockFixedIntBlock(blockSize=552), d69=MockRandom, b65=Pulsing(freqCutoff=12), b64=MockFixedIntBlock(blockSize=552), b67=MockVariableIntBlock(baseBlockSize=43), b66=MockRandom, d70=SimpleText, b69=MockRandom, b68=SimpleText, d72=MockSep, d71=Pulsing(freqCutoff=12), c71=Pulsing(freqCutoff=12), c70=MockFixedIntBlock(blockSize=552), a69=Pulsing(freqCutoff=12), c73=MockVariableIntBlock(baseBlockSize=43), c72=MockRandom, a66=MockRandom, a65=SimpleText, a68=Standard, a67=MockSep, c32=MockSep, c33=Standard, c30=SimpleText, c31=MockRandom, c36=MockVariableIntBlock(baseBlockSize=43), a41=Pulsing(freqCutoff=12), c37=MockFixedIntBlock(blockSize=552), a42=MockSep, a0=MockRandom, c34=Pulsing(freqCutoff=12), c35=MockSep, a40=SimpleText, b84=MockSep, d79=MockFixedIntBlock(blockSize=552), b85=Standard, b82=SimpleText, d77=MockSep, c38=Standard, b83=MockRandom, d78=Standard, c39=SimpleText, b80=MockRandom, d75=Standard, b81=MockVariableIntBlock(baseBlockSize=43), d76=SimpleText, d73=MockRandom, d74=MockVariableIntBlock(baseBlockSize=43), d83=MockRandom, a9=MockFixedIntBlock(blockSize=552), d82=SimpleText, d81=MockFixedIntBlock(blockSize=552), d80=MockVariableIntBlock(baseBlockSize=43), b79=MockFixedIntBlock(blockSize=552), b78=MockSep, b77=Pulsing(freqCutoff=12), b76=SimpleText, b75=Standard, a1=Pulsing(freqCutoff=12), a35=Pulsing(freqCutoff=12), a2=MockSep, a34=MockFixedIntBlock(blockSize=552), a3=MockVariableIntBlock(baseBlockSize=43), a33=Standard, a4=MockFixedIntBlock(blockSize=552), a32=MockSep, a5=MockRandom, a39=MockRandom, c40=SimpleText, a6=MockVariableIntBlock(baseBlockSize=43), a38=SimpleText, a7=Standard, a37=MockFixedIntBlock(blockSize=552), a8=SimpleText, a36=MockVariableIntBlock(baseBlockSize=43), c41=MockFixedIntBlock(blockSize=552), c42=Pulsing(freqCutoff=12), c43=MockRandom, c44=MockVariableIntBlock(baseBlockSize=43), c45=SimpleText, a50=MockVariableIntBlock(baseBlockSize=43), c46=MockRandom, a51=MockFixedIntBlock(blockSize=552), c47=MockSep, a52=SimpleText, c48=Standard, a53=MockRandom, b93=MockFixedIntBlock(blockSize=552), d88=MockRandom, c49=MockVariableIntBlock(baseBlockSize=43), b94=Pulsing(freqCutoff=12), d89=MockVariableIntBlock(baseBlockSize=43), b95=MockRandom, b96=MockVariableIntBlock(baseBlockSize=43), d84=Pulsing(freqCutoff=12), b90=SimpleText, d85=MockSep, b91=Pulsing(freqCutoff=12), d86=MockVariableIntBlock(baseBlockSize=43), b92=MockSep, d87=MockFixedIntBlock(blockSize=552), d92=Standard, d91=MockSep, d94=Pulsing(freqCutoff=12), d93=MockFixedIntBlock(blockSize=552), b87=MockFixedIntBlock(blockSize=552), b86=MockVariableIntBlock(baseBlockSize=43), d90=SimpleText, b89=MockRandom, b88=SimpleText, a44=MockVariableIntBlock(baseBlockSize=43), a43=MockRandom, a46=SimpleText, a45=Standard, a48=Standard, a47=MockSep, c51=MockFixedIntBlock(blockSize=552), a49=MockFixedIntBlock(blockSize=552), c50=MockVariableIntBlock(baseBlockSize=43), d98=MockFixedIntBlock(blockSize=552), d97=MockVariableIntBlock(baseBlockSize=43), d96=MockSep, d95=Pulsing(freqCutoff=12), d99=MockRandom, a20=MockVariableIntBlock(baseBlockSize=43), c99=SimpleText, c98=Standard, c97=MockVariableIntBlock(baseBlockSize=43), c96=MockRandom, b19=MockVariableIntBlock(baseBlockSize=43), a16=Pulsing(freqCutoff=12), a17=MockSep, b17=Pulsing(freqCutoff=12), a14=Standard, b18=MockSep, a15=SimpleText, a12=SimpleText, a13=MockRandom, a10=MockVariableIntBlock(baseBlockSize=43), a11=MockFixedIntBlock(blockSize=552), b11=MockFixedIntBlock(blockSize=552), b12=Pulsing(freqCutoff=12), b10=Standard, b15=SimpleText, b16=MockRandom, a18=MockRandom, b13=MockVariableIntBlock(baseBlockSize=43), a19=MockVariableIntBlock(baseBlockSize=43), b14=MockFixedIntBlock(blockSize=552), b30=MockRandom, a31=MockSep, a30=Pulsing(freqCutoff=12), b28=SimpleText, a25=MockVariableIntBlock(baseBlockSize=43), b29=MockRandom, a26=MockFixedIntBlock(blockSize=552), a27=SimpleText, a28=MockRandom, a21=MockSep, a22=Standard, a23=MockFixedIntBlock(blockSize=552), a24=Pulsing(freqCutoff=12), b20=MockRandom, b21=MockVariableIntBlock(baseBlockSize=43), b22=Standard, b23=SimpleText, a29=Pulsing(freqCutoff=12), b24=MockSep, b25=Standard, b26=MockFixedIntBlock(blockSize=552), b27=Pulsing(freqCutoff=12), b41=Pulsing(freqCutoff=12), b40=MockFixedIntBlock(blockSize=552), c77=MockRandom, c76=SimpleText, c75=MockFixedIntBlock(blockSize=552), c74=MockVariableIntBlock(baseBlockSize=43), c79=SimpleText, c78=Standard, c80=MockSep, c83=MockRandom, c84=MockVariableIntBlock(baseBlockSize=43), c81=MockFixedIntBlock(blockSize=552), b39=MockFixedIntBlock(blockSize=552), c82=Pulsing(freqCutoff=12), b37=Standard, b38=SimpleText, b35=MockRandom, b36=MockVariableIntBlock(baseBlockSize=43), b33=MockVariableIntBlock(baseBlockSize=43), b34=MockFixedIntBlock(blockSize=552), b31=Pulsing(freqCutoff=12), b32=MockSep, str2=MockSep, b50=MockVariableIntBlock(baseBlockSize=43), b52=SimpleText, str3=MockVariableIntBlock(baseBlockSize=43), b51=Standard, c86=Standard, tvtest=MockSep, c85=MockSep, c88=Pulsing(freqCutoff=12), c87=MockFixedIntBlock(blockSize=552), c89=MockVariableIntBlock(baseBlockSize=43), c90=SimpleText, c91=MockRandom, c92=Standard, c93=SimpleText, c94=Pulsing(freqCutoff=12), c95=MockSep, content1=MockRandom, b46=Pulsing(freqCutoff=12), b47=MockSep, content3=MockFixedIntBlock(blockSize=552), b48=MockVariableIntBlock(baseBlockSize=43), content4=MockVariableIntBlock(baseBlockSize=43), b49=MockFixedIntBlock(blockSize=552), content5=Pulsing(freqCutoff=12), b42=SimpleText, b43=MockRandom, b44=MockSep, b45=Standard}, locale=sk, timezone=America/Rainy_River
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDateTools, TestDeletionPolicy, TestDocsAndPositions, TestFlex, TestIndexReaderCloneNorms, TestIndexWriter]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=86065896,total=127401984
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.index.TestIndexWriter FAILED
{code}"
"LUCENE-2128","IMPROVEMENT","IMPROVEMENT","Further parallelizaton of ParallelMultiSearcher","When calling {{search(Query, Filter, int)}} on a ParallelMultiSearcher, the {{createWeights}} function of MultiSearcher is called, and sequentially calls {{docFreqs()}} on every sub-searcher. This can take a significant amount of time when there are lots of remote sub-searchers.

"
"LUCENE-3232","REFACTORING","","Move MutableValues to Common Module","Solr makes use of the MutableValue* series of classes to improve performance of grouping by FunctionQuery (I think).  As such they are used in ValueSource implementations.  Consequently we need to move these classes in order to move the ValueSources.

As Yonik pointed out, these classes have use beyond just FunctionQuerys and might be used by both Solr and other modules.  However I don't think they belong in Lucene core, since they aren't really related to search functionality.  Therefore I think we should put them into a Common module, which can serve as a dependency to Solr and any module."
"LUCENE-3685","RFE","IMPROVEMENT","Add top-down version of BlockJoinQuery","Today, BlockJoinQuery can join from child docIDs up to parent docIDs.
EG this works well for product (parent) + many SKUs (child) search.

But the reverse, which BJQ cannot do, is also useful in some cases.
EG say you index songs (child) within albums (parent), but you want to
search and present by song not album while involving some fields from
the album in the query.  In this case you want to wrap a parent query
(against album), joining down to the child document space.
"
"LUCENE-825","BUG","BUG","NullPointerException from SegmentInfos.FindSegmentsFile.run() if FSDirectory.list() returns NULL ","Found this bug while running unit tests to verify an upgrade of our system from 1.4.3 to 2.1.0.  This bug did *not* occur during 1.4.3, it is new to 2.x (I'm pretty sure it's 2.1-only)

If the index directory gets deleted out from under Lucene after the FSDirectory has been created, then attempts to open an IndexWriter or IndexReader will result in an NPE.  Lucene should be throwing an IOException in this case.

Repro:
    1) Create an FSDirectory pointing somewhere in the filesystem (e.g. /foo/index/1)
    2) rm -rf the parent dir (rm -rf /foo/index)
    3) Try to open an IndexReader

Result: NullPointerException on line ""for(int i=0;i<files.length;i++) { "" -- 'files' is NULL.
 
Expect: IOException


....  

This is happening because of a missing NULL check in SegmentInfos$FindSegmentsFile.run():

        if (0 == method) {
          if (directory != null) {
            files = directory.list();
          } else {
            files = fileDirectory.list();
          }

          gen = getCurrentSegmentGeneration(files);

          if (gen == -1) {
            String s = """";
            for(int i=0;i<files.length;i++) { 
              s += "" "" + files[i];
            }
            throw new FileNotFoundException(""no segments* file found: files:"" + s);
          }
        }


The FSDirectory constructor will make sure the index dir exists, but if it is for some reason deleted out from underneath Lucene after the FSDirectory is instantiated, then java.io.File.list() will return NULL.  Probably better to fix FSDirectory.list() to just check for null and return a 0-length array:

(in org/apache/lucene/store/FSDirectory.java)
314c314,317
<         return directory.list(IndexFileNameFilter.getFilter());
---
>     String[] toRet = directory.list(IndexFileNameFilter.getFilter());
>     if (toRet == null)
>         return new String[]{};
>     return toRet;
"
"LUCENE-2743","IMPROVEMENT","IMPROVEMENT","SimpleText is too slow","If you are unlucky enough to get SimpleText codec on the TestBasics (span query) test then it runs very slowly..."
"LUCENE-3566","BUG","IMPROVEMENT","Parametrizing H1 and H2","The DFR normalizations {{H1}} and {{H2}} are parameter-free. This is in line with the [original article|http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.101.742], but not with the [thesis|http://theses.gla.ac.uk/1570/], where H2 accepts a {{c}} parameter, nor with [information-based models|http://dl.acm.org/citation.cfm?id=1835490], where H1 also accepts a {{c}} parameter."
"LUCENE-3030","IMPROVEMENT","IMPROVEMENT","Block tree terms dict & index","Our default terms index today breaks terms into blocks of fixed size
(ie, every 32 terms is a new block), and then we build an index on top
of that (holding the start term for each block).

But, it should be better to instead break terms according to how they
share prefixes.  This results in variable sized blocks, but means
within each block we maximize the shared prefix and minimize the
resulting terms index.  It should also be a speedup for terms dict
intensive queries because the terms index becomes a ""true"" prefix
trie, and can be used to fast-fail on term lookup (ie returning
NOT_FOUND without having to seek/scan a terms block).

Having a true prefix trie should also enable much faster intersection
with automaton (but this will be a new issue).

I've made an initial impl for this (called
BlockTreeTermsWriter/Reader).  It's still a work in progress... lots
of nocommits, and hairy code, but tests pass (at least once!).

I made two new codecs, temporarily called StandardTree, PulsingTree,
that are just like their counterparts but use this new terms dict.

I added a new ""exactOnly"" boolean to TermsEnum.seek.  If that's true
and the term is NOT_FOUND, we will (quickly) return NOT_FOUND and the
enum is unpositioned (ie you should not call next(), docs(), etc.).

In this approach the index and dict are tightly connected, so it does
not support a pluggable index impl like BlockTermsWriter/Reader.
Blocks are stored on certain nodes of the prefix trie, and can contain
both terms and pointers to sub-blocks (ie, if the block is not a leaf
block).  So there are two trees, tied to one another -- the index
trie, and the blocks.  Only certain nodes in the trie map to a block
in the block tree.

I think this algorithm is similar to burst tries
(http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.3499),
except it allows terms to be stored on inner blocks (not just leaf
blocks).  This is important for Lucene because an [accidental]
""adversary"" could produce a terms dict with way too many blocks (way
too much RAM used by the terms index).  Still, with my current patch,
an adversary can produce too-big blocks... which we may need to fix,
by letting the terms index not be a true prefix trie on it's leaf
edges.

Exactly how the blocks are picked can be factored out as its own
policy (but I haven't done that yet).  Then, burst trie is one policy,
my current approach is another, etc.  The policy can be tuned to
the terms' expected distribution, eg if it's a primary key field and
you only use base 10 for each character then you want block sizes of
size 10.  This can make a sizable difference on lookup cost.

I modified the FST Builder to allow for a ""plugin"" that freezes the
""tail"" (changed suffix) of each added term, because I use this to find
the blocks.
"
"LUCENE-3622","REFACTORING","TASK","separate IndexDocValues interface from implementation","Currently the o.a.l.index.values contains both the abstract apis and Lucene40's current implementation.

I think we should move the implementation underneath Lucene40Codec, leaving only the abstract apis.

For example, simpletext might have a different implementation, and we might make a int8 implementation
underneath preflexcodec to support norms."
"LUCENE-2708","TEST","TEST","when a test Assume fails, display information","Currently if a test uses Assume.assumeTrue, it silently passes.

I think we should output something, at *least* if you have VERBOSE set, maybe always.

Here's an example of what the output might look like:
{noformat}
junit-sequential:
    [junit] Testsuite: org.apache.solr.servlet.SolrRequestParserTest
    [junit] Tests run: 4, Failures: 0, Errors: 0, Time elapsed: 1.582 sec
    [junit]
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: testStreamURL Assume failed (ignored):
    [junit] org.junit.internal.AssumptionViolatedException: got: <java.io.FileNotFoundException: http://www.apdfgdfgache
.org/dist/lucene/solr/>, expected: null
    [junit]     at org.junit.Assume.assumeThat(Assume.java:70)
    [junit]     at org.junit.Assume.assumeNoException(Assume.java:92)
    [junit]     at org.apache.solr.servlet.SolrRequestParserTest.testStreamURL(SolrRequestParserTest.java:123)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:802)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:775)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)
    [junit] Caused by: java.io.FileNotFoundException: http://www.apdfgdfgache.org/dist/lucene/solr/
    [junit]     at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1311)
    [junit]     at org.apache.solr.servlet.SolrRequestParserTest.testStreamURL(SolrRequestParserTest.java:120)
    [junit]     ... 26 more
    [junit] ------------- ---------------- ---------------
{noformat}"
"LUCENE-716","RFE","IMPROVEMENT","Support unicode escapes in QueryParser","As suggested by Yonik in http://issues.apache.org/jira/browse/LUCENE-573 the QueryParser should be able to handle unicode escapes, i. e. \uXXXX.

I have already working and tested code. It is based on the patch i submitted for LUCENE-573, so once this is (hopefully ;-)) committed, I will submit another patch here."
"LUCENE-290","DESIGN_DEFECT","BUG","[PATCH] public static members in class TermVectorsWriter","hi all,

looking at the implementation of TermVectorsWriter, you'll find a bunch of
public static final members where the visibility could be reduced to be
protected. I don't see a reason for having them public if the class itself is
protected and all members are final values. May be somebody could check and
either commit or enlighten me ;-)

thx
Bernhard"
"LUCENE-645","BUG","BUG","Highligter fails to include non-token at end of string to be highlighted","The following code extract show the problem


		TermQuery query= new TermQuery( new Term( ""data"", ""help"" )); 
		Highlighter hg = new Highlighter(new SimpleHTMLFormatter(), new QueryScorer( query ));
		hg.setTextFragmenter( new NullFragmenter() );
		
		String match = null;
		try {
			match = hg.getBestFragment( new StandardAnalyzer(), ""data"", ""help me [54-65]"" );
		} catch (IOException e) {
			e.printStackTrace();
		}
		System.out.println( match );


The sytsem outputs 

<B>help</B> me [54-65


would expect 

<B>help</B> me [54-65]



"
"LUCENE-449","IMPROVEMENT","IMPROVEMENT","NullPointerException when temporary directory not readable","We have customers reporting errors such as:

Caused by: java.lang.NullPointerException
	at org.apache.lucene.store.FSDirectory.create(FSDirectory.java:200)
	at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:144)
	at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:117)
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:205)
	at com.atlassian.jira.util.LuceneUtils.getIndexWriter(LuceneUtils.java:46)
	at com.atlassian.jira.issue.index.DefaultIndexManager.getIndexWriter(DefaultIndexManager.java:568)
	at com.atlassian.jira.issue.index.DefaultIndexManager.indexIssuesAndComments(DefaultIndexManager.java:287)
	... 59 more


This occurs when the lock directory is unreadable (eg. because Tomcat sets java.io.tmpdir to temp/ and the permissions here are broken). Attached is "
"LUCENE-818","SPEC","BUG","IndexWriter should detect when it's used after being closed","Spinoff from this thread on java-user:

    http://www.gossamer-threads.com/lists/lucene/java-user/45986

If you call addDocument on IndexWriter after it's closed you'll hit a
hard-to-explain NullPointerException (because the RAMDirectory was
closed).  Before 2.1, apparently you won't hit any exception and the
IndexWrite will keep running but will have released it's write lock (I
think).

I plan to fix IndexWriter methods to throw an IllegalStateException if
it has been closed.
"
"LUCENE-2744","BUG","BUG","CheckIndex overstates how many fields have norms enabled","It simply tells you how many unique fields there are... it should instead only say how many have norms."
"LUCENE-3454","REFACTORING","IMPROVEMENT","rename optimize to a less cool-sounding name","I think users see the name optimize and feel they must do this, because who wants a suboptimal system? but this probably just results in wasted time and resources.

maybe rename to collapseSegments or something?"
"LUCENE-2582","RFE","IMPROVEMENT","allow an alg file to specify the default codec","I already committed this one by accident so I better open an issue!

I added this:

  default.codec = Pulsing

so that your alg file can specify the codec to be used when writing new segments in an index."
"LUCENE-1989","DESIGN_DEFECT","BUG","CharArraySet cannot be made generic, because it violates the Set<char[]> interface","I tried to make CharArraySet using generics (extends AbstractSet<char[]>) but this is not possible, as it e.g. returns sometimes String instances in the Iterator instead of []. Also its addAll method accepts both String and char[]. I think this class is a complete mis-design and violates almost everything (sorry).

What to do? Make it Set<?> or just place a big @SuppressWarnings(""unchecked""> in front of it?

Because of this problem also a lot of Set declarations inside StopAnalyzer cannot be made generic as you never know whats inside."
"LUCENE-2489","REFACTORING","IMPROVEMENT","promote TestExternalCodecs.PerFieldCodecWrapper to core","PerFieldCodecWrapper lets you set the Codec for each field; I'll promote to core & mark experimental."
"LUCENE-2639","TEST","TEST","remove random juggling in tests, add -Dtests.seed","Since we added newIndexWriterConfig/newDirectory, etc, a lot of tests are juggling randoms around.

Instead this patch:
* changes it so LuceneTestCase[J4] manage the random.
* allow you to set -Dtests.seed=23432432432 to reproduce a test, rather than editing the code
* removes random arguments from newIndexWriterConfig, newDirectory.

I want to do this before looking at doing things like newField so we can vary term vectors, etc.

I also fixed the solr contrib builds so they arent hiding the exceptions i noted in SOLR-2002."
"LUCENE-3301","BUG","BUG","add workaround for jre breakiterator bugs","on some inputs, the java breakiterator support will internally crash.

for example: ant test -Dtestcase=TestThaiAnalyzer -Dtestmethod=testRandomStrings -Dtests.seed=-8005471002120855329:-2517344653287596566 -Dtests.multiplier=3"
"LUCENE-3598","IMPROVEMENT","IMPROVEMENT","Improve InfoStream class in trunk to be more consistent with logging-frameworks like slf4j/log4j/commons-logging","Followup on a [thread by Shai Erea on java-dev@lao|http://lucene.472066.n3.nabble.com/IndexWriter-infoStream-is-final-td3537485.html]: I already discussed with Robert about that, that there is one thing missing. Currently the IW only checks if the infoStream!=null and then passes the message to the method, and that *may* ignore it. For your requirement it is the case that this is enabled or disabled dynamically. Unfortunately if the construction of the message is heavy, then this wastes resources.

I would like to add another method to this class: abstract boolean isEnabled() that can also be implemented. I would then replace all null checks in IW by this method. The default config in IW would be changed to use a NoOutputInfoStream that returns false here and ignores the message.

A simple logger wrapper for e.g. log4j / slf4j then could look like (ignoring component, could be enabled):

{code:java}
Loger log = YourLoggingFramework.getLogger(IndexWriter.class);

public void message(String component, String message) {
  log.debug(component + "": "" + message);
}

public boolean isEnabled(String component) {
  return log.isDebugEnabled();
}
{code}

Using this you could enable/disable logging live by e.g. the log4j management console of your app server by enabling/disabling IndexWriter.class logging.

The changes are really simple:
- PrintStreamInfoStream returns true, always, mabye make it dynamically enable/disable to allow Shai's request
- infoStream.getDefault() is never null and can never be set to null. Instead the default is a singleton NoOutputInfoStream that returns false of isEnabled(component).
- All null checks on infoStream should be replaced by infoStream.isEanbled(component), this is possible as always != null. There are no slowdowns by this - it's like Collections.emptyList() instead stupid null checks."
"LUCENE-2494","IMPROVEMENT","IMPROVEMENT","Modify ParallelMultiSearcher to use a CompletionService instead of slowly polling for results","Right now, the parallel multi searcher creates an array/list of Future<V> representing each of the searchables that's being concurrently searched (and its corresponding search task).

As it stands, once the tasks are all submitted to the executor, the array is iterated over, FIFO, and Future.get() is called iteratively.  This obviously works, but isn't ideal.  It's entirely possible (a situation I've run into) where one of the first searchables represents a large index that takes a long time to search, so the results of the other searchables can't be processed until the large index is done searching.  In my case, we have two indexes with several million records that get searched in front of some other indexes, the smallest of which has only a few ten thousand entries and I didn't think it was ideal for the results of the other indexes to wait.

I've modified ParallelMultiSearcher to use CompletionServices instead, so that results are processed in the order they are completed, rather than the order that they are submitted.  All the tests still pass, and to the best of my knowledge this won't break anything.  This have several advantages:
1) Speed - the thread owning the executor doesn't have to wait for the first submitted task to finish in order to process the results of the other tasks, which may have finished first
2) Removed several warnings (even if they are annotated away) due to the ugliness of typecasting generic arrays.
3) Decreased the complexity of the code in some cases, usually by removing the necessity of allocating and filling arrays.

With a primed ""cache"" of searchables, I was getting 700-1200 ms per search, and using the same phrases, with this patch, I am now getting 400-500ms per search :)

Patch is attached."
"LUCENE-481","BUG","BUG","IndexReader.getCurrentVersion() and isCurrent should use commit lock.","There is a race condition if one machine is checking the current version of an index while another wants to update the segments file in IndexWriter.close().

java.io.IOException: Cannot delete segments
	at org.apache.lucene.store.FSDirectory.renameFile(FSDirectory.java:213)
	at org.apache.lucene.index.SegmentInfos.write(SegmentInfos.java:90)
	at org.apache.lucene.index.IndexWriter$3.doBody(IndexWriter.java:503)
	at org.apache.lucene.store.Lock$With.run(Lock.java:109)
	at org.apache.lucene.index.IndexWriter.mergeSegments(IndexWriter.java:501)
	at org.apache.lucene.index.IndexWriter.flushRamSegments(IndexWriter.java:440)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:242)

On the windows platform reading the contents of a file disallows deleting the file.

I use Lucene to maintain an index of +-700.000 documents, one server adds documents, while other servers handle the searches.
The search servers poll the index version regularly to check if they have to reopen their IndexSearcher.
Once in a while (about once every two days on average), IndexWriter.close() fails because it cannot delete the previous segments file, even though it hold the commit lock.
The reason is probably that search servers are reading the segments file to check the version without using the commit lock.
"
"LUCENE-2440","RFE","IMPROVEMENT","Add support for custom ExecutorServices in ParallelMultiSearcher","Right now, the ParallelMultiSearcher uses a cachedThreadPool, which is limitless and a poor choice for a web application, given the threaded nature of the requests (say a webapp with tomcat-default 200 threads and 100 indexes could be looking at 2000 searching threads pretty easily).  Support for adding a custom ExecutorService is pretty trivial.  Patch forthcoming."
"LUCENE-3811","CLEANUP","BUG","remove unused benchmark dependencies","Benchmark has a huge number of jar files in its lib/ (some of which even have different versions than the same libs used in e.g. solr)

But the worst thing is, most of these it doesn't even use.
* commons-collection: unused
* commons-beanutils: unused
* commons-logging: unused
* commons-digetser: unused

"
"LUCENE-2416","IMPROVEMENT","IMPROVEMENT","Some improvements to Benchmark","I've noticed that WriteLineDocTask declares it does not support multi-threading, but taking a closer look I think this is really for no good reason. Most of the work is done by reading from the ContentSource and constructing the document. If those two are mult-threaded (and I think all ContentSources are), then we can synchronize only around writing the actual document to the line file.

While investigating that, I've noticed some 1.5 TODOs and some other minor improvements that can be made. If you've wanted to make some minor improvements to benchmark, let me know :). I intend to include only minor and trivial ones."
"LUCENE-3894","IMPROVEMENT","IMPROVEMENT","Make BaseTokenStreamTestCase a bit more evil","Throw an exception from the Reader while tokenizing, stop after not consuming all tokens, sometimes spoon-feed chars from the reader..."
"LUCENE-1709","TEST","IMPROVEMENT","Parallelize Tests","The Lucene tests can be parallelized to make for a faster testing system.  

This task from ANT can be used: http://ant.apache.org/manual/CoreTasks/parallel.html

Previous discussion: http://www.gossamer-threads.com/lists/lucene/java-dev/69669

Notes from Mike M.:
{quote}
I'd love to see a clean solution here (the tests are embarrassingly
parallelizable, and we all have machines with good concurrency these
days)... I have a rather hacked up solution now, that uses
""-Dtestpackage=XXX"" to split the tests up.

Ideally I would be able to say ""use N threads"" and it'd do the right
thing... like the -j flag to make.
{quote}"
"LUCENE-367","OTHER","BUG","Can not subscribe","Hello, 
I have sent email to lucene-dev-subscribe@jakarta.apache.org and it always 
returns failed: 
 
<lucene-dev-subscribe@jakarta.apache.org>: 
Sorry, no mailbox here by that name. (#5.1.1) 
 
Please help me subscribe."
"LUCENE-2402","RFE","IMPROVEMENT","Add an explicit method to invoke IndexDeletionPolicy","Today, if one uses an IDP which holds onto segments, such as SnapshotDeletionPolicy, or any other IDP in the tests, those segments are left in the index even if the IDP no longer references them, until IW.commit() is called (and actually does something). I'd like to add a specific method to IW which will invoke the IDP's logic and get rid of the unused segments w/o forcing the user to call IW.commit(). There are a couple of reasons for that:

* Segments take up sometimes valuable HD space, and the application may wish to reclaim that space immediately. In some scenarios, the index is updated once in several hours (or even days), and waiting until then may not be acceptable.
* I think it's a cleaner solution than waiting for the next commit() to happen. One can still wait for it if one wants, but otherwise it will give you the ability to immediately get rid of those segments.
* TestSnapshotDeletionPolicy includes this code, which only strengthens (IMO) the need for such method:
{code}
// Add one more document to force writer to commit a
// final segment, so deletion policy has a chance to
// delete again:
Document doc = new Document();
doc.add(new Field(""content"", ""aaa"", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
writer.addDocument(doc);
{code}

If IW had an explicit method, that code would not need to exist there at all ...

Here comes the fun part - naming the baby:
* invokeDeletionPolicy -- describes exactly what is going to happen. However, if the user did not set IDP at all (relying on default, which I think many do), users won't understand what is it.
* deleteUnusedSegments - more user-friendly, assuming users understand what 'segments' are.

BTW, IW already has deleteUnusedFiles() which only tries to delete unreferenced files that failed to delete before (such as on Windows, due to e.g. open readers). Perhaps instead of inventing a new name, we can change IW.deleteUnusedFiles to call IndexFileDeleter.checkpoint (instead of deletePendingFiles) which deletes those files + calls IDP.onCommit()."
"LUCENE-1862","DOCUMENTATION","BUG","duplicate package.html files in queryParser and analsysis.cn packages","These files conflict with eachother when building the javadocs. there can be only one (of each) ...

{code}
hossman@brunner:~/lucene/java$ find src contrib -name package.html | perl -ple 's{.*src/java/}{}' | sort | uniq -c | grep -v "" 1 ""
   2 org/apache/lucene/analysis/cn/package.html
   2 org/apache/lucene/queryParser/package.html
hossman@brunner:~/lucene/java$ find src contrib -path \*queryParser/package.html
src/java/org/apache/lucene/queryParser/package.html
contrib/queryparser/src/java/org/apache/lucene/queryParser/package.html
hossman@brunner:~/lucene/java$ find src contrib -path \*cn/package.html
contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/package.html
contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/package.html
{code}

"
"LUCENE-3681","IMPROVEMENT","IMPROVEMENT","FST.BYTE2 should save as fixed 2 byte not as vInt","We currently write BYTE1 as a single byte, but BYTE2/4 as vInt, but I think that's confusing.  Also, for the FST for the new Kuromoji analyzer (LUCENE-3305), writing as 2 bytes instead shrank the FST and ran faster, presumably because more values were >= 16384 than were < 128.

Separately the whole INPUT_TYPE is very confusing... really all it's doing is ""declaring"" the allowed range of the characters of the input alphabet, and then the only thing that uses that is the write/readLabel methods (well and some confusing sugar methods in Builder!).  Not sure how to fix that yet...

It's a simple change but it changes the FST binary format so any users w/ FSTs out there will have to rebuild (FST is marked experimental...).
"
"LUCENE-706","DOCUMENTATION","IMPROVEMENT","Index File Format - Example for frequency file .frq is wrong","Reported by Johan Stuyts - http://www.nabble.com/Possible-documentation-error--p7012445.html - 

Frequency file example says: 

     For example, the TermFreqs for a term which occurs once in document seven and three times in document eleven would be the following sequence of VInts: 
         15, 22, 3 

It should be: 

     For example, the TermFreqs for a term which occurs once in document seven and three times in document eleven would be the following sequence of VInts: 
         15, 8, 3 


"
"LUCENE-2611","BUILD_SYSTEM","RFE","IntelliJ IDEA and Eclipse setup","Setting up Lucene/Solr in IntelliJ IDEA or Eclipse can be time-consuming.

The attached patches add a new top level directory {{dev-tools/}} with sub-dirs {{idea/}} and {{eclipse/}} containing basic setup files for trunk, as well as top-level ant targets named ""idea"" and ""eclipse"" that copy these files into the proper locations.  This arrangement avoids the messiness attendant to in-place project configuration files directly checked into source control.

The IDEA configuration includes modules for Lucene and Solr, each Lucene and Solr contrib, and each analysis module.  A JUnit run configuration per module is included.

The Eclipse configuration includes a source entry for each source/test/resource location and classpath setup: a library entry for each jar.

For IDEA, once {{ant idea}} has been run, the only configuration that must be performed manually is configuring the project-level JDK.  For Eclipse, once {{ant eclipse}} has been run, the user has to refresh the project (right-click on the project and choose Refresh).

If these patches is committed, Subversion svn:ignore properties should be added/modified to ignore the destination IDEA and Eclipse configuration locations.

Iam Jambour has written up on the Lucene wiki a detailed set of instructions for applying the 3.X branch patch for IDEA: http://wiki.apache.org/lucene-java/HowtoConfigureIntelliJ"
"LUCENE-1995","BUG","BUG","ArrayIndexOutOfBoundsException during indexing","http://search.lucidimagination.com/search/document/f29fc52348ab9b63/arrayindexoutofboundsexception_during_indexing"
"LUCENE-3913","BUG","BUG","HTMLStripCharFilter produces invalid final offset","Nightly build found this... I boiled it down to a small test case that doesn't require the big line file docs."
"LUCENE-3328","RFE","IMPROVEMENT","Specialize BooleanQuery if all clauses are TermQueries","During work on LUCENE-3319 I ran into issues with BooleanQuery compared to PhraseQuery in the exact case. If I disable scoring on PhraseQuery and bypass the position matching, essentially doing a conjunction match, ExactPhraseScorer beats plain boolean scorer by 40% which is a sizeable gain. I converted a ConjunctionScorer to use DocsEnum directly but still didn't get all the 40% from PhraseQuery. Yet, it turned out with further optimizations this gets very close to PhraseQuery. The biggest gain here came from converting the hand crafted loop in ConjunctionScorer#doNext to a for loop which seems to be less confusing to hotspot. In this particular case I think code specialization makes lots of sense since BQ with TQ is by far one of the most common queries.

I will upload a patch shortly"
"LUCENE-1739","BUG","BUG","2.4.x index cannot be opened with 2.9-dev","Sorry for the lack of proper testcase.

In 2.4.1, if you created an index with the (stupid) options below, then it will not create a .prx file. 2.9 expects this file and will not open the index.
The reason i used these stupid options is because i changed the field from indexed=yes to indexed=no, but forgot to remove the .setOmitTf()

{code}
public class Testcase {
	public static void main(String args[]) throws Exception {
		/* run this part with lucene 2.4.1 */
		IndexWriter iw = new IndexWriter(""test"", new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
		iw.setUseCompoundFile(false);
		Document doc = new Document();
		Field field1 = new Field(""field1"", ""foo"", Field.Store.YES, Field.Index.NO);
		field1.setOmitTf(true); // 2.9 will create a 0-byte .prx file, but 2.4.x will NOT. This is the problem. 2.9 expects this file!
		doc.add(field1);
		iw.addDocument(doc);
		iw.close(); 
		/* run this with lucene 2.9 */
		IndexReader ir = IndexReader.open(FSDirectory.getDirectory(""test""), true); 
	}
}
{code}"
"LUCENE-2004","BUG","BUG","Constants.LUCENE_MAIN_VERSION is inlined in code compiled against Lucene JAR, so version detection is incorrect","When you compile your own code against the Lucene 2.9 version of the JARs and use the LUCENE_MAIN_VERSION constant and then run the code against the 3.0 JAR, the constant still contains 2.9, because javac inlines primitives and Strings into the class files if they are public static final and are generated by a constant (not method).

The attached fix will fix this by using a ident(String) functions that return the String itsself to prevent this inlining.

Will apply to 2.9, trunk and 2.9 BW branch. No I can also reenable one test I removed because of this."
"LUCENE-2773","IMPROVEMENT","IMPROVEMENT","Don't create compound file for large segments by default","Spinoff from LUCENE-2762.

CFS is useful for keeping the open file count down.  But, it costs
some added time during indexing to build, and also ties up temporary
disk space, causing eg a large spike on the final merge of an
optimize.

Since MergePolicy dictates which segments should be CFS, we can
change it to only build CFS for ""smallish"" merges.

I think we should also set a maxMergeMB by default so that very large
merges aren't done.
"
"LUCENE-1851","BUILD_SYSTEM","IMPROVEMENT","'ant javacc' in root project should also properly create contrib/surround Java files","For consistency after LUCENE-1829 which did the same for contrib/queryparser"
"LUCENE-2198","RFE","IMPROVEMENT","support protected words in Stemming TokenFilters","This is from LUCENE-1515

I propose that all stemming TokenFilters have an 'exclusion set' that bypasses any stemming for words in this set.
Some stemming tokenfilters have this, some do not.

This would be one way for Karl to implement his new swedish stemmer (as a text file of ignore words).
Additionally, it would remove duplication between lucene and solr, as they reimplement snowballfilter since it does not have this functionality.
Finally, I think this is a pretty common use case, where people want to ignore things like proper nouns in the stemming.

As an alternative design I considered a case where we generalized this to CharArrayMap (and ignoring words would mean mapping them to themselves), which would also provide a mechanism to override the stemming algorithm. But I think this is too expert, could be its own filter, and the only example of this i can find is in the Dutch stemmer.

So I think we should just provide ignore with CharArraySet, but if you feel otherwise please comment.
"
"LUCENE-572","BUG","BUG","SpanNotQuery.hashCode ignores exclude","filing as bug for tracking/refrence...

On May 16, 2006, at 3:33 AM, Chris Hostetter wrote:

> SpanNodeQuery's hashCode method makes two refrences to  
> include.hashCode(),
> but none to exclude.hashCode() ... this is a mistake yes/no?

Date: Tue, 16 May 2006 05:57:15 -0400
From: Erik Hatcher
To: java-dev@lucene.apache.org
Subject: Re: SpanNotQuery.hashCode cut/paste error?

Yes, this is a mistake.  I'm happy to fix it, but looks like you have  
other patches in progress.

"
"LUCENE-1808","RFE","IMPROVEMENT","make Query.createWeight public (or add back Query.createQueryWeight())","Now that the QueryWeight class has been removed, the public QueryWeight createQueryWeight() method on Query was also removed

i have cases where i want to create a weight for a sub query (outside of the org.apache.lucene.search package) and i don't want the weight normalized (think BooleanQuery outside of the o.a.l.search package)

in order to do this, i have to create a static Utils class inside o.a.l.search, pass in the Query and searcher, and have the static method call the protected createWeight method
this should not be necessary

This could be fixed in one of 2 ways:
1. make createWeight() public on Query (breaks back compat)
2. add the following method:
{code}
public Weight createQueryWeight(Searcher searcher) throws IOException {
  return createWeight(searcher);
}
{code}

createWeight(Searcher) should then be deprectated in favor of the publicly accessible method
"
"LUCENE-516","TEST","BUG","TestFSDirectory fails on Windows","""ant test"" generates the following error consistently when run on a Windows machine even when run as user with Administrator privileges

    [junit] Testcase: testTmpDirIsPlainFile(org.apache.lucene.index.store.TestFSDirectory):     Caused an ERROR
    [junit] Access is denied
    [junit] java.io.IOException: Access is denied
    [junit]     at java.io.WinNTFileSystem.createFileExclusively(Native Method)
    [junit]     at java.io.File.createNewFile(File.java:828)
    [junit]     at org.apache.lucene.index.store.TestFSDirectory.testTmpDirIsPlainFile(TestFSDirectory.java:66)"
"LUCENE-3631","RFE","TASK","Remove write access from SegmentReader and possibly move to separate class or IndexWriter/BufferedDeletes/...","After LUCENE-3606 is finished, there are some TODOs:

SegmentReader still contains (package-private) all delete logic including crazy copyOnWrite for validDocs Bits. It would be good, if SegmentReader itsself could be read-only like all other IndexReaders.

There are two possibilities to do this:
# the simple one: Subclass SegmentReader and make a RWSegmentReader that is only used by IndexWriter/BufferedDeletes/... DirectoryReader will only use the read-only SegmentReader. This would move all TODOs to a separate class. It's reopen/clone method would always create a RO-SegmentReader (for NRT).
# Remove all write and commit stuff from SegmentReader completely and move it to IndexWriter's readerPool (it must be in readerPool as deletions need a not-changing view on an index snapshot).

Unfortunately the code is so complicated and I have no real experience in those internals of IndexWriter so I did not want to do it with LUCENE-3606, I just separated the code in SegmentReader and marked with TODO. Maybe Mike McCandless can help :-)"
"LUCENE-1412","BUG","BUG","FileNotFoundException thrown by Directory.copy()","java.io.FileNotFoundException: segments_bu
        at org.apache.lucene.store.RAMDirectory.openInput(RAMDirectory.java:234)
        at org.apache.lucene.store.Directory.copy(Directory.java:190)

"
"LUCENE-669","BUG","BUG","finalize()-methods of FSDirectory.FSIndexInput and FSDirectory.FSIndexOutput try to close already closed file","Hi all,

I found a small problem in FSDirectory: The finalize()-methods of FSDirectory.FSIndexInput and FSDirectory.FSIndexOutput try to close the underlying file. This is not a problem unless the file has been closed before by calling the close() method. If it has been closed before, the finalize method throws an IOException saying that the file is already closed. Usually this IOException would go unnoticed, because the GarbageCollector, which calls finalize(), just eats it. However, if I use the Eclipse debugger the execution of my code will always be suspended when this exception is thrown.

Even though this exception probably won't cause problems during normal execution of Lucene, the code becomes cleaner if we apply this small patch. Might this IOException also have a performance impact, if it is thrown very frequently?

I attached the patch which applies cleanly on the current svn HEAD. All testcases pass and I verfied with the Eclipse debugger that the IOException is not longer thrown."
"LUCENE-2415","DESIGN_DEFECT","IMPROVEMENT","Remove JakarteRegExCapabilities shim to access package protected field","To access the prefix in Jakarta RegExes we use a shim class in the same package as jakarta. I will remove this and replace by reflection like Robert does in his ICUTokenizer rule compiler.

Shim classes have the problem wth signed artifacts, as you cannot insert a new class into a foreign package if you sign regex classes.

This shim-removal also allows users to use later jakarta regex versions, if they are in classpath and cannot be removed (even if they have bugs). Performance is no problem, as the prefix is only get once per TermEnum."
"LUCENE-3027","BUG","BUG","TestOmitTf.testMixedMerge random seed failure","Version: trunk r1091638

ant test -Dtests.seed=-6595054217575280191:5576532348905930588


    [junit] ------------- Standard Error -----------------
    [junit] WARNING: test method: 'testDeMorgan' left thread running: Thread[NRT search threads-1691-thread-2,5,main]
    [junit] RESOURCE LEAK: test method: 'testDeMorgan' left 1 thread(s) running
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestBooleanQuery -Dtestmethod=testDeMorgan -Dtests.seed=-6595054217575280191:5576532348905930588
    [junit] ------------- ---------------- ---------------
    [junit] Testsuite: org.apache.lucene.index.TestNorms
    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 5.064 sec
    [junit] 
    [junit] Testsuite: org.apache.lucene.index.TestOmitTf
    [junit] Testcase: testMixedMerge(org.apache.lucene.index.TestOmitTf):	Caused an ERROR
    [junit] CheckIndex failed
    [junit] java.lang.RuntimeException: CheckIndex failed
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:152)
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:138)
    [junit] 	at org.apache.lucene.index.TestOmitTf.testMixedMerge(TestOmitTf.java:155)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1232)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1160)
    [junit] 
    [junit] 
    [junit] Tests run: 5, Failures: 0, Errors: 1, Time elapsed: 0.851 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] CheckIndex failed
    [junit] Segments file=segments_1 numSegments=1 version=FORMAT_4_0 [Lucene 4.0]
    [junit]   1 of 1: name=_12 docCount=60
    [junit]     codec=SegmentCodecs [codecs=[MockRandom, MockVariableIntBlock(baseBlockSize=112)], provider=RandomCodecProvider: {f1=MockRandom, f2=MockVariableIntBlock(baseBlockSize=112)}]
    [junit]     compound=false
    [junit]     hasProx=false
    [junit]     numFiles=16
    [junit]     size (MB)=0,01
    [junit]     diagnostics = {optimize=true, mergeFactor=2, os.version=2.6.37-gentoo, os=Linux, lucene.version=4.0-SNAPSHOT, source=merge, os.arch=amd64, java.version=1.6.0_24, java.vendor=Sun Microsystems Inc.}
    [junit]     no deletions
    [junit]     test: open reader.........OK
    [junit]     test: fields..............OK [2 fields]
    [junit]     test: field norms.........OK [2 fields]
    [junit]     test: terms, freq, prox...ERROR: java.io.IOException: Read past EOF
    [junit] java.io.IOException: Read past EOF
    [junit] 	at org.apache.lucene.store.RAMInputStream.switchCurrentBuffer(RAMInputStream.java:90)
    [junit] 	at org.apache.lucene.store.RAMInputStream.readByte(RAMInputStream.java:63)
    [junit] 	at org.apache.lucene.store.MockIndexInputWrapper.readByte(MockIndexInputWrapper.java:105)
    [junit] 	at org.apache.lucene.store.DataInput.readVInt(DataInput.java:94)
    [junit] 	at org.apache.lucene.index.codecs.sep.SepSkipListReader.readSkipData(SepSkipListReader.java:188)
    [junit] 	at org.apache.lucene.index.codecs.MultiLevelSkipListReader.loadNextSkip(MultiLevelSkipListReader.java:142)
    [junit] 	at org.apache.lucene.index.codecs.MultiLevelSkipListReader.skipTo(MultiLevelSkipListReader.java:112)
    [junit] 	at org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl$SepDocsEnum.advance(SepPostingsReaderImpl.java:454)
    [junit] 	at org.apache.lucene.index.CheckIndex.testTermIndex(CheckIndex.java:782)
    [junit] 	at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:495)
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:148)
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:138)
    [junit] 	at org.apache.lucene.index.TestOmitTf.testMixedMerge(TestOmitTf.java:155)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit] 	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1232)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1160)
    [junit] 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit] 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit] 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:758)
    [junit]     test: stored fields.......OK [60 total field count; avg 1 fields per doc]
    [junit]     test: term vectors........OK [120 total vector count; avg 2 term/freq vector fields per doc]
    [junit] FAILED
    [junit]     WARNING: fixIndex() would remove reference to this segment; full exception:
    [junit] java.lang.RuntimeException: Term Index test failed
    [junit] 	at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:508)
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:148)
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:138)
    [junit] 	at org.apache.lucene.index.TestOmitTf.testMixedMerge(TestOmitTf.java:155)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit] 	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1232)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1160)
    [junit] 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit] 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit] 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:758)
    [junit] 
    [junit] WARNING: 1 broken segments (containing 60 documents) detected
    [junit] 
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestOmitTf -Dtestmethod=testMixedMerge -Dtests.seed=-6595054217575280191:5576532348905930588
    [junit] NOTE: test params are: codec=RandomCodecProvider: {noTf=MockSep, tf=Standard, f1=MockRandom, f2=MockVariableIntBlock(baseBlockSize=112)}, locale=cs_CZ, timezone=Chile/Continental
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestAssertions, TestCachingTokenFilter, TestDocument, TestDirectoryReader, TestFlex, TestIndexWriterConfig, TestIndexWriterMerging, TestIndexWriterOnJRECrash, TestMultiReader, TestNewestSegment, TestNorms, TestOmitTf]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=94021800,total=126484480
    [junit] ------------- ---------------- ---------------"
"LUCENE-563","DOCUMENTATION","IMPROVEMENT","IndexReader currently has javadoc errors","Current trunk has some javadoc errors in IndexReader and some more in contrib.
"
"LUCENE-3580","DESIGN_DEFECT","BUG","Most 4.0 PostingsReaders don't obey DISI contract","While trying to do some refactoring, I noticed funky things going on with some codecs.

One problem is that DocIdSetIterator says the following:
{noformat}
Returns the following:
   * <ul>
   * <li>-1 or {@link #NO_MORE_DOCS} if {@link #nextDoc()} or
   * {@link #advance(int)} were not called yet.
{noformat}

But most 4.0 Docs/DocsAndPositionsEnums don't actually do this (e.g. return 0). instead we 
are relying on Scorers to 'cover' for them, which is inconsistent. Some scorers actually rely
upon this behavior, for example look at ReqExclScorer.toNonExcluded(), it calls docId() on the
excluded part before it calls nextDoc()

So we need to either fix these enums, change these enums to not extend DocIdSetIterator (and redefine
what the actual contract should be for these enums), change DocIdSetIterator, or something else.

Fixing the enums to return -1 here when they are uninitialized kinda sucks for the ones summing up
document deltas...
"
"LUCENE-2245","CLEANUP","TASK","Remaining contrib testcases should use Version based ctors instead of deprecated ones","Many testcases in contrib use deprecated ctors for WhitespaceTokenizer / Analyzer etc."
"LUCENE-1838","DESIGN_DEFECT","BUG","BoostingNearQuery must implement clone - now it clones to a NearSpanQuery - cloning is required for Span container classes",""
"LUCENE-2090","IMPROVEMENT","IMPROVEMENT","convert automaton to char[] based processing and TermRef / TermsEnum api","The automaton processing is currently done with String, mostly because TermEnum is based on String.
it is easy to change the processing to work with char[], since behind the scenes this is used anyway.

in general I think we should make sure char[] based processing is exposed in the automaton pkg anyway, for things like pattern-based tokenizers and such.
"
"LUCENE-415","BUG","BUG","Merge error during add to index (IndexOutOfBoundsException)","I've been batch-building indexes, and I've build a couple hundred indexes with 
a total of around 150 million records.  This only happened once, so it's 
probably impossible to reproduce, but anyway... I was building an index with 
around 9.6 million records, and towards the end I got this:

java.lang.IndexOutOfBoundsException: Index: 54, Size: 24
        at java.util.ArrayList.RangeCheck(ArrayList.java:547)
        at java.util.ArrayList.get(ArrayList.java:322)
        at org.apache.lucene.index.FieldInfos.fieldInfo(FieldInfos.java:155)
        at org.apache.lucene.index.FieldInfos.fieldName(FieldInfos.java:151)
        at org.apache.lucene.index.SegmentTermEnum.readTerm(SegmentTermEnum.java
:149)
        at org.apache.lucene.index.SegmentTermEnum.next
(SegmentTermEnum.java:115)
        at org.apache.lucene.index.SegmentMergeInfo.next
(SegmentMergeInfo.java:52)
        at org.apache.lucene.index.SegmentMerger.mergeTermInfos
(SegmentMerger.java:294)
        at org.apache.lucene.index.SegmentMerger.mergeTerms
(SegmentMerger.java:254)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:93)
        at org.apache.lucene.index.IndexWriter.mergeSegments
(IndexWriter.java:487)
        at org.apache.lucene.index.IndexWriter.maybeMergeSegments
(IndexWriter.java:458)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:310)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:294)"
"LUCENE-3583","TEST","BUG","benchmark tests always fail on windows because directory cannot be removed","This seems to be a bug recently introduced. I have no idea what's wrong. Attached is a log file, reproduces everytime.

"
"LUCENE-2631","BUG","BUG","Fix small perf issues with String/TermOrdValComparator","Uncovered some silliness when working on LUCENE-2504, eg we are doing unnecessary binarySearch on a single-segment reader."
"LUCENE-1794","RFE","IMPROVEMENT","implement reusableTokenStream for all contrib analyzers","most contrib analyzers do not have an impl for reusableTokenStream

regardless of how expensive the back compat reflection is for indexing speed, I think we should do this to mitigate any performance costs. hey, overall it might even be an improvement!

the back compat code for non-final analyzers is already in place so this is easy money in my opinion."
"LUCENE-2816","IMPROVEMENT","IMPROVEMENT","MMapDirectory speedups","MMapDirectory has some performance problems:
# When the file is larger than Integer.MAX_VALUE, we use MultiMMapIndexInput, 
which does a lot of unnecessary bounds-checks for its buffer-switching etc. 
Instead, like MMapIndexInput, it should rely upon the contract of these operations
in ByteBuffer (which will do a bounds check always and throw BufferUnderflowException).
Our 'buffer' is so large (Integer.MAX_VALUE) that its rare this happens and doing
our own bounds checks just slows things down.
# the readInt()/readLong()/readShort() are slow and should just defer to ByteBuffer.readInt(), etc
This isn't very important since we don't much use these, but I think there's no reason
users (e.g. codec writers) should have to readBytes() + wrap as bytebuffer + get an 
IntBuffer view when readInt() can be almost as fast..."
"LUCENE-3765","RFE","BUG","trappy ignoreCase behavior with StopFilter/ignoreCase","Spinoff from LUCENE-3751:

{code}
* If <code>stopWords</code> is an instance of {@link CharArraySet} (true if
* <code>makeStopSet()</code> was used to construct the set) it will be
* directly used and <code>ignoreCase</code> will be ignored since
* <code>CharArraySet</code> directly controls case sensitivity.
{code}

This is really confusing and trappy... we need to change something here."
"LUCENE-1234","DESIGN_DEFECT","BUG","BoostingTermQuery's BoostingSpanScorer class should be protected instead of package access","Currently, BoostingTermScorer, an inner class of BoostingTermQuery is not accessible from outside the search.payloads
making it difficult to write an extension of BoostingTermQuery. The other inner classes are protected already, as they should be."
"LUCENE-1236","DOCUMENTATION","IMPROVEMENT","EdgeNGram* documentation improvement","To clarify what ""edge"" means, I added some description. That edge means the beggining edge of a term or ending edge of a term."
"LUCENE-3453","CLEANUP","BUG","remove IndexDocValuesField","Its confusing how we present CSF functionality to the user, its actually not a ""field"" but an ""attribute"" of a field like  STORED or INDEXED.

Otherwise, its really hard to think about CSF because there is a mismatch between the APIs and the index format."
"LUCENE-2051","REFACTORING","IMPROVEMENT","Contrib Analyzer Setters should be deprecated and replace with ctor arguments","Some analyzers in contrib provide setters for stopword / stem exclusion sets / hashtables etc. Those setters should be deprecated as they yield unexpected behaviour. The way they work is they set the reusable token stream instance to null in a thread local cache which only affects the tokenstream in the current thread. Analyzers itself should be immutable except of the threadlocal. 

will attach a patch soon."
"LUCENE-948","BUG","BUG","Writers on two machines over NFS can hit FNFE due to stale NFS client caching","Issue spawned from this thread:

  http://www.gossamer-threads.com/lists/lucene/java-user/50680

When IndexFileDeleter lists the directory, looking for segments_X
files to load, if it hits a FNFE on opening such a file it should
catch this and treat it as if the file does not exist.

On NFS (and possibly other file systems), a directory listing is not
guaranteed to be ""current""/coherent.  Specifically, if machine #1 has
just removed file ""segments_n"" and shortly thereafer machine #2 does a
dir listing, it's possible (likely?) that the dir listing will still
show that segments_n exists.

I think the fix is simple: catch the FNFE and just handle it as if the
segments_n does not in fact exist.

"
"LUCENE-1554","BUG","BUG","Problem with IndexWriter.mergeFinish","I'm getting a (very) infrequent assert in IndexWriter.mergeFinish from TestIndexWriter.testAddIndexOnDiskFull. The problem occurs during the rollback when the merge hasn't been registered. I'm not 100% sure this is the correct fix, because it's such an infrequent event. 

{code:java}
  final synchronized void mergeFinish(MergePolicy.OneMerge merge) throws IOException {
    
    // Optimize, addIndexes or finishMerges may be waiting
    // on merges to finish.
    notifyAll();

    if (merge.increfDone)
      decrefMergeSegments(merge);

    assert merge.registerDone;

    final SegmentInfos sourceSegments = merge.segments;
    final int end = sourceSegments.size();
    for(int i=0;i<end;i++)
      mergingSegments.remove(sourceSegments.info(i));
    mergingSegments.remove(merge.info);
    merge.registerDone = false;
  }
{code}

Should  be something like:

{code:java}
  final synchronized void mergeFinish(MergePolicy.OneMerge merge) throws IOException {
    
    // Optimize, addIndexes or finishMerges may be waiting
    // on merges to finish.
    notifyAll();

    if (merge.increfDone)
      decrefMergeSegments(merge);

    if (merge.registerDone) {
      final SegmentInfos sourceSegments = merge.segments;
      final int end = sourceSegments.size();
      for(int i=0;i<end;i++)
        mergingSegments.remove(sourceSegments.info(i));
      mergingSegments.remove(merge.info);
      merge.registerDone = false;
    }
  }
{code}"
"LUCENE-2662","REFACTORING","IMPROVEMENT","BytesHash","This issue will have the BytesHash separated out from LUCENE-2186"
"LUCENE-1379","BUG","BUG","SpanScorer fails when sloppyFreq() returns 0","I think we should fix this for 2.4 (now back to 10)?"
"LUCENE-2885","IMPROVEMENT","IMPROVEMENT","Add WaitForMergesTask","When building an index, if you just .close the IW, you may leave merges still needing to be done... so a WaitForMerges task lets algs fix this."
"LUCENE-349","DOCUMENTATION","BUG","Documentation for tii and tis files seems to be out of sync with code","The documentation on the .tii file in fileformats.xml seems to be out of sync
with the actual code in TermInfosReader.java.

Specifically, the docs for the TermInfosIndex file seems to leave out several
fields that are read from the file in the readIndex() method (well, specifically
they're read in by the SegmentTermEnum constructor, but you get the idea)."
"LUCENE-340","RFE","IMPROVEMENT","[PATCH] Highlighter: Delegate output escaping to Formatter","Patch for jakarta-lucene-sandbox/contributions/highlighter
CVS version 3rd February 2005

This patch allows the highlighter Formatter to control escaping of the non
highlighted text as well as the highlighting of the matching text.

The example formatters highlight the matching text using XML/HTML tags. This
works fine if the plain text does not contain any characters that need to be
escaped for HTML output (i.e. <, &, and ""), however this cannot be guaranteed.
As the formatter controls the method of highlighting (in the examples this is
HTML, but it could be any other form of markup) it should also be responsible
for escaping the rest of the output.

This patch adds a method, encodeText(String), to the Formatter interface. This
is a breaking change. This method is called from the Highlighter with the text
that is not passed to the formatter's highlightTerm method. 
The SimpleHTMLFormatter has a public static method for performing simple HTML
escaping called htmlEncode. 
The SimpleHTMLFormatter, GradientFormatter, and SpanGradientFormatter have been
updated to implement the encodeText method and call the htmlEncode method to
escape the output.

For existing formatter to maintain exactly the same behaviour as before applying
this patch they would need to implement the encodeText method to return the
argument value without modification, e.g.:

public String encodeText(String originalText)
{
  return originalText;
}"
"LUCENE-3268","BUG","BUG","TestScoredDocIDsUtils.testWithDeletions test failure","ant test -Dtestcase=TestScoredDocIDsUtils -Dtestmethod=testWithDeletions -Dtests.seed=-2216133137948616963:2693740419732273624 -Dtests.multiplier=5

In general, on both 3.x and trunk, if you run this test with -Dtests.iter=100 it tends to fail 2% of the time.

"
"LUCENE-1841","DOCUMENTATION","IMPROVEMENT","Provide Summary Information on the Files in the Lucene index","I find myself often having to remember, by file extension, what is in a particular index file.  The information is all contained in the File Formats, but not summarized.  This patch provides a simple table that describes the extensions and provides links to the relevant section."
"LUCENE-2467","BUG","BUG","IndexWriter memory leak when large docs are indexed","Spinoff from the java-user thread ""IndexWriter and memory usage""...

IndexWriter has had a long standing memory leak, since LUCENE-843.

When the byte/char/int blocks are recycled to the common pool, the
per-thread DW classes incorrectly still hold a reference to them.

This normally is not a problem, since these buffers will be re-used
again.

But, if you index a massive document, causing IW to allocate more than
the RAM buffer allocated to it, then the leak happens.  So you could
have a 16 MB RAM buffer set, but if a huge doc required allocation of
200 MB worth of arrays, those 200 MB are never freed (well, until you
close the IW and deref it from the app).

It's even worse if you use multiple threads: if each thread has ever
had to index a massive document, then that thread incorrectly holds
onto the extra arrays.
"
"LUCENE-3600","BUG","BUG","BlockJoinQuery advance fails on an assert in case of a single parent with child segment","The BlockJoinQuery will fail on an assert when advance in called on a segment with a single parent with a child. The call to parentBits.prevSetBit(parentTarget - 1) will cause -1 to be returned, and the assert will fail, though its valid. Just removing the assert fixes the problem, since nextDoc will handle it properly.

Also, I don't understand the ""assert parentTarget != 0;"", with a comment of each parent must have one child. There isn't really a reason to add this constraint, as far as I can tell..., just call nextDoc in this case, no?"
"LUCENE-3400","CLEANUP","","Deprecate / Remove DutchAnalyzer.setStemDictionary","DutchAnalyzer.setStemDictionary(File) prevents reuse of TokenStreams (and also uses a File which isn't ideal).  It should be deprecated in 3x, removed in trunk."
"LUCENE-1593","IMPROVEMENT","IMPROVEMENT","Optimizations to TopScoreDocCollector and TopFieldCollector","This is a spin-off of LUCENE-1575 and proposes to optimize TSDC and TFC code to remove unnecessary checks. The plan is:
# Ensure that IndexSearcher returns segements in increasing doc Id order, instead of numDocs().
# Change TSDC and TFC's code to not use the doc id as a tie breaker. New docs will always have larger ids and therefore cannot compete.
# Pre-populate HitQueue with sentinel values in TSDC (score = Float.NEG_INF) and remove the check if reusableSD == null.
# Also move to use ""changing top"" and then call adjustTop(), in case we update the queue.
# some methods in Sort explicitly add SortField.FIELD_DOC as a ""tie breaker"" for the last SortField. But, doing so should not be necessary (since we already break ties by docID), and is in fact less efficient (once the above optimization is in).
# Investigate PQ - can we deprecate insert() and have only insertWithOverflow()? Add a addDummyObjects method which will populate the queue without ""arranging"" it, just store the objects in the array (this can be used to pre-populate sentinel values)?

I will post a patch as well as some perf measurements as soon as I have them."
"LUCENE-3462","BUG","BUG","Jenkins builds hang quite often in TestIndexWriterWithThreads.testCloseWithThreads","Last hung test run: [https://builds.apache.org/job/Lucene-Solr-tests-only-trunk/10638/console]

{noformat}
[junit] ""main"" prio=5 tid=0x0000000801ef3800 nid=0x1965c waiting on condition [0x00007fffffbfd000]
[junit]    java.lang.Thread.State: WAITING (parking)
[junit] 	at sun.misc.Unsafe.park(Native Method)
[junit] 	- parking to wait for  <0x0000000825d853a8> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
[junit] 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
[junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:838)
[junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:871)
[junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1201)
[junit] 	at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
[junit] 	at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
[junit] 	at org.apache.lucene.index.DocumentsWriterFlushControl.markForFullFlush(DocumentsWriterFlushControl.java:403)
[junit] 	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:557)
[junit] 	- locked <0x0000000825d81998> (a org.apache.lucene.index.DocumentsWriter)
[junit] 	at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2776)
[junit] 	- locked <0x0000000825d7d840> (a java.lang.Object)
[junit] 	at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2904)
[junit] 	- locked <0x0000000825d7d830> (a java.lang.Object)
[junit] 	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1156)
[junit] 	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1099)
[junit] 	at org.apache.lucene.index.TestIndexWriterWithThreads.testCloseWithThreads(TestIndexWriterWithThreads.java:200)
[junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
[junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[junit] 	at java.lang.reflect.Method.invoke(Method.java:616)
[junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
[junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
[junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
[junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
[junit] 	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
[junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
[junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
[junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
[junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
[junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
[junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
[junit] 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
[junit] 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
[junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
[junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
[junit] 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
[junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
[junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
[junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
[junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
[junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
[junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
[junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
{noformat}"
"LUCENE-1905","BUG","BUG","Instantiating SimpleFSLockFactory by its String param constructor throws an IllegalStateException",""
"LUCENE-2311","RFE","IMPROVEMENT","Pass potent SR to IRWarmer.warm(), and also call warm() for new segments","Currently warm() receives a SegmentReader without terms index and docstores.
It would be arguably more useful for the app to receive a fully loaded reader, so it can actually fire up some caches. If the warmer is undefined on IW, we probably leave things as they are.

It is also arguably more concise and clear to call warm() on all newly created segments, so there is a single point of warming readers in NRT context, and every subreader coming from getReader is guaranteed to be warmed up -> you don't have to introduce even more mess in your code by rechecking it.

"
"LUCENE-635","REFACTORING","IMPROVEMENT","[PATCH] Decouple locking implementation from Directory implementation","This is a spinoff of http://issues.apache.org/jira/browse/LUCENE-305.

I've opened this new issue to capture that it's wider scope than
LUCENE-305.

This is a patch originally created by Jeff Patterson (see above link)
and then modified as described here:

  http://issues.apache.org/jira/browse/LUCENE-305#action_12418493

with some small additional changes:

  * For each FSDirectory.getDirectory(), I made a corresponding
    version that also accepts a LockFactory instance.  So, you can
    construct an FSDirectory with your own LockFactory.

  * Cascaded defaulting for FSDirectory's LockFactory implementation:
    if you pass in a LockFactory instance, it's used; else if
    setDisableLocks was called, we use NoLockFactory; else, if the
    system property ""org.apache.lucene.store.FSDirectoryLockFactoryClass""
    is defined, we use that; finally, we'll use the original locking
    implementation (SimpleFSLockFactory).

The gist is that all locking code has been moved out of *Directory and
into subclasses of a new abstract LockFactory class.  You can now set
the LockFactory of a Directory to change how it does locking.  For
example, you can create an FSDirectory but set its locking to
SingleInstanceLockFactory (if you know all writing/reading will take
place a single JVM).

The changes pass all unit tests (on Ubuntu Linux Sun Java 1.5 and
Windows XP Sun Java 1.4), and I added another TestCase to test the
LockFactory code.

Note that LockFactory defaults are not changed: FSDirectory defaults
to SimpleFSLockFactory and RAMDirectory defaults to
SingleInstanceLockFactory.

Next step (separate issue) is to create a LockFactory that uses the OS
native locks (through java.nio).
"
"LUCENE-1902","DOCUMENTATION","BUG","Changes.html not explicitly included in release","None of the release related ant targets explicitly call cahnges-to-html ... this seems like an oversight.  (currently it's only called as part of the nightly target)

"
"LUCENE-2111","CLEANUP","IMPROVEMENT","Wrapup flexible indexing","Spinoff from LUCENE-1458.

The flex branch is in fairly good shape -- all tests pass, initial search performance testing looks good, it survived several visits from the Unicode policeman ;)

But it still has a number of nocommits, could use some more scrutiny especially on the ""emulate old API on flex index"" and vice/versa code paths, and still needs some more performance testing.  I'll do these under this issue, and we should open separate issues for other self contained fixes.

The end is in sight!"
"LUCENE-2092","BUG","BUG","BooleanQuery.hashCode and equals ignore isCoordDisabled","BooleanQuery.isCoordDisabled() is not considered by BooleanQuery's hashCode() or equals() methods ... this can cause serious badness to happen when caching BooleanQueries.

bug traces back to at least 1.9"
"LUCENE-1727","IMPROVEMENT","BUG","Order of stored Fields not maintained","As noted in these threads...

http://www.nabble.com/Order-of-fields-returned-by-Document.getFields%28%29-to21034652.html
http://www.nabble.com/Order-of-fields-within-a-Document-in-Lucene-2.4%2B-to24210597.html

somewhere prior to Lucene 2.4.1 a change was introduced that prevents the Stored fields of a Document from being returned in same order that they were originally added in.  This can cause serious performance problems for people attempting to use LoadFirstFieldSelector or a custom FieldSelector with the LOAD_AND_BREAK, or the SIZE_AND_BREAK options (since the fields don't come back in the order they expect)

Speculation in the email threads is that the origin of this bug is code introduced by LUCENE-1301 -- but the purpose of that issue was refactoring, so if it really is the cause of the change this would seem to be a bug, and not a side affect of a conscious implementation change.

Someone who understands indexing internals should investigate this.  At a minimum, if it's decided that this is not actual a bug, then prior to resolving this bug the wiki docs and some of the FIeldSelector javadocs should be updated to make it clear what order Fields will be returned in.

"
"LUCENE-3615","TEST","IMPROVEMENT","Make it easier to run Test2BTerms","Currently, Test2BTerms has an @Ignore annotation which means that the only way to run it as a test is to edit the file.

There are a couple of options to fix this:
# Add a main() so it can be invoked via the command line outside of the test framework
# Add some new annotations that mark it as slow or weekly or something like that and have the test target ignore @slow (or whatever) by default, but can also turn it on."
"LUCENE-1992","BUG","BUG","intermittent failure in TestIndexWriter. testExceptionDuringSync ","{code}
common.test:

    [mkdir] Created dir: C:\Projects\lucene\trunk-full1\build\test

    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter

    [junit] Tests run: 102, Failures: 0, Errors: 1, Time elapsed: 100,297sec

    [junit]

    [junit] Testcase: testExceptionDuringSync(org.apache.lucene.index.TestIndexWriter): Caused an ERROR

    [junit] _a.fnm

    [junit] java.io.FileNotFoundException: _a.fnm

    [junit]     at org.apache.lucene.store.MockRAMDirectory.openInput(MockRAMDirectory.java:226)

    [junit]     at org.apache.lucene.index.FieldInfos.<init>(FieldInfos.java:68)

    [junit]     at org.apache.lucene.index.SegmentReader$CoreReaders.<init>(SegmentReader.java:116)

    [junit]     at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:620)

    [junit]     at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:590)

    [junit]     at org.apache.lucene.index.DirectoryReader.<init>(DirectoryReader.java:104)

    [junit]     at org.apache.lucene.index.ReadOnlyDirectoryReader.<init>(ReadOnlyDirectoryReader.java:27)

    [junit]     at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:74)

    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:704)

    [junit]     at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:69)

    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:307)

    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:193)

    [junit]     at org.apache.lucene.index.TestIndexWriter.testExceptionDuringSync(TestIndexWriter.java:2723)

    [junit]     at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:206)

    [junit]

    [junit]

    [junit] Test org.apache.lucene.index.TestIndexWriter FAILED
{code}"
"LUCENE-3184","TEST","TEST","add LuceneTestCase.rarely()/LuceneTestCase.atLeast()","in LUCENE-3175, the tests were sped up a lot by using reasonable number of iterations normally, but cranking up for NIGHTLY.
we also do crazy things more 'rarely' for normal builds (e.g. simpletext, payloads, crazy merge params, etc)
also, we found some bugs by doing this, because in general our parameters are too fixed.

however, it made the code look messy... I propose some new methods:
instead of some crazy code in your test like:
{code}
int numdocs = (TEST_NIGHTLY ? 1000 : 100) * RANDOM_MULTIPLIER;
{code}

you use:
{code}
int numdocs = atLeast(100);
{code}

this will apply the multiplier, also factor in nightly, and finally add some random fudge... so e.g. in local runs its sometimes 127 docs, sometimes 113 docs, etc.

additionally instead of code like:
{code}
if ((TEST_NIGHTLY && random.nextBoolean()) || (random.nextInt(20) == 17)) {
{code}

you do
{code}
if (rarely()) {
{code}

which applies NIGHTLY and also the multiplier (logarithmic growth).
"
"LUCENE-1663","DOCUMENTATION","IMPROVEMENT","Documentation bug.  The 2.4.1 query parser syntax wiki page says it is for 1.9","This page:

http://lucene.apache.org/java/2_4_1/queryparsersyntax.html

says this:
.bq
This page provides the Query Parser syntax in Lucene 1.9. If you are using a different version of Lucene, please consult the copy of docs/queryparsersyntax.html that was distributed with the version you are using. 

This is misleading on a doc page for 2.4.1"
"LUCENE-3625","RFE","TASK","FieldValueFitler should expose the field it uses","FieldValueFitler should expose the field it uses. It currently hides this entirely."
"LUCENE-2975","BUG","BUG","hotspot bug in readvint gives wrong results","When testing the 3.1-RC1 made by Yonik on the PANGAEA (www.pangaea.de) productive system I figured out that suddenly on a large segment (about 5 GiB) some stored fiels suddenly produce a strange deflate decompression problem (CompressionTools) although the stored fields are no longer pre-3.0 compressed. It seems that the header of the stored field is read incorrectly at the buffer boundary in MultiMMapDir and then FieldsReader just incorrectly detects a deflate-compressed field (CompressionTools).

The error occurs reproducible on CheckIndex with MMapDirectory, but not with NIODir or SimpleDir. The FDT file of that segment is 2.6 GiB, on Solaris the chunk size is Integer.MAX_VALUE, so we have 2 MultiMMap IndexInputs.

Robert and me have the index ready as a tar file, we will do tests on our local machines and hopefully solve the bug, maybe introduced by Robert's recent changes to MMap."
"LUCENE-2321","IMPROVEMENT","IMPROVEMENT","use packed ints for the terms dict index","Terms dict index needs to store large RAM resident arrays of ints, but, because their size is bound & variable (depending on the segment/docs), we should used packed ints for them."
"LUCENE-2476","BUG","BUG","Constructor of IndexWriter let's runtime exceptions pop up, while keeping the writeLock obtained","Constructor of IndexWriter let's runtime exceptions pop up, while keeping the writeLock obtained.

The init method in IndexWriter catches IOException only (I got NegativeArraySize by reading up a _corrupt_ index), and now, there is no way to recover, since the writeLock will be kept obtained. Moreover, I don't have IndexWriter instance either, to ""grab"" the lock somehow, since the init() method is called from IndexWriter constructor.

Either broaden the catch to all exceptions, or at least provide some circumvention to clear up. In my case, I'd like to ""fallback"", just delete the corrupted index from disk and recreate it, but it is impossible, since the LOCK_HELD NativeFSLockFactory's entry about obtained WriteLock is _never_ cleaned out and is no (at least apparent) way to clean it out forcibly. I can't create new IndexWriter, since it will always fail with LockObtainFailedException."
"LUCENE-3571","IMPROVEMENT","TASK","nuke IndexSearcher(directory)","IndexSearcher is supposed to be a cheap wrapper around a reader,
but sometimes it is, sometimes it isn't.

I think its confusing tangling of a heavyweight and lightweight
object that it sometimes 'houses' a reader and must close it in that case.
"
"LUCENE-295","BUG","BUG","[PATCH] MultiSearcher problems with Similarity.docFreq()","When MultiSearcher invokes its subsearchers, it is the subsearchers' docFreq()
that is accessed by Similarity.docFreq().  This causes idf's to be computed
local to each index rather than globally, which causes ranking across multiple
indices to not be equivalent to ranking across the entire global collection.

The attached files (if I can figure out how to attach them) provide a potential
partial solution for this.  They properly fix a simple test case, RankingTest,
that was provided by Daniel Naber.

The changes are:
  1.  Searcher:  Add topmostSearcher() field with getter and setter to record
the outermost Searcher.  Default to this.
  2.  MultiSearcher:  Pass down the topmostSearcher when creating the subsearchers.
  3.  IndexSearcher:  Call Query.weight() everywhere with the topmostSearcher
instead of this.
  4.  Query:  Provide a default implementation of Query.combine() so that
MultiSearcher works with all queries.

Problems or possible problems I see:
  1.  This does not address the same issue with RemoteSearchable. 
RemoteSearchable is not a Searcher, nor can it be due to lack of multiple
inheritance in Java, but Query.weight() requires a Searcher.  Perhaps
Query.weight() should be changed to take a Searchable, but this requires
changing many places and I suspect would break apps.
  2.  There may be other places that topmostSearcher should be used instead of this.
  3.  The default implementation for Query.combine() is a guess on my part - it
works for TermQuery.  It's fragile in that the default implementation will hide
bugs caused by queries that inadvertently omit a more precise Query.combine()
method.
  4.  The prior comment on Query.combine() indicates that whoever wrote it was
fully aware of this problem and so probably had another usage in mind, so the
whole issue may just be Daniel's usage in the test case.  It's not apparent to
me, so I probably don't understand something."
"LUCENE-3606","RFE","TASK","Make IndexReader really read-only in Lucene 4.0","As we change API completely in Lucene 4.0 we are also free to remove read-write access and commits from IndexReader. This code is so hairy and buggy (as investigated by Robert and Mike today) when you work on SegmentReader level but forget to flush in the DirectoryReader, so its better to really make IndexReaders readonly.

Currently with IndexReader you can do things like:
- delete/undelete Documents -> Can be done by with IndexWriter, too (using deleteByQuery)
- change norms -> this is a bad idea in general, but when we remove norms at all and replace by DocValues this is obsolete already. Changing DocValues should also be done using IndexWriter in trunk (once it is ready)"
"LUCENE-2383","BUG","BUG","Some small fixes after the flex merge...","Changes:

  * Re-introduced specialization optimization to FieldCacheRangeQuery;
    also fixed bug (was failing to check deletions in advance)

  * Changes 2 checkIndex methods from protected -> public

  * Add some missing null checks when calling MultiFields.getFields or
    IndexReader.fields()

  * Tweak'd CHANGES a bit

  * Removed some small dead code
"
"LUCENE-3909","IMPROVEMENT","IMPROVEMENT","Move Kuromoji to analysis.ja and introduce Japanese* naming","Lucene/Solr 3.6 and 4.0 will get out-of-the-box Japanese language support through {{KuromojiAnalyzer}}, {{KuromojiTokenizer}} and various other filters.  These filters currently live in {{org.apache.lucene.analysis.kuromoji}}.

I'm proposing that we move Kuromoji to a new Japanese package {{org.apache.lucene.analysis.ja}} in line with how other languages are organized.  As part of this, I also think we should rename {{KuromojiAnalyzer}} to {{JapaneseAnalyzer}}, etc. to further align naming to our conventions by making it very clear that these analyzers are for Japanese.  (As much as I like the name ""Kuromoji"", I think ""Japanese"" is more fitting.)

A potential issue I see with this that I'd like to raise and get feedback on, is that end-users in Japan and elsewhere who use lucene-gosen could have issues after an upgrade since lucene-gosen is in fact releasing its analyzers under the {{org.apache.lucene.analysis.ja}} namespace (and we'd have a name clash).

I believe users should have the freedom to choose whichever Japanese analyzer, filter, etc. they'd like to use, and I don't want to propose a name change that just creates unnecessary problems for users, but I think the naming proposed above is most fitting for a Lucene/Solr release.
"
"LUCENE-1147","RFE","IMPROVEMENT","add option to CheckIndex to only check certain segments","Simple patch to add -segment option to CheckIndex tool, to have it only check the particular segment, instead of all segments, from your index."
"LUCENE-2935","RFE","IMPROVEMENT","Let Codec consume entire document","Currently the codec API is limited to consume Terms & Postings upon a segment flush. To enable stored fields & DocValues to make use of the Codec abstraction codecs should allow to pull a consumer ahead of flush time and consume all values from a document's field though a consumer API. An alternative to consuming the entire document would be extending FieldsConsumer to return a StoredValueConsumer / DocValuesConsumer like it is done in DocValues - Branch right now side by side to the TermsConsumer. Yet, extending this has proven to be very tricky and error prone for several reasons:
* FieldsConsumer requires SegmentWriteState which might be different upon flush compared to when the document is consumed. SegmentWriteState must therefor be created twice 1. when the first docvalues field is indexed 2. when flushed. 
* FieldsConsumer are current pulled for each indexed field no matter if there are terms to be indexed or not. Yet, if we use something like DocValuesCodec which essentially wraps another codec and creates FieldConsumer on demand the wrapped codecs consumer might not be initialized even if the field is indexed. This causes problems once such a field is opened but missing the required files for that codec. I added some harsh logic to work around this which should be prevented.
* SegmentCodecs are created for each SegmentWriteState which might yield wrong codec IDs depending on how fields numbers are assigned. We currently depend on the fact that all fields for a segment and therefore their codecs are known when SegmentCodecs are build. To enable consuming perDoc values in codecs we need to do that incrementally

Codecs should instead provide a DocumentConsumer side by side with the FieldsConsumer created prior to flush. This is also a prerequisite for LUCENE-2621"
"LUCENE-2970","IMPROVEMENT","BUG","SpecialOperations.isFinite can have TERRIBLE TERRIBLE runtime in certain situations","in an application of mine, i experienced some very slow query times with finite automata (all the DFAs are acyclic)

It turned out, the slowdown is some terrible runtime in SpecialOperations.isFinite <-- this is used to determine if the DFA is acyclic or not.

(in this case I am talking about even up to minutes of cpu).
"
"LUCENE-2089","IMPROVEMENT","IMPROVEMENT","explore using automaton for fuzzyquery","we can optimize fuzzyquery by using AutomatonTermsEnum. The idea is to speed up the core FuzzyQuery in similar fashion to Wildcard and Regex speedups, maintaining all backwards compatibility.

The advantages are:
* we can seek to terms that are useful, instead of brute-forcing the entire terms dict
* we can determine matches faster, as true/false from a DFA is array lookup, don't even need to run levenshtein.

We build Levenshtein DFAs in linear time with respect to the length of the word: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652

To implement support for 'prefix' length, we simply concatenate two DFAs, which doesn't require us to do NFA->DFA conversion, as the prefix portion is a singleton. the concatenation is also constant time with respect to the size of the fuzzy DFA, it only need examine its start state.

with this algorithm, parametric tables are precomputed so that DFAs can be constructed very quickly.
if the required number of edits is too large (we don't have a table for it), we use ""dumb mode"" at first (no seeking, no DFA, just brute force like now).

As the priority queue fills up during enumeration, the similarity score required to be a competitive term increases, so, the enum gets faster and faster as this happens. This is because terms in core FuzzyQuery are sorted by boost value, then by term (in lexicographic order).

For a large term dictionary with a low minimal similarity, you will fill the pq very quickly since you will match many terms. 
This not only provides a mechanism to switch to more efficient DFAs (edit distance of 2 -> edit distance of 1 -> edit distance of 0) during enumeration, but also to switch from ""dumb mode"" to ""smart mode"".

With this design, we can add more DFAs at any time by adding additional tables. The tradeoff is the tables get rather large, so for very high K, we would start to increase the size of Lucene's jar file. The idea is we don't have include large tables for very high K, by using the 'competitive boost' attribute of the priority queue.

For more information, see http://en.wikipedia.org/wiki/Levenshtein_automaton"
"LUCENE-1542","BUG","BUG","Lucene can incorrectly set the position of tokens that start a field with positonInc 0.","More info in LUCENE-1465"
"LUCENE-1247","CLEANUP","IMPROVEMENT","Unnecessary assert in org.apache.lucene.index.DocumentsWriterThreadState.trimFields()","In org.apache.lucene.index.DocumentsWriterThreadState.trimFields() is the following code:

      if (fp.lastGen == -1) {
        // This field was not seen since the previous
        // flush, so, free up its resources now

        // Unhash
        final int hashPos = fp.fieldInfo.name.hashCode() & fieldDataHashMask;
        DocumentsWriterFieldData last = null;
        DocumentsWriterFieldData fp0 = fieldDataHash[hashPos];
        while(fp0 != fp) {
          last = fp0;
          fp0 = fp0.next;
        }
        assert fp0 != null;

The assert at the end is not necessary as fp0 cannot be null.  The first line in the above code guarantees that fp is not null by the time the while loop is hit.  The while loop is exited when fp0 and fp are equal.  Since fp is not null then fp0 cannot be null when the while loop is exited, thus the assert is guaranteed to never occur.

This was detected by FindBugs."
"LUCENE-1979","CLEANUP","TASK","Remove remaining deprecations from indexer package",""
"LUCENE-3807","REFACTORING","IMPROVEMENT","Cleanup suggester API","Currently the suggester api and especially TermFreqIterator don't play that nice with BytesRef and other paradigms we use in lucene, further the java iterator pattern isn't that useful when it gets to work with TermsEnum, BytesRef etc. We should try to clean up this api step by step moving over to BytesRef including the Lookup class and its interface..."
"LUCENE-2574","IMPROVEMENT","IMPROVEMENT","Optimize copies between IndexInput and Output","We've created an optimized copy of files from Directory to Directory. We've also optimized copyBytes recently. However, we're missing the opposite side of the copy - from IndexInput to Output. I'd like to mimic the FileChannel API by having copyTo on IndexInput and copyFrom on IndexOutput. That way, both sides can optimize the copy process, depending on the type of the IndexInput/Output that they need to copy to/from.

FSIndexInput/Output can use FileChannel if the two are FS types. RAMInput/OutputStream can copy to/from the buffers directly, w/o going through intermediate ones. Actually, for RAMIn/Out this might be a big win, because it doesn't care about the type of IndexInput/Output given - it just needs to copy to its buffer directly.

If we do this, I think we can consolidate all Dir.copy() impls down to one (in Directory), and rely on the In/Out ones to do the optimized copy. Plus, it will enable someone to do optimized copies between In/Out outside the scope of Directory.

If this somehow turns out to be impossible, or won't make sense, then I'd like to optimize RAMDirectory.copy(Dir, src, dest) to not use an intermediate buffer."
"LUCENE-919","DESIGN_DEFECT","","DefaultSkipListReader should not be public","There's no need for org.apache.lucene.index.DefaultSkipListReader to be public.
This class hasn't been released yet, so we should fix this before 2.2."
"LUCENE-2024","BUILD_SYSTEM","BUG","""ant dist"" no longer generates md5's for the top-level artifacts","Mark hit this for 2.9.0, and I just hit it again for 2.9.1.  It used to work..."
"LUCENE-652","REFACTORING","IMPROVEMENT","Compressed fields should be ""externalized"" (from Fields into Document)","Right now, as of 2.0 release, Lucene supports compressed stored fields.  However, after discussion on java-dev, the suggestion arose, from Robert Engels, that it would be better if this logic were moved into the Document level.  This way the indexing level just stores opaque binary fields, and then Document handles compress/uncompressing as needed.

This approach would have prevented issues like LUCENE-629 because merging of segments would never need to decompress.

See this thread for the recent discussion:

    http://www.gossamer-threads.com/lists/lucene/java-dev/38836

When we do this we should also work on related issue LUCENE-648."
"LUCENE-1079","CLEANUP","IMPROVEMENT","DocValues cleanup: constructor & getInnerArray()","DocValues constructor taking a numDocs parameter is not very clean.
Get rid of this.

Also, it's optional getInnerArray() method is not very clean.
This is necessary for better testing, but currently tests will fail if it is not implemented.
Modify it to throw UnSupportedOp exception (rather than returning an empty array).
Modify tests to not fail but just warn if the tested iml does not override it.

These changes should make it easier to implement DocValues for other ValueSource's, e.g. above payloads, with or without caching.
"
"LUCENE-241","BUG","BUG","NullPointerException in CompoundFileReader","Hello,

we have got a NullPointerException in the Lucene-class CompoundFileReader:

java.lang.NullPointerException
        at
org.apache.lucene.index.CompoundFileReader.<init>(CompoundFileReader.java:94)
        at org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:97)
        at org.apache.lucene.index.IndexWriter.mergeSegments(IndexWriter.java:466)
        at
org.apache.lucene.index.IndexWriter.flushRamSegments(IndexWriter.java:426)
        at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:236)

Lucene has been working fine for some days, until this NullPointerException
has occured which has corrupted the complete index.

The reason for this NullPointerException is the following Code 
in Lucenes source file CompoundFileReader.java:

    public CompoundFileReader(Directory dir, String name)
    throws IOException
    {
        boolean success = false;
        ...

        try {
            stream = dir.openFile(name);

            // read the directory and init files
            ...

            success = true;

        } finally {
            if (! success) {
                try {
                    stream.close();
                } catch (IOException e) { }
            }
        }
    }

If the IO-method-call ""dir.openFile()"" throws an IOExeption,
then the variable ""stream"" remains its null value.
The statement ""stream.close()"" in the finally clause will then cause a
NullPointerException.

I would suggest that you change the code from:
    stream.close();
to:
    if ( stream != null ) {
        stream.close();
    }

There are a lot of reasons why an IO-operation like ""dir.openFile()""
could throw an IOException.
I cannot guarantee that such an IO exception will never occur again.
Therefore it is better to handle such an IO exception correctly.

This issue is similar to bug# 29774, except that I recommand an easy way
to solve this problem."
"LUCENE-1963","IMPROVEMENT","IMPROVEMENT","ArabicAnalyzer: Lowercase before Stopfilter","ArabicAnalyzer lowercases text in case you have some non-Arabic text around.
It also allows you to set a custom stopword list (you might augment the Arabic list with some English ones, for example).

In this case its helpful for these non-Arabic stopwords, to lowercase before stopfilter.
"
"LUCENE-258","RFE","BUG","[PATCH] HTMLParser doesn't parse hexadecimal character references","I recently inherited a project from an ex-colleague; it uses Lucene and in
particular the HTML Parser.  I've found that she had made an amendment to the
parser to allow it to parse and decode hexadecimal character references, which
we depend on, but had not reported a bug.  If she had, someone might have
pointed out that her correction was wrong ...

I don't seem to be able to attach the (fairly trivial) patch to an initial bug
report (and in any case I've failed to find the instructions for generating a
diff file in the right format, even though I'm sure I've seen it somewhere)."
"LUCENE-1965","IMPROVEMENT","IMPROVEMENT","Lazy Atomic Loading Stopwords in SmartCN ","The default constructor in SmartChineseAnalyzer loads the default (jar embedded) stopwords each time the constructor is invoked. 
This should be atomically loaded only once in an unmodifiable set.

"
"LUCENE-2804","TEST","TEST","check all tests that use FSDirectory.open","In LUCENE-2471 we were discussing the copyBytes issue, and Shai and I had a discussion about how we could prevent such bugs in the future.

One thing that lead to the bug existing in our code for so long, was that it only happened on windows (e.g. never failed in hudson!)
This was because the bug only happened if you were copying from SimpleFSDirectory, and the test used FSDirectory.open

Today the situation is improving: most tests use newDirectory() which is random by default and never use FSDir.open,
it always uses SimpleFS or NIOFS so that the same random seed will reproduce across both windows and unix.

So I think we need to review all uses of FSDirectory.open in our tests, and minimize these.
In general tests should use newDirectory().
If the test comes with say a zip-file and wants to explicitly open stuff from disk, I think it should open the contents with say SimpleFSDir,
and then call newDirectory(Directory) to copy into a new ""random"" implementation for actual testing. This method already exists:
{noformat}
  /**
   * Returns a new Dictionary instance, with contents copied from the
   * provided directory. See {@link #newDirectory()} for more
   * information.
   */
  public static MockDirectoryWrapper newDirectory(Directory d) throws IOException {
{noformat}
"
"LUCENE-951","BUG","BUG","PATCH MultiLevelSkipListReader NullPointerException"," When Reconstructing Document Using Luke Tool, received NullPointerException.

java.lang.NullPointerException
        at org.apache.lucene.index.MultiLevelSkipListReader.loadSkipLevels(MultiLevelSkipListReader.java:188)
        at org.apache.lucene.index.MultiLevelSkipListReader.skipTo(MultiLevelSkipListReader.java:97)
        at org.apache.lucene.index.SegmentTermDocs.skipTo(SegmentTermDocs.java:164)
        at org.getopt.luke.Luke$2.run(Unknown Source)

Luke version 0.7.1

I emailed with Luke author Andrzej Bialecki and he suggested the attached patch file which fixed the problem.
"
"LUCENE-865","BUG","BUG","SpellChecker not working because of stale IndexSearcher","The SpellChecker unit test did not work, because of a stale IndexReader and IndexSearcher instance after calling indexDictionary(Dictionary)."
"LUCENE-2358","REFACTORING","TASK","rename KeywordMarkerTokenFilter","I would like to rename KeywordMarkerTokenFilter to KeywordMarkerFilter.
We havent released it yet, so its a good time to keep the name brief and consistent."
"LUCENE-3012","BUG","BUG","if you use setNorm, lucene writes a headerless separate norms file","In this case SR.reWrite just writes the bytes with no header...
we should write it always.

we can detect in these cases (segment written <= 3.1) with a 
sketchy length == maxDoc check.
"
"LUCENE-1883","DOCUMENTATION","IMPROVEMENT","Fix typos in CHANGES.txt and contrib/CHANGES.txt prior to 2.9 release","I noticed a few typos in CHANGES.txt and contrib/CHANGES.txt.  (Once they make it past a release, they're set in stone...)

Will attach a patch shortly."
"LUCENE-1869","IMPROVEMENT","IMPROVEMENT","when checking tvx/fdx size mismatch, also include whether the file exists","IndexWriter checks, during flush and during merge, that the size of the index file for stored fields (*.fdx) and term vectors (*.tvx) matches how many bytes it has just written.

This originally was added for LUCENE-1282, ie, as a safety to catch the nasty ""off by 1"" JRE hotspot bug that would otherwise silently corrupt the index.

However, this check also seems to catch a different case, where the size of the file is zero.   The most recent example is LUCENE-1521.  I'd like to improve the message in the exception to include whether or not the file exists, to help understand why users are sometimes hitting this exception.  My best theory at this point is something external is removing the file out from under the IndexWriter.
"
"LUCENE-2667","IMPROVEMENT","IMPROVEMENT","Fix FuzzyQuery's defaults, so its fast.","We worked a lot on FuzzyQuery, but you need to be a rocket scientist to ensure good results.

The main problem is that the default distance is 0.5f, which doesn't take into account the length of the string.
To add insult to injury, the default number of expansions is 1024 (traditionally from BooleanQuery maxClauseCount)

I propose:
* The syntax of FuzzyQuery is enhanced, so that you can specify raw edits too: such as foobar~2 (all terms within 2 levenshtein edits of foobar). Previously if you specified any amount >=1, you got IllegalArgumentException, so this won't break anyone. You can still use foobar~0.5, and it works just as before
* The default for minimumSimilarity then becomes LevenshteinAutomata.MAXIMUM_SUPPORTED_DISTANCE, which is 2. This way if you just do foobar~, its always fast.
* The size of the priority queue is reduced by default from 1024 to a much more reasonable value: 50. This is what FuzzyLikeThis uses.

I think its best to just change the defaults for this query, since it was so aweful before. We can add notes in migrate.txt that if you care about using the old values, then you should provide them explicitly, and you will get the same results!
"
"LUCENE-3327","BUG","BUG","TestFSTs.testRandomWords throws AIOBE when ""verbose""=true","Seems like invalid utf-8 sometimes gets passed to Bytesref.utf8ToString() in the verbose ""println""s."
"LUCENE-3437","TEST","","Detect the test thread by reference, not by name.","Get rid of this:
{code}
      if (doFail && (Thread.currentThread().getName().equals(""main"") 
          || Thread.currentThread().getName().equals(""Main Thread""))) {
{code}"
"LUCENE-1169","BUG","BUG","Search with Filter does not work!","See attached JUnitTest, self-explanatory


"
"LUCENE-3911","TEST","TASK","improve BaseTokenStreamTestCase random string generation","Most analysis tests use mocktokenizer (which splits on whitespace), but
its rare that we generate a string with 'many tokens'. So I think we should
try to generate more realistic test strings."
"LUCENE-1107","OTHER","TASK","Clean up old JIRA issues in component ""Other""","A list of all JIRA issues in component ""Other"" that haven't been updated in 2007:

   *	 LUCENE-746  	 Incorrect error message in AnalyzingQueryParser.getPrefixQuery   
   *	LUCENE-644 	Contrib: another highlighter approach 
   *	LUCENE-574 	support for vjc java compiler, also known as J# 
   *	LUCENE-471 	gcj ant target doesn't work on windows 
   *	LUCENE-434 	Lucene database bindings 
   *	LUCENE-254 	[PATCH] pseudo-relevance feedback enhancement 
   *	LUCENE-180 	[PATCH] Language guesser contribution 
"
"LUCENE-2803","BUG","BUG","FieldCache should not pay attention to deleted docs when creating entries","The FieldCache uses a key that ignores deleted docs, so it's actually a bug to use deleted docs when creating an entry.  It can lead to incorrect values when the same entry is used with a different reader."
"LUCENE-1202","BUILD_SYSTEM","BUG","Clover setup currently has some problems","(tracking as a bug before it get lost in email...
  http://www.nabble.com/Clover-reports-missing-from-hudson--to15510616.html#a15510616
)

The clover setup for Lucene currently has some problems, 3 i think...

1) instrumentation fails on contrib/db/ because it contains java packages the ASF Clover lscence doesn't allow instrumentation of.  i have a patch for this.

2) running instrumented contrib tests for other contribs produce strange errors...

{{monospaced}}
    [junit] Testsuite: org.apache.lucene.analysis.el.GreekAnalyzerTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.126 sec
    [junit]
    [junit] ------------- Standard Error -----------------
    [junit] [CLOVER] FATAL ERROR: Clover could not be initialised. Are you sure you have Clover
in the runtime classpath? (class
java.lang.NoClassDefFoundError:com_cenqua_clover/CloverVersionInfo)
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAnalyzer(org.apache.lucene.analysis.el.GreekAnalyzerTest):    Caused
an ERROR
    [junit] com_cenqua_clover/g
    [junit] java.lang.NoClassDefFoundError: com_cenqua_clover/g
    [junit]     at org.apache.lucene.analysis.el.GreekAnalyzer.<init>(GreekAnalyzer.java:157)
    [junit]     at
org.apache.lucene.analysis.el.GreekAnalyzerTest.testAnalyzer(GreekAnalyzerTest.java:60)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.analysis.el.GreekAnalyzerTest FAILED
{{monospaced}}

...i'm not sure what's going on here.  the error seems to happen both when
trying to run clover on just a single contrib, or when doing the full
build ... i suspect there is an issue with the way the batchtests fork
off, but I can't see why it would only happen to contribs (the regular
tests fork as well)

3) according to Grant...

{{quote}}
...There is also a bit of a change on Hudson during the migration to the new servers that needs to be ironed  out. 
{{quote}}
"
"LUCENE-847","REFACTORING","IMPROVEMENT","Factor merge policy out of IndexWriter","If we factor the merge policy out of IndexWriter, we can make it pluggable, making it possible for apps to choose a custom merge policy and for easier experimenting with merge policy variants."
"LUCENE-2862","IMPROVEMENT","IMPROVEMENT","Track total term freq per term","Right now we track docFreq for each term (how many docs have the
term), but the totalTermFreq (total number of occurrences of this
term, ie sum of freq() for each doc that has the term) is also a
useful stat (for flex scoring, PulsingCodec, etc.).
"
"LUCENE-389","RFE","IMPROVEMENT","MatchAllDocsQuery to return all documents","It would be nice to have a type of query just return all documents from an index."
"LUCENE-501","TASK","TASK","need DOAP file for Lucene","Can someone please draft a DOAP file for Lucene, so that we're listed at http://projects.apache.org/?

A DOAP generator is at:

http://projects.apache.org/create.html

Please attach it to this bug report.  Thanks."
"LUCENE-2373","RFE","IMPROVEMENT","Create a Codec to work with streaming and append-only filesystems","Since early 2.x times Lucene used a skip/seek/write trick to patch the length of the terms dict into a place near the start of the output data file. This however made it impossible to use Lucene with append-only filesystems such as HDFS.

In the post-flex trunk the following code in StandardTermsDictWriter initiates this:
{code}
    // Count indexed fields up front
    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT); 

    out.writeLong(0);                             // leave space for end index pointer
{code}
and completes this in close():
{code}
      out.seek(CodecUtil.headerLength(CODEC_NAME));
      out.writeLong(dirStart);
{code}

I propose to change this layout so that this pointer is stored simply at the end of the file. It's always 8 bytes long, and we known the final length of the file from Directory, so it's a single additional seek(length - 8) to read it, which is not much considering the benefits."
"LUCENE-910","DOCUMENTATION","TASK","Add/change warning comments in the javadocs of Payload APIs","Since the payload API is still experimental we should change the comments
in the javadocs similar to the new search/function package."
"LUCENE-875","DOCUMENTATION","TASK","javadocs creation has too many warnings/errors","Currently running 'ant javadocs' creates so many warnings that you have to grep the output to verify that new code did not add more.

While most current errors might be minor, they might hide a few serious ones that we will never know abut until someone complains. 

Best if we fix all of them and keep it always clean..."
"LUCENE-836","RFE","RFE","Benchmarks Enhancements (precision/recall, TREC, Wikipedia)","Would be great if the benchmark contrib had a way of providing precision/recall benchmark information ala TREC.  I don't know what the copyright issues are for the TREC queries/data (I think the queries are available, but not sure about the data), so not sure if the is even feasible, but I could imagine we could at least incorporate support for it for those who have access to the data.  It has been a long time since I have participated in TREC, so perhaps someone more familiar w/ the latest can fill in the blanks here.

Another option is to ask for volunteers to create queries and make judgments for the Reuters data, but that is a bit more complex and probably not necessary.  Even so, an Apache licensed set of benchmarks may be useful for the community as a whole.  Hmmm.... 

Wikipedia might be another option instead of Reuters to setup as a download for benchmarking, as it is quite large and I believe the licensing terms are quite amenable.  Having a larger collection would be good for stressing Lucene more and would give many users a demonstration of how Lucene handles large collections.

At any rate, this kind of information could be useful for people looking at different indexing schemes, formats, payloads and different query strategies.

"
"LUCENE-1925","DESIGN_DEFECT","IMPROVEMENT","In IndexSearcher class, make subReader and docCount arrays protected so sub classes can access them","Please make these two member variables protected so subclasses can access them, e.g.:

  protected IndexReader[] subReaders;
  protected int[] docStarts;

Thanks"
"LUCENE-2088","DESIGN_DEFECT","BUG","AttributeSource.addAttribute should only accept interfaces, the missing test leads to problems with Token.TOKEN_ATTRIBUTE_FACTORY","This is a blocker, because you can call addAttribute(Token.class) without getting an error message.

I will commit the fix and restart the vote for 3.0. This also applies to 2.9, but there is no Token Attribute Factory. But I will merge to 2.9, too, if a 2.9.2 comes."
"LUCENE-1201","RFE","RFE","Add getIndexCommit method to IndexReader","Spinoff from this thread:

  http://markmail.org/message/bojgqfgyxkkv4fyb

I think it makes sense ask an IndexReader for the commit point it has
open.  This enables the use case described in the above thread, which
is to create a deletion policy that is able to query all open readers
for what commit points they are using, and prevent deletion of them.

"
"LUCENE-1740","RFE","RFE","Lucli: Command to change the Analyzer","Currently, Lucli is hardcoded to use StandardAnalyzer. The provided patch introduces a command ""analyzer"" to specify a different Analyzer class. 
If something fails, StandardAnalyzer is the fall-back."
"LUCENE-2325","TEST","TEST","investigate solr test failures using flex","We have a branch of Solr located here: https://svn.apache.org/repos/asf/lucene/solr/branches/solr

Currently all the tests pass with lucene trunk jars.

I plopped in the flex jars and they do not, so I thought these might be interesting to look at.
"
"LUCENE-1875","DOCUMENTATION","BUG","Javadoc of TokenStream.end() somehow confusing","The Javadocs of TokenStream.end() are somehow confusing, because they also refer to the old TokenStream API (""after next() returned null""). But one who implements his TokenStream with the old API cannot make use of the end() feature, as he would not use attributes and so cannot update the end offsets (he could, but then he should rewrite the whole TokenStream). To be conform to the old API, there must be an end(Token) method, which we will not add.

I would drop the old API from this docs."
"LUCENE-773","CLEANUP","IMPROVEMENT","Deprecate ""create"" method in FSDirectory.getDirectory in favor of IndexWriter's ""create""","It's confusing that there is a create=true|false at the FSDirectory
level and then also another create=true|false at the IndexWriter
level.  Which one should you use when creating an index?

Our users have been confused by this in the past:

  http://www.gossamer-threads.com/lists/lucene/java-user/4792

I think in general we should try to have one obvious way to achieve
something (like Python: http://en.wikipedia.org/wiki/Python_philosophy).

And the fact that there are now two code paths that are supposed to do
the same (similar?) thing, can more easily lead to sneaky bugs.  One
case of LUCENE-140 (already fixed in trunk but not past releases),
which inspired this issue, can happen if you send create=false to the
FSDirectory and create=true to the IndexWriter.

Finally, as of lockless commits, it is now possible to open an
existing index for ""create"" while readers are still using the old
""point in time"" index, on Windows.  (At least one user had tried this
previously and failed).  To do this, we use the IndexFileDeleter class
(which retries on failure) and we also look at the segments file to
determine the next segments_N file to write to.

With future issues like LUCENE-710 even more ""smarts"" may be required
to know what it takes to ""create"" a new index into an existing
directory.  Given that we have have quite a few Directory
implemenations, I think these ""smarts"" logically should live in
IndexWriter (not replicated in each Directory implementation), and we
should leave the Directory as an interface that knows how to make
changes to some backing store but does not itself try to make any
changes.
"
"LUCENE-3274","REFACTORING","IMPROVEMENT","Collapse Common module into Lucene core util","It was suggested by Robert in [http://markmail.org/message/wbfuzfamtn2qdvii] that we should try to limit the dependency graph between modules and where there is something 'common' it should probably go into lucene core.  Given that I haven't added anything to this module except the MutableValue classes, I'm going to collapse them into the util package, remove the module, and correct the dependencies."
"LUCENE-2255","IMPROVEMENT","BUG","IndexWriter.getReader() allocates file handles","I am not sure if this is a ""bug"" or really just me not reading the Javadocs right...

The IR returned by IW.getReader() leaks file handles if you do not close() it, leading to starvation of the available file handles/process. If it was clear from the docs that this was a *new* reader and not some reference owned by the writer then this would probably be ok. But as I read the docs the reader is internally managed by the IW, which at first shot lead me to believe that I shouldn't close it.

So perhaps the docs should be amended to clearly state that this is a caller-owns reader that *must* be closed? Attaching a simple app that illustrates the problem."
"LUCENE-3686","BUG","BUG","EnhancementsPayloadIterator.getCategoryData(CategoryEnhancement) problematic usage of Object.equals()","EnhancementsPayloadIterator has an internal list of category enhancemnets, and in getCategoryData(CategoryEnhancement) there is a lookup of the given CategoryEnhancement in the list. In order to make sure this lookup works, CategoryEnhancement must override Object.equals(Object)."
"LUCENE-1044","IMPROVEMENT","BUG","Behavior on hard power shutdown","When indexing a large number of documents, upon a hard power failure  (e.g. pull the power cord), the index seems to get corrupted. We start a Java application as an Windows Service, and feed it documents. In some cases (after an index size of 1.7GB, with 30-40 index segment .cfs files) , the following is observed.

The 'segments' file contains only zeros. Its size is 265 bytes - all bytes are zeros.
The 'deleted' file also contains only zeros. Its size is 85 bytes - all bytes are zeros.

Before corruption, the segments file and deleted file appear to be correct. After this corruption, the index is corrupted and lost.

This is a problem observed in Lucene 1.4.3. We are not able to upgrade our customer deployments to 1.9 or later version, but would be happy to back-port a patch, if the patch is small enough and if this problem is already solved.
"
"LUCENE-528","IMPROVEMENT","IMPROVEMENT","Optimization for IndexWriter.addIndexes()","One big performance problem with IndexWriter.addIndexes() is that it has to optimize the index both before and after adding the segments.  When you have a very large index, to which you are adding batches of small updates, these calls to optimize make using addIndexes() impossible.  It makes parallel updates very frustrating.

Here is an optimized function that helps out by calling mergeSegments only on the newly added documents.  It will try to avoid calling mergeSegments until the end, unless you're adding a lot of documents at once.

I also have an extensive unit test that verifies that this function works correctly if people are interested.  I gave it a different name because it has very different performance characteristics which can make querying take longer."
"LUCENE-2640","TEST","TEST","add LuceneTestCase[J4].newField","I think it would be good to vary the different field options in tests.

For example, we do this with IW settings (newIndexWriterConfig), and directories (newDirectory).

This patch adds newField(), it works just like new Field(), except it will sometimes turns on extra options:
Stored fields, term vectors, additional term vectors data, etc.
"
"LUCENE-2913","DESIGN_DEFECT","","Add missing getter methods to NumericField, NumericTokenStream, NumericRangeQuery, NumericRangeFilter","These classes are missing bean-style getter methods for some basic properties. This is inconsistent and should be fixed."
"LUCENE-2261","RFE","IMPROVEMENT","configurable MultiTermQuery TopTermsScoringBooleanRewrite pq size","MultiTermQuery has a TopTermsScoringBooleanRewrite, that uses a priority queue to expand the query to the top-N terms.

currently N is hardcoded at BooleanQuery.getMaxClauseCount(), but it would be nice to be able to set this for top-N MultiTermQueries: e.g. expand a fuzzy query to at most only the 50 closest terms.

at a glance it seems one way would be to expose TopTermsScoringBooleanRewrite (it is private right now) and add a ctor to it, so a MultiTermQuery can instantiate one with its own limit."
"LUCENE-641","BUG","BUG","maxFieldLength actual limit is 1 greater than expected value.","
// Prepare document.
Document document = new Document();
document.add(new Field(""name"",
            ""pattern oriented software architecture"", Store.NO,
            Index.TOKENIZED, TermVector.WITH_POSITIONS_OFFSETS));

// Set max field length to 2.
indexWriter.setMaxFieldLength(2);

// Add document into index.
indexWriter.addDocument(document, new StandardAnalyzer());

// Create a query.
QueryParser queryParser = new QueryParser(""name"", new StandardAnalyzer());
Query query = queryParser.parse(""software"");

// Search the 3rd term.
Hits hits = indexSearcher.search(query);

Assert.assertEquals(0, hits.length());
// failed. Actual hits.length() == 1, but expect 0."
"LUCENE-2957","BUILD_SYSTEM","IMPROVEMENT","generate-maven-artifacts target should include all non-Mavenized Lucene & Solr dependencies","Currently, in addition to deploying artifacts for all of the Lucene and Solr modules to a repository (by default local), the {{generate-maven-artifacts}} target also deploys artifacts for the following non-Mavenized Solr dependencies (lucene_solr_3_1 version given here):

# {{solr/lib/commons-csv-1.0-SNAPSHOT-r966014.jar}} as org.apache.solr:solr-commons-csv:3.1
# {{solr/lib/apache-solr-noggit-r944541.jar}} as org.apache.solr:solr-noggit:3.1
\\ \\
The following {{.jar}}'s should be added to the above list (lucene_solr_3_1 version given here):
\\ \\
# {{lucene/contrib/icu/lib/icu4j-4_6.jar}}
# {{lucene/contrib/benchmark/lib/xercesImpl-2.9.1-patched-XERCESJ}}{{-1257.jar}}
# {{solr/contrib/clustering/lib/carrot2-core-3.4.2.jar}}**
# {{solr/contrib/uima/lib/uima-an-alchemy.jar}}
# {{solr/contrib/uima/lib/uima-an-calais.jar}}
# {{solr/contrib/uima/lib/uima-an-tagger.jar}}
# {{solr/contrib/uima/lib/uima-an-wst.jar}}
# {{solr/contrib/uima/lib/uima-core.jar}}
\\ \\
I think it makes sense to follow the same model as the current non-Mavenized dependencies:
\\ \\
* {{groupId}} = {{org.apache.solr/.lucene}}
* {{artifactId}} = {{solr-/lucene-}}<original-name>,
* {{version}} = <lucene-solr-release-version>.

**The carrot2-core jar doesn't need to be included in trunk's release artifacts, since there already is a Mavenized Java6-compiled jar.  branch_3x and lucene_solr_3_1 will need this Solr-specific Java5-compiled maven artifact, though."
"LUCENE-2398","TEST","TEST","Improve tests to work easier from IDEs","As reported by Paolo Castagna on the mailing lists, some tests fail when you run them from eclipse.

Some of the failures he reports are actually code problems such as base test classes not being 
abstract when they should be... we should fix things like that."
"LUCENE-1873","DOCUMENTATION","BUG","Update site lucene-sandbox page","The page has misleading/bad info. One thing I would like to do - but I won't attempt now (prob good for the modules issue) - is commit to one word - contrib or sandbox. I think sandbox should be purged myself.

The current page says that the sandbox is kind of a rats nest with various early stage software that one day may make it into core - that info is outdated I think. We should replace it, and also specify how the back compat policy works in contrib eg each contrib can have its own policy, with the default being no policy.

We should also drop the piece about being open to Lucene's committers and others - a bit outdated.

We should also either include the other contribs, or change the wording to indicate that the list is only a sampling of the many contribs."
"LUCENE-1424","RFE","RFE","Change all multi-term querys so that they extend MultiTermQuery and allow for a constant score mode","Cleans up a bunch of code duplication, closer to how things should be - design wise, gives us constant score for all the multi term queries, and allows us at least the option of highlighting the constant score queries without much further work."
"LUCENE-1297","RFE","RFE","Allow other string distance measures in spellchecker","Updated spelling code to allow for other string distance measures to be used.

Created StringDistance interface.
Modified existing Levenshtein distance measure to implement interface (and renamed class).
Verified that change to Levenshtein distance didn't impact runtime performance.
Implemented Jaro/Winkler distance metric
Modified SpellChecker to take distacne measure as in constructor or in set method and to use interface when calling.
"
"LUCENE-3271","REFACTORING","IMPROVEMENT","Move 'good' contrib/queries classes to Queries module","With the Queries module now filled with the FunctionQuery stuff, we should look at closing down contrib/queries.  While not a huge contrib, it contains a number of pretty useful classes and some that should go elsewhere.

Heres my proposed plan:

- similar.* -> suggest module
- regex.* -> queries module
- BooleanFilter -> queries module under .filters package
- BoostingQuery -> queries module
- ChainedFilter -> queries module under .filters package
- DuplicateFilter -> queries module under .filters package
- FieldCacheRewriteMethod -> This doesn't belong in this contrib or the queries module.  I think we should push it to contrib/misc for the time being.  It seems to have quite a few constraints on when its useful.  If indeed CONSTANT_SCORE_AUTO rewrite is better, then I dont see a purpose for it.
- FilterClause -> class inside BooleanFilter
- FuzzyLikeThisQuery -> suggest module. This class seems a mess with its Similarity hardcoded.  With all that said, it does seem to do what it claims and with some cleanup, it could be good.
- TermsFilter -> queries module under .filters package
- SlowCollated* -> They can stay in the module till we have a better place to nuke them.

One of the implications of the above moves, is that the xml-query-parser, which supports many of the queries, will need to have a dependency on the queries module.  But that seems unavoidable at this stage.



"
"LUCENE-2329","IMPROVEMENT","IMPROVEMENT","Use parallel arrays instead of PostingList objects","This is Mike's idea that was discussed in LUCENE-2293 and LUCENE-2324.

In order to avoid having very many long-living PostingList objects in TermsHashPerField we want to switch to parallel arrays.  The termsHash will simply be a int[] which maps each term to dense termIDs.

All data that the PostingList classes currently hold will then we placed in parallel arrays, where the termID is the index into the arrays.  This will avoid the need for object pooling, will remove the overhead of object initialization and garbage collection.  Especially garbage collection should benefit significantly when the JVM runs out of memory, because in such a situation the gc mark times can get very long if there is a big number of long-living objects in memory.

Another benefit could be to build more efficient TermVectors.  We could avoid the need of having to store the term string per document in the TermVector.  Instead we could just store the segment-wide termIDs.  This would reduce the size and also make it easier to implement efficient algorithms that use TermVectors, because no term mapping across documents in a segment would be necessary.  Though this improvement we can make with a separate jira issue."
"LUCENE-802","BUILD_SYSTEM","IMPROVEMENT","lucene jars should include LiCENSE and NOTICE","The Lucene jars created by the build should include the LICENSE and NOTICE files in META-INF."
"LUCENE-2881","IMPROVEMENT","IMPROVEMENT","Track FieldInfo per segment instead of per-IW-session","Currently FieldInfo is tracked per IW session to guarantee consistent global field-naming / ordering. IW carries FI instances over from previous segments which also carries over field properties like isIndexed etc. While having consistent field ordering per IW session appears to be important due to bulk merging stored fields etc. carrying over other properties might become problematic with Lucene's Codec support.  Codecs that rely on consistent properties in FI will fail if FI properties are carried over.

The DocValuesCodec (DocValuesBranch) for instance writes files per segment and field (using the field id within the file name). Yet, if a segment has no DocValues indexed in a particular segment but a previous segment in the same IW session had DocValues, FieldInfo#docValues will be true  since those values are reused from previous segments. 

We already work around this ""limitation"" in SegmentInfo with properties like hasVectors or hasProx which is really something we should manage per Codec & Segment. Ideally FieldInfo would be managed per Segment and Codec such that its properties are valid per segment. It also seems to be necessary to bind FieldInfoS to SegmentInfo logically since its really just per segment metadata.  "
"LUCENE-2269","BUILD_SYSTEM","TEST","don't download/extract 20,000 files when doing the build","When you build lucene, it downloads and extracts some data for contrib/benchmark, especially the 20,000+ files for the reuters corpus.
this is only needed for one test, and these 20,000 files drive IDEs and such crazy.
instead of doing this by default, we should only download/extract data if you specifically ask (like wikipedia, collation do, etc)

for the qualityrun test, instead use a linedoc formatted 587-line text file, similar to reuters.first20.lines.txt already used by benchmark.
"
"LUCENE-1876","DOCUMENTATION","IMPROVEMENT","Some contrib packages are missing a package.html","Dunno if we will get to this one this release, but a few contribs don't have a package.html (or a good overview that would work as a replacement) - I don't think this is hugely important, but I think it is important - you should be able to easily and quickly read a quick overview for each contrib I think.

So far I have identified collation and spatial."
"LUCENE-1686","CLEANUP","IMPROVEMENT","Remove Unnecessary NULL check in FindSegmentsFile - cleanup","FindSegmentsFile accesses the member ""directory"" in line 579 while performing a null check in 592. The null check is unnecessary as if directory is null line 579 would throw a NPE.
I removed the null check and made the member ""directory"" final. In addition I added a null check in the constructor as If the value is null we should catch it asap. 

"
"LUCENE-749","BUG","IMPROVEMENT","ChainedFilter does not work well in the event of filters in ANDNOT","First ANDNOT operation takes place against a completely false bitset and will always return zero results.  "
"LUCENE-3851","TEST","BUG","TestTermInfosReaderIndex failing (always reproducible)","Always fails on branch (use reproduce string below):
git clone --depth 1 -b rr git@github.com:dweiss/lucene_solr.git

{noformat}
[junit4] Running org.apache.lucene.codecs.lucene3x.TestTermInfosReaderIndex
[junit4] FAILURE 0.04s J0 | TestTermInfosReaderIndex.testSeekEnum
[junit4]    > Throwable #1: java.lang.AssertionError: expected:<field9:z91ob3wozm6d> but was:<:>
[junit4]    > 	at __randomizedtesting.SeedInfo.seed([C7597DFBBE0B3D7D:C6D9CEDD0700AAFF]:0)
[junit4]    > 	at org.junit.Assert.fail(Assert.java:93)
[junit4]    > 	at org.junit.Assert.failNotEquals(Assert.java:647)
[junit4]    > 	at org.junit.Assert.assertEquals(Assert.java:128)
[junit4]    > 	at org.junit.Assert.assertEquals(Assert.java:147)
[junit4]    > 	at org.apache.lucene.codecs.lucene3x.TestTermInfosReaderIndex.testSeekEnum(TestTermInfosReaderIndex.java:137)
[junit4]    > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit4]    > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
[junit4]    > 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
[junit4]    > 	at java.lang.reflect.Method.invoke(Method.java:597)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1766)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.access$1000(RandomizedRunner.java:141)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:728)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:789)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:803)
[junit4]    > 	at org.apache.lucene.util.LuceneTestCase$SubclassSetupTeardownRule$1.evaluate(LuceneTestCase.java:744)
[junit4]    > 	at org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:636)
[junit4]    > 	at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
[junit4]    > 	at org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:550)
[junit4]    > 	at org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:600)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:735)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.access$600(RandomizedRunner.java:141)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$3$1.run(RandomizedRunner.java:586)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:605)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:641)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:652)
[junit4]    > 	at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSuite(RandomizedRunner.java:533)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.access$400(RandomizedRunner.java:141)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$2.run(RandomizedRunner.java:479)
[junit4]    > 
[junit4]   2> NOTE: reproduce with: ant test -Dtests.filter=*.TestTermInfosReaderIndex -Dtests.filter.method=testSeekEnum -Drt.seed=C7597DFBBE0B3D7D -Dargs=""-Dfile.encoding=UTF-8""
[junit4]   2>
[junit4]    > (@AfterClass output)
[junit4]   2> NOTE: test params are: codec=Appending, sim=DefaultSimilarity, locale=en, timezone=Atlantic/Stanley
[junit4]   2> NOTE: all tests run in this JVM:
[junit4]   2> [TestLock, TestFileSwitchDirectory, TestWildcardRandom, TestVersionComparator, TestTermdocPerf, TestBitVector, TestParallelTermEnum, TestSimpleSearchEquivalence, TestNumericRangeQuery64, TestSort, TestIsCurrent, TestToken, TestIntBlockCodec, TestDocumentsWriterDeleteQueue, TestPagedBytes, TestThreadedForceMerge, TestOmitTf, TestSegmentTermEnum, TestIndexWriterConfig, TestCheckIndex, TestTermVectorsWriter, TestNumericTokenStream, TestSearchAfter, TestRegexpQuery, InBeforeClass, InAfterClass, InTestMethod, NonStringProperties, TestIndexWriterMergePolicy, TestVirtualMethod, TestFieldCache, TestSurrogates, TestSegmentTermDocs, TestMultiValuedNumericRangeQuery, TestBasicOperations, TestCodecs, TestDateSort, TestPositiveScoresOnlyCollector, TestBooleanQuery, TestIndexInput, TestMinimize, TestNumericRangeQuery32, TestBoolean2, TestSloppyPhraseQuery, TestNoDeletionPolicy, TestFieldCacheTermsFilter, TestRandomStoredFields, TestDocBoost, TestTransactionRollback, TestUnicodeUtil, TestIndexWriterLockRelease, TestUTF32ToUTF8, TestFixedBitSet, TestDoubleBarrelLRUCache, TestTimeLimitingCollector, TestSpanFirstQuery, TestDirectory, TestSpansAdvanced2, TestConcurrentMergeScheduler, TestIndexWriterExceptions, TestDocValues, TestCustomNorms, TestFieldValueFilter, TestTermVectors, TestTermInfosReaderIndex]
[junit4]   2> NOTE: Linux 2.6.32-38-server amd64/Sun Microsystems Inc. 1.6.0_20 (64-bit)/cpus=4,threads=1,free=100102360,total=243859456
[junit4]   2> 
{noformat}"
"LUCENE-569","BUG","BUG","NearSpans skipTo bug","NearSpans appears to have a bug in skipTo that causes it to skip over some matching documents completely.  I discovered this bug while investigating problems with SpanWeight.explain, but as far as I can tell the Bug is not specific to Explanations ... it seems like it could potentially result in incorrect matching in some situations where a SpanNearQuery is nested in another query such thatskipTo will be used ... I tried to create a high level test case to exploit the bug when searching, but i could not.  TestCase exploiting the class using NearSpan and SpanScorer will follow..."
"LUCENE-2503","RFE","RFE","light/minimal stemming for euro languages","The snowball stemmers are very aggressive and it would be nice if there were lighter alternatives.

Some applications may want to perform less aggressive stemming, for example:
http://www.lucidimagination.com/search/document/5d16391e21ca6faf/plural_only_stemmer

Good, relevance tested algorithms exist and I think we should provide these alternatives."
"LUCENE-2097","BUG","BUG","In NRT mode, and CFS enabled, IndexWriter incorrectly ties up disk space","Spinoff of java-user thread titled ""searching while optimize""...

If IndexWriter is in NRT mode (you've called getReader() at least
once), and CFS is enabled, then internally the writer pools readers.
However, after a merge completes, it opens the reader against het
non-CFS segment files, and pools that.  It then builds the CFS file,
as well, thus tying up the storage for that segment twice.

Functionally the bug is harmless (it's only a disk space issue).
Also, when the segment is merged, the disk space is released again
(though the newly merged segment will also be double-tied-up).

Simple workaround is to use non-CFS mode, or, don't use getReader."
"LUCENE-3073","DESIGN_DEFECT","IMPROVEMENT","make compoundfilewriter public","CompoundFileReader is public, but CompoundFileWriter is not.

I propose we make it public + @lucene.internal instead (just in case someone 
else finds themselves wanting to manipulate cfs files)
"
"LUCENE-1692","TEST","TEST","Contrib analyzers need tests","The analyzers in contrib need tests, preferably ones that test the behavior of all the Token 'attributes' involved (offsets, type, etc) and not just what they do with token text.

This way, they can be converted to the new api without breakage."
"LUCENE-1244","IMPROVEMENT","IMPROVEMENT","Get rid of (another) hard coded path",""
"LUCENE-523","BUG","BUG","FSDirectory.openFile(String) causes ClassCastException","When you call FSDirectory.openFile(String) you get a ClassCastException since FSIndexInput is not an org.apache.lucene.store.InputStream

The workaround is to reimplement using openInput(String). I personally don't need this to be fixed but wanted to document it here in case anyone else runs into this for any reason.

The reason I'm calling this is that I have a requirement on my project to create read only indexes and name the index segments consistently from one build to the next. So, after creating and optimizing the index, I rename the files and rewrite the segments file. It would be nice if I had an API that would allow me to say ""I only want one segment and I want its name to be 'foo'"". For instance IndexWriter.optimize(String segmentName)"
"LUCENE-3619","BUG","BUG","in trunk if you switch up omitNorms while indexing, you get a corrumpt norms file","document 1 has 
  body: norms=true
  title: norms=true
document 2 has 
  body: norms=false
  title: norms=true

when seeing 'body' for the first time, normswriterperfield gets 'initial fieldinfo' and 
saves it away, which says norms=true

however, at flush time we dont check, so we write the norms happily anyway.
then SegmentReader reads the norms later: it skips ""body"" since it omits norms
and if you ask for the norms of 'title' it instead returns the bogus ""body"" norms.

asserting that SegmentReader ""plans to"" read the whole .nrm file exposes the bug."
"LUCENE-1538","IMPROVEMENT","BUG","ValueSourceQuery hits synchronization bottleneck in IndexReader.isDeleted","I plan to fix it the same way we did in LUCENE-1316 for MatchAllDocsQuery (use TermDocs(null))."
"LUCENE-3509","IMPROVEMENT","IMPROVEMENT","Add settings to IWC to optimize IDV indices for CPU or RAM respectivly","spinnoff from LUCENE-3496 - we are seeing much better performance if required bits for PackedInts are rounded up to a 8/16/32/64. We should add this option to IWC and default to round up ie. more RAM & faster lookups."
"LUCENE-2854","CLEANUP","IMPROVEMENT","Deprecate SimilarityDelegator and Similarity.lengthNorm","SimilarityDelegator is a back compat trap (see LUCENE-2828).

Apps should just [statically] subclass Sim or DefaultSim; if they really need ""runtime subclassing"" then they can make their own app-level delegator.

Also, Sim.computeNorm subsumes lengthNorm, so we should deprecate lengthNorm in favor of computeNorm."
"LUCENE-987","TEST","TEST","Deprecate IndexModifier","See discussion at http://www.gossamer-threads.com/lists/lucene/java-dev/52017?search_string=deprecating%20indexmodifier;#52017

This is to deprecate IndexModifier before 3.0 and remove it in 3.0.

This patch includes:
  1 IndexModifier and TestIndexModifier are deprecated.
  2 TestIndexWriterModify is added. It is similar to TestIndexModifer but uses IndexWriter and has a few other changes. The changes are because of the difference between IndexModifier and IndexWriter.
  3 TestIndexWriterLockRelease and TestStressIndexing are switched to use IndexWriter instead of IndexModifier."
"LUCENE-345","BUG","BUG","Weird BooleanQuery behavior","Here's a simple OR-connected query.

T:files T:deleting C:thanks C:exists

The query above hits 1 document. But following *same* query only
with parenthesis results nothing.

(T:files T:deleting) (C:thanks C:exists)

Another combinations of MUST and SHOULD.

""T:files T:deleting +C:production +C:optimize"" hits 1 document.
""(T:files T:deleting) (+C:production +C:optimize)"" hits 1 document."
"LUCENE-1131","RFE","RFE","Add numDeletedDocs to IndexReader","Add numDeletedDocs to IndexReader. Basically, the implementation is as simple as doing:
public int numDeletedDocs() {
  return deletedDocs == null ? 0 : deletedDocs.count();
}
in SegmentReader.
Patch to follow to include in all IndexReader extensions."
"LUCENE-757","BUILD_SYSTEM","BUG","Source packaging fails if ${dist.dir} does not exist","package-tgz-src and package-zip-src fail if ${dist.dir} does not exist, since these two targets do not call the package target, which is responsible for making the dir.

I have a fix and will commit shortly."
"LUCENE-1429","BUG","BUG","close() throws incorrect IllegalStateEx after IndexWriter hit an OOME when autoCommit is true","Spinoff from http://www.nabble.com/IllegalStateEx-thrown-when-calling-close-to20201825.html

When IndexWriter hits an OOME, it records this and then if close() is
called it calls rollback() instead.  This is a defensive measure, in
case the OOME corrupted the internal buffered state (added/deleted
docs).

But there's a bug: if you opened IndexWriter with autoCommit true,
close() then incorrectly throws an IllegalStatException.

This fix is simple: allow rollback to be called even if autoCommit is
true, internally during close.  (External calls to rollback with
autoCommmit true is still not allowed).
"
"LUCENE-1458","RFE","RFE","Further steps towards flexible indexing","I attached a very rough checkpoint of my current patch, to get early
feedback.  All tests pass, though back compat tests don't pass due to
changes to package-private APIs plus certain bugs in tests that
happened to work (eg call TermPostions.nextPosition() too many times,
which the new API asserts against).

[Aside: I think, when we commit changes to package-private APIs such
that back-compat tests don't pass, we could go back, make a branch on
the back-compat tag, commit changes to the tests to use the new
package private APIs on that branch, then fix nightly build to use the
tip of that branch?o]

There's still plenty to do before this is committable! This is a
rather large change:

  * Switches to a new more efficient terms dict format.  This still
    uses tii/tis files, but the tii only stores term & long offset
    (not a TermInfo).  At seek points, tis encodes term & freq/prox
    offsets absolutely instead of with deltas delta.  Also, tis/tii
    are structured by field, so we don't have to record field number
    in every term.
.
    On first 1 M docs of Wikipedia, tii file is 36% smaller (0.99 MB
    -> 0.64 MB) and tis file is 9% smaller (75.5 MB -> 68.5 MB).
.
    RAM usage when loading terms dict index is significantly less
    since we only load an array of offsets and an array of String (no
    more TermInfo array).  It should be faster to init too.
.
    This part is basically done.

  * Introduces modular reader codec that strongly decouples terms dict
    from docs/positions readers.  EG there is no more TermInfo used
    when reading the new format.
.
    There's nice symmetry now between reading & writing in the codec
    chain -- the current docs/prox format is captured in:
{code}
FormatPostingsTermsDictWriter/Reader
FormatPostingsDocsWriter/Reader (.frq file) and
FormatPostingsPositionsWriter/Reader (.prx file).
{code}
    This part is basically done.

  * Introduces a new ""flex"" API for iterating through the fields,
    terms, docs and positions:
{code}
FieldProducer -> TermsEnum -> DocsEnum -> PostingsEnum
{code}
    This replaces TermEnum/Docs/Positions.  SegmentReader emulates the
    old API on top of the new API to keep back-compat.
    
Next steps:

  * Plug in new codecs (pulsing, pfor) to exercise the modularity /
    fix any hidden assumptions.

  * Expose new API out of IndexReader, deprecate old API but emulate
    old API on top of new one, switch all core/contrib users to the
    new API.

  * Maybe switch to AttributeSources as the base class for TermsEnum,
    DocsEnum, PostingsEnum -- this would give readers API flexibility
    (not just index-file-format flexibility).  EG if someone wanted
    to store payload at the term-doc level instead of
    term-doc-position level, you could just add a new attribute.

  * Test performance & iterate.
"
"LUCENE-2397","BUG","IMPROVEMENT","SnapshotDeletionPolicy.snapshot() throws NPE if no commits happened","SDP throws NPE if no commits occurred and snapshot() was called. I will replace it w/ throwing IllegalStateException. I'll also move TestSDP from o.a.l to o.a.l,index. I'll post a patch soon"
"LUCENE-2907","BUG","BUG","automaton termsenum bug when running with multithreaded search","This one popped in hudson (with a test that runs the same query against fieldcache, and with a filter rewrite, and compares results)

However, its actually worse and unrelated to the fieldcache: you can set both to filter rewrite and it will still fail.
"
"LUCENE-1916","DOCUMENTATION","TASK","smartcn HHMM doc translation","My coworker Patricia Peng translated the documentation and code comments for smartcn HHMM package.
"
"LUCENE-3575","BUG","BUG","Field names can be wrong for stored fields / term vectors after merging","The good news is this bug only exists in trunk... the bad news is it's
been here for some time (created by accident in LUCENE-2881).  But the
good news is it should strike fairly rarely.

SegmentMerger sometimes incorrectly thinks it can bulk-copy TVs/stored
fields when it cannot (because field numbers don't map to the same
names across segments).

I think it happens only with addIndexes, or indexes that have
pre-trunk segments, and then SM falsely thinks it can bulk-merge only
when the last field number has the same field name across segments.
"
"LUCENE-448","RFE","RFE","optional norms","For applications with many indexed fields, the norms cause memory problems both during indexing and querying.
This patch makes norms optional on a per-field basis, in the same way that term vectors are optional per-field.

Overview of changes:
 - Field.omitNorms that defaults to false
 - backward compatible lucene file format change: FieldInfos.FieldBits has a bit for omitNorms
 - IndexReader.hasNorms() method
 - During merging, if any segment includes norms, then norms are included.
 - methods to get norms return the equivalent 1.0f array for backward compatibility

The patch was designed for backward compatibility:
 - all current unit tests pass w/o any modifications required
 - compatible with old indexes since the default is omitNorms=false
 - compatible with older/custom subclasses of IndexReader since a default hasNorms() is provided
 - compatible with older/custom users of IndexReader such as Weight/Scorer/explain since a norm array is produced on demand, even if norms were not stored

If this patch is accepted (or if the direction is acceptable), performance for scoring  could be improved by assuming 1.0f when hasNorms(field)==false.
"
"LUCENE-3280","RFE","IMPROVEMENT","Add new bit set impl for caching filters","I think OpenBitSet is trying to satisfy too many audiences, and it's
confusing/error-proned as a result.  It has int/long variants of many
methods.  Some methods require in-bound access, others don't; of those
others, some methods auto-grow the bits, some don't.  OpenBitSet
doesn't always know its numBits.

I'd like to factor out a more ""focused"" bit set impl whose primary
target usage is a cached Lucene Filter, ie a bit set indexed by docID
(int, not long) whose size is known and fixed up front (backed by
final long[]) and is always accessed in-bounds.
"
"LUCENE-1974","BUG","BUG","BooleanQuery can not find all matches in special condition","query: (name:tang*)
doc=5137 score=1.0  doc:Document<stored,indexed<name:tangfulin>>
doc=11377 score=1.0  doc:Document<stored,indexed<name:tangfulin>>
query: name:tang* name:notexistnames
doc=5137 score=0.048133932  doc:Document<stored,indexed<name:tangfulin>>

It is two queries on the same index, one is just a prefix query in a
boolean query, and the other is a prefix query plus a term query in a
boolean query, all with Occur.SHOULD .

what I wonder is why the later query can not find the doc=11377 doc ?

the problem can be repreduced by the code in the attachment ."
"LUCENE-1831","DESIGN_DEFECT","BUG","TokenWrapperAttributeFactory, CachingWrapperFilterHelper implements equals and so should also implement hashCode","its part of the contract of Object 

bq. If two objects are equal according to the equals(Object) method, then calling the hashCode method on each of the two objects must produce the same integer result."
"LUCENE-2284","BUG","BUG","MatchAllDocsQueryNode toString() creates invalid XML-Tag","MatchAllDocsQueryNode.toString() returns ""<matchAllDocs field='*' term='*'>"", which is inavlid XML should read ""<matchAllDocs field='*' term='*' />.
"
"LUCENE-1630","RFE","IMPROVEMENT","Mating Collector and Scorer on doc Id orderness","This is a spin off of LUCENE-1593. This issue proposes to expose appropriate API on Scorer and Collector such that one can create an optimized Collector based on a given Scorer's doc-id orderness and vice versa. Copied from LUCENE-1593, here is the list of changes:

# Deprecate Weight and create QueryWeight (abstract class) with a new scorer(reader, scoreDocsInOrder), replacing the current scorer(reader) method. QueryWeight implements Weight, while score(reader) calls score(reader, false /* out-of-order */) and scorer(reader, scoreDocsInOrder) is defined abstract.
#* Also add QueryWeightWrapper to wrap a given Weight implementation. This one will also be deprecated, as well as package-private.
#* Add to Query variants of createWeight and weight which return QueryWeight. For now, I prefer to add a default impl which wraps the Weight variant instead of overriding in all Query extensions, and in 3.0 when we remove the Weight variants - override in all extending classes.
# Add to Scorer isOutOfOrder with a default to false, and override in BS to true.
# Modify BooleanWeight to extend QueryWeight and implement the new scorer method to return BS2 or BS based on the number of required scorers and setAllowOutOfOrder.
# Add to Collector an abstract _acceptsDocsOutOfOrder_ which returns true/false.
#* Use it in IndexSearcher.search methods, that accept a Collector, in order to create the appropriate Scorer, using the new QueryWeight.
#* Provide a static create method to TFC and TSDC which accept this as an argument and creates the proper instance.
#* Wherever we create a Collector (TSDC or TFC), always ask for out-of-order Scorer and check on the resulting Scorer isOutOfOrder(), so that we can create the optimized Collector instance.
# Modify IndexSearcher to use all of the above logic.

The only class I'm worried about, and would like to verify with you, is Searchable. If we want to deprecate all the search methods on IndexSearcher, Searcher and Searchable which accept Weight and add new ones which accept QueryWeight, we must do the following:
* Deprecate Searchable in favor of Searcher.
* Add to Searcher the new QueryWeight variants. Here we have two choices: (1) break back-compat and add them as abstract (like we've done with the new Collector method) or (2) add them with a default impl to call the Weight versions, documenting these will become abstract in 3.0.
* Have Searcher extend UnicastRemoteObject and have RemoteSearchable extend Searcher. That's the part I'm a little bit worried about - Searchable implements java.rmi.Remote, which means there could be an implementation out there which implements Searchable and extends something different than UnicastRemoteObject, like Activeable. I think there is very small chance this has actually happened, but would like to confirm with you guys first.
* Add a deprecated, package-private, SearchableWrapper which extends Searcher and delegates all calls to the Searchable member.
* Deprecate all uses of Searchable and add Searcher instead, defaulting the old ones to use SearchableWrapper.
* Make all the necessary changes to IndexSearcher, MultiSearcher etc. regarding overriding these new methods.

One other optimization that was discussed in LUCENE-1593 is to expose a topScorer() API (on Weight) which returns a Scorer that its score(Collector) will be called, and additionally add a start() method to DISI. That will allow Scorers to initialize either on start() or score(Collector). This was proposed mainly because of BS and BS2 which check if they are initialized in every call to next(), skipTo() and score(). Personally I prefer to see that in a separate issue, following that one (as it might add methods to QueryWeight)."
"LUCENE-722","REFACTORING","BUG","DEFAULT spelled DEFALT in MoreLikeThis.java","DEFAULT is spelled DEFALT in contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis.java"
"LUCENE-1191","IMPROVEMENT","IMPROVEMENT","If IndexWriter hits OutOfMemoryError it should not commit","While progress has been made making IndexWriter robust to OOME, I
think there is still a real risk that an OOME at a bad time could put
IndexWriter into a bad state such that if close() is called and
somehow it succeeds without hitting another OOME, it risks
introducing messing up the index.

I'd like to detect if OOME has been hit in any of the methods that
alter IW's state, and if so, do not commit changes to the index.  If
close is called after hitting OOME, I think writer should instead
abort.

Attached patch just adds try/catch clauses to catch OOME, note that
it was hit, and re-throw it.  Then, sync() refuses to commit a new
segments_N if OOME was hit, and close instead calls abort when OOME
was hit.  All tests pass.  I plan to commit in a day or two."
"LUCENE-946","OTHER","TASK","replace text from an online collection (used in few test cases) with text that is surely 100% free.","Text from an online firstaid collection (firstaid . ie . eu . org)  is used as arbitrary text for test documents creation, in:
   o.a.l.analysis.Analyzer.FunctionTestSetup.DOC_TEXT_LINES
   o.a.l.benchmark.byTask.feeds.SimpleDocMaker.DOC_TEXT

I once got this text from Project Gutenberg and was sure that it is free. But now the referred Web site does not seem to respond, and I can no more find that firstaid eBook in the Project Gutenberg site.

Since it doesn't matter what text we use there, I will just replace that with some of my own words..."
"LUCENE-3433","RFE","RFE","Random access non RAM resident IndexDocValues (CSF)","There should be a way to get specific IndexDocValues by going through the Directory rather than loading all of the values into memory."
"LUCENE-306","BUG","BUG","[PATCH]multiple wildcards ? at the end of search pattern return incorrect hits","The problem is if you search on ""ca??"", the hit includes 'cat', 'CA', 
etc, while the user only wants 4 letter words start with CA, such as 
'card', 'cash', to be returned. This happens only when multiple '?' at 
the end of search pattern. The solution is to check if the word that is 
matching against search pattern ends while there is still '?' left. If 
this is the case, match should return false.

Attached is the patch code I generated use 'diff'
********************************************************************

--- WildcardTermEnum.org	2004-05-11 11:42:10.000000000 -0400
+++ WildcardTermEnum.java	2004-11-08 14:35:14.823610500 -0500
@@ -132,6 +132,10 @@
             }
             else
             {
+	      //to prevent ""cat"" matches ""ca??""
+	      if(wildchar == WILDCARD_CHAR){
+		return false;
+	      }	      
               // Look at the next character
               wildcardSearchPos++;
             }
**********************************************************************"
"LUCENE-429","IMPROVEMENT","IMPROVEMENT","Little improvement for SimpleHTMLEncoder","The SimpleHTMLEncoder could be improved slightly: all characters with code >=
128 should be encoded as character entities. The reason is, that the encoder
does not know the encoding that is used for the response. Therefore it is safer
to encode all characters beyond ASCII as character entities.

Here is the necessary modification of SimpleHTMLEncoder:

       default:
         if (c < 128) {
           result.append(c);
         } else {
           result.append(""&#"").append((int)c).append("";"");
         }"
"LUCENE-1935","REFACTORING","TASK","Generify PriorityQueue","Priority Queue should use generics like all other Java 5 Collection API classes. This very simple, but makes code more readable."
"LUCENE-3141","IMPROVEMENT","IMPROVEMENT","FastVectorHighlighter - expose FieldFragList.fragInfo for user-customizable FragmentsBuilder","Needed to build a custom highlightable snippet - snippet should start with the sentence containing the first match, then continue for 250 characters.

So created a custom FragmentsBuilder extending SimpleFragmentsBuilder and overriding the createFragments(IndexReader reader, int docId, String fieldName, FieldFragList fieldFragList) method - unit test containing the code is attached to the JIRA.

To get this to work, needed to expose (make public) the FieldFragList.fragInfo member variable. This is currently package private, so only FragmentsBuilder implementations within the lucene-highlighter o.a.l.s.vectorhighlight package (such as SimpleFragmentsBuilder) can access it. Since I am just using the lucene-highlighter.jar as an external dependency to my application, the simplest way to access FieldFragList.fragInfo in my class was to make it public.
"
"LUCENE-439","BUG","BUG","Filters need hashCode() and equals()","Filters need to implement hashCode() and equals(), esp since certain query types can contain a filter (FilteredQuery, ConstantScoreQuery)"
"LUCENE-3086","RFE","IMPROVEMENT","add ElisionsFilter to ItalianAnalyzer","we set this up for french by default, but we don't for italian.
we should enable it with the standard italian contractions (e.g. definite articles).

the various stemmers for these languages assume this is already being taken care of
and don't do anything about it... in general things like snowball assume really dumb
tokenization, that you will split on the word-internal ', and they add these to stoplists."
"LUCENE-3464","REFACTORING","BUG","Rename IndexReader.reopen to make it clear that reopen may not happen","Spinoff from LUCENE-3454 where Shai noted this inconsistency.

IR.reopen sounds like an unconditional operation, which has trapped users in the past into always closing the old reader instead of only closing it if the returned reader is new.

I think this hidden maybe-ness is trappy and we should rename it (maybeReopen?  reopenIfNeeded?).

In addition, instead of returning ""this"" when the reopen didn't happen, I think we should return null to enforce proper usage of the maybe-ness of this API."
"LUCENE-2034","REFACTORING","IMPROVEMENT","Massive Code Duplication in Contrib Analyzers - unifly the analyzer ctors","Due to the variouse tokenStream APIs we had in lucene analyzer subclasses need to implement at least one of the methodes returning a tokenStream. When you look at the code it appears to be almost identical if both are implemented in the same analyzer.  Each analyzer defnes the same inner class (SavedStreams) which is unnecessary.
In contrib almost every analyzer uses stopwords and each of them creates his own way of loading them or defines a large number of ctors to load stopwords from a file, set, arrays etc.. those ctors should be removed / deprecated and eventually removed.


"
"LUCENE-1536","IMPROVEMENT","IMPROVEMENT","if a filter can support random access API, we should use it","I ran some performance tests, comparing applying a filter via
random-access API instead of current trunk's iterator API.

This was inspired by LUCENE-1476, where we realized deletions should
really be implemented just like a filter, but then in testing found
that switching deletions to iterator was a very sizable performance
hit.

Some notes on the test:

  * Index is first 2M docs of Wikipedia.  Test machine is Mac OS X
    10.5.6, quad core Intel CPU, 6 GB RAM, java 1.6.0_07-b06-153.

  * I test across multiple queries.  1-X means an OR query, eg 1-4
    means 1 OR 2 OR 3 OR 4, whereas +1-4 is an AND query, ie 1 AND 2
    AND 3 AND 4.  ""u s"" means ""united states"" (phrase search).

  * I test with multiple filter densities (0, 1, 2, 5, 10, 25, 75, 90,
    95, 98, 99, 99.99999 (filter is non-null but all bits are set),
    100 (filter=null, control)).

  * Method high means I use random-access filter API in
    IndexSearcher's main loop.  Method low means I use random-access
    filter API down in SegmentTermDocs (just like deleted docs
    today).

  * Baseline (QPS) is current trunk, where filter is applied as iterator up
    ""high"" (ie in IndexSearcher's search loop)."
"LUCENE-1047","REFACTORING","IMPROVEMENT","Change MergePolicy & MergeScheduler to be abstract base classes instead of an interfaces","This gives us freedom to add methods with default base implementation over time w/o breaking backwards compatibility.

Thanks to Hoss for raising this!"
"LUCENE-2162","RFE","IMPROVEMENT","ExtendableQueryParser should allow extensions to access the toplevel parser settings/ properties","Based on the latest discussions in LUCENE-2039 this issue will expose the toplevel parser via the ExtensionQuery so that ExtensionParsers can access properties like getAllowLeadingWildcards() from the top level parser."
"LUCENE-3898","TEST","BUG","possible SynonymFilter bug: hudson fail","See https://builds.apache.org/job/Lucene-trunk/1867/consoleText (no seed)"
"LUCENE-1194","RFE","RFE","Add deleteByQuery to IndexWriter","This has been discussed several times recently:

  http://markmail.org/message/awlt4lmk3533epbe
  http://www.gossamer-threads.com/lists/lucene/java-user/57384#57384

If we add deleteByQuery to IndexWriter then this is a big step towards
allowing IndexReader to be readonly.

I took the approach suggested in that first thread: I buffer delete
queries just like we now buffer delete terms, holding the max docID
that the delete should apply to.

Then, I also decoupled flushing deletes (mapping term or query -->
actual docIDs that need deleting) from flushing added documents, and
now I flush deletes only when a merge is started, or on commit() or
close().  SegmentMerger now exports the docID map it used when
merging, and I use that to renumber the max docIDs of all pending
deletes.

Finally, I turned off tracking of memory usage of pending deletes
since they now live beyond each flush.  Deletes are now only
explicitly flushed if you set maxBufferedDeleteTerms to something
other than DISABLE_AUTO_FLUSH.  Otherwise they are flushed at the
start of every merge."
"LUCENE-3635","RFE","IMPROVEMENT","Allow setting arbitrary objects on PerfRunData","PerfRunData is used as the intermediary objects between PerfRunTasks. Just like we can set IndexReader/Writer on it, it will be good if it allows setting other arbitrary objects that are e.g. created by one task and used by another.

A recent example is the enhancement to the benchmark package following the addition of the facet module. We had to add TaxoReader/Writer.

The proposal is to add a HashMap<String, Object> that custom PerfTasks can set()/get(). I do not propose to move IR/IW/TR/TW etc. into that map. If however people think that we should, I can do that as well."
"LUCENE-2627","BUG","BUG","MMapDirectory chunking is buggy","MMapDirectory uses chunking with MultiMMapIndexInput.
 
Because Java's ByteBuffer uses an int to address the
values, it's necessary to access a file >
Integer.MAX_VALUE in size using multiple byte buffers.

But i noticed from the clover report the entire MultiMMapIndexInput class is completely untested: no surprise since all tests make tiny indexes.
"
"LUCENE-1415","BUG","BUG","MultiPhraseQuery has incorrect hashCode() implementation - Leads to Solr Cache misses","I found this while hunting for the cause of Solr Cache misses.

The MultiPhraseQuery class hashCode() implementation is non-deterministic. It uses termArrays.hashCode() in the computation. The contents of that ArrayList are actually arrays themselves, which return there reference ID as a hashCode instead of returning a hashCode which is based on the contents of the array. I would suggest an implementation involving the Arrays.hashCode() method.

I will try to submit a patch soon, off for today."
"LUCENE-359","RFE","BUG","[PATCH] add term index interval accessors","It should be possible for folks to set the index interval used when writing indexes."
"LUCENE-1770","RFE","RFE","EnwikiQueryMaker",""
"LUCENE-2514","RFE","TASK","Change Term to use bytes","in LUCENE-2426, the sort order was changed to codepoint order.

unfortunately, Term is still using string internally, and more importantly its compareTo() uses the wrong order [utf-16].
So MultiTermQuery, etc (especially its priority queues) are currently wrong.

By changing Term to use bytes, we can also support terms encoded as bytes such as numerics, instead of using
strange string encodings.
"
"LUCENE-2052","CLEANUP","IMPROVEMENT","Scan method signatures and add varargs where possible","I changed a lot of signatures, but there may be more. The important ones like MultiReader and MultiSearcher are already done. This applies also to contrib. Varargs are no backwards break, they stay arrays as before."
"LUCENE-294","RFE","IMPROVEMENT","DisjunctionScorer","This disjunction scorer can match a minimum nr. of docs, 
it provides skipTo() and it uses skipTo() on the subscorers. 
The score() method is abstract in DisjunctionScorer and implemented 
in DisjunctionSumScorer as an example."
"LUCENE-3471","BUG","BUG","TestNRTManager test failure","reproduces for me"
"LUCENE-337","BUG","BUG","Combination of BooleanQuery and PhrasePrefixQuery can provoke UnsupportedOperationException","A BooleanQuery including a PhrasePrefixQuery can cause an exception to be thrown
from BooleanScorer#skipTo when the search is executed:  

java.lang.UnsupportedOperationException
	at org.apache.lucene.search.BooleanScorer.skipTo(BooleanScorer.java:189)
	at org.apache.lucene.search.ConjunctionScorer.doNext(ConjunctionScorer.java:53)
	at org.apache.lucene.search.ConjunctionScorer.next(ConjunctionScorer.java:48)
	at org.apache.lucene.search.Scorer.score(Scorer.java:37)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:92)
	at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:64)
	at org.apache.lucene.search.Hits.<init>(Hits.java:43)
	at org.apache.lucene.search.Searcher.search(Searcher.java:33)
	at org.apache.lucene.search.Searcher.search(Searcher.java:27)
        ... (non-lucene code)

The problem appears to be that PhrasePrefixQuery optimizes itself into a
BooleanQuery when it contains only one term.  However, it does this in the
createWeight() method of its scorer instead of in the rewrite method of the
query itself.  Thus it bypasses the boolean typecheck when BooleanQuery is
deciding whether to use ConjunctionScorer or BooleanScorer, eventually resulting
in the UOE."
"LUCENE-667","BUILD_SYSTEM","BUG","javacc skeleton files not regenerated","Copies of the the character stream files for javacc are checked into svn. These files were generated under javacc 3.0 (at least that's what they say, though javacc 3.2 says this too). javacc 4 complains that they are out of date but won't replace them; they must be removed before it will regenerate them.

There is one side effect of removing them: local changes are lost.  r387550 removed a couple of deprecated methods. By using the files as generated by javacc, these deprecated  methods will be readded (at least until the javacc team removes them totally). There are other changes being made to the stream files, so I woudl think it's better to live with them unmodified than to keep local versions just for this change.

If we want javacc to recreate the files, the attached patch will remove them before running javacc.

All the tests pass using both javacc3.2 and 4.0.


"
"LUCENE-3240","REFACTORING","","Move FunctionQuery, ValueSources and DocValues to Queries module","Having resolved the FunctionQuery sorting issue and moved the MutableValue classes, we can now move FunctionQuery, ValueSources and DocValues to a Queries module."
"LUCENE-561","BUG","BUG","ParallelReader fails on deletes and on seeks of previously unused fields","In using ParallelReader I've hit two bugs:

1.  ParallelReader.doDelete() and doUndeleteAll() call doDelete() and doUndeleteAll() on the subreaders, but these methods do not set hasChanges.  Thus the changes are lost when the readers are closed.  The fix is to call deleteDocument() and undeleteAll() on the subreaders instead.

2.  ParallelReader discovers the fields in each subindex by using IndexReader.getFieldNames() which only finds fields that have occurred on at least one document.  In general a parallel index is designed with assignments of fields to sub-indexes and term seeks (including searches) may be done on any of those fields, even if no documents in a particular state of the index have yet had an assigned field.  Seeks/searches on fields that have not yet been indexed generated an NPE in ParallelReader's various inner class seek() and next() methods because fieldToReader.get() returns null on the unseen field.  The fix is to extend the add() methods to supply the correct list of fields for each subindex.

Patch that corrects both of these issues attached.
"
"LUCENE-1830","BUG","BUG","BoostingNearQuery doesn't have hashCode/equals",""
"LUCENE-674","IMPROVEMENT","BUG","Error in FSDirectory if java.io.tmpdir incorrectly specified","A user of the JAMWiki project (http://jamwiki.org/) reported an error with the following stack trace:

SEVERE: Unable to create search instance /usr/share/tomcat5/webapps/jamwiki-0.3.4-beta7/test/base/search/indexen
java.io.IOException: Cannot create directory: /temp
        at org.apache.lucene.store.FSDirectory.init(FSDirectory.java:171)
        at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:141)
        at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:117)
        at org.jamwiki.search.LuceneSearchEngine.getSearchIndexPath(LuceneSearchEngine.java:318)

The culprit is that the java.io.tmpdir property was incorrectly specified on the user's system.  Lucene could easily handle this issue by modifying the FSDirectory.init() method.  Currently the code uses the index directory if java.io.tmpdir and org.apache.lucene.lockDir are unspecified, but it could use that directory if those values are unspecified OR if they are invalid.  Doing so would make Lucene a bit more robust without breaking any existing installations.
"
"LUCENE-3308","CLEANUP","","Cleanup 'good' queries code","Before moving some of the classes from the queries contrib to the queries module, I want to just pass over them and clean them up, since we want code in modules to be of the same calibre as core code."
"LUCENE-499","DOCUMENTATION","IMPROVEMENT","Documentation improvements for 1.9 release","I've poked arround the 1.9-rc1 builds and noticed a few simple documentation things that could be cleaned up, a patch will follow that...

1) Adds some additional info to the README.txt
2) Updates the version info in queryparsersyntax.xml and fileformats.html, and advises people 
     with older versions how to find the correct documentation for their version
3) Builds javadocs for all of the contrib modules (the list was incomplete)"
"LUCENE-2542","DESIGN_DEFECT","IMPROVEMENT","TopDocsCollector should be abstract super class that is the real ""TopDocsCollector"" contract, a subclass should implement the priority-queue logic. e.g. PQTopDocsCollector","TopDocsCollector is both an abstract interface for producing TopDocs as well as a PriorityQueue based implementation.
Not all Collectors that could produce TopDocs must use a PriorityQueue, and it would be advantageous to allow the TopDocsCollector to be an ""interface"" type abstract class, with a PQTopDocsCollector sub-class.
While doing this, it'd be good to clean up the generics uses in these classes. As it's odd to create a TopFieldCollector and have to case the TopDocs object, when this can be fixed with generics."
"LUCENE-441","IMPROVEMENT","BUG","IntParser and FloatParser unused by FieldCacheImpl","FieldCacheImpl doesn't use IntParser or FloatParser to parse values"
"LUCENE-3198","IMPROVEMENT","IMPROVEMENT","Change default Directory impl on 64bit linux to MMap","Consistently in my NRT testing on Fedora 13 Linux, 64 bit JVM (Oracle 1.6.0_21) I see MMapDir getting better search and merge performance when compared to NIOFSDir.

I think we should fix the default."
"LUCENE-1046","BUG","BUG","Dead code in SpellChecker.java (branch never executes)","SpellChecker contains the following lines of code:

    final int goalFreq = (morePopular && ir != null) ? ir.docFreq(new Term(field, word)) : 0;
    // if the word exists in the real index and we don't care for word frequency, return the word itself
    if (!morePopular && goalFreq > 0) {
      return new String[] { word };
    }

The branch will never execute: the only way for goalFreq to be greater than zero is if morePopular is true, but if morePopular is true, the expression in the if statement evaluates to false.

"
"LUCENE-421","RFE","IMPROVEMENT","Numeric range searching with large value sets","I have a set of enhancements that build on the numeric sorting cache introduced
by Tim Jones and that provide integer and floating point range searches over
numeric ranges that are far too large to be implemented via the current term
range rewrite mechanism.  I'm new to Apache and trying to find out how to attach
the source files for the changes for your consideration."
"LUCENE-2869","CLEANUP","TASK","remove Query.getSimilarity()","Spinoff of LUCENE-2854.

See LUCENE-2828 and LUCENE-2854 for reference.

In general, the SimilarityDelegator was problematic with regards to back-compat, and if queries
want to score differently, trying to runtime subclass Similarity only causes trouble.

The reason we could not fix this in LUCENE-2854 is because:
{noformat}
Michael McCandless added a comment - 08/Jan/11 01:53 PM
bq. Is it possible to remove this method Query.getSimilarity also? I don't understand why we need this method!

I would love to! But I think that's for another day...

I looked into this and got stuck with BoostingQuery, which rewrites to an anon 
subclass of BQ overriding its getSimilarity in turn override its coord method. 
Rather twisted... if we can do this differently I think we could remove Query.getSimilarity.
{noformat}

here is the method in question:

{noformat}
/** Expert: Returns the Similarity implementation to be used for this query.
 * Subclasses may override this method to specify their own Similarity
 * implementation, perhaps one that delegates through that of the Searcher.
 * By default the Searcher's Similarity implementation is returned.*/
public Similarity getSimilarity(IndexSearcher searcher) {
  return searcher.getSimilarity();
}
{noformat}
"
"LUCENE-1713","REFACTORING","IMPROVEMENT","Rename RangeQuery -> TermRangeQuery","Since we now have NumericRangeQuery (LUCENE-1701) we should rename RangeQuery to TextRangeQuery to make it clear that TextRangeQuery (TermRangeQuery?  StringRangeQuery) is based entirely on text comparison.

And, existing users on upgrading to 2.9 and using RangeQuery for [slow] numeric searching would realize they now have a good option for numeric range searching."
"LUCENE-1628","RFE","RFE","Persian Analyzer","A simple persian analyzer.

i measured trec scores with the benchmark package below against http://ece.ut.ac.ir/DBRG/Hamshahri/ :

SimpleAnalyzer:
SUMMARY
  Search Seconds:         0.012
  DocName Seconds:        0.020
  Num Points:           981.015
  Num Good Points:       33.738
  Max Good Points:       36.185
  Average Precision:      0.374
  MRR:                    0.667
  Recall:                 0.905
  Precision At 1:         0.585
  Precision At 2:         0.531
  Precision At 3:         0.513
  Precision At 4:         0.496
  Precision At 5:         0.486
  Precision At 6:         0.487
  Precision At 7:         0.479
  Precision At 8:         0.465
  Precision At 9:         0.458
  Precision At 10:        0.460
  Precision At 11:        0.453
  Precision At 12:        0.453
  Precision At 13:        0.445
  Precision At 14:        0.438
  Precision At 15:        0.438
  Precision At 16:        0.438
  Precision At 17:        0.429
  Precision At 18:        0.429
  Precision At 19:        0.419
  Precision At 20:        0.415

PersianAnalyzer:
SUMMARY
  Search Seconds:         0.004
  DocName Seconds:        0.011
  Num Points:           987.692
  Num Good Points:       36.123
  Max Good Points:       36.185
  Average Precision:      0.481
  MRR:                    0.833
  Recall:                 0.998
  Precision At 1:         0.754
  Precision At 2:         0.715
  Precision At 3:         0.646
  Precision At 4:         0.646
  Precision At 5:         0.631
  Precision At 6:         0.621
  Precision At 7:         0.593
  Precision At 8:         0.577
  Precision At 9:         0.573
  Precision At 10:        0.566
  Precision At 11:        0.572
  Precision At 12:        0.562
  Precision At 13:        0.554
  Precision At 14:        0.549
  Precision At 15:        0.542
  Precision At 16:        0.538
  Precision At 17:        0.533
  Precision At 18:        0.527
  Precision At 19:        0.525
  Precision At 20:        0.518

"
"LUCENE-3294","BUG","BUG","Some code still compares string equality instead using equals","I found a couple of places where we still use string == otherstring which don't look correct. I will attache a patch soon."
"LUCENE-1392","DOCUMENTATION","IMPROVEMENT","Some small javadocs/extra import fixes","Two things that Uwe Schindler caught, plus fixes for javadoc warnings in core.  I plan to commit to trunk & 2.4."
"LUCENE-742","CLEANUP","IMPROVEMENT","SpanOrQuery.java: simplification and test","The current SpanOrQuery.java has some unnessary attributes. After removing these, I found that there was no existing test for it, so I added some tests to TestSpans.java."
"LUCENE-469","BUG","BUG","(Parallel-)MultiSearcher: using Sort object changes the scores","Example: 
Hits hits=multiSearcher.search(query);
returns different scores for some documents than
Hits hits=multiSearcher.search(query, Sort.RELEVANCE);
(both for MultiSearcher and ParallelMultiSearcher)

The documents returned will be the same and in the same order, but the scores in the second case will seem out of order.

Inspecting the Explanation objects shows that the scores themselves are ok, but there's a bug in the normalization of the scores.

The document with the highest score should have score 1.0, so all document scores are divided by the highest score.  (Assuming the highest score was>1.0)

However, for MultiSearcher and ParallelMultiSearcher, this normalization factor is applied *per index*, before merging the results together (the merge itself is ok though).

An example: if you use
Hits hits=multiSearcher.search(query, Sort.RELEVANCE);
for a MultiSearcher with two subsearchers, the first document will have score 1.0.
The next documents from the same subsearcher will have decreasing scores.
The first document from the other subsearcher will however have score 1.0 again !

The same applies for other Sort objects, but it is less visible.

I will post a TestCase demonstrating the problem and suggested patches to solve it in a moment..."
"LUCENE-2683","BUILD_SYSTEM","IMPROVEMENT","upgrade icu libraries to 4.4.2","modules/analysis uses 4.4
solr/contrib/extraction uses 4.2.1

I think we should keep them the same version, for consistency, and go to 4.4.2 since it has bugfixes."
"LUCENE-3764","REFACTORING","TASK","Remove oal.util.MapBackedSet (Java 6 offsers Collections.newSetFromMap())","Easy search and replace job. In 3.x we still need the class, as Java 5 does not have Collections.newSetFromMap()."
"LUCENE-3646","RFE","TASK","throw exception for fieldcache on a non-atomic reader","In Lucene 4.0, we go through a lot of effort to prevent slow uses of non-atomic readers:

DirectoryReader/MultiReader etc throw exception if you don't try to access postings or docvalues apis per-segment, etc.

But the biggest trap of all is still too easy to fall into, we don't do the same for FieldCache.

I think we should throw exception, forcing the user to either change their code or use a SlowMultiReaderWrapper.
"
"LUCENE-3856","RFE","IMPROVEMENT","Create docvalues based grouped facet collector","Create docvalues based grouped facet collector. Currently only term based collectors have been implemented (LUCENE-3802)."
"LUCENE-1270","BUG","BUG","After IW.addIndexesNoOptimize, IW.close may hang","Spinoff from here:

  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200804.mbox/%3c43128.192.168.1.71.1208561409.webmail@192.168.1.71%3e

The addIndexesNoOptimize method first merges eligible segments
according to the MergePolicy, and then copies over one by one any
remaining ""external"" segments.

That copy can possibly (rather rarely) result in new merges becoming
eligible because its size can change if the index being added was
created with autoCommit=false.

However, we fail to then invoke the MergeScheduler to run these
merges.  As a result, in close, where we wait until all running and
pending merges complete, we will never return.

The fix is simple: invoke the merge scheduler inside
copyExternalSegments() if any segments were copied.  I also added
defensive invocation of the merge scheduler during close, just in case
other code paths could allow for a merge to be added to the pending
queue but not scheduled.

"
"LUCENE-2077","DOCUMENTATION","IMPROVEMENT","changes-to-html: better handling of bulleted lists in CHANGES.txt","- bulleted lists
- should be rendered
- as such
- in output HTML"
"LUCENE-442","TEST","BUG","TestIndexModifier.testIndexWithThreads is not valid?","I recently started playing with the trunk of SVN, and noticed that intermitently, TestIndexModifier.testIndexWithThreads (revision 292010) would fail.

The basic premise of the test seems to be that 3 pairs of IndexThread instances can be started in parallel, each pair using the same instance of IndexModifier to concurrently and randomly add/delete/optimize a single FSDirectory index.  
The test is considered a success if the sum of additions-deletions recorded by each pair of threads equals the final docCount() for the IndexModifier instance used by that pair of threads.

Now I freely admit that I'm not 100% familiar with the code for IndexModifier, but at a glance, the basic premise seems to be: 
   a) If method for IndexWriter is called, open it if needed, close the IndexReader first if needed.
   b) if method for IndexReader is called, open it if needed, close the IndexWriter first if needed.

If I'm understnading that correctly, I see no reason to assume this test will pass.  
It seems like there could be plenty of scenerios in which the number of additions-deletions != docCount(). The most trivial example I can think of is:
   1) the first IndexThread instance which has a chance to run adds a document, and optimizes before any other IndexThreads ever open the Directory.
   2) a subsequent pair of IndexThread instances open their IndexModifier instance before any documents are deleted.
   3) the IndexThread instances from #2 do nothing but add documents
...that pair of IndexThreads is now garunteed to have recorded a differnet number of additions then the docCount returned by their IndexModifier.

Am I missing something, or should this test be removed?

"
"LUCENE-1173","BUG","BUG","index corruption autoCommit=false","In both Lucene 2.3 and trunk, the index becomes corrupted when autoCommit=false"
"LUCENE-586","IMPROVEMENT","IMPROVEMENT","Very inefficient implementation of MultiTermDocs.skipTo","In our application anytime the index was unoptimized/contained more than one segment there was a sharp drop in performance, which amounted to over 50ms per search on average.  We would consistently see this drop anytime an index went from an optimized state to an unoptimized state.

I tracked down the issue to the implementation of MultiTermDocs.skipTo function (found in MultiReader.java).  Optimized indexes do not use this class during search but unoptimized indexes do.  The comment on this function even explicitly states 'As yet unoptimized implementation.'  It was implemented just by calling 'next' over and over so even if it knew it could skip ahead hundreds of thousands of hits it would not.

So I re-implemented the function very similar to how the MultiTermDocs.next function was implemented and tested it out on or application for correctness and performance and it passed all our tests and the performance penalty of having multiple segments vanished.  We have already put the new jar onto our production machines.

Here is my implementation of skipTo, which closely mirrors the accepted implementation of 'next', please feel free to test it and commit it.

  /** Much more optimized implementation. Could be
   * optimized fairly easily to skip entire segments */
  public boolean skipTo(int target) throws IOException {
    if (current != null && current.skipTo(target-base)) {
      return true;
    } else if (pointer < readers.length) {
      base = starts[pointer];
      current = termDocs(pointer++);
      return skipTo(target);
    } else
      return false;
  }"
"LUCENE-556","BUG","BUG","MatchAllDocsQuery, MultiSearcher and a custom HitCollector throwing exception","I have encountered an issue with lucene1.9.1. It involves MatchAllDocsQuery, MultiSearcher and a custom HitCollector. The following code throws  java.lang.UnsupportedOperationException.

If I remove the MatchAllDocsQuery  condition (comment whole //1 block), or if I dont use the custom hitcollector (ms.search(mbq); instead of ms.search(mbq, allcoll);) the exception goes away. By stepping into the source I can see it seems due to MatchAllDocsQuery no implementing extractTerms()....


           Searcher searcher = new
IndexSearcher(""c:\\projects\\mig\\runtime\\index\\01Aug16\\"");
           Searchable[] indexes = new IndexSearcher[1];
           indexes[0] = searcher;
           MultiSearcher ms = new MultiSearcher(indexes);

           AllCollector allcoll = new AllCollector(ms);

           BooleanQuery mbq = new BooleanQuery();
           mbq.add(new TermQuery(new Term(""body"", ""value1"")),
BooleanClause.Occur.MUST_NOT);
// 1
           MatchAllDocsQuery alld = new MatchAllDocsQuery();
           mbq.add(alld, BooleanClause.Occur.MUST);
//

           System.out.println(""Query: "" + mbq.toString());

           // 2
           ms.search(mbq, allcoll);
           //ms.search(mbq);"
"LUCENE-3522","BUG","BUG","TermsFilter.getDocIdSet(context) NPE on missing field","If the context does not contain the field for a term when calling TermsFilter.getDocIdSet(AtomicReaderContext context) then a NullPointerException is thrown due to not checking for null Terms before getting iterator."
"LUCENE-632","BUG","BUG","The creation of a spell index from a LuceneDictionary via SpellChecker.indexDictionary (Dictionary dict) fails starting with 1.9.1 (up to current svn version)","Two different errors in 1.9.1/2.0.0 and current svn version.

1.9.1/2.0.0:
at the end of indexDictionary (Dictionary dict) 
the IndexReader-instance reader is closed.
This causes a NullpointerException because reader has not been initialized before (neither in that method nor in the constructor).
Uncommenting this line (reader.close()) seems to resolve that issue.

current svn:
the constructor tries to create an IndexSearcher-instance for the specified path;
as there is no index in that path - it is not created yet -  an exception is thrown.

"
"LUCENE-2286","IMPROVEMENT","IMPROVEMENT","enable DefaultSimilarity.setDiscountOverlaps by default","I think we should enable setDiscountOverlaps in DefaultSimilarity by default.

If you are using synonyms or commongrams or a number of other 0-posInc-term-injecting methods, these currently screw up your length normalization.
These terms have a position increment of zero, so they shouldnt count towards the length of the document.

I've done relevance tests with persian showing the difference is significant, and i think its a big trap to anyone using synonyms, etc: your relevance can actually get worse if you don't flip this boolean flag."
"LUCENE-3213","REFACTORING","IMPROVEMENT","Use AtomicReaderContext also for CustomScoreProvider","When moving to AtomicReaderContext, one place was not changed to use it: CustomScoreQuery's CustomScoreProvider. It should also take AtomicReaderContext instead of IndexReader, as this may help users to effectively implement custom scoring there absolute DocIds are needed."
"LUCENE-228","IMPROVEMENT","BUG","encoding of GermanAnalyzer.java and GermanStemmer.java isn't utf-8","For PyLucene, the gcj/swig - based python integration of java lucene, it would
be good if java source files didn't use encodings other than utf-8.
On Windows - and systems without iconv support in general - compiling code  
with gcj where the java source text is in another encoding than utf-8 is    
difficult if not impossible.

To change the encoding on these files:

 iconv -f iso-8859-1 -t utf-8 GermanAnalyzer.java > GermanAnalyzer.java.utf-8
 iconv -f iso-8859-1 -t utf-8 GermanStemmer.java > GermanStemmer.java.utf-8"
"LUCENE-3170","BUG","BUG","TestDocValuesIndexing reproducible  test failure","docvalues branch: r1131275

{code}
    [junit] Testsuite: org.apache.lucene.index.values.TestDocValuesIndexing
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.81 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestDocValuesIndexing -Dtestmethod=testAddIndexes -Dtests.seed=-3253978684351194958:-8331223747763543724
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=Standard, BYTES_VAR_STRAIGHT=Pulsing(freqCutoff=12), BYTES_FIXED_SORTED=MockRandom}, locale=es_MX, timezone=Pacific/Chatham
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDocValuesIndexing]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=89168480,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAddIndexes(org.apache.lucene.index.values.TestDocValuesIndexing):     FAILED
    [junit] [first=BYTES_FIXED_SORTED, second=BYTES_VAR_STRAIGHT] expected:<9> but was:<10>
    [junit] junit.framework.AssertionFailedError: [first=BYTES_FIXED_SORTED, second=BYTES_VAR_STRAIGHT] expected:<9> but was:<10>
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.testAddIndexes(TestDocValuesIndexing.java:208)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1348)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1266)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.values.TestDocValuesIndexing FAILED
{code}"
"LUCENE-551","BUILD_SYSTEM","TASK","Make Lucene - Java 1.9.1 Available in Maven2 repository in iBibilio.org","Please upload 1.9.1 release to iBiblio so that Maven users can easily use the latest release.  Currently 1.4.3 is the most recently available version: http://www.ibiblio.org/maven2/lucene/lucene/

Please read the following FAQ for more information: http://maven.apache.org/project-faq.html"
"LUCENE-914","SPEC","BUG","Scorer.skipTo(current) remains on current for some scorers","Background in http://www.nabble.com/scorer.skipTo%28%29-contr-tf3880986.html

It appears that several scorers do not strictly follow the spec of Scorer.skipTo(n), and skip to current location remain in current location whereas the spec says: ""beyond current"". 

We should (probably) either relax the spec or fix the implementations."
"LUCENE-2596","IMPROVEMENT","IMPROVEMENT","Impl toString() in MergePolicy and its extensions","These can be important to see for debugging.

We lost them in the cutover to IWC.

Just opening this issue to remind us to get them back, before releasing..."
"LUCENE-2201","IMPROVEMENT","IMPROVEMENT","more performance improvements for snowball","i took a more serious look at snowball after LUCENE-2194.

This gives greatly improved performance, but note it has some minor breaks to snowball internals:
* Among.s becomes a char[] instead of a string
* SnowballProgram.current becomes a char[] instead of a StringBuilder
* SnowballProgram.eq_s(int, String) becomes eq_s(int, CharSequence), so that eq_v(StringBuilder) doesnt need to create an extra string.
* same as the above with eq_s_b and eq_v_b
* replace_s(int, int, String) becomes replace_s(int, int, CharSequence), so that StringBuilder-based slice and insertion methods don't need to create an extra string.

all of these ""breaks"" imho are only theoretical, the problem is just that pretty much everything is public or protected in the snowball internals.

the performance improvement here depends heavily upon the snowball language in use, but its way more significant than LUCENE-2194.
"
"LUCENE-1655","CLEANUP","BUG","remove 1.5 only unit test code that snuck in","I just tried to run unit tests w/ Java 1.4.2, but hit this:

{code}
common.compile-test:
    [mkdir] Created dir: /lucene/src/diagnostics.1654/build/classes/test
    [javac] Compiling 191 source files to /lucene/src/diagnostics.1654/build/classes/test
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:26: package java.util.concurrent.atomic does not exist
    [javac] import java.util.concurrent.atomic.AtomicInteger;
    [javac]                                    ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:262: cannot resolve symbol
    [javac] symbol  : class AtomicInteger 
    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.DeleteThreads
    [javac]     AtomicInteger delCount = new AtomicInteger();
    [javac]     ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:332: cannot resolve symbol
    [javac] symbol  : class AtomicInteger 
    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.AddDirectoriesThreads
    [javac]     AtomicInteger count = new AtomicInteger(0);
    [javac]     ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:333: cannot resolve symbol
    [javac] symbol  : class AtomicInteger 
    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.AddDirectoriesThreads
    [javac]     AtomicInteger numAddIndexesNoOptimize = new AtomicInteger(0);
    [javac]     ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:262: cannot resolve symbol
    [javac] symbol  : class AtomicInteger 
    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.DeleteThreads
    [javac]     AtomicInteger delCount = new AtomicInteger();
    [javac]                                  ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:332: cannot resolve symbol
    [javac] symbol  : class AtomicInteger 
    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.AddDirectoriesThreads
    [javac]     AtomicInteger count = new AtomicInteger(0);
    [javac]                               ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:333: cannot resolve symbol
    [javac] symbol  : class AtomicInteger 
    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.AddDirectoriesThreads
    [javac]     AtomicInteger numAddIndexesNoOptimize = new AtomicInteger(0);
    [javac]                                                 ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/search/TestSort.java:932: cannot resolve symbol
    [javac] symbol  : method getSimpleName ()
    [javac] location: class java.lang.Class
    [javac]           assertEquals(actualTFCClasses[j], tdc.getClass().getSimpleName());
    [javac]                                                         ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/search/TestTopScoreDocCollector.java:70: cannot resolve symbol
    [javac] symbol  : method getSimpleName ()
    [javac] location: class java.lang.Class
    [javac]         assertEquals(actualTSDCClass[i], tdc.getClass().getSimpleName());
    [javac]                                                      ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -deprecation for details.
    [javac] 9 errors
{code}"
"LUCENE-3044","BUG","BUG","thaiwordfilter uses attributesource.copyTo incorrectly","The bug can be seen by https://builds.apache.org/hudson/job/Lucene-Solr-tests-only-3.x/7367/

It looks like the issue is this lazy initialization of the cloned token: if the tokenstream is reused
and the consumer is interested in a different set of attributes, it could be a problem.

one probably-probably-not-totally-correct fix would be to add 'clonedToken = null;' to reset(), at 
least it would solve this case?"
"LUCENE-747","BUILD_SYSTEM","BUG","nightly build failed","javadoc tasked failed due to new project structure in contrib/gdata-server
added correct package structure to java/trunk/build.xml

javadoc creation successful.

Patch added as attachment.

regards simon"
"LUCENE-870","RFE","RFE","add concurrent merge policy","Provide the ability to handle merges in one or more concurrent threads, i.e., concurrent with other IndexWriter operations.

I'm factoring the code from LUCENE-847 for this."
"LUCENE-2622","BUG","BUG","Random Test Failure org.apache.lucene.TestExternalCodecs.testPerFieldCodec (from TestExternalCodecs)","Error Message

state.ord=54 startOrd=0 ir.isIndexTerm=true state.docFreq=1
Stacktrace

junit.framework.AssertionFailedError: state.ord=54 startOrd=0 ir.isIndexTerm=true state.docFreq=1
	at org.apache.lucene.index.codecs.standard.StandardTermsDictReader$FieldReader$SegmentTermsEnum.seek(StandardTermsDictReader.java:395)
	at org.apache.lucene.index.DocumentsWriter.applyDeletes(DocumentsWriter.java:1099)
	at org.apache.lucene.index.DocumentsWriter.applyDeletes(DocumentsWriter.java:1028)
	at org.apache.lucene.index.IndexWriter.applyDeletes(IndexWriter.java:4213)
	at org.apache.lucene.index.IndexWriter.doFlushInternal(IndexWriter.java:3381)
	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3221)
	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3211)
	at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:2345)
	at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:2323)
	at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:2293)
	at org.apache.lucene.TestExternalCodecs.testPerFieldCodec(TestExternalCodecs.java:645)
	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:381)
	at org.apache.lucene.util.LuceneTestCase.run(LuceneTestCase.java:373)
Standard Output

NOTE: random codec of testcase 'testPerFieldCodec' was: MockFixedIntBlock(blockSize=1327)
NOTE: random locale of testcase 'testPerFieldCodec' was: lt_LT
NOTE: random timezone of testcase 'testPerFieldCodec' was: Africa/Lusaka
NOTE: random seed of testcase 'testPerFieldCodec' was: 812019387131615618"
"LUCENE-512","BUG","BUG","ClassCastException in ParallelReader class","ClassCastException in ParalleReader when calling getTermFreqVectors on line 153

Reason : 

 cast of key and value is swapped

Fixed with : 

      IndexReader reader = (IndexReader)e.getValue();
      String field = (String)e.getKey();
"
"LUCENE-3678","BUG","BUG","TestAddIndexes fails (norms file not found)","ant test-core -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithRollback -Dtests.seed=2f55291b308dc34b:-4d72bfad34f3f357:4bc5ec894269c041 -Dargs=""-Dfile.encoding=UTF-8"" -Dtests.iter=100

fails about 4 or 5 times out of 100."
"LUCENE-2137","RFE","IMPROVEMENT","Replace SegmentReader.Ref with AtomicInteger","I think the patch should be applied to backcompat tag in its entirety."
"LUCENE-257","IMPROVEMENT","BUG","[PATCH] TermVectorReader and TermVectorWriter","TermVectorReader.close() closes all streams now under any condition. If an
excpetion is catched, it is remembered an thrown when all streams are closed.
Unnecessary variable assignment removed from code. 
Fix typo in TermVectorReader and TermVectorWriter."
"LUCENE-3152","TEST","BUG","MockDirectoryWrapper should wrap the lockfactory","After applying the patch from LUCENE-3147, I added a line to make the test fail if it cannot remove its temporary directory.

I ran 'ant test' on linux 50 times, and it passed all 50 times.
But on windows, it failed often because of write.lock... this is because of unclosed writers in the test.

MockDirectoryWrapper is currently unaware of this write.lock, I think it should wrap the lockfactory so that .close() will fail if there are any outstanding locks.
Then hopefully these tests would fail on linux too.
"
"LUCENE-2462","CLEANUP","BUG","remove DocsAndPositionsEnum.getPayloadLength","This was an accidental leftover; now that getPayload returns a BytesRef, this method is not needed."
"LUCENE-362","IMPROVEMENT","BUG","[PATCH] Extension to binary Fields that allows fixed byte buffer","This is a very simple patch that supports storing binary values in the index
more efficiently.  A new Field constructor accepts a length argument, allowing a
fixed byte[] to be reused acrossed multiple calls with arguments of different
sizes.  A companion change to FieldsWriter uses this length when storing and/or
compressing the field.

There is one remaining case in Document.  Intentionally, no direct accessor to
the length of a binary field is provided from Document, only from Field.  This
is because Field's created by FieldReader will never have a specified length and
this is usual case for Field's read from Document.  It seems less confusing for
most users.

I don't believe any upward incompatibility is introduced here (e.g., from the
possibility of getting a larger byte[] than actually holds the value from
Document), since no such byte[] values are possible without this patch anyway.

The compression case is still inefficient (much copying), but it is hard to see
how Lucene can do too much better.  However, the application can do the
compression externally and pass in the reused compression-output buffer as a
binary value (which is what I'm doing).  This represents a substantialy
allocation savings for storing large documents bodies (compressed) into the
Lucene index.

Two patch files are attached, both created by svn on 3/17/05."
"LUCENE-2200","DESIGN_DEFECT","IMPROVEMENT","Several final classes have non-overriding protected members","Protected member access in final classes, except where a protected method overrides a superclass's protected method, makes little sense.  The attached patch converts final classes' protected access on fields to private, removes two final classes' unused protected constructors, and converts one final class's protected final method to private."
"LUCENE-1646","IMPROVEMENT","IMPROVEMENT","QueryParser throws new exceptions even if custom parsing logic threw a better one","We have subclassed QueryParser and have various custom fields.  When these fields contain invalid values, we throw a subclass of ParseException which has a more useful message (and also a localised message.)

Problem is, Lucene's QueryParser is doing this:

{code}
    catch (ParseException tme) {
        // rethrow to include the original query:
        throw new ParseException(""Cannot parse '"" +query+ ""': "" + tme.getMessage());
    }
{code}

Thus, our nice and useful ParseException is thrown away, replaced by one with no information about what's actually wrong with the query (it does append getMessage() but that isn't localised.  And it also throws away the underlying cause for the exception.)

I am about to patch our copy to simply remove these four lines; the caller knows what the query string was (they have to have a copy of it because they are passing it in!) so having it in the error message itself is not useful.  Furthermore, when the query string is very big, what the user wants to know is not that the whole query was bad, but which part of it was bad.

"
"LUCENE-2897","IMPROVEMENT","IMPROVEMENT","apply delete-by-Term and docID immediately to newly flushed segments","Spinoff from LUCENE-2324.

When we flush deletes today, we keep them as buffered Term/Query/docIDs that need to be deleted.  But, for a newly flushed segment (ie fresh out of the DWPT), this is silly, because during flush we visit all terms and we know their docIDs.  So it's more efficient to apply the deletes (for this one segment) at that time.

We still must buffer deletes for all prior segments, but these deletes don't need to map to a docIDUpto anymore; ie we just need a Set.

This issue should wait until LUCENE-1076 is in since that issue cuts over buffered deletes to a transactional stream."
"LUCENE-2608","RFE","IMPROVEMENT","Allow for specification of spell checker accuracy when calling suggestSimilar","There is really no need for accuracy to be a class variable in the Spellchecker"
"LUCENE-1615","CLEANUP","IMPROVEMENT","deprecated method used in fieldsReader / setOmitTf()","setOmitTf(boolean) is deprecated and should not be used by core classes. One place where it appears is FieldsReader , this patch fixes it. It was necessary to change Fieldable to AbstractField at two places, only local variables.   "
"LUCENE-1197","BUG","BUG","IndexWriter can flush too early when flushing by RAM usage","There is a silly bug in how DocumentsWriter tracks its RAM usage:
whenever term vectors are enabled, it incorrectly counts the space
used by term vectors towards flushing, when in fact this space is
recycled per document.

This is not a functionality bug.  All it causes is flushes to happen
too frequently, and, IndexWriter will use less RAM than you asked it
to.  To work around it you can simply give it a bigger RAM buffer.

I will commit a fix shortly."
"LUCENE-2013","DESIGN_DEFECT","BUG","QueryScorer and SpanRegexQuery are incompatible.","Since the resolution of #LUCENE-1685, users are not supposed to rewrite their queries before submitting them to QueryScorer:

bq.------------------------------------------------------------------------
bq.r800796 | markrmiller | 2009-08-04 06:56:11 -0700 (Tue, 04 Aug 2009) | 1 line
bq.
bq.LUCENE-1685: The position aware SpanScorer has become the default scorer for Highlighting. The SpanScorer implementation has replaced QueryScorer and the old term highlighting QueryScorer has been renamed to QueryTermScorer. Multi-term queries are also now expanded by default. If you were previously rewritting the query for multi-term query highlighting, you should no longer do that (unless you switch to using QueryTermScorer). The SpanScorer API (now QueryScorer) has also been improved to more closely match the API of the previous QueryScorer implementation.
bq.------------------------------------------------------------------------

This is a great convenience for the most part, but it's causing me difficulties with SpanRegexQuerys, as the WeightedSpanTermExtractor uses Query.extractTerms() to collect the fields used in the query, but SpanRegexQuery does not implement this method, so highlighting any query with a SpanRegexQuery throws an UnsupportedOpertationException.  If this issue is circumvented, there is still the issue of SpanRegexQuery throwing an exception when someone calls its getSpans() method.

I can provide the patch that I am currently using, but I'm not sure that my solution is optimal.  It adds two methods to SpanQuery: extractFields(Set<String> fields) which is equivalent to fields.add(getField()) except when MaskedFieldQuerys get involved, and mustBeRewrittenToGetSpans() which returns true for SpanQuery, false for SpanTermQuery, and is overridden in each composite SpanQuery to return a value depending on its components.  In this way SpanRegexQuery (and any other custom SpanQuerys) do not need to be adjusted.

Currently the collection of fields and non-weighted terms are done in a single step.  In the proposed patch the WeightedSpanTerm extraction from a SpanQuery proceeds in two steps.  First, if the QueryScorer's field is null, then the fields are collected from the SpanQuery using the extractFields() method.  Second the terms are collected using extractTerms(), rewriting the query for each field if mustBeRewrittenToGetSpans() returns true."
"LUCENE-3026","BUG","BUG","smartcn analyzer throw NullPointer exception when the length of analysed text over 32767","That's all because of org.apache.lucene.analysis.cn.smart.hhmm.SegGraph's makeIndex() method:
  public List<SegToken> makeIndex() {
    List<SegToken> result = new ArrayList<SegToken>();
    int s = -1, count = 0, size = tokenListTable.size();
    List<SegToken> tokenList;
    short index = 0;
    while (count < size) {
      if (isStartExist(s)) {
        tokenList = tokenListTable.get(s);
        for (SegToken st : tokenList) {
          st.index = index;
          result.add(st);
          index++;
        }
        count++;
      }
      s++;
    }
    return result;
  }

here 'short index = 0;' should be 'int index = 0;'. And that's reported here http://code.google.com/p/imdict-chinese-analyzer/issues/detail?id=2 and http://code.google.com/p/imdict-chinese-analyzer/issues/detail?id=11, the author XiaoPingGao have already fixed this bug:http://code.google.com/p/imdict-chinese-analyzer/source/browse/trunk/src/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java"
"LUCENE-1462","BUG","BUG","Instantiated/IndexWriter discrepanies"," * RAMDirectory seems to do a reset on tokenStreams the first time, this permits to initialise some objects before starting streaming, InstantiatedIndex does not.
 * I can Serialize a RAMDirectory but I cannot on a InstantiatedIndex because of : java.io.NotSerializableException: org.apache.lucene.index.TermVectorOffsetInfo

http://www.nabble.com/InstatiatedIndex-questions-to20576722.html

"
"LUCENE-2650","IMPROVEMENT","IMPROVEMENT","improve windows defaults in FSDirectory","Currently windows defaults to SimpleFSDirectory, but this is a problem due to the synchronization.

I have been benchmarking queries *sequentially* and was pretty surprised at how much faster
MMapDirectory is, for example for cases that do many seeks.

I think we should change the defaults for windows as such:

if (WINDOWS and UNMAP_SUPPORTED and 64-bit)
  use MMapDirectory
else
  use SimpleFSDirectory 

I think we should just consider doing this for 4.0 only and see how it goes.
"
"LUCENE-2032","IMPROVEMENT","BUG","Spatial Filters not Serializable","I am using Lucene in a distributed setup. 

The Filters in the spatial project aren't Serializable even though it inherits it from Filter. Filter is a Serializable class. 

DistanceFilter contains the non-Serializable class WeakHashMap.
CartesianShapeFilter contains the non-Serializable class java.util.logging.Logger
"
"LUCENE-1343","IMPROVEMENT","IMPROVEMENT","A replacement for AsciiFoldingFilter that does a more thorough job of removing diacritical marks or non-spacing modifiers.","The ISOLatin1AccentFilter takes Unicode characters that have diacritical marks and replaces them with a version of that character with the diacritical mark removed.  For example  becomes e.  However another equally valid way of representing an accented character in Unicode is to have the unaccented character followed by a non-spacing modifier character (like this:    )    The ISOLatin1AccentFilter doesn't handle the accents in decomposed unicode characters at all.    Additionally there are some instances where a word will contain what looks like an accented character, that is actually considered to be a separate unaccented character  such as    but which to make searching easier you want to fold onto the latin1  lookalike  version   L  .   

The UnicodeNormalizationFilter can filter out accents and diacritical marks whether they occur as composed characters or decomposed characters, it can also handle cases where as described above characters that look like they have diacritics (but don't) are to be folded onto the letter that they look like (   -> L )"
"LUCENE-767","IMPROVEMENT","IMPROVEMENT","maxDoc should be explicitly stored in the index, not derived from file length","This is a spinoff of LUCENE-140

In general we should rely on ""as little as possible"" from the file system.  Right now, maxDoc is derived by checking the file length of the FieldsReader index file (.fdx) which makes me nervous.  I think we should explicitly store it instead.

Note that there are no known cases where this is actually causing a problem. There was some speculation in the discussion of LUCENE-140 that it could be one of the possible, but in digging / discussion there were no specifically relevant JVM bugs found (yet!).  So this would be a defensive fix at this point."
"LUCENE-3340","BUG","BUG","Buffered deletes are not flushed by RAM or count","When a segment is flushed, we will generally NOT flush the deletes, ie we simply buffer up the pending delete terms/queries, and the only apply them if 1) a segment is going to be merged (so we can remove the del docs in that segment), or 2) the buffered deletes' RAM exceeds 1/2 of IW's RAM limit when we are flushing a segment, or 3) the buffered deletes count exceeds IWC's maxBufferedDeleteTerms.

But the latter 2 triggers are currently broken on trunk; I suspect (but I'm not sure) when we landed DWPT we introduced this bug."
"LUCENE-2932","DOCUMENTATION","TASK","clean up obselete information on the website","When searching for information on 'lucene indexing speed' I get back some really out of date stuff:
1. on the features page it proudly proclaims 20MB/minute, on some really old hardware. I think we should
change this to 95GB/hour: http://blog.mikemccandless.com/2010/09/lucenes-indexing-is-fast.html
2. there are ancient benchmarks results from versioned data we link to the website. We list versioned
websites for ancient versions going back to 1.4.3. Also i noticed when just casually googling for
API documentation I tend to get results going to these ancient versions. I think we should remove
stuff for all versions prior to 2.9"
"LUCENE-631","RFE","RFE","GData Server - Milestone 3 Patch, Bugfixes, Documentation","For Milestone 3 added Features:

- Update Delete Concurrency
- Version control
- Second storage impl. based on Db4o. (Distributed Storage)
- moved all configuration in one single config file.
- removed dependencies in testcases.
- added schema validation for and all  xml files in the project (Configuration etc.)
- added JavaDoc
- much better Performance after reusing some resources
- added recovering component to lucene based storage to recover entries after a server crash or OOM Error (very simple)

- solved test case fail on hyperthread / multi core machines (@ hossman: give it a go)

@Yonik && Doug could you get that stuff in the svn please

regards simon

"
"LUCENE-1827","REFACTORING","IMPROVEMENT","Make the Payload Boosting Queries consistent","BoostingFunctionTermQuery should be consistent with BoostingNearQuery -

Renaming to PayloadNearQuery and PayloadTermQuery"
"LUCENE-2399","RFE","RFE","Add support for ICU's Normalizer2","While there are separate Case Folding, Normalization, and Ignorable-removal filters in LUCENE-1488,
the new ICU Normalizer2 API does this all at once with nfkc_cf (based on the new NFKC_Casefold property in Unicode).

This is great, because it provides a ton of unicode functionality that is really needed.
And the new Normalizer2 API takes CharSequence and writes to Appendable...
"
"LUCENE-1266","BUG","BUG","IndexWriter.optimize(boolean doWait) ignores doWait parameter","{{IndexWriter.optimize(boolean doWait)}} ignores the doWait parameter and always calls {{optimize(1, true)}}.

That does not seem to be the intended behavior, based on the doc comment."
"LUCENE-2206","RFE","RFE","integrate snowball stopword lists","The snowball project creates stopword lists as well as stemmers, example: http://svn.tartarus.org/snowball/trunk/website/algorithms/english/stop.txt?view=markup

This patch includes the following:
* snowball stopword lists for 13 languages in contrib/snowball/resources
* all stoplists are unmodified, only added license header and converted each one from whatever encoding it was in to UTF-8
* added getSnowballWordSet  to WordListLoader, this is because the format of these files is very different, for example it supports multiple words per line and embedded comments.

I did not add any changes to SnowballAnalyzer to actually automatically use these lists yet, i would like us to discuss this in a future issue proposing integrating snowball with contrib/analyzers.
"
"LUCENE-1428","BUILD_SYSTEM","BUG","Highlighter dist jar includes memory binary class files","Mark Harwood sent me a note about this issue noticed by a colleague. Previous releases have the memory class files in the Highlighter distribution jar. The Highlighter uses the same contrib dependency method that the xml query parser does - the problem doesn't manifest there because of the alphabetical order of build though. Fix is to not inheritAll when launching the ant task to build memory contrib."
"LUCENE-461","BUG","BUG","StandardTokenizer splitting all of Korean words into separate characters","StandardTokenizer splits all those Korean words inth separate character tokens. For example, ""?????"" is one Korean word that means ""Hello"", but StandardAnalyzer separates it into five tokens of ""?"", ""?"", ""?"", ""?"", ""?""."
"LUCENE-2732","REFACTORING","BUG","Fix charset problems in XML loading in HyphenationCompoundWordTokenFilter (also Solr's loader from schema)","As said in LUCENE-2731, the handling of XML in HyphenationCompoundWordTokenFilter is broken and breaks XML 1.0 (5th edition) spec totally. You should never supply a Reader to any XML api, unless you have internal character data (e.g. created programmatically). Also you should supply a system id, as resolving external entities does not work. The loader from files is much more broken, it always open the file as a Reader and then passes it to InputSource. Instead it should point filename directly to InputSource.

This issue will fix it in trunk and use InputSource in Solr, but will still supply the Reader possibility in previous versions (deprecated)."
"LUCENE-2572","BUILD_SYSTEM","BUG","Maven artifacts for Lucene 4 are not stored in the correct path","Hello,

I would like to use the maven artifacts for Lucene 4.0 produced by the Hudson build machine. The artifacts are correctly produced (http://hudson.zones.apache.org/hudson/view/Lucene/job/Lucene-trunk/lastSuccessfulBuild/artifact/maven_artifacts/lucene/).
However, the artifacts which should be stored under the path ""org/apache/lucene/"" are currently stored under ""lucene"" which prevents a project using maven to correctly download the Lucene 4.0 artifacts.

Thanks again for your help.  "
"LUCENE-3428","BUG","BUG","trunk tests hang/deadlock TestIndexWriterWithThreads","trunk tests have been hanging often lately in hudson, this time i was careful to kill and get a good stacktrace:"
"LUCENE-1668","IMPROVEMENT","BUG","Trunk fails tests, FSD.open() - related","    [junit] Testcase: testReadAfterClose(org.apache.lucene.index.TestCompoundFile):	FAILED
    [junit] expected readByte() to throw exception
    [junit] junit.framework.AssertionFailedError: expected readByte() to throw exception
    [junit] 	at org.apache.lucene.index.TestCompoundFile.demo_FSIndexInputBug(TestCompoundFile.java:345)
    [junit] 	at org.apache.lucene.index.TestCompoundFile.testReadAfterClose(TestCompoundFile.java:313)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)

This one is a non-bug, if you ask me. The test should fail on SimpleFSD, but on my system FSD.open() creates MMapD and that one cannot be closed, so the read succeeds.

    [junit] ------------- Standard Output ---------------
    [junit] Thread[Thread-34,5,main]: exc
    [junit] java.nio.BufferUnderflowException
    [junit] 	at java.nio.Buffer.nextGetIndex(Buffer.java:474)
    [junit] 	at java.nio.DirectByteBuffer.get(DirectByteBuffer.java:229)
    [junit] 	at org.apache.lucene.store.MMapDirectory$MMapIndexInput.readByte(MMapDirectory.java:67)
    [junit] 	at org.apache.lucene.store.ChecksumIndexInput.readByte(ChecksumIndexInput.java:36)
    [junit] 	at org.apache.lucene.store.IndexInput.readInt(IndexInput.java:70)
    [junit] 	at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:238)
    [junit] 	at org.apache.lucene.index.DirectoryIndexReader$1.doBody(DirectoryIndexReader.java:106)
    [junit] 	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:699)
    [junit] 	at org.apache.lucene.index.DirectoryIndexReader.open(DirectoryIndexReader.java:126)
    [junit] 	at org.apache.lucene.index.IndexReader.open(IndexReader.java:374)
    [junit] 	at org.apache.lucene.index.IndexReader.open(IndexReader.java:260)
    [junit] 	at org.apache.lucene.search.IndexSearcher.<init>(IndexSearcher.java:76)
    [junit] 	at org.apache.lucene.index.TestStressIndexing$SearcherThread.doWork(TestStressIndexing.java:109)
    [junit] 	at org.apache.lucene.index.TestStressIndexing$TimedThread.run(TestStressIndexing.java:52)
    [junit] NOTE: random seed of testcase 'testStressIndexAndSearching' was: -7374705829444180151
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testStressIndexAndSearching(org.apache.lucene.index.TestStressIndexing):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.lucene.index.TestStressIndexing.runStressTest(TestStressIndexing.java:155)
    [junit] 	at org.apache.lucene.index.TestStressIndexing.testStressIndexAndSearching(TestStressIndexing.java:178)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)

This one suceeds sometimes, sometimes (mostly) fails. Is obviously linked with switch to MMapD, but what is the real cause - I don't know.

    [junit] ------------- Standard Output ---------------
    [junit] NOTE: random seed of testcase 'testSetBufferSize' was: 8481546620770090440
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testSetBufferSize(org.apache.lucene.store.TestBufferedIndexInput):	Caused an ERROR
    [junit] org.apache.lucene.store.MMapDirectory$MMapIndexInput cannot be cast to org.apache.lucene.store.BufferedIndexInput
    [junit] java.lang.ClassCastException: org.apache.lucene.store.MMapDirectory$MMapIndexInput cannot be cast to org.apache.lucene.store.BufferedIndexInput
    [junit] 	at org.apache.lucene.store.TestBufferedIndexInput$MockFSDirectory.tweakBufferSizes(TestBufferedIndexInput.java:226)
    [junit] 	at org.apache.lucene.store.TestBufferedIndexInput.testSetBufferSize(TestBufferedIndexInput.java:181)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)

Broken assumptions.
"
"LUCENE-3700","RFE","IMPROVEMENT","optionally support naist-jdic for kuromoji","This is an alternative dictionary, somewhat larger (~25%).

we can support it in build.xml so if a user wants to build with it, they can (the resulting jar file will be 500KB larger)"
"LUCENE-1703","RFE","IMPROVEMENT","Add a waitForMerges() method to IndexWriter","It would be very useful to have a waitForMerges() method on the IndexWriter.

Right now, the only way i can see to achieve this is to call IndexWriter.close()

ideally, there would be a method on the IndexWriter to wait for merges without actually closing the index.
This would make it so that background merges (or optimize) can be waited for without closing the IndexWriter, and then reopening a new IndexWriter

the close() reopen IndexWriter method can be problematic if the close() fails as the write lock won't be released
this could then result in the following sequence:
* close() - fails
* force unlock the write lock (per close() documentation)
* new IndexWriter() (acquires write lock)
* finalize() on old IndexWriter releases the write lock
* Index is now not locked, and another IndexWriter pointing to the same directory could be opened

If you don't force unlock the write lock, opening a new IndexWriter will fail until garbage collection calls finalize() the old IndexWriter

If the waitForMerges() method is available, i would likely never need to close() the IndexWriter until right before the process being shutdown, so this issue would not occur (worst case scenario, the waitForMerges() fails)


"
"LUCENE-2331","RFE","RFE","Add NoOpMergePolicy","I'd like to add a simple and useful MP implementation which does .... nothing ! :). I've came across many places where either the following is documented or implemented: ""if you want to prevent merges, set mergeFactor to a high enough value"". I think a NoOpMergePolicy is just as good, and can REALLY allow you disable merges (except for maybe set mergeFactor to Int.MAX_VAL).

As such, NoOpMergePolicy will be introduced as a singleton, and can be used for convenience purposes only. Also, for Parallel Index it's important, because I'd like the slices to never do any merges, unless ParallelWriter decides so. So they should be set w/ that MP.

I have a patch ready. Waiting for LUCENE-2320 to go in, so that I don't need to change it afterwards.

About the name - I like the name, but suggestions are welcome. I thought of a NullMergePolicy, but I don't like 'Null' used for a NoOp."
"LUCENE-1573","RFE","BUG","IndexWriter does not do the right thing when a Thread is interrupt()'d","Spinoff from here:

    http://www.nabble.com/Deadlock-with-concurrent-merges-and-IndexWriter--Lucene-2.4--to22714290.html

When a Thread is interrupt()'d while inside Lucene, there is a risk currently that it will cause a spinloop and starve BG merges from completing.

Instead, when possible, we should allow interruption.  But unfortunately for back-compat, we will need to wrap the exception in an unchecked version.  In 3.0 we can change that to simply throw InterruptedException."
"LUCENE-634","DOCUMENTATION","BUG","QueryParser is not applicable for the arguments (String, String, Analyzer) error in results.jsp when executing search in the browser (demo from Lucene 2.0)","When executing search in the browser (as described in demo3.html Lucene demo) I get error, because the demo uses the method (QueryParser with three arguments) which is deleted (it was deprecated).
I checked the demo from Lucene 1.4-final it with Lucene 1.4-final - it works, because those time the method was there.
But demo from Lucene 2.0 does not work with Lucene 2.0

The error stack is here:
TTP Status 500 -

type Exception report

message

description The server encountered an internal error () that prevented it from fulfilling this request.

exception

org.apache.jasper.JasperException: Unable to compile class for JSP

An error occurred at line: 60 in the jsp file: /results.jsp
Generated servlet error:
The method parse(String) in the type QueryParser is not applicable for the arguments (String, String, Analyzer)


org.apache.jasper.servlet.JspServletWrapper.handleJspException(JspServletWrapper.java:510)
org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:375)
org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:314)
org.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)
javax.servlet.http.HttpServlet.service(HttpServlet.java:802)

root cause

org.apache.jasper.JasperException: Unable to compile class for JSP

An error occurred at line: 60 in the jsp file: /results.jsp
Generated servlet error:
The method parse(String) in the type QueryParser is not applicable for the arguments (String, String, Analyzer)


org.apache.jasper.compiler.DefaultErrorHandler.javacError(DefaultErrorHandler.java:84)
org.apache.jasper.compiler.ErrorDispatcher.javacError(ErrorDispatcher.java:328)
org.apache.jasper.compiler.JDTCompiler.generateClass(JDTCompiler.java:409)
org.apache.jasper.compiler.Compiler.compile(Compiler.java:297)
org.apache.jasper.compiler.Compiler.compile(Compiler.java:276)
org.apache.jasper.compiler.Compiler.compile(Compiler.java:264)
org.apache.jasper.JspCompilationContext.compile(JspCompilationContext.java:563)
org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:303)
org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:314)
org.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)
javax.servlet.http.HttpServlet.service(HttpServlet.java:802)

note The full stack trace of the root cause is available in the Apache Tomcat/5.5.15 logs."
"LUCENE-1880","REFACTORING","IMPROVEMENT","Make contrib/collation/(ICU)CollationKeyAnalyzer constructors public","In contrib/collation, the constructors for CollationKeyAnalyzer and ICUCollationKeyAnalyzer are package private, and so are effectively unusable."
"LUCENE-330","IMPROVEMENT","IMPROVEMENT","[PATCH] Use filter bits for next() and skipTo() in FilteredQuery","This improves performance of FilteredQuery by not calling score() 
on documents that do not pass the filter. 
This passes the current tests for FilteredQuery, but these tests 
have not been adapted/extended."
"LUCENE-347","DOCUMENTATION","IMPROVEMENT","[PATCH] Javadoc correction for Scorer.java"," "
"LUCENE-353","BUG","BUG","Locking bug","In org.apache.lucene.store.Lock, line 57 (lucene_1_4_final branch):

if (++sleepCount == maxSleepCount)

is incorrect, the sleepCount is incremented before the compare causing it
throwing the exception with out waiting for at least 1 interation.

Should be changed instead to:
if (sleepCount++ == maxSleepCount)

As this is a self-contained simple fix, I am not submitting a patch.

Thanks

-John"
"LUCENE-644","RFE","IMPROVEMENT","Contrib: another highlighter approach","Mark Harwoods highlighter package is a great contribution to Lucene, I've used it a lot! However, when you have *large* documents (fields), highlighting can be quite time consuming if you increase the number of bytes to analyze with setMaxDocBytesToAnalyze(int). The default value of 50k is often too low for indexed PDFs etcetera, which results in empty highlight strings.

This is an alternative approach using term position vectors only to build fragment info objects. Then a StringReader can read the relevant fragments and skip() between them. This is a lot faster. Also, this method uses the *entire* field for finding the best fragments so you're always guaranteed to get a highlight snippet.

Because this method only works with fields which have term positions stored one can check if this method works for a particular field using following code (taken from TokenSources.java):

        TermFreqVector tfv = (TermFreqVector) reader.getTermFreqVector(docId, field);
        if (tfv != null && tfv instanceof TermPositionVector)
        {
          // use FulltextHighlighter
        }
        else
        {
          // use standard Highlighter
        }

Someone else might find this useful so I'm posting the code here."
"LUCENE-3227","RFE","IMPROVEMENT","Add Rewriteable Support to SortField.toString","I missed adding support for the new Rewriteable SortField type to toString()."
"LUCENE-419","REFACTORING","BUG","[REFACTORING] FieldSortedHitQueue has too much duplicated code","There's 40LOC duplicated in FieldDocSortedHitQueue::lessThan just to handle 
the reverse sort. It would be more readable to actually do something like 
(YMMV):

if (field.getReverse()) {
    c = -c;
}"
"LUCENE-803","OTHER","IMPROVEMENT","add svn ignores for eclipse artifacts","Be nice to ignore the files eclipse puts into the project root as we do the .idea file for intellij.

The two files are

.project
.classpath

I'm gonna lie and say there's a patch available for this because an svn diff patch with propery changes can't be applied with patch anyway."
"LUCENE-1388","RFE","IMPROVEMENT","Add init method to CloseableThreadLocal","Java ThreadLocal has an init method that allows subclasses to easily instantiate an initial value.  "
"LUCENE-1648","BUG","BUG","when you clone or reopen an IndexReader with pending changes, the new reader doesn't commit the changes","While working on LUCENE-1647, I came across this issue... we are failing to carry over hasChanges, norms/deletionsDirty, etc, when cloning the new reader."
"LUCENE-1443","IMPROVEMENT","IMPROVEMENT","Performance improvement in OpenBitSetDISI.inPlaceAnd()",""
"LUCENE-1020","RFE","RFE","Basic tool for checking & repairing an index","This has been requested a number of times on the mailing lists.  Most
recently here:

  http://www.gossamer-threads.com/lists/lucene/java-user/53474

I think we should provide a basic tool out of the box.
"
"LUCENE-438","REFACTORING","IMPROVEMENT","add Token.setTermText(), remove final","The Token class should be more friendly to classes not in it's package:
  1) add setTermText()
  2) remove final from class and toString()
  3) add clone()

Support for (1):
  TokenFilters in the same package as Token are able to do things like 
   ""t.termText = t.termText.toLowerCase();"" which is more efficient, but more importantly less error prone.  Without the ability to change *only* the term text, a new Token must be created, and one must remember to set all the properties correctly.  This exact issue caused this bug:
http://issues.apache.org/jira/browse/LUCENE-437

Support for (2):
  Removing final allows one to subclass Token.  I didn't see any performance impact after removing final.
I can go into more detail on why I want to subclass Token if anyone is interested.

Support for (3):
  - support for a synonym TokenFilter, where one needs to make two tokens from one (same args that support (1), and esp important if instance is a subclass of Token)."
"LUCENE-1714","BUG","BUG","WriteLineDocTask incorrectly normalizes fields","WriteLineDocTask normalizes the body, title and date fields by replacing any ""\t"" with a space. However, if any one of them contains newlines, LineDocMaker will fail, since the first line read will include some of the text, however the second line, which it now expects to be a new document, will include other parts of the text.

I don't know how we didn't hit it so far. Maybe the wikipedia text doesn't have such lines, however when I ran over the TREC collection I hit a lot of those.

I will attach a patch shortly."
"LUCENE-1028","DESIGN_DEFECT","BUG","Weight is not serializable for some of the queries","In order to work with ParallelMultiSearcher, Query weights need to be serializable.  The interface Weight extends java.io.Serializable, but it appears that some of the newer queries unnecessarily store global state in their weights, thus causing serialization errors."
"LUCENE-1505","IMPROVEMENT","IMPROVEMENT","Change contrib/spatial to use trie's NumericUtils, and remove NumberUtils","Currently spatial contrib includes a copy of NumberUtils from solr (otherwise it would depend on solr)

Once LUCENE-1496 is sorted out, this copy should be removed."
"LUCENE-1452","BUG","BUG","Binary field content lost during optimize","Scenario:

* create an index with arbitrary content, and close it
* open IndexWriter again, and add a document with binary field (stored but not compressed)
* close IndexWriter _without_ optimizing, so that the new document is in a separate segment.
* open IndexReader. You can read the last document and its binary field just fine.
* open IndexWriter, optimize the index, close IndexWriter
* open IndexReader. Now the field is still present (not null) and is marked as binary, but the data is not there - Field.getBinaryLength() returns 0.
"
"LUCENE-511","BUG","BUG","New BufferedIndexOutput optimization fails to update bufferStart","New BufferIndexOutput optimization of writeBytes fails to update bufferStart under some conditions. Test case and fix attached."
"LUCENE-3601","BUG","BUG","testIWondiskfull unreferenced files failure","NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterOnDiskFull -Dtestmethod=testAddDocumentOnDiskFull -Dtests.seed=aff9b14dd518cfb:4d2f112726e2947f:-2b03094a43a947ee -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1""

Reproduces some of the time..."
"LUCENE-2690","IMPROVEMENT","IMPROVEMENT","Do MultiTermQuery boolean rewrites per segment","MultiTermQuery currently rewrites FuzzyQuery (using TopTermsBooleanQueryRewrite), the auto constant rewrite method and the ScoringBQ rewrite methods using a MultiFields wrapper on the top-level reader. This is inefficient.

This patch changes the rewrite modes to do the rewrites per segment and uses some additional datastructures (hashed sets/maps) to exclude duplicate terms. All tests currently pass, but FuzzyQuery's tests should not, because it depends for the minimum score handling, that the terms are collected in order..

Robert will fix FuzzyQuery in this issue, too. This patch is just a start."
"LUCENE-3906","IMPROVEMENT","TASK","allow specifying -Dbootclasspath for javac/javadocs","So that you can compile/javadoc against the actual target JRE libraries
even if you have a newer compiler."
"LUCENE-1376","BUG","BUG","sometimes if a BG merge hits an exception, optimize() will fail to forward the exception","I was seeing an intermittant failure, only on a Windows instance running inside VMWare, of TestIndexWriter.testAddIndexOnDiskFull.

It is happening because the while loop that checks for merge exceptions that had occurred during optimize fails to catch the case where all the BG optimize merges completed (or hit exceptions) before the while loop begins.  IE, all BG threads finished before the FG thread advanced to the while loop.  In that case the code fails to check if there were any exceptions.

The fix is straightforward: change the while loop so that it always checks, at least once, whether there were exceptions."
"LUCENE-1973","CLEANUP","TASK","Remove deprecated query components","Remove the rest of the deprecated query components."
"LUCENE-500","CLEANUP","TASK","Lucene 2.0 requirements - Remove all deprecated code","Per the move to Lucene 2.0 from 1.9, remove all deprecated code and update documentation, etc.

Patch to follow shortly."
"LUCENE-895","TEST","BUG","Exclude PrecedenceQueryParser from build or disable failing test cases","As Erik commented in LUCENE-885 the PrecendenceQueryParser is currently
unmaintained. Since some tests are failing we should either exclude PQP from the 
build or simply disable the failing tests."
"LUCENE-980","BUG","BUG","Formatting error in ReportTask in contrib/benchmark","I am building a new Task, AnalyzerTask, that lets you change the Analyzer in the loop, thus allowing for the comparison of the same Analyzers over the set of documents.

My algorithm declaration looks like:
NewAnalyzer(WhitespaceAnalyzer, SimpleAnalyzer, StopAnalyzer, standard.StandardAnalyzer)

And it could be longer.

The exception is:
Error: cannot execute the algorithm! String index out of range: 85
java.lang.StringIndexOutOfBoundsException: String index out of range: 85
	at java.lang.String.substring(String.java:1765)
	at org.apache.lucene.benchmark.byTask.utils.Format.format(Format.java:85)
	at org.apache.lucene.benchmark.byTask.tasks.ReportTask.tableTitle(ReportTask.java:85)
	at org.apache.lucene.benchmark.byTask.tasks.ReportTask.genPartialReport(ReportTask.java:140)
	at org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask.reportSumByName(RepSumByNameTask.java:77)
	at org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask.doLogic(RepSumByNameTask.java:39)
	at org.apache.lucene.benchmark.byTask.tasks.PerfTask.runAndMaybeStats(PerfTask.java:83)
	at org.apache.lucene.benchmark.byTask.tasks.TaskSequence.doSerialTasks(TaskSequence.java:112)
	at org.apache.lucene.benchmark.byTask.tasks.TaskSequence.doLogic(TaskSequence.java:93)
	at org.apache.lucene.benchmark.byTask.utils.Algorithm.execute(Algorithm.java:228)
	at org.apache.lucene.benchmark.byTask.Benchmark.execute(Benchmark.java:73)
	at org.apache.lucene.benchmark.byTask.Benchmark.main(Benchmark.java:109)

The error seems to be caused by the fact that ReportTask uses the OP (operation) column for the String, but then uses the length of the algorithm declaration to index into the String, resulting in the index out of bounds exception.

The line in question is:
return (s + padd).substring(0, col.length());

And probably should be changed to something like:
    String s1 = (s + padd);
    return s1.substring(0, Math.min(col.length(), s1.length()));

Either that or the column should be trimmed.  The workaround is to explicitly name the task.

If no objections, I will make the change, tomorrow.  "
"LUCENE-2984","REFACTORING","IMPROVEMENT","Move hasVectors() & hasProx() responsibility out of SegmentInfo to FieldInfos ","Spin-off from LUCENE-2881 which had this change already but due to some random failures related to this change I remove this part of the patch to make it more isolated and easier to test. "
"LUCENE-2487","DESIGN_DEFECT","IMPROVEMENT","IndexReader subclasses must implement flex APIs","To be fixed only on trunk...

I made IndexReader's base flex APIs abstract, fixed all core/contrib/solr places that subclassed IR and didn't already implement flex (including contrib/memory, contrib/instantiated), and remove all the classes for the back-compat layer that emulated flex APIs on top of pre-flex APIs."
"LUCENE-939","IMPROVEMENT","IMPROVEMENT","Check for boundary conditions in FieldInfos","In FieldInfos there are three methods in which we don't check for
boundary conditions but catch e. g. an IndexOutOfBoundsException
or a NPE. I think this isn't good code style and is probably not
even faster than checking explicitly.

""Exceptions should not be used to alter the flow of a program as 
part of normal execution.""

Also this can be irritating when you're trying to debug an 
IndexOutOfBoundsException that is thrown somewhere else in your
program and you place a breakpoint on that exception.

The three methods are:

  public int fieldNumber(String fieldName) {
    try {
      FieldInfo fi = fieldInfo(fieldName);
      if (fi != null)
        return fi.number;
    }
    catch (IndexOutOfBoundsException ioobe) {
      return -1;
    }
    return -1;
  }
  

  public String fieldName(int fieldNumber) {
    try {
      return fieldInfo(fieldNumber).name;
    }
    catch (NullPointerException npe) {
      return """";
    }
  }
  
  
  public FieldInfo fieldInfo(int fieldNumber) {
    try {
      return (FieldInfo) byNumber.get(fieldNumber);
    }
    catch (IndexOutOfBoundsException ioobe) {
      return null;
    }
  }"
"LUCENE-204","DOCUMENTATION","IMPROVEMENT","[PATCH] usage feedback for IndexFiles demo","Just a small patch that adds ""usage"" output if the demo is called without a 
parameter, makes it a little bit friendlier to beginners."
"LUCENE-3362","TEST","BUG","Initialization error of Junit tests with solr-test-framework with IDEs and Maven","I'm currently developping a new component for Solr. And in my Netbeans project, I have created two Test classes for this component: one class for simple unit tests (derived from  SolrTestCaseJ4 class) and a second one for tests with sharding (derived from  BaseDistributedSearchTestCase).
When I launch a test with these two classes, I have an error in the initialization of the second class of tests (no matter the class is, this is always the second executed class which fails). The error comes from an ""assert"" which failed in the begining of the function ""initRandom()"" of LuceneTestCase class :

assert !random.initialized;

But, if I launch each test class separatly, all the tests succeed!

After a discussion with Mr. Muir, the problems seems to be related to the incompatibility of the class LuceneTestCase with the functioning of Maven projects in IDEs.

According to mister Muir:

""
The problem is that via ant, tests work like this (e.g. for 3 test classes):
computeTestMethods
beforeClass
afterClass
computeTestMethods
beforeClass
AfterClass
computeTestMethods
beforeClass
afterClass

but via an IDE, if you run it from a folder like you did, then it does this:
computeTestMethods
computeTestMethods
computeTestMethods
beforeClass
afterClass
beforeClass
afterClass
beforeClass
afterClass 
"""
"LUCENE-2496","BUG","BUG","NPE if you open IW with CREATE on an index with no segments file","I have a simple test case that hits this NPE:

{noformat}
    [junit] java.lang.NullPointerException
    [junit] 	at java.io.File.<init>(File.java:305)
    [junit] 	at org.apache.lucene.store.NIOFSDirectory.openInput(NIOFSDirectory.java:67)
    [junit] 	at org.apache.lucene.store.FSDirectory.openInput(FSDirectory.java:333)
    [junit] 	at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:213)
    [junit] 	at org.apache.lucene.index.IndexFileDeleter.<init>(IndexFileDeleter.java:218)
    [junit] 	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:1113)
    [junit] 	at org.apache.lucene.index.TestIndexWriter.testNoSegmentFile(TestIndexWriter.java:4975)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:277)
{noformat}

It happens if you have an aborted index, ie, there are segment files in there (*.frq, *.tis, etc.) but no segments_N file, and then you try to open an IW with CREATE on that index."
"LUCENE-1049","IMPROVEMENT","IMPROVEMENT","Simple toString() for BooleanFilter","While working with BooleanFilter I wanted a basic toString() for debugging.

This is what I came up.  It works ok for me."
"LUCENE-1943","IMPROVEMENT","IMPROVEMENT","ChineseFilter is inefficient","trivial patch to use CharArraySet, so it can use termBuffer() instead of term()
"
"LUCENE-3324","BUG","BUG","checkindex fails if docfreq >= skipInterval and term is indexed more than once at same position","This is a bad check in the skipping verification logic"
"LUCENE-2181","RFE","RFE","benchmark for collation","Steven Rowe attached a contrib/benchmark-based benchmark for collation (both jdk and icu) under LUCENE-2084, along with some instructions to run it... 

I think it would be a nice if we could turn this into a committable patch and add it to benchmark.
"
"LUCENE-1401","IMPROVEMENT","BUG","Deprecation of autoCommit in 2.4 leads to compile problems, when autoCommit should be false","I am currently changing my code to be most compatible with 2.4. I switched on deprecation warnings and got a warning about the autoCommit parameter in IndexWriter constructors.

My code *should* use autoCommit=false, so I want to use the new semantics. The default of IndexWriter is still autoCommit=true. My problem now: How to disable autoCommit whithout deprecation warnings?

Maybe, the ""old"" constructors, that are deprecated should use autoCommit=true. But there are new constructors with this ""IndexWriter.MaxFieldLength mfl"" in it, that appear new in 2.4 but are deprecated:

IndexWriter(Directory d, boolean autoCommit, Analyzer a, boolean create, IndexDeletionPolicy deletionPolicy, IndexWriter.MaxFieldLength mfl) 
          Deprecated. This will be removed in 3.0, when autoCommit will be hardwired to false. Use IndexWriter(Directory,Analyzer,boolean,IndexDeletionPolicy,MaxFieldLength) instead, and call commit() when needed.

What the hell is meant by this, a new constructor that is deprecated? And the hint is wrong. If I use the other constructor in the warning, I get autoCommit=true.

There is something completely wrong.

It should be clear, which constructors set autoCommit=true, which set it per default to false (perhaps new ones), and the Deprecated text is wrong, if autoCommit does not default to false."
"LUCENE-3302","CLEANUP","IMPROVEMENT","Leftover legacy enum in IndexReader","In IndexReader we still have some leftover ""handmade"" enum from pre-Java5 times. Unfortunately the Generics/Java5 Policeman did not notice it.

This patch is just code cleanup, no baclkwards breaks, as code using this enum would not see any difference (because only superclass changes).

I will commit this asap."
"LUCENE-2455","CLEANUP","IMPROVEMENT","Some house cleaning in addIndexes*","Today, the use of addIndexes and addIndexesNoOptimize is confusing - 
especially on when to invoke each. Also, addIndexes calls optimize() in 
the beginning, but only on the target index. It also includes the 
following jdoc statement, which from how I understand the code, is 
wrong: _After this completes, the index is optimized._ -- optimize() is 
called in the beginning and not in the end. 

On the other hand, addIndexesNoOptimize does not call optimize(), and 
relies on the MergeScheduler and MergePolicy to handle the merges. 

After a short discussion about that on the list (Thanks Mike for the 
clarifications!) I understand that there are really two core differences 
between the two: 
* addIndexes supports IndexReader extensions
* addIndexesNoOptimize performs better

This issue proposes the following:
# Clear up the documentation of each, spelling out the pros/cons of 
  calling them clearly in the javadocs.
# Rename addIndexesNoOptimize to addIndexes
# Remove optimize() call from addIndexes(IndexReader...)
# Document that clearly in both, w/ a recommendation to call optimize() 
  before on any of the Directories/Indexes if it's a concern. 

That way, we maintain all the flexibility in the API - 
addIndexes(IndexReader...) allows for using IR extensions, 
addIndexes(Directory...) is considered more efficient, by allowing the 
merges to happen concurrently (depending on MS) and also factors in the 
MP. So unless you have an IR extension, addDirectories is really the one 
you should be using. And you have the freedom to call optimize() before 
each if you care about it, or don't if you don't care. Either way, 
incurring the cost of optimize() is entirely in the user's hands. 

BTW, addIndexes(IndexReader...) does not use neither the MergeScheduler 
nor MergePolicy, but rather call SegmentMerger directly. This might be 
another place for improvement. I'll look into it, and if it's not too 
complicated, I may cover it by this issue as well. If you have any hints 
that can give me a good head start on that, please don't be shy :). "
"LUCENE-985","BUG","BUG","AIOOB thrown when length of termText is longer than 16384 characters (ArrayIndexOutOfBoundsException)","DocumentsWriter has a max term length of 16384; if you cross that you
get an unfriendly ArrayIndexOutOfBoundsException.  We should fix to raise a clearer exception."
"LUCENE-1233","DESIGN_DEFECT","IMPROVEMENT","Fix Document.getFieldables and others to never return null","Document.getFieldables (and other similar methods) returns null if there are no fields matching the name.  We can avoid NPE in consumers of this API if instead we return an empty array.

Spinoff from http://markmail.org/message/g2nzstmce4cnf3zj"
"LUCENE-371","BUG","BUG","RangeQuery - add equals and hashCode methods","I'm attaching a patch with an equals() and hashCode() implementation for 
RangeQuery.java, and a new unit test for TestRangeQuery.java, as per a recent 
message on lucene-user mailing list (subject ""RangeQuery doesn't override 
equals() or hashCode() - intentional?"")

patches to follow"
"LUCENE-1959","RFE","RFE","Index Splitter","If an index has multiple segments, this tool allows splitting those segments into separate directories.  "
"LUCENE-1936","CLEANUP","TASK","Remove deprecated charset support from Greek and Russian analyzers","This removes the deprecated support for custom charsets.

One thing I found is that once these charsets are removed, RussianLowerCaseFilter is the same as LowerCaseFilter.
So I marked it deprecated to be removed in 3.1
"
"LUCENE-3255","BUG","BUG","Corrupted segment file not detected and wipes index contents","Lucene will happily wipe an existing index if presented with a latest generation segments_n file of all zeros. File format documentation says segments_N files should start with a format of -9 but SegmentInfos.read accepts >=0 as valid for backward compatibility reasons.

"
"LUCENE-3463","IMPROVEMENT","BUG","Jenkins trunk tests (nightly only) fail quite often with OOM in Automaton/FST tests","The nightly Job Lucene-trunk quite often fails with OOM (in several methods, not always in the same test):

Example from last night (this time a huge Automaton):

{noformat}
[junit] java.lang.OutOfMemoryError: Java heap space
[junit] Dumping heap to /home/hudson/hudson-slave/workspace/Lucene-trunk/heapdumps/java_pid38855.hprof ...
[junit] Heap dump file created [86965954 bytes in 1.186 secs]
[junit] Testsuite: org.apache.lucene.index.TestTermsEnum
[junit] Testcase: testIntersectRandom(org.apache.lucene.index.TestTermsEnum):	Caused an ERROR
[junit] Java heap space
[junit] java.lang.OutOfMemoryError: Java heap space
[junit] 	at org.apache.lucene.util.automaton.RunAutomaton.<init>(RunAutomaton.java:128)
[junit] 	at org.apache.lucene.util.automaton.ByteRunAutomaton.<init>(ByteRunAutomaton.java:28)
[junit] 	at org.apache.lucene.util.automaton.CompiledAutomaton.<init>(CompiledAutomaton.java:134)
[junit] 	at org.apache.lucene.index.TestTermsEnum.testIntersectRandom(TestTermsEnum.java:266)
[junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
[junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
[junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
[junit] 
[junit] 
[junit] Tests run: 6, Failures: 0, Errors: 1, Time elapsed: 11.699 sec
{noformat}

Other traces:

{noformat}
[junit] Testsuite: org.apache.lucene.util.fst.TestFSTs
[junit] Testcase: testRealTerms(org.apache.lucene.util.fst.TestFSTs):	Caused an ERROR
[junit] Java heap space
[junit] java.lang.OutOfMemoryError: Java heap space
[junit] 	at org.apache.lucene.util.ArrayUtil.grow(ArrayUtil.java:338)
[junit] 	at org.apache.lucene.util.fst.FST$BytesWriter.writeBytes(FST.java:927)
[junit] 	at org.apache.lucene.util.fst.ByteSequenceOutputs.write(ByteSequenceOutputs.java:113)
[junit] 	at org.apache.lucene.util.fst.ByteSequenceOutputs.write(ByteSequenceOutputs.java:32)
[junit] 	at org.apache.lucene.util.fst.FST.addNode(FST.java:451)
[junit] 	at org.apache.lucene.util.fst.NodeHash.add(NodeHash.java:122)
[junit] 	at org.apache.lucene.util.fst.Builder.compileNode(Builder.java:180)
[junit] 	at org.apache.lucene.util.fst.Builder.finish(Builder.java:495)
[junit] 	at org.apache.lucene.index.codecs.memory.MemoryCodec$TermsWriter.finish(MemoryCodec.java:232)
[junit] 	at org.apache.lucene.index.FreqProxTermsWriterPerField.flush(FreqProxTermsWriterPerField.java:414)
[junit] 	at org.apache.lucene.index.FreqProxTermsWriter.flush(FreqProxTermsWriter.java:92)
[junit] 	at org.apache.lucene.index.TermsHash.flush(TermsHash.java:117)
[junit] 	at org.apache.lucene.index.DocInverter.flush(DocInverter.java:80)
[junit] 	at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:78)
[junit] 	at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:472)
[junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:420)
[junit] 	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:568)
[junit] 	at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:366)
[junit] 	at org.apache.lucene.index.IndexReader.open(IndexReader.java:317)
[junit] 	at org.apache.lucene.util.fst.TestFSTs.testRealTerms(TestFSTs.java:1034)
[junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
{noformat}

or:

{noformat}
[junit] Testsuite: org.apache.lucene.util.automaton.TestCompiledAutomaton
[junit] Testcase: testRandom(org.apache.lucene.util.automaton.TestCompiledAutomaton):	Caused an ERROR
[junit] Java heap space
[junit] java.lang.OutOfMemoryError: Java heap space
[junit] 	at org.apache.lucene.util.automaton.RunAutomaton.<init>(RunAutomaton.java:128)
[junit] 	at org.apache.lucene.util.automaton.ByteRunAutomaton.<init>(ByteRunAutomaton.java:28)
[junit] 	at org.apache.lucene.util.automaton.CompiledAutomaton.<init>(CompiledAutomaton.java:134)
[junit] 	at org.apache.lucene.util.automaton.TestCompiledAutomaton.build(TestCompiledAutomaton.java:39)
[junit] 	at org.apache.lucene.util.automaton.TestCompiledAutomaton.testTerms(TestCompiledAutomaton.java:55)
[junit] 	at org.apache.lucene.util.automaton.TestCompiledAutomaton.testRandom(TestCompiledAutomaton.java:101)
[junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
[junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
[junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
{noformat}

Almost every nightly test fails, history: [https://builds.apache.org/job/Lucene-trunk]

We should maybe raise the max heap space or reduce doc counts/..."
"LUCENE-1528","RFE","IMPROVEMENT","Add support for Ideographic Space to the queryparser - also know as fullwith space and wide-space","The Ideographic Space is a space character that is as wide as a normal CJK character cell.
It is also known as wide-space or fullwith space.This type of space is used in CJK languages.

This patch adds support for the wide space, making the queryparser component more friendly
to queries that contain CJK text.

Reference:
'http://en.wikipedia.org/wiki/Space_(punctuation)' - see Table of spaces, char U+3000.

I also added a new testcase that fails before the patch.
After the patch is applied all junits pass."
"LUCENE-2324","RFE","IMPROVEMENT","Per thread DocumentsWriters that write their own private segments","See LUCENE-2293 for motivation and more details.

I'm copying here Mike's summary he posted on 2293:

Change the approach for how we buffer in RAM to a more isolated
approach, whereby IW has N fully independent RAM segments
in-process and when a doc needs to be indexed it's added to one of
them. Each segment would also write its own doc stores and
""normal"" segment merging (not the inefficient merge we now do on
flush) would merge them. This should be a good simplification in
the chain (eg maybe we can remove the *PerThread classes). The
segments can flush independently, letting us make much better
concurrent use of IO & CPU."
"LUCENE-1813","RFE","IMPROVEMENT","Add option to ReverseStringFilter to mark reversed tokens","This patch implements additional functionality in the filter to ""mark"" reversed tokens with a special marker character (Unicode 0001). This is useful when indexing both straight and reversed tokens (e.g. to implement efficient leading wildcards search)."
"LUCENE-1961","CLEANUP","TASK","Remove remaining deprecations in document package","Remove different deprecated APIs:
- Field.Index.NO_NORMS, etc.
- Field.binaryValue()
- getOmitTf()/setOmitTf()
"
"LUCENE-2536","BUG","BUG","Rollback doesn't preserve integrity of original index","After several ""updateDocuments"" calls a rollback call does not return the index to the prior state.
This seems to occur if the number of updates exceeds the RAM buffer size i.e. when some flushing of updates occurs.

Test fails in Lucene 2.4, 2.9, 3.0.1 and 3.0.2

JUnit to follow.
"
"LUCENE-2211","TEST","BUG","Improve BaseTokenStreamTestCase to uses a fake attribute to check if clearAttributes() was called correctly - found bugs in contrib/analyzers","Robert had the idea to use a fake attribute inside BaseTokenStreamTestCase that records if its clear() method was called. If this is not the case after incrementToken(), asserTokenStreamContents fails. It also uses the attribute in TeeSinkTokenFilter, because there a lot of copying, captureState and restoreState() is used. By the attribute, you can track wonderful, if save/restore and clearAttributes is correctly implemented. It also verifies that *before* a captureState() it was also cleared (as the state will also contain the clear call). Because if you consume tokens in a filter, capture the consumed tokens and insert them, the capturedStates must also be cleared before.

In contrib analyzers are some test that fail to pass this additional assertion. They are not fixed in the attached patch."
"LUCENE-2695","IMPROVEMENT","IMPROVEMENT","DisjunctionMaxScorer allocates 2 arrays per scored doc","It has this:
{noformat}
  @Override
  public float score() throws IOException {
    int doc = subScorers[0].docID();
    float[] sum = { subScorers[0].score() }, max = { sum[0] };
    int size = numScorers;
    scoreAll(1, size, doc, sum, max);
    scoreAll(2, size, doc, sum, max);
    return max[0] + (sum[0] - max[0]) * tieBreakerMultiplier;
  }
{noformat}

They are thread-private arrays so possibly/likely JVM can optimize this case (allocate only on the stack) but still I think instead it should have private instance vars for the score/max."
"LUCENE-2792","RFE","RFE","Add a simple FST impl to Lucene","
I implemented the algo described at
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.3698 for
incrementally building a finite state transducer (FST) from sorted
inputs.

This is not a fully general FST impl -- it's only able to build up an
FST incrementally from input/output pairs that are pre-sorted.

Currently the inputs are BytesRefs, and the outputs are pluggable --
NoOutputs gets you a simple FSA, PositiveIntOutputs maps to a long,
ByteSequenceOutput maps to a BytesRef.

The implementation has a low memory overhead, so that it can handle a
fairly large set of terms.  For example, it can build the FSA for the
9.8M terms from a 10M document wikipedia index in ~8 seconds (on
beast), using ~256 MB peak RAM, resulting in an FSA that's ~60 MB.

It packs the FST as-it-builds into a compact byte[], and then exposes
the API to read nodes/arcs directly from the byte[].  The FST can be
quickly saved/loaded to/from a Directory since it's just a big byte[].

The format is similar to what Morfologik uses
(http://sourceforge.net/projects/morfologik/).

I think there are a number of possible places we can use this in
Lucene.  For example, I think many apps could hold the entire terms
dict in RAM, either at the multi-reader level or maybe per-segment
(mapping to file offset or to something else custom to the app), which
may possibly be a good speedup for certain MTQs (though, because the
format is packed into a byte[], there is a decode cost when visiting
arcs).

The builder can also prune as it goes, so you get a prefix trie pruned
according to how many terms run through the nodes, which makes it
faster and even less memory consuming.  This may be useful as a
replacement for our current binary search terms index since it can
achieve higher term density for the same RAM consumption of our
current index.

As an initial usage to make sure this is exercised, I cutover the
SimpleText codec, which currently fully loads all terms into a
TreeMap (and has caused intermittent OOME in some tests), to use an FST
instead.  SimpleText uses a PairOutputs which is able to ""pair up"" any
two other outputs, since it needs to map each input term to an int
docFreq and long filePosition.

All tests pass w/ SimpleText forced codec, and I think this is
committable except I'd love to get some help w/ the generics
(confession to the policeman: I had to add
@SuppressWarnings({""unchecked""})) all over!!  Ideally an FST is
parameterized by its output type (Integer, BytesRef, etc.).

I even added a new @nightly test that makes a largeish set of random
terms and tests the resulting FST on different outputs :)

I think it would also be easy to make a variant that uses char[]
instead of byte[] as its inputs, so we could eg use this during analysis
(Robert's idea).  It's already be easy to have a CharSequence
output type since the outputs are pluggable.

Dawid Weiss (author of HPPC -- http://labs.carrotsearch.com/hppc.html -- and
Morfologik -- http://sourceforge.net/projects/morfologik/)
was very helpful iterating with me on this (thank you!).
"
"LUCENE-3668","BUG","BUG","offsets issues with multiword synonyms","as reported on the list, there are some strange offsets with FSTSynonyms, in the case of multiword synonyms.

as a workaround it was suggested to use the older synonym impl, but it has bugs too (just in a different way).
"
"LUCENE-748","BUG","BUG","Exception during IndexWriter.close() prevents release of the write.lock","After encountering a case of index corruption - see http://issues.apache.org/jira/browse/LUCENE-140 - when the close() method encounters an exception in the flushRamSegments() method, the index write.lock is not released (ie. it is not really closed).

The writelock is only released when the IndexWriter is GC'd and finalize() is called."
"LUCENE-3584","IMPROVEMENT","TASK","bulk postings should be codec private","In LUCENE-2723, a lot of work was done to speed up Lucene's bulk postings read API.

There were some upsides:
* you could specify things like 'i dont care about frequency data up front'.
  This made things like multitermquery->filter and other consumers that don't
  care about freqs faster. But this is unrelated to 'bulkness' and we have a
  separate patch now for this on LUCENE-2929.
* the buffersize for standardcodec was increased to 128, increasing performance
  for TermQueries, but this was unrelated too.

But there were serious downsides/nocommits:
* the API was hairy because it tried to be 'one-size-fits-all'. This made consumer code crazy.
* the API could not really be specialized to your codec: e.g. could never take advantage that e.g. docs and freqs are aligned.
* the API forced codecs to implement delta encoding for things like documents and positions. 
  But this is totally up to the codec how it wants to encode! Some codecs might not use delta encoding.
* using such an API for positions was only theoretical, it would have been super complicated and I doubt ever
  performant or maintainable.
* there was a regression with advance(), probably because the api forced you to do both a linear scan thru
  the remaining buffer, then refill...

I think a cleaner approach is to let codecs do whatever they want to implement the DISI
contract. This lets codecs have the freedom to implement whatever compression/buffering they want
for the best performance, and keeps consumers simple. If a codec uses delta encoding, or if it wants
to defer this to the last possible minute or do it at decode time, thats its own business. Maybe a codec
doesn't want to do any buffering at all.
"
"LUCENE-2356","RFE","BUG","Enable setting the terms index divisor used by IndexWriter whenever it opens internal readers","Opening a place holder issue... if all the refactoring being discussed don't make this possible, then we should add a setting to IWC to do so.

Apps with very large numbers of unique terms must set the terms index divisor to control RAM usage.

(NOTE: flex's RAM terms dict index RAM usage is more efficient, so this will help such apps).

But, when IW resolves deletes internally it always uses default 1 terms index divisor, and the app cannot change that.  Though one workaround is to call getReader(termInfosIndexDivisor) which will pool the reader with the right divisor."
"LUCENE-3405","REFACTORING","IMPROVEMENT","Rename IOUtils.close methods","The closeSafely methods that take a boolean suppressExceptions are dangerous... I've renamed to .close (no suppression) and .closeWhileHandlingException (suppresses all exceptions)."
"LUCENE-3446","BUG","BUG","NullPointerException in BooleanFilter ","BooleanFilter getDISI() method used with QueryWrapperFilter occur NullPointerException,
if any QueryWrapperFilter not match terms in IndexReader.

---------------------------------------------------
java.lang.NullPointerException
	at org.apache.lucene.util.OpenBitSetDISI.inPlaceAnd(OpenBitSetDISI.java:66)
	at org.apache.lucene.search.BooleanFilter.getDocIdSet(BooleanFilter.java:102)
	at org.apache.lucene.search.IndexSearcher.searchWithFilter(IndexSearcher.java:551)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:532)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:463)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:433)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:356)
	at test.BooleanFilterTest.main(BooleanFilterTest.java:50)
---------------------------------------------------

null-check below lines.
---------------------------------------------------
res = new OpenBitSetDISI(getDISI(shouldFilters, i, reader), reader.maxDoc());
res.inPlaceOr(getDISI(shouldFilters, i, reader));
res = new OpenBitSetDISI(getDISI(notFilters, i, reader), reader.maxDoc());
res.inPlaceNot(getDISI(notFilters, i, reader));
res = new OpenBitSetDISI(getDISI(mustFilters, i, reader), reader.maxDoc());
res.inPlaceAnd(getDISI(mustFilters, i, reader));
---------------------------------------------------"
"LUCENE-496","RFE","RFE","New tool for  reseting the (length)norm of fields after changing Similarity","I've written a little tool that seems like it can/will be very handy as I tweak my custom similarity.  I think it would make a good addition to contrib/miscellaneous.

Class and Tests to be attached shortly..."
"LUCENE-3435","RFE","TASK","Create a Size Estimator model for Lucene and Solr","It is often handy to be able to estimate the amount of memory and disk space that both Lucene and Solr use, given certain assumptions.  I intend to check in an Excel spreadsheet that allows people to estimate memory and disk usage for trunk.  I propose to put it under dev-tools, as I don't think it should be official documentation just yet and like the IDE stuff, we'll see how well it gets maintained."
"LUCENE-3356","BUG","BUG","trunk TestRollingUpdates.testRollingUpdates seed failure","trunk r1152892
reproducable: always

{code}
junit-sequential:
    [junit] Testsuite: org.apache.lucene.index.TestRollingUpdates
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 1.168 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestRollingUpdates -Dtestmethod=testRollingUpdates -Dtests.seed=-5322802004404580273:-4001225075726350391
    [junit] WARNING: test method: 'testRollingUpdates' left thread running: merge thread: _c(4.0):cv3/2 _h(4.0):cv3 into _k
    [junit] RESOURCE LEAK: test method: 'testRollingUpdates' left 1 thread(s) running
    [junit] NOTE: test params are: codec=RandomCodecProvider: {docid=Standard, body=SimpleText, title=MockSep, titleTokenized=Pulsing(freqCutoff=20), date=MockFixedIntBlock(blockSize=1474)}, locale=lv_LV, timezone=Pacific/Fiji
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestRollingUpdates]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_26 (64-bit)/cpus=8,threads=1,free=128782656,total=158400512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testRollingUpdates(org.apache.lucene.index.TestRollingUpdates):   FAILED
    [junit] expected:<20> but was:<21>
    [junit] junit.framework.AssertionFailedError: expected:<20> but was:<21>
    [junit]     at org.apache.lucene.index.TestRollingUpdates.testRollingUpdates(TestRollingUpdates.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1522)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1427)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestRollingUpdates FAILED

{code}"
"LUCENE-296","IMPROVEMENT","IMPROVEMENT","[PATCH] FuzzyTermEnum optimization and refactor","I took a look at it to see if it could be improved.  I saw speed improvements of
20% - 40% by making a couple changes.  

The patch is here: http://www.hagerfamily.com/patches/FuzzyTermEnumOptimizePatch.txt

The Patch is based on the HEAD of the CVS tree as of Oct 22, 2004.

What Changed?

Since the word was discarded if the edit distance for the word was
above a certain threshold, I updated the distance algorithm to abort
if at any time during the calculation it is determined that the best
possible outcome of the edit distance algorithm is above this
threshold.  The source code has a great explanation.

I also reduced the amount of floating point math, reduced the amount
of potential space the array takes in its first dimension, removed the
potential divide by 0 error when one term is an empty string, and
fixed a bug where an IllegalArgumentException was thrown if the class
was somehow initialized wrong, instead of looking at the arguments.

The behavior is almost identical.  The exception is that similarity is
set to 0.0 when it is guaranteed to be below the minimum similarity.

Results

I saw the biggest improvement from longer words, which makes a sense.
My long word was ""bridgetown"" and I saw a 60% improvement on this.
The biggest improvement are for words that are farthest away from the
median length of the words in the index.  Short words (1-3 characters)
saw a 30% improvement.  Medium words saw a 10% improvement (5-7
characters).  These improvements are with the prefix set to 0."
"LUCENE-213","BUG","BUG","[PATCH] Ordered spanquery with slop can fail","In CVS of 7 April 2004. 
An ordered SpanQuery with slop 1 querying: w1 w2 w3 
in document: w1 w3 w2 w3 
fails. It should match as: w1 . w2 w3"
"LUCENE-1857","REFACTORING","IMPROVEMENT","Change NumericRangeQuery to generics (for the numeric type)","NumericRangeQuery/Filter can use generics for more type-safety: NumericRangeQuery<T extends Number>"
"LUCENE-3496","RFE","RFE","Support grouping by IndexDocValues","Although IDV is not yet finalized (More particular the SortedSource). I think we already can discuss / investigate implementing grouping by IDV."
"LUCENE-2466","BUG","BUG","fix some more locale problems in lucene/solr","set ANT_ARGS=""-Dargs=-Duser.language=tr -Duser.country=TR""
ant clean test

We should make sure this works across all of lucene/solr"
"LUCENE-783","RFE","IMPROVEMENT","Store all metadata in human-readable segments file","Various index-reading components in Lucene need metadata in addition to data.
This metadata is presently stored in arbitrary binary headers and spread out
over several files.  We should move to concentrate it in a single file, and 
this file should be encoded using a human-readable, extensible, standardized 
data serialization language -- either XML or YAML.

* Making metadata human-readable makes debugging easier.  Centralizing it
  makes debugging easier still.  Developers benefit from being able to scan
  and locate relevant information quickly and with less debug printing.  Users
  get a new window through which to peer into the index structure.
* Since metadata is written to a separate file, there would no longer be a 
  need to seek back to the beginning of any data file to finish a header, 
  solving issue LUCENE-532.
* Special-case parsing code needed for extracting metadata supplied by 
  different index formats can be pared down.  If a value is no longer 
  necessary, it can just be ignored/discarded.
* Removing headers from the data files simplifies them and makes the file
  format easier to implement. 
* With headers removed, all or nearly all data structures can take the
  form of records stacked end to end, so that once a decoder has been
  selected, an iterator can read the file from top to tail.  To an extent,
  this allows us to separate our data-processing algorithms from our
  serialization algorithms, decoupling Lucene's code base from its file
  format.  For instance, instead of further subclassing TermDocs to deal with
  ""flexible indexing"" formats, we might replace it with a PostingList which
  returns a subclass of Posting.  The deserialization code would be wholly
  contained within the Posting subclass rather than spread out over several
  subclasses of TermDocs.
* YAML and XML are equally well suited for the task of storing metadata, 
  but in either case a complete parser would not be needed -- a small subset 
  of the language will do.  KinoSearch 0.20's custom-coded YAML parser 
  occupies about 600 lines of C -- not too bad, considering how miserable C's 
  string handling capabilities are. "
"LUCENE-1433","DOCUMENTATION","IMPROVEMENT","Changes.html generation improvements","Bug fixes for and improvements to changes2html.pl, which generates Changes.html from CHANGES.txt:

# When the current location has a fragment identifier, expand parent sections, so that the linked-to section is visible.
# Properly handle beginning-of-release comments that don't fall under a section heading (previously: some content in release ""1.9 final"" was invisible).
# Auto-linkify SOLR-XXX and INFRA-XXX JIRA issues (previously: only LUCENE-XXX issues).
# Auto-linkify Bugzilla bugs prefaced with ""Issue"" (previously: only ""Bug"" and ""Patch"").
# Auto-linkify Bugzilla bugs in the form ""bugs XXXXX and YYYYY"".
# Auto-linkify issues that follow attributions.
"
"LUCENE-2155","TEST","TASK","random localization test failures","Some tests fail randomly (hard to reproduce). It appears to me that this is caused by uninitialized date fields. For example Uwe reported a failure today in this test of TestQueryParser:

{code}
 /** for testing legacy DateField support */
  public void testLegacyDateRange() throws Exception {
    String startDate = getLocalizedDate(2002, 1, 1, false);
    String endDate = getLocalizedDate(2002, 1, 4, false);
{code}

if you look at the helper getLocalizedDate, you can see if the 4th argument is false, it does not initialize all date field functions.
{code}
  private String getLocalizedDate(int year, int month, int day, boolean extendLastDate) {
 Calendar calendar = new GregorianCalendar();
 calendar.set(year, month, day);
 if (extendLastDate) {
      calendar.set(Calendar.HOUR_OF_DAY, 23);
      calendar.set(Calendar.MINUTE, 59);
      calendar.set(Calendar.SECOND, 59);
 ...
}
{code}

I think the solution to this is that in all tests, whereever we create new GregorianCalendar(), it should be followed by a call to Calendar.clear().
This will ensure that we always initialize unused calendar fields to zero, rather than being dependent on the local time."
"LUCENE-3200","CLEANUP","IMPROVEMENT","Cleanup MMapDirectory to use only one MMapIndexInput impl with mapping sized of powers of 2","Robert and me discussed a little bit after Mike's investigations, that using SingleMMapIndexinput together with MultiMMapIndexInput leads to hotspot slowdowns sometimes.

We had the following ideas:
- MultiMMapIndexInput is almost as fast as SingleMMapIndexInput, as the switching between buffer boundaries is done in exception catch blocks. So normal code path is always the same like for Single*
- Only the seek method uses strange calculations (the modulo is totally bogus, it could be simply: int bufOffset = (int) (pos % maxBufSize); - very strange way of calculating modulo in the original code)
- Because of speed we suggest to no longer use arbitrary buffer sizes. We should pass only the power of 2 to the indexinput as size. All calculations in seek and anywhere else would be simple bit shifts and AND operations (the and masks for the modulo can be calculated in the ctor like NumericUtils does when calculating precisionSteps).
- the maximum buffer size will now be 2^30, not 2^31-1. But thats not an issue at all. In my opinion, a buffer size of 2^31-1 is stupid in all cases, as it will no longer fit page boundaries and mmapping gets harder for the O/S.

We will provide a patch with those cleanups."
"LUCENE-3054","IMPROVEMENT","TASK","SorterTemplate.quickSort stack overflows on broken comparators that produce only few disticnt values in large arrays","Looking at Otis's sort problem on the mailing list, he said:
{noformat}
* looked for other places where this call is made - found it in
MultiPhraseQuery$MultiPhraseWeight and changed that call from
ArrayUtil.quickSort to ArrayUtil.mergeSort
* now we no longer see SorterTemplate.quickSort in deep recursion when we do a
thread dump
{noformat}

I thought this was interesting because PostingsAndFreq's comparator
looks like it needs a tiebreaker.

I think in our sorts we should add some asserts to try to catch some of these broken comparators."
"LUCENE-1177","BUG","BUG","IW.optimize() can do too many merges at the very end","This was fixed on trunk in LUCENE-1044 but I'd like to separately
backport it to 2.3.

With ConcurrentMergeScheduler there is a bug, only when CFS is on,
whereby after the final merge of an optimize has finished and while
it's building its CFS, the merge policy may incorrectly ask for
another merge to collapse that segment into a compound file.  The net
effect is optimize can spend many extra iterations unecessarily
merging a single segment to collapse it to compound file.

I believe the case is rare (hard to hit), and maybe only if you have
multiple threads calling optimize at once (the TestThreadedOptimize
test can hit it), but it's a low-risk fix so I plan to commit to 2.3
shortly.

"
"LUCENE-360","TEST","BUG","Test compilation error","The Lucence test fails with the following error: (latest revision from SVN)

compile-test:
   [mkdir] Created dir: /opt/lucene/lucene/build/classes/test
   [javac] Compiling 79 source files to /opt/lucene/lucene/build/classes/test
   [javac]
/opt/lucene/lucene/src/test/org/apache/lucene/index/TermInfosTest.java:89:
cannot resolve symbol
   [javac] symbol  : constructor TermInfosWriter
(org.apache.lucene.store.Directory,java.lang.String,org.apache.lucene.index.FieldInfos)
   [javac] location: class org.apache.lucene.index.TermInfosWriter
   [javac]     TermInfosWriter writer = new TermInfosWriter(store,
""words"", fis);

I see that TermInfosWriter was changed two days back to add another
argument (interval) to its constructor.

Mailing lists for Lucene do not seem to be responding (sending emails to
subscribe bounces back and also I do not see any mails after March 2nd being
archived in either the user or dev lists). So I am using the bug database to
inform the test failure and submit a simple patch that uses the value of 128
(the default in the TermInfosWriter class) as interval in the test case."
"LUCENE-642","RFE","RFE","GData Server IndexComponent","New Feature added:

-> Indexcomponent.
-> Content extraction from entries.
-> Custom content ext. strategies added.
-> user defined index schema.
-> extended gdata-config.xml schema (xsd)
-> Indexcomponent UnitTests
-> Spellchecking on some JavaDoc.

##############
New jars included:

nekoHTML.jar 
xercesImpl.jar

@yonik: don't miss the '+' button to add directories :)"
"LUCENE-1874","DOCUMENTATION","BUG","Further updates to the site scoring page","update the site scoring page - see Appendix:
{quote}
Class Diagrams
Karl Wettin's UML on the Wiki
{quote}
Karl's diagrams are outdated - I think this link should be pulled for 2.9

{quote}
Sequence Diagrams
FILL IN HERE. Volunteers?
{quote}
I think this should be pulled - I say put something like this as a task in JIRA - not the published site docs."
"LUCENE-2517","DOCUMENTATION","IMPROVEMENT","changes-to-html: fixes and improvements","The [Lucene Hudson Changes.html|http://hudson.zones.apache.org/hudson/job/Lucene-trunk/lastSuccessfulBuild/artifact/lucene/build/docs/changes/Changes.html] looks bad because changes2html.pl doesn't properly handle some new usages in CHANGES.txt."
"LUCENE-1741","RFE","IMPROVEMENT","Make MMapDirectory.MAX_BBUF user configureable to support chunking the index files in smaller parts","This is a followup for java-user thred: http://www.lucidimagination.com/search/document/9ba9137bb5d8cb78/oom_with_2_9#9bf3b5b8f3b1fb9b

It is easy to implement, just add a setter method for this parameter to MMapDir."
"LUCENE-376","BUILD_SYSTEM","IMPROVEMENT","[PATCH] Ant macro for javadocs and javadocs-internal","This removes the duplication introduced in build.xml 
when the javadocs-internal build target was added. 
 
Regards, 
Paul Elschot"
"LUCENE-246","RFE","BUG","[PATCH] Make a getter for SortField[] fields in org.apache.lucene.search.Sort","I'm have my own Collector and I would like to use the Sort object within my
collector, but SortField[] fields; is not accessible outside Lucene's package.
Can you please consider making a public getFields() method in the Sort object so
we can use it in our implementation?"
"LUCENE-1605","RFE","RFE","Add subset method to BitVector","Recently I needed the ability to efficiently compute subsets of a BitVector. The method is:
  public BitVector subset(int start, int end)
where ""start"" is the starting index, inclusive and ""end"" is the ending index, exclusive.

Attached is a patch including the subset method as well as relevant unit tests."
"LUCENE-3494","IMPROVEMENT","IMPROVEMENT","Remove per-document multiply in FilteredQuery","Spinoff of LUCENE-1536.

In LUCENE-1536, Uwe suggested using FilteredQuery under-the-hood to implement filtered search.

But this query is inefficient, it does a per-document multiplication (wrapped.score() * boost()).

Instead, it should just pass the boost down in its weight, like BooleanQuery does to avoid this per-document multiply."
"LUCENE-2556","IMPROVEMENT","IMPROVEMENT","(Char)TermAttribute cloning memory consumption","The memory consumption problem with cloning a (Char)TermAttributeImpl object was raised on thread http://markmail.org/thread/bybuerugbk5w2u6z"
"LUCENE-3203","IMPROVEMENT","IMPROVEMENT","Rate-limit IO used by merging","Large merges can mess up searches and increase NRT reopen time (see
http://blog.mikemccandless.com/2011/06/lucenes-near-real-time-search-is-fast.html).

A simple rate limiter improves the spikey NRT reopen times during big
merges, so I think we should somehow make this possible.  Likely this
would reduce impact on searches as well.

Typically apps that do indexing and searching on same box are in no
rush to see the merges complete so this is a good tradeoff.
"
"LUCENE-1286","RFE","IMPROVEMENT","LargeDocHighlighter - another span highlighter optimized for large documents","The existing Highlighter API is rich and well designed, but the approach taken is not very efficient for large documents.

I believe that this is because the current Highlighter rebuilds the document by running through and scoring every every token in the tokenstream.

With a break in the current API, an alternate approach can be taken: rebuild the document by running through the query terms by using their offsets. The benefit is clear - a large doc will have a large tokenstream, but a query will likely be very small in comparison.

I expect this approach to be quite a bit faster for very large documents, while still supporting Phrase and Span queries.

First rough patch to follow shortly."
"LUCENE-1187","CLEANUP","IMPROVEMENT","Things to be done now that Filter is independent from BitSet","(Aside: where is the documentation on how to mark up text in jira comments?)

The following things are left over after LUCENE-584 :

For Lucene 3.0  Filter.bits() will have to be removed.

There is a CHECKME in IndexSearcher about using ConjunctionScorer to have the boolean behaviour of a Filter.

I have not looked into Filter caching yet, but I suppose there will be some room for improvement there.
Iirc the current core has moved to use OpenBitSetFilter and that is probably what is being cached.
In some cases it might be better to cache a SortedVIntList instead.

Boolean logic on DocIdSetIterator is already available for Scorers (that inherit from DocIdSetIterator) in the search package. This is currently implemented by ConjunctionScorer, DisjunctionSumScorer,
ReqOptSumScorer and ReqExclScorer.
Boolean logic on BitSets is available in contrib/misc and contrib/queries

DisjunctionSumScorer calls score() on its subscorers before the score value actually needed.
This could be a reason to introduce a DisjunctionDocIdSetIterator, perhaps as a superclass of DisjunctionSumScorer.

To fully implement non scoring queries a TermDocIdSetIterator will be needed, perhaps as a superclass of TermScorer.

The javadocs in org.apache.lucene.search using matching vs non-zero score:
I'll investigate this soon, and provide a patch when necessary.

An early version of the patches of LUCENE-584 contained a class Matcher,
that differs from the current DocIdSet in that Matcher has an explain() method.
It remains to be seen whether such a Matcher could be useful between
DocIdSet and Scorer.

The semantics of scorer.skipTo(scorer.doc()) was discussed briefly.
This was also discussed at another issue recently, so perhaps it is wortwhile to open a separate issue for this.

Skipping on a SortedVIntList is done using linear search, this could be improved by adding multilevel skiplist info much like in the Lucene index for documents containing a term.

One comment by me of 3 Dec 2008:

A few complete (test) classes are deprecated, it might be good to add the target release for removal there.
"
"LUCENE-3778","DESIGN_DEFECT","IMPROVEMENT","Create a grouping convenience class","Currently the grouping module has many collector classes with a lot of different options per class. I think it would be a good idea to have a GroupUtil (Or another name?) convenience class. I think this could be a builder, because of the many options (sort,sortWithinGroup,groupOffset,groupCount and more) and implementations (term/dv/function) grouping has."
"LUCENE-3718","BUG","BUG","SamplingWrapperTest failure with certain test seed","Build: https://builds.apache.org/job/Lucene-Solr-tests-only-trunk/12231/

1 tests failed.
REGRESSION:  org.apache.lucene.facet.search.SamplingWrapperTest.testCountUsingSamping

Error Message:
Results are not the same!

Stack Trace:
org.apache.lucene.facet.FacetTestBase$NotSameResultError: Results are not the same!
       at org.apache.lucene.facet.FacetTestBase.assertSameResults(FacetTestBase.java:333)
       at org.apache.lucene.facet.search.sampling.BaseSampleTestTopK.assertSampling(BaseSampleTestTopK.java:104)
       at org.apache.lucene.facet.search.sampling.BaseSampleTestTopK.testCountUsingSamping(BaseSampleTestTopK.java:82)
       at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:529)
       at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
       at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)

NOTE: reproduce with: ant test -Dtestcase=SamplingWrapperTest -Dtestmethod=testCountUsingSamping -Dtests.seed=4a5994491f79fc80:-18509d134c89c159:-34f6ecbb32e930f7 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=UTF-8""
NOTE: test params are: codec=Lucene40: {$facets=PostingsFormat(name=MockRandom), $full_path$=PostingsFormat(name=MockSep), content=Pulsing40(freqCutoff=19 minBlockSize=65 maxBlockSize=209), $payloads$=PostingsFormat(name=Lucene40WithOrds)}, sim=RandomSimilarityProvider(queryNorm=true,coord=true): {$facets=LM Jelinek-Mercer(0.700000), content=DFR I(n)B3(800.0)}, locale=bg, timezone=Asia/Manila
"
"LUCENE-2846","RFE","IMPROVEMENT","omitTF is viral, but omitNorms is anti-viral.","omitTF is viral. if you add document 1 with field ""foo"" as omitTF, then document 2 has field ""foo"" without omitTF, they are both treated as omitTF.

but omitNorms is the opposite. if you have a million documents with field ""foo"" with omitNorms, then you add just one document without omitting norms, 
now you suddenly have a million 'real norms'.

I think it would be good for omitNorms to be viral too, just for consistency, and also to prevent huge byte[]'s.
but another option is to make omitTF anti-viral, which is more ""schemaless"" i guess.
"
"LUCENE-3549","CLEANUP","IMPROVEMENT","Remove DocumentBuilder interface from facet module","The facet module contains an interface called DocumentBuilder, which contains a single method, build(Document) (it's a builder API). We use it in my company to standardize how different modules populate a Document object. I've included it with the facet contribution so that things will compile with as few code changes as possible.

Now it's time to do some cleanup and I'd like to start with this interface. If people think that this interface is useful to reside in 'core', then I don't mind moving it there. But otherwise, let's remove it from the code. It has only one impl in the facet module: CategoryDocumentBuilder, and we can certainly do without the interface.

More so, it's under o.a.l package which is inappropriate IMO. If it's moved to 'core', it should be under o.a.l.document.

If people see any problem with that, please speak up. I will do the changes and post a patch here shortly."
"LUCENE-1387","RFE","RFE","Add LocalLucene","Local Lucene (Geo-search) has been donated to the Lucene project, per https://issues.apache.org/jira/browse/INCUBATOR-77.  This issue is to handle the Lucene portion of integration.

See http://lucene.markmail.org/message/orzro22sqdj3wows?q=LocalLucene
"
"LUCENE-3876","BUG","BUG","TestIndexWriterExceptions fails (reproducible)","{noformat}
ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testIllegalPositions -Dtests.seed=-228094d3d2f35cf2:-496e33eec9bbd57c:36a1c54f4e1bb32 -Dargs=""-Dfile.encoding=UTF-8""

    [junit] junit.framework.AssertionFailedError: position=-2 lastPosition=0
    [junit]     at org.apache.lucene.codecs.lucene40.Lucene40PostingsWriter.addPosition(Lucene40PostingsWriter.java:215)
    [junit]     at org.apache.lucene.index.FreqProxTermsWriterPerField.flush(FreqProxTermsWriterPerField.java:519)
    [junit]     at org.apache.lucene.index.FreqProxTermsWriter.flush(FreqProxTermsWriter.java:92)
    [junit]     at org.apache.lucene.index.TermsHash.flush(TermsHash.java:117)
    [junit]     at org.apache.lucene.index.DocInverter.flush(DocInverter.java:53)
    [junit]     at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:81)
    [junit]     at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:475)
    [junit]     at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:422)
    [junit]     at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:553)
    [junit]     at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:2640)
    [junit]     at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:2616)
    [junit]     at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:851)
    [junit]     at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:810)
    [junit]     at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:774)
    [junit]     at org.apache.lucene.index.TestIndexWriterExceptions.testIllegalPositions(TestIndexWriterExceptions.java:1517)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
    [junit]     at org.apache.lucene.util.LuceneTestCase$SubclassSetupTeardownRule$1.evaluate(LuceneTestCase.java:729)
    [junit]     at org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:645)
    [junit]     at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
    [junit]     at org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:556)
    [junit]     at org.apache.lucene.util.UncaughtExceptionsRule$1.evaluate(UncaughtExceptionsRule.java:51)
    [junit]     at org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:618)
    [junit]     at org.junit.rules.RunRules.evaluate(RunRules.java:18)
    [junit]     at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
    [junit]     at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:164)
    [junit]     at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
    [junit]     at org.apache.lucene.util.UncaughtExceptionsRule$1.evaluate(UncaughtExceptionsRule.java:51)
    [junit]     at org.apache.lucene.util.StoreClassNameRule$1.evaluate(StoreClassNameRule.java:21)
    [junit]     at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
    [junit]     at org.junit.rules.RunRules.evaluate(RunRules.java:18)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:518)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1052)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:906)
    [junit] 
{noformat}"
"LUCENE-3707","RFE","TASK","Add a Lucene3x private SegmentInfosFormat implemenation","we still don't have a Lucene3x & preflex version of segment infos format. we need this before we release 4.0"
"LUCENE-1164","IMPROVEMENT","BUG","Improve how ConcurrentMergeScheduler handles too-many-merges case","CMS now lets you set ""maxMergeThreads"" to control max # simultaneous
merges.

However, when CMS hits that max, it still allows further merges to
run, by running them in the foreground thread.  So if you set this max
to 1, and use 1 thread to add docs, you can get 2 merges running at
once (which I think is broken).

I think, instead, CMS should pause the foreground thread, waiting
until the number of merge threads drops below the limit.  Then, kick
off the backlog merge in a thread and return control back to primary
thread.
"
"LUCENE-1404","BUG","BUG","NPE in NearSpansUnordered.isPayloadAvailable() ","Using RC1 of lucene 2.4 resulted in null pointer exception with some constructed SpanNearQueries

Implementation of isPayloadAvailable() (results in exception)
{code}
 public boolean isPayloadAvailable() {
   SpansCell pointer = min();
   do {
     if(pointer.isPayloadAvailable()) {
       return true;
     }
     pointer = pointer.next;
   } while(pointer.next != null);

   return false;
  }
{code}

""Fixed"" isPayloadAvailable()
{code}
 public boolean isPayloadAvailable() {
   SpansCell pointer = min();
   while (pointer != null) {
     if(pointer.isPayloadAvailable()) {
       return true;
     }
     pointer = pointer.next;
   }

   return false;
  }
{code}

Exception produced:
{code}
  [junit] java.lang.NullPointerException
    [junit]     at org.apache.lucene.search.spans.NearSpansUnordered$SpansCell.access$300(NearSpansUnordered.java:65)
    [junit]     at org.apache.lucene.search.spans.NearSpansUnordered.isPayloadAvailable(NearSpansUnordered.java:235)
    [junit]     at org.apache.lucene.search.spans.NearSpansOrdered.shrinkToAfterShortestMatch(NearSpansOrdered.java:246)
    [junit]     at org.apache.lucene.search.spans.NearSpansOrdered.advanceAfterOrdered(NearSpansOrdered.java:154)
    [junit]     at org.apache.lucene.search.spans.NearSpansOrdered.next(NearSpansOrdered.java:122)
    [junit]     at org.apache.lucene.search.spans.SpanScorer.next(SpanScorer.java:54)
    [junit]     at org.apache.lucene.search.Scorer.score(Scorer.java:57)
    [junit]     at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:137)
    [junit]     at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:113)
    [junit]     at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:113)
    [junit]     at org.apache.lucene.search.Hits.<init>(Hits.java:80)
    [junit]     at org.apache.lucene.search.Searcher.search(Searcher.java:50)
    [junit]     at org.apache.lucene.search.Searcher.search(Searcher.java:40)
    [junit]     at com.attivio.lucene.SpanQueryTest.search(SpanQueryTest.java:79)
    [junit]     at com.attivio.lucene.SpanQueryTest.assertHitCount(SpanQueryTest.java:75)
    [junit]     at com.attivio.lucene.SpanQueryTest.test(SpanQueryTest.java:67)
{code}

will attach unit test that causes exception (and passes with updated isPayloadAvailable())
"
"LUCENE-2278","BUG","BUG","FastVectorHighlighter: highlighted term is out of alignment in multi-valued NOT_ANALYZED field",""
"LUCENE-3883","RFE","RFE","Analysis for Irish","Adds analysis for Irish.

The stemmer is generated from a snowball stemmer. I've sent it to Martin Porter, who says it will be added during the week."
"LUCENE-392","IMPROVEMENT","BUG","[PATCH] Some Field methods use Classcast check instead of instanceof which is slow","I am not sure if this is because Lucene historically needed to work with older
JVM's but with modern JVM's, instanceof is much quicker. 

The Field.stringValue(), .readerValue(), and .binaryValue() methods all use
ClassCastException checking.

Using the following test-bed class, you will see that instanceof is miles quicker:

package com.aconex.index;

public class ClassCastExceptionTest {

    private static final long ITERATIONS = 100000;

    /**
     * @param args
     */
    public static void main(String[] args) {

        runClassCastTest(1); // once for warm up
        runClassCastTest(2);
        
        runInstanceOfCheck(1);
        runInstanceOfCheck(2);

    }
    private static void runInstanceOfCheck(int run) {
        long start = System.currentTimeMillis();

        Object foo = new Foo();
        for (int i = 0; i < ITERATIONS; i++) {
            String test;
            if(foo instanceof String) {
                System.out.println(""Is a string""); // should never print
            }
        }
        long end = System.currentTimeMillis();
        long diff = end - start;
        System.out.println(""InstanceOf checking run #"" + run + "": "" + diff + ""ms"");
        
    }

    private static void runClassCastTest(int run) {
        long start = System.currentTimeMillis();

        Object foo = new Foo();
        for (int i = 0; i < ITERATIONS; i++) {
            String test;
            try {
                test = (String)foo;
            } catch (ClassCastException c) {
                // ignore
            }
        }
        long end = System.currentTimeMillis();
        long diff = end - start;
        System.out.println(""ClassCast checking run #"" + run + "": "" + diff + ""ms"");
    }

    private static final class Foo {
    }

}


Results
=======

Run #1

ClassCast checking run #1: 1660ms
ClassCast checking run #2: 1374ms
InstanceOf checking run #1: 8ms
InstanceOf checking run #2: 4ms


Run #2
ClassCast checking run #1: 1280ms
ClassCast checking run #2: 1344ms
InstanceOf checking run #1: 7ms
InstanceOf checking run #2: 2ms


Run #3
ClassCast checking run #1: 1347ms
ClassCast checking run #2: 1250ms
InstanceOf checking run #1: 7ms
InstanceOf checking run #2: 2ms

This could explain why Documents with more Fields scales worse, as in, for lots
of Documents with lots of Fields, the effect is exacerbated."
"LUCENE-2692","RFE","RFE","Position Checking Span Queries","I've created a bunch of new SpanQuery classes that allow one to do things like check to see if a SpanQuery falls between two positions (which is a more general form of SpanFirstQuery) and I've also added one that only includes a match if the payload located at the span match also matches a given payload.  With the latter, one can do queries for items w/ specific payloads."
"LUCENE-689","BUG","BUG","NullPointerException thrown by equals method in SpanOrQuery","Part of our code utilizes the equals method in SpanOrQuery and, in certain cases (details to follow, if necessary), a NullPointerException gets thrown as a result of the String ""field"" being null.  After applying the following patch, the problem disappeared:

Index: src/java/org/apache/lucene/search/spans/SpanOrQuery.java
===================================================================
--- src/java/org/apache/lucene/search/spans/SpanOrQuery.java    (revision 465065)
+++ src/java/org/apache/lucene/search/spans/SpanOrQuery.java    (working copy)
@@ -121,7 +121,8 @@
     final SpanOrQuery that = (SpanOrQuery) o;

     if (!clauses.equals(that.clauses)) return false;
-    if (!field.equals(that.field)) return false;
+    if (field != null && !field.equals(that.field)) return false;
+    if (field == null && that.field != null) return false;

     return getBoost() == that.getBoost();
   }

"
"LUCENE-1069","BUG","BUG","CheckIndex incorrectly sees deletes as index corruption","There is a silly bug in CheckIndex whereby any segment with deletes is
considered corrupt.

Thanks to Bogdan Ghidireac for reporting this."
"LUCENE-3348","BUG","BUG","IndexWriter applies wrong deletes during concurrent flush-all","Yonik uncovered this with the TestRealTimeGet test: if a flush-all is
underway, it is possible for an incoming update to pick a DWPT that is
stale, ie, not yet pulled/marked for flushing, yet the DW has cutover
to a new deletes queue.  If this happens, and the deleted term was
also updated in one of the non-stale DWPTs, then the wrong document is
deleted and the test fails by detecting the wrong value.

There's a 2nd failure mode that I haven't figured out yet, whereby 2
docs are returned when searching by id (there should only ever be 1
doc since the test uses updateDocument which is atomic wrt
commit/reopen).

Yonik verified the test passes pre-DWPT, so my guess is (but I
have yet to verify) this test also passes on 3.x.  I'll backport
the test to 3.x to be sure.
"
"LUCENE-3071","RFE","RFE","PathHierarchyTokenizer adaptation for urls: splits reversed","{{PathHierarchyTokenizer}} should be usable to split urls the a ""reversed"" way (useful for faceted search against urls):
{{www.site.com}} -> {{www.site.com, site.com, com}}

Moreover, it should be able to skip a given number of first (or last, if reversed) tokens:
{{/usr/share/doc/somesoftware/INTERESTING/PART}}
Should give with 4 tokens skipped:
{{INTERESTING}}
{{INTERESTING/PART}}"
"LUCENE-1253","IMPROVEMENT","BUG","LengthFilter and other TokenFilters that skip tokens ignore relative positionIncrement","See for reference:
http://www.nabble.com/WordDelimiterFilter%2BLenghtFilter-results-in-termPosition%3D%3D-1-td16306788.html
and http://www.nabble.com/Lucene---Java-f24284.html

It seems that LengthFilter (at least) could produce a stream in which the first Token has a positionIncrement of 0, which make CheckIndex and Luke function ""Reconstruct&Edit"" to generate exception.

Should something be done to avoid this situation, or could the error be ignored (by allowing Term with a position of -1, and relaxing CheckIndex checks?)
"
"LUCENE-3742","BUG","BUG","SynFilter doesn't set offsets for outputs that hang off the end of the input tokens","If you have syn rule a -> x y and input a then output is a/x y but... what should y's offsets be?  Right now we set to 0/0."
"LUCENE-3802","RFE","RFE","Grouping collector that computes grouped facet counts","Spinoff from issue SOLR-2898. "
"LUCENE-3381","CLEANUP","IMPROVEMENT","Sandbox remaining contrib queries","In LUCENE-3271, I moved the 'good' queries from the queries contrib to new destinations (primarily the queries module).  The remnants now need to find their home.  As suggested in LUCENE-3271, these classes are not bad per se, just odd.  So lets create a sandbox contrib that they and other 'odd' contrib classes can go to.  We can then decide their fate at another time."
"LUCENE-1960","CLEANUP","TASK","Remove deprecated Field.Store.COMPRESS","Also remove FieldForMerge and related code."
"LUCENE-2205","IMPROVEMENT","IMPROVEMENT","Rework of the TermInfosReader class to remove the Terms[], TermInfos[], and the index pointer long[] and create a more memory efficient data structure.","Basically packing those three arrays into a byte array with an int array as an index offset.  

The performance benefits are stagering on my test index (of size 6.2 GB, with ~1,000,000 documents and ~175,000,000 terms), the memory needed to load the terminfos into memory were reduced to 17% of there original size.  From 291.5 MB to 49.7 MB.  The random access speed has been made better by 1-2%, load time of the segments are ~40% faster as well, and full GC's on my JVM were made 7 times faster.

I have already performed the work and am offering this code as a patch.  Currently all test in the trunk pass with this new code enabled.  I did write a system property switch to allow for the original implementation to be used as well.

-Dorg.apache.lucene.index.TermInfosReader=default or small

I have also written a blog about this patch here is the link.

http://www.nearinfinity.com/blogs/aaron_mccurry/my_first_lucene_patch.html



"
"LUCENE-2341","RFE","RFE","explore morfologik integration","Dawid Weiss mentioned on LUCENE-2298 that there is another Polish stemmer available:
http://sourceforge.net/projects/morfologik/

This works differently than LUCENE-2298, and ideally would be another option for users.
"
"LUCENE-3418","BUG","BUG","Lucene is not fsync'ing files on commit","Thanks to hurricane Irene, when Mark's electricity became unreliable, he discovered that on power loss Lucene could easily corrumpt the index, which of course should never happen...

I was able to easily repro, by pulling the plug on an Ubuntu box during indexing.  On digging, I discovered, to my horror, that Lucene is failing to fsync any files, ever!

This bug was unfortunately created when we committed LUCENE-2328... that issue added tracking, in FSDir, of which files have been closed but not sync'd, so that when sync is called during IW.commit we only sync those files that haven't already been sync'd.

That tracking is done via the FSDir.onIndexOutputClosed callback, called when an FSIndexOutput is closed.  The bug is that we only call it on exception during close:

{noformat}

    @Override
    public void close() throws IOException {
      // only close the file if it has not been closed yet
      if (isOpen) {
        boolean success = false;
        try {
          super.close();
          success = true;
        } finally {
          isOpen = false;
          if (!success) {
            try {
              file.close();
              parent.onIndexOutputClosed(this);
            } catch (Throwable t) {
              // Suppress so we don't mask original exception
            }
          } else
            file.close();
        }
      }
    }
{noformat}

And so FSDir thinks no files need syncing when its sync method is called....

I think instead we should call it up-front; better to over-sync than under-sync.

The fix is trivial (move the callback up-front), but I'd love to somehow have a test that can catch such a bad regression in the future.... still I think we can do that test separately and commit this fix first.

Note that even though LUCENE-2328 was backported to 2.9.x and 3.0.x, this bug wasn't, ie the backport was a much simpler fix (to just address the original memory leak); it's 3.1, 3.2, 3.3 and trunk when this bug is present."
"LUCENE-2641","TEST","TEST","BaseTestRangeFilter can be extremely slow","The tests that extend BaseTestRangeFilter can sometimes be very slow:
TestFieldCacheRangeFilter, TestMultiTermConstantScore, TestTermRangeFilter

for example, TestFieldCacheRangeFilter just ran for 10 minutes on my computer before I killed it,
but i noticed these tests frequently run for over a minute.

I think at the least we should change these to junit4 so the index is built once in @beforeClass"
"LUCENE-1179","BUG","BUG","AssertionError on creating doc containing field with empty string as field name","Spinoff from here:

  http://www.gossamer-threads.com/lists/lucene/java-user/58496

Pre-2.3 you were allowed to add Fields to a Document where the field name is the empty string.  In 2.3.0 it broke: you hit this during flush:

{code}
java.lang.AssertionError
    at org.apache.lucene.index.TermInfosWriter.add(TermInfosWriter.java:143)
    at org.apache.lucene.index.DocumentsWriter.appendPostings(DocumentsWriter.java:2290)
    at org.apache.lucene.index.DocumentsWriter.writeSegment(DocumentsWriter.java:1985)
    at org.apache.lucene.index.DocumentsWriter.flush(DocumentsWriter.java:539)
    at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:2497)
    at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:2397)
    at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1204)
    at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1178)
    at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1153) 
{code}

The bug is just an over-aggressive assert statement.  I'll commit a fix shortly & port to 2.3 branch for 2.3.1 release."
"LUCENE-447","BUILD_SYSTEM","IMPROVEMENT","Make ""ant -projecthelp"" show the javadocs and docs targets as well","Added a description to the targets ""javadocs"" and ""docs"".
This makes ant show them when the executes ""ant -projecthelp""
"
"LUCENE-220","BUG","BUG","Inconsistent behaviour sorting against field with no related documents","In StringSortedHitQueue - generateSortIndex seems to mistake 
the TermEnum having values as indicating that the sort field 
has entries in the index.

In the case where the search has matching results an ArrayIndexOutOfBounds
exception is thrown in sortValue (line 177 StringSortedHitQueue)
as generateSortIndex creates a terms array of zero length and fieldOrder
contains 0 for all documents.

It would seem more helpful if:
a) generateSortIndex catches the lack of any documents with the sort field.

or

b) reserve terms[0] as a special value for documents that do not have
matching sort field values. ie Change the current implementation to add 1
to the index and change terms[0] to ensure it sorts ""untagged"" documents to
first or last.

For my application Id much prefer solution (b) as it allows much smaller 
indexes and make searching using sort values less brittle.

Thats the best my communication skills can muster just now. Could change
current code to something like:

private final int[] generateSortIndex()
throws IOException {

	final int[] retArray = new int[reader.maxDoc()];
	final String[] mterms = new String[reader.maxDoc() + 1];  // guess length
	if (retArray.length > 0) {
		TermDocs termDocs = reader.termDocs();
		// change this value to control if documents without sort field come first or last
		mterms[0] = """";  // XXXXXXXXX change
		int t = 1;  // current term number  XXXXXXXXXXXXX change
		try {
	

			do {
				Term term = enumerator.term();
				if (term.field() != field) break;

				// store term text
				// we expect that there is at most one term per document
				if (t >= mterms.length) throw new RuntimeException (""there are more terms
than documents in field \""""+field+""\"""");
				mterms[t] = term.text();

				// store which documents use this term
				termDocs.seek (enumerator);
				while (termDocs.next()) {
					retArray[termDocs.doc()] = t;
				}

				t++;
			} while (enumerator.next());
		} finally {
			termDocs.close();
		}

		// if there are less terms than documents,
		// trim off the dead array space
		if (t < mterms.length) {
			terms = new String[t];
			System.arraycopy (mterms, 0, terms, 0, t);
		} else {
			terms = mterms;
		}
	}
	return retArray;
}

Having very quick look at IntegerSortedHitQueue would seem possible
to do same thing. Maybe creating Integer wrapper objects once.

Hope that made some sort of sense. Im not very familiar with the code
or Lucene terminology.
If the above seems like a useful approach Id be glad to generate patches
for a cleaned up version.

Thanks

Sam"
"LUCENE-1064","DESIGN_DEFECT","IMPROVEMENT","Make TopDocs constructor public","TopDocs constructor is package visible. This prevents instantiating it from outside this package. For example, I wrote a HitColletor that couldn't extend directly from TopDocCollector. I need to create a new TopDocs instance, however since the c'tor is package visible, I can't do that.
For now, I completely duplicated the code, but I hope you'll fix it soon."
"LUCENE-2740","BUG","BUG","PerFieldCodecWrapper causes crashes if not all per field codes have been used","If a PerFieldCodecWrapper is used an SegmentMerger tries to merge two segments where one segment only has a subset of the field PerFieldCodecWrapper defines SegmentMerger tries to open non-existing files since Codec#files(Directory, SegmentInfo, Set<String>) blindly copies the expected files into the given set. This also hits exceptions in CheckIndex and addIndexes(). 
The reason for this is that PerFieldCodecWrapper simply iterates over the codecs it knows and adds all files without checking if they are present in the given Directory. We need to have some mechnanism that check if the ""required"" files for a codec are present and only add the files to the set if that field is really there.

"
"LUCENE-801","BUILD_SYSTEM","IMPROVEMENT","build.xml in cnotrib/benchmark should auto build core java and demo if required","Currently one needs to build core jar and demo jar before building/running benchmark.
This is not very convenient. 
Change it to 
- use core classes and demo classes (instead of jars).
- build core and demo by dependency if required."
"LUCENE-215","BUG","BUG","addIndexes unexpectedly closes index","It seems that in 1.4rc2, a call to IndexWriter.addIndexes (IndexReader[]) will
close the provided IndexReader; in 1.3-final this does not happen.  So my code
which uses addIndexes to merge new information into an index and then calls
close() on the IndexReader now crashes with an ""already closed"" exception.  I
can attach test code which works in 1.3 but not in 1.4rc2 if that would be helpful.

If this is an intentional change in behavior, it needs to be documented.  Thanks!"
"LUCENE-2619","TEST","TEST","simple improvements to tests","Simon had requested some docs on what all our test options do, so lets clean it up and doc it.

i propose:
# change all vars to be tests.xxx (e.g. tests.threadspercpu, tests.multiplier, ...)
# ensure all 6 build systems (lucene, solr, each solr contrib) respect these.
# add a simple wiki page listing what these do."
"LUCENE-3726","IMPROVEMENT","IMPROVEMENT","Default KuromojiAnalyzer to use search mode","Kuromoji supports an option to segment text in a way more suitable for search,
by preventing long compound nouns as indexing terms.

In general 'how you segment' can be important depending on the application 
(see http://nlp.stanford.edu/pubs/acl-wmt08-cws.pdf for some studies on this in chinese)

The current algorithm punishes the cost based on some parameters (SEARCH_MODE_PENALTY, SEARCH_MODE_LENGTH, etc)
for long runs of kanji.

Some questions (these can be separate future issues if any useful ideas come out):
* should these parameters continue to be static-final, or configurable?
* should POS also play a role in the algorithm (can/should we refine exactly what we decompound)?
* is the Tokenizer the best place to do this, or should we do it in a tokenfilter? or both?
  with a tokenfilter, one idea would be to also preserve the original indexing term, overlapping it: e.g. ABCD -> AB, CD, ABCD(posInc=0)
  from my understanding this tends to help with noun compounds in other languages, because IDF of the original term boosts 'exact' compound matches.
  but does a tokenfilter provide the segmenter enough 'context' to do this properly?

Either way, I think as a start we should turn on what we have by default: its likely a very easy win.
"
"LUCENE-3602","RFE","RFE","Add join query to Lucene","Solr has (psuedo) join query for a while now. I think this should also be available in Lucene.  "
"LUCENE-2966","BUG","BUG","SegmentReader.doCommit should be sync'd; norms methods need not be sync'd","I fixed the failure in TestNRTThreads, but in the process tripped an assert because SegmentReader.doCommit isn't sync'd.

So I sync'd it, but I don't think the norms APIs need to be sync'd -- we populate norms up front and then never change them.  Un-sync'ing them is important so that in the NRT case calling IW.commit doesn't block searches trying to pull norms.

Also some small code refactoring."
"LUCENE-3127","REFACTORING","IMPROVEMENT","pull CoreReaders out of SegmentReader","Similar to LUCENE-3117, I think we should pull the CoreReaders class out of SR,
to make it easier to navigate the code."
"LUCENE-2102","RFE","IMPROVEMENT","LowerCaseFilter for Turkish language","java.lang.Character.toLowerCase() converts 'I' to 'i' however in Turkish alphabet lowercase of 'I' is not 'i'. It is LATIN SMALL LETTER DOTLESS I.

"
"LUCENE-1566","BUG","BUG","Large Lucene index can hit false OOM due to Sun JRE issue","This is not a Lucene issue, but I want to open this so future google
diggers can more easily find it.

There's this nasty bug in Sun's JRE:

  http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6478546

The gist seems to be, if you try to read a large (eg 200 MB) number of
bytes during a single RandomAccessFile.read call, you can incorrectly
hit OOM.  Lucene does this, with norms, since we read in one byte per
doc per field with norms, as a contiguous array of length maxDoc().

The workaround was a custom patch to do large file reads as several
smaller reads.

Background here:

  http://www.nabble.com/problems-with-large-Lucene-index-td22347854.html
"
"LUCENE-2134","BUILD_SYSTEM","BUG","[PATCH] ant-task ""javadocs-all"" fails with OutOfMemoryError","Hi all,

the current nightly build's ""ant dist"" fails with an OutOfMemoryError at ant task javadocs-all (see below).
Apparently javadoc needs more memory.

A similar case has been reported in HADOOP-5561 (add a maxmemory statement to the javadoc task), and I propose the same change for Lucene as well.

Cheers,
Christian


javadocs-all:
  [javadoc] Generating Javadoc
  [javadoc] Javadoc execution
  [javadoc] Loading source files for package org.apache.lucene...
  [javadoc] Loading source files for package org.apache.lucene.analysis...
(...)
  [javadoc] Loading source files for package org.apache.lucene.queryParser.standard.config...
  [javadoc] Loading source files for package org.apache.lucene.queryParser.standard.nodes...
  [javadoc] Loading source files for package org.apache.lucene.queryParser.standard.parser...
  [javadoc] Loading source files for package org.apache.lucene.queryParser.standard.processors...
  [javadoc] Constructing Javadoc information...
  [javadoc] Standard Doclet version 1.6.0_15
  [javadoc] Building tree for all the packages and classes...

 (takes a long time here until OOME)

  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] 	at java.lang.Throwable.getStackTraceElement(Native Method)
  [javadoc] 	at java.lang.Throwable.getOurStackTrace(Throwable.java:591)
  [javadoc] 	at java.lang.Throwable.printStackTrace(Throwable.java:462)
  [javadoc] 	at java.lang.Throwable.printStackTrace(Throwable.java:451)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:103)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.AbstractMemberBuilder.build(AbstractMemberBuilder.java:56)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildMemberSummary(ClassBuilder.java:279)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  [javadoc] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  [javadoc] 	at java.lang.reflect.Method.invoke(Method.java:597)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.invokeMethod(ClassBuilder.java:101)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:90)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildClassDoc(ClassBuilder.java:124)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  [javadoc] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  [javadoc] 	at java.lang.reflect.Method.invoke(Method.java:597)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.invokeMethod(ClassBuilder.java:101)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:90)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.build(ClassBuilder.java:108)
  [javadoc] 	at com.sun.tools.doclets.formats.html.HtmlDoclet.generateClassFiles(HtmlDoclet.java:155)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.AbstractDoclet.generateClassFiles(AbstractDoclet.java:164)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.AbstractDoclet.startGeneration(AbstractDoclet.java:106)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.AbstractDoclet.start(AbstractDoclet.java:64)
  [javadoc] 	at com.sun.tools.doclets.formats.html.HtmlDoclet.start(HtmlDoclet.java:42)
  [javadoc] 	at com.sun.tools.doclets.standard.Standard.start(Standard.java:23)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  [javadoc] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  [javadoc] 	at java.lang.reflect.Method.invoke(Method.java:597)
  [javadoc] 	at com.sun.tools.javadoc.DocletInvoker.invoke(DocletInvoker.java:269)
  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] 	at java.util.Arrays.copyOfRange(Arrays.java:3209)
  [javadoc] 	at java.lang.String.<init>(String.java:215)
  [javadoc] 	at com.sun.tools.javac.util.Convert.utf2string(Convert.java:131)
  [javadoc] 	at com.sun.tools.javac.util.Name.toString(Name.java:164)
  [javadoc] 	at com.sun.tools.javadoc.ClassDocImpl.getClassName(ClassDocImpl.java:341)
  [javadoc] 	at com.sun.tools.javadoc.TypeMaker.getTypeName(TypeMaker.java:100)
  [javadoc] 	at com.sun.tools.javadoc.ParameterizedTypeImpl.parameterizedTypeToString(ParameterizedTypeImpl.java:117)
  [javadoc] 	at com.sun.tools.javadoc.TypeMaker.getTypeString(TypeMaker.java:121)
  [javadoc] 	at com.sun.tools.javadoc.ExecutableMemberDocImpl.makeSignature(ExecutableMemberDocImpl.java:217)
  [javadoc] 	at com.sun.tools.javadoc.ExecutableMemberDocImpl.signature(ExecutableMemberDocImpl.java:198)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap.getMemberKey(VisibleMemberMap.java:485)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap.access$1000(VisibleMemberMap.java:28)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap$ClassMembers.isOverridden(VisibleMemberMap.java:442)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap$ClassMembers.addMembers(VisibleMemberMap.java:316)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap$ClassMembers.build(VisibleMemberMap.java:278)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap$ClassMembers.access$100(VisibleMemberMap.java:230)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap.<init>(VisibleMemberMap.java:93)
  [javadoc] 	at com.sun.tools.doclets.formats.html.ConstructorWriterImpl.<init>(ConstructorWriterImpl.java:38)
  [javadoc] 	at com.sun.tools.doclets.formats.html.WriterFactoryImpl.getConstructorWriter(WriterFactoryImpl.java:129)
  [javadoc] 	at com.sun.tools.doclets.formats.html.WriterFactoryImpl.getMemberSummaryWriter(WriterFactoryImpl.java:141)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.MemberSummaryBuilder.init(MemberSummaryBuilder.java:104)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.MemberSummaryBuilder.getInstance(MemberSummaryBuilder.java:64)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.BuilderFactory.getMemberSummaryBuilder(BuilderFactory.java:191)
  [javadoc] 	at com.sun.tools.doclets.formats.html.ClassWriterImpl.navSummaryLinks(ClassWriterImpl.java:474)
  [javadoc] 	at com.sun.tools.doclets.formats.html.ClassWriterImpl.printSummaryDetailLinks(ClassWriterImpl.java:456)
  [javadoc] 	at com.sun.tools.doclets.formats.html.HtmlDocletWriter.navLinks(HtmlDocletWriter.java:462)
  [javadoc] 	at com.sun.tools.doclets.formats.html.ClassWriterImpl.writeFooter(ClassWriterImpl.java:146)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildClassFooter(ClassBuilder.java:330)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  [javadoc] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  [javadoc] 	at java.lang.reflect.Method.invoke(Method.java:597)
  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] 	at java.util.LinkedHashMap.createEntry(LinkedHashMap.java:424)
  [javadoc] 	at java.util.LinkedHashMap.addEntry(LinkedHashMap.java:406)
  [javadoc] 	at java.util.HashMap.put(HashMap.java:385)
  [javadoc] 	at sun.util.resources.OpenListResourceBundle.loadLookup(OpenListResourceBundle.java:118)
  [javadoc] 	at sun.util.resources.OpenListResourceBundle.loadLookupTablesIfNecessary(OpenListResourceBundle.java:97)
  [javadoc] 	at sun.util.resources.OpenListResourceBundle.handleGetObject(OpenListResourceBundle.java:58)
  [javadoc] 	at sun.util.resources.TimeZoneNamesBundle.handleGetObject(TimeZoneNamesBundle.java:59)
  [javadoc] 	at java.util.ResourceBundle.getObject(ResourceBundle.java:378)
  [javadoc] 	at java.util.ResourceBundle.getObject(ResourceBundle.java:381)
  [javadoc] 	at java.util.ResourceBundle.getStringArray(ResourceBundle.java:361)
  [javadoc] 	at sun.util.TimeZoneNameUtility.retrieveDisplayNames(TimeZoneNameUtility.java:100)
  [javadoc] 	at sun.util.TimeZoneNameUtility.retrieveDisplayNames(TimeZoneNameUtility.java:81)
  [javadoc] 	at java.util.TimeZone.getDisplayNames(TimeZone.java:399)
  [javadoc] 	at java.util.TimeZone.getDisplayName(TimeZone.java:350)
  [javadoc] 	at java.util.Date.toString(Date.java:1025)
  [javadoc] 	at com.sun.tools.doclets.formats.html.markup.HtmlDocWriter.today(HtmlDocWriter.java:337)
  [javadoc] 	at com.sun.tools.doclets.formats.html.HtmlDocletWriter.printHtmlHeader(HtmlDocletWriter.java:281)
  [javadoc] 	at com.sun.tools.doclets.formats.html.ClassWriterImpl.writeHeader(ClassWriterImpl.java:122)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildClassHeader(ClassBuilder.java:164)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  [javadoc] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  [javadoc] 	at java.lang.reflect.Method.invoke(Method.java:597)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.invokeMethod(ClassBuilder.java:101)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:90)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildClassDoc(ClassBuilder.java:124)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  [javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  [javadoc] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  [javadoc] 	at java.lang.reflect.Method.invoke(Method.java:597)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.invokeMethod(ClassBuilder.java:101)
  [javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:90)
  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] java.lang.OutOfMemoryError: Java heap space
  [javadoc] java.lang.OutOfMemoryError: Java heap space
"
"LUCENE-959","IMPROVEMENT","IMPROVEMENT","Document Vector->ArrayList","Document Vector should be changed to ArrayList.
Document is not advertised to be thread safe, and it's doubtful that anyone modifies a Document from multiple threads."
"LUCENE-2377","BUG","IMPROVEMENT","Enable the use of NoMergePolicy and NoMergeScheduler by Benchmark","Benchmark allows one to set the MP and MS to use, by defining the class name and then use reflection to instantiate them. However NoMP and NoMS are singletons and therefore reflection does not work for them. Easy fix in CreateIndexTask. I'll post a patch soon."
"LUCENE-1394","DOCUMENTATION","BUG","Include all CHANGES.txt under contrib","We already include to root CHANGES.txt but fail to include the ones under contrib."
"LUCENE-1031","DOCUMENTATION","IMPROVEMENT","Fixes a handful of misspellings/mistakes in changes.txt","There are a handful of misspellings/mistakes in changes.txt. This patch fixes them. Avoided the one or two British to English conversions <g>"
"LUCENE-3031","BUG","BUG","setFlushPending fails if we concurrently hit a aborting exception","If we select a DWPT for flushing but that DWPT is currently in flight and hits an exception after we selected them for flushing the num of docs is reset to 0 and we trip that exception. So we rather check if it is > 0 than assert on it here.
{noformat}
[junit] Testsuite: org.apache.lucene.index.TestIndexWriterExceptions
    [junit] Testcase: testRandomExceptionsThreads(org.apache.lucene.index.TestIndexWriterExceptions):	FAILED
    [junit] thread Indexer 3: hit unexpected failure
    [junit] junit.framework.AssertionFailedError: thread Indexer 3: hit unexpected failure
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:227)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1226)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1154)
    [junit] 
    [junit] 
    [junit] Tests run: 18, Failures: 1, Errors: 0, Time elapsed: 30.287 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] Indexer 3: unexpected exception2
    [junit] java.lang.AssertionError
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushControl.setFlushPending(DocumentsWriterFlushControl.java:170)
    [junit] 	at org.apache.lucene.index.FlushPolicy.markLargestWriterPending(FlushPolicy.java:108)
    [junit] 	at org.apache.lucene.index.FlushByRamOrCountsPolicy.onInsert(FlushByRamOrCountsPolicy.java:61)
    [junit] 	at org.apache.lucene.index.FlushPolicy.onUpdate(FlushPolicy.java:77)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushControl.doAfterDocument(DocumentsWriterFlushControl.java:115)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:341)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1367)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1339)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:92)
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testRandomExceptionsThreads -Dtests.seed=3493970007652348212:2010109588873167237
    [junit] WARNING: test method: 'testRandomExceptionsThreads' left thread running: merge thread: _1v(4.0):Cv2 _27(4.0):cv1 into _2h
    [junit] WARNING: test method: 'testRandomExceptionsThreads' left thread running: merge thread: _2c(4.0):cv1 into _2m
    [junit] RESOURCE LEAK: test method: 'testRandomExceptionsThreads' left 2 thread(s) running
    [junit] NOTE: test params are: codec=RandomCodecProvider: {content=MockFixedIntBlock(blockSize=421), field=MockSep, id=SimpleText, other=MockSep, contents=MockRandom, content1=Pulsing(freqCutoff=11), content2=MockSep, content4=SimpleText, content5=SimpleText, content6=MockRandom, crash=MockRandom, content7=MockVariableIntBlock(baseBlockSize=109)}, locale=mk_MK, timezone=Europe/Malta
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestToken, TestDateTools, Test2BTerms, TestAddIndexes, TestFilterIndexReader, TestIndexWriterExceptions]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=78897400,total=195821568
{noformat}"
"LUCENE-2918","IMPROVEMENT","IMPROVEMENT","IndexWriter should prune 100% deleted segs even in the NRT case","We now prune 100% deleted segs on commit from IW or IR (LUCENE-2010),
but this isn't quite aggressive enough, because in the NRT case you
rarely call commit.

Instead, the moment we delete the last doc of a segment, it should be
pruned from the in-memory segmentInfos.  This way, if you open an NRT
reader, or a merge kicks off, or commit is called, the 100% deleted
segment is already gone.
"
"LUCENE-1564","BUG","BUG","Field.setValue(...) doesn't properly handle switching between byte[] and other types","This came up in PyLucene testing, based on Lucene 2.4.1.  Thread here:

  http://pylucene.markmail.org/message/75jzxzqi3smp2s4z

The problem is that Field.setValue does not fix up the isBinary
boolean, so if you create a String field, and then do
setValue(byte[]), you'll get an exception when adding a document
containing that field to the index."
"LUCENE-3382","BUG","BUG","fix compound-file/NoSuchDirectoryException bugs in NRTCachingDir","found some bugs over on LUCENE-3374, but we should fix these separately from whether or not we move it to core,
and the bugs apply to 3.x too.

here we can just add explicit tests for the problems."
"LUCENE-3701","RFE","IMPROVEMENT","FST apis out of sync between trunk/3.x","Looks like the offender is LUCENE-3030 :)

Not sure if everything is generally useful but it does change the public API (e.g. you can specify FreezeTail to the super-scary Builder ctor among other things).

Maybe we should sync up for 3.x? "
"LUCENE-1134","BUG","BUG","BooleanQuery.rewrite does not work properly for minNumberShouldMatch","BooleanQuery.rewrite does not respect minNumberShouldMatch if the number of clauses is 1. This causes inconsistencies for the queries ""+def"" and ""+abc +def"", while setting the minNumShouldMatch to '1' for both.
For the first query, results are returned although there are no SHOULD clauses in the query.
For the second query no results are returned.
The reason lies in the optimization BooleanQuery.rewrite has for one clauses queries.
Patch included - optimize the query for a single clause only if the minNumShouldMatch <= 0."
"LUCENE-925","DOCUMENTATION","","Analysis Package Level Javadocs","Analysis package level javadocs need improving.  An overview of what an Analyzer does, and maybe some sample code showing how to write you own Analyzer, Tokenizer and TokenFilter would be really helpful.  Bonus would be some discussion on best practices for achieving performance during analysis. "
"LUCENE-1196","BUG","BUG","RAMDirectory reports incorrect EOF on seek","If you create a file whose length is a multiple of 1024 (BUFFER_SIZE),
and then try to seek to the very end of the file, you hit
EOFException.

But this is actually ""legal"" as long as you don't try to read any
bytes at that point.

I'm hitting this (rarely) with the bulk-merging logic for term vectors
(LUCENE-1120), which can seek to the very end of the file but not read
any bytes if conditions are right.

"
"LUCENE-2488","BUILD_SYSTEM","BUG","Fix 2.9 contrib builds to succeed when JDK 1.4 is used (leaving out contribs that require 1.5)","When you build and test Lucene 2.9 with Java 1.4, building and testing of contrib fails. This patch fixes this to repect the current compiler version and disables all contribs that need Java 1.5 by checking their javac.source property.

This patch can be ported to 3.x or trunk, when 1.6 contribs will appear."
"LUCENE-1087","BUG","BUG","MultiSearcher.explain returns incorrect score/explanation relating to docFreq","Creating 2 different indexes, searching  each individually and print score details and compare to searching both indexes with MulitSearcher and printing score details.  
 
The ""docFreq"" value printed isn't correct - the values it prints are as if each index was searched individually.

Code is like:
{code}
MultiSearcher multi = new MultiSearcher(searchables);
Hits hits = multi.search(query);
for(int i=0; i<hits.length(); i++)
{
  Explanation expl = multi.explain(query, hits.id(i));
  System.out.println(expl.toString());
}
{code}

I raised this in the Lucene user mailing list and was advised to log a bug, email thread given below.

{noformat} 
-----Original Message-----
From: Chris Hostetter  
Sent: Friday, December 07, 2007 10:30 PM
To: java-user
Subject: Re: does the MultiSearcher class calculate IDF properly?


a quick glance at the code seems to indicate that MultiSearcher has code 
for calcuating the docFreq accross all of the Searchables when searching 
(or when the docFreq method is explicitly called) but that explain method 
just delegates to Searchable that the specific docid came from.

if you compare that Explanation score you got with the score returned by 
a HitCollector (or TopDocs) they probably won't match.

So i would say ""yes MultiSearcher calculates IDF properly, but 
MultiSeracher.explain is broken.  Please file a bug about this, i can't 
think of an easy way to fix it, but it certianly seems broken to me.


: Subject: does the MultiSearcher class calculate IDF properly?
: 
: I tried the following.  Creating 2 different indexes, search each
: individually and print score details and compare to searching both
: indexes with MulitSearcher and printing score details.  
: 
: The ""docFreq"" value printed don't seem right - is this just a problem
: with using Explain together with the MultiSearcher?
: 
: 
: Code is like:
: MultiSearcher multi = new MultiSearcher(searchables);
: Hits hits = multi.search(query);
: for(int i=0; i<hits.length(); i++)
: {
:   Explanation expl = multi.explain(query, hits.id(i));
:   System.out.println(expl.toString());
: }
: 
: 
: Output:
: id = 14 score = 0.071
: 0.07073946 = (MATCH) fieldWeight(contents:climate in 2), product of:
:   1.0 = tf(termFreq(contents:climate)=1)
:   1.8109303 = idf(docFreq=1)
:   0.0390625 = fieldNorm(field=contents, doc=2)
{noformat} "
"LUCENE-404","BUG","BUG","Crash when querying an index using multiple term positions.","file: MultipleTermPositions.java, line: 201, function: skipTo(int).

This refers to the source that can currently be downloaded from the lucene site,
Lucene v. 1.4.3.

The function peek() returns null (because top() also retruned null). There is no
check for this, as far as I can understand. The function doc() is called on a
null-object, which results in a NullPointerException.

I switched the specified line to this one:

while(_termPositionsQueue.peek() != null && target >
_termPositionsQueue.peek().doc())

This got rid of the crash for me."
"LUCENE-1696","RFE","IMPROVEMENT","Added New Token API impl for ASCIIFoldingFilter","I added an implementation of incrementToken to ASCIIFoldingFilter.java and extended the existing  testcase for it.
I will attach the patch shortly.
Beside this improvement I would like to start up a small discussion about this filter. ASCIIFoldingFitler is meant to be a replacement for ISOLatin1AccentFilter which is quite nice as it covers a superset of the latter. I have used this filter quite often but never on a as it is basis. In the most cases this filter does the correct thing (replace a special char with its ascii correspondent) but in some cases like for German umlaut it does not return the expected result. A german umlaut  like '' does not translate to a but rather to 'ae'. I would like to change this but I'n not 100% sure if that is expected by all users of that filter. Another way of doing it would be to make it configurable with a flag. This would not affect performance as we only check if such a umlaut char is found. 
Further it would be really helpful if that filter could ""inject"" the original/unmodified token with the same position increment into the token stream on demand. I think its a valid use-case to index the modified and unmodified token. For instance, the german word ""sd"" would be folded to ""sud"". In a query q:(sd) the filter would also fold to sud and therefore find sud which has a totally different meaning. Folding works quite well but for special cases would could add those options to make users life easier. The latter could be done in a subclass while the umlaut problem should be fixed in the base class.

simon "
"LUCENE-1858","DOCUMENTATION","TASK","Update site level documentation","ugg - a fun one - my brain is sliding to the bottom of my skull from excitement.

Must update all of the site level pages to current API usage."
"LUCENE-1331","BUG","BUG","FSDirectory doesn't detect double-close nor usage after close","FSDirectory.close implements logic to ensure only a single instance of FSDirectory per canonical directory exists.  This means code that synchronizes on the FSDirectory instance is also synchronized against that canonical directory.  I think only IndexModifier (now deprecated) actually makes use of this, but I'm not certain. 

But, the close() method doesn't detect double close, and doesn't catch usage after being closed, and so one can easily get two instances of FSDirectory for the same canonical directory."
"LUCENE-2761","RFE","IMPROVEMENT","specialize payload processing from of DocsAndPositionsEnum","In LUCENE-2760 i started working to try to improve the speed of a few spanqueries.
In general the trick there is to avoid processing positions if you dont have to.

But, we can improve queries that read lots of positions further by cleaning up SegmentDocsAndPositionsEnum, 
in nextPosition() this has no less than 3 payloads-related checks.

however, a large majority of users/fields have no payloads at all.
I think we should specialize this case into a separate implementation and speed up the common case.

edit: dyslexia with the jira issue number."
"LUCENE-2029","RFE","IMPROVEMENT","Allow separate control over whether body is stored or analyzed","Simple enhancement to DocMaker."
"LUCENE-1208","BUG","BUG","Deadlock case in IndexWriter on exception just before flush","If a document hits a non-aborting exception, eg something goes wrong
in tokenStream.next(), and, that document had triggered a flush
(due to RAM or doc count) then DocumentsWriter will deadlock because
that thread marks the flush as pending but fails to clear it on
exception.

I have a simple test case showing this, and a fix fixing it."
"LUCENE-1549","IMPROVEMENT","IMPROVEMENT","Strengthen CheckIndex a bit","A few small improvements to CheckIndex to detect possible ""docs out of order"" cases."
"LUCENE-2299","BUG","BUG","if you open an NRT reader while addIndexes* is running it may miss segments","Earwin spotted this in pending ongoing refactoring of Dir/MultiReader, but I wanted to open this separately just to make sure we fix it for 3.1...

This is the fix:
{code}
Index: src/java/org/apache/lucene/index/DirectoryReader.java
===================================================================
--- src/java/org/apache/lucene/index/DirectoryReader.java	(revision 919119)
+++ src/java/org/apache/lucene/index/DirectoryReader.java	(working copy)
@@ -145,7 +145,7 @@
     for (int i=0;i<numSegments;i++) {
       boolean success = false;
       try {
-        final SegmentInfo info = infos.info(upto);
+        final SegmentInfo info = infos.info(i);
         if (info.dir == dir) {
           readers[upto++] = writer.readerPool.getReadOnlyClone(info, true, termInfosIndexDivisor);
         }
{code}

"
"LUCENE-1209","RFE","IMPROVEMENT","If setConfig(Config config) is called in resetInputs(), you can turn term vectors off and on by round","I want to be able to run one benchmark that tests things using term vectors and not using term vectors.

Currently this is not easy because you cannot specify term vectors per round.

While you do have to create a new index per round, this automation is preferable to me in comparison to running two separate tests.

If it doesn't affect anything else, it would be great to have setConfig(Config config) called in BasicDocMaker.resetInputs(). This would keep the term vector options up to date per round if you reset.

- Mark"
"LUCENE-2294","RFE","IMPROVEMENT","Create IndexWriterConfiguration and store all of IW configuration there","I would like to factor out of all IW configuration parameters into a single configuration class, which I propose to name IndexWriterConfiguration (or IndexWriterConfig). I want to store there almost everything besides the Directory, and to reduce all the ctors down to one: IndexWriter(Directory, IndexWriterConfiguration). What I was thinking of storing there are the following parameters:
* All of ctors parameters, except for Directory.
* The different setters where it makes sense. For example I still think infoStream should be set on IW directly.

I'm thinking that IWC should expose everything in a setter/getter methods, and defaults to whatever IW defaults today. Except for Analyzer which will need to be defined in the ctor of IWC and won't have a setter.

I am not sure why MaxFieldLength is required in all IW ctors, yet IW declares a DEFAULT (which is an int and not MaxFieldLength). Do we still think that 10000 should be the default? Why not default to UNLIMITED and otherwise let the application decide what LIMITED means for it? I would like to make MFL optional on IWC and default to something, and I hope that default will be UNLIMITED. We can document that on IWC, so that if anyone chooses to move to the new API, he should be aware of that ...

I plan to deprecate all the ctors and getters/setters and replace them by:
* One ctor as described above
* getIndexWriterConfiguration, or simply getConfig, which can then be queried for the setting of interest.
* About the setters, I think maybe we can just introduce a setConfig method which will override everything that is overridable today, except for Analyzer. So someone could do iw.getConfig().setSomething(); iw.setConfig(newConfig);
** The setters on IWC can return an IWC to allow chaining set calls ... so the above will turn into iw.setConfig(iw.getConfig().setSomething1().setSomething2()); 

BTW, this is needed for Parallel Indexing (see LUCENE-1879), but I think it will greatly simplify IW's API.

I'll start to work on a patch."
"LUCENE-672","RFE","RFE","new merge policy","New merge policy developed in the course of 
http://issues.apache.org/jira/browse/LUCENE-565
http://issues.apache.org/jira/secure/attachment/12340475/newMergePolicy.Sept08.patch"
"LUCENE-2244","IMPROVEMENT","BUG","Improve StandardTokenizer's understanding of non ASCII punctuation and quotes","In the vein of LUCENE-1126 and LUCENE-1390, StandardTokenizerImpl.jflex should do a better job at understanding non-ASCII punctuation characters.

For example, its understanding of the single-quote character ""'"" is currently limited to that character only. It will set a token's type to APOSTROPHE only if the ""'"" was used.
In the patch attached, I added all the characters that ASCIIFoldingFilter would change into ""'"".

I'm not sure that this is the right approach so I didn't write a complete patch for all the other hardcoded characters used in jflex rules such as ""."", ""-"" which have some variants in ASCIIFoldingFilter that could be used as well.

Maybe a better approach would be to make it possible to have an ASCIIFoldingFilter-like reader as a character filter that could be in inserted in front of StandardTokenizer ?"
"LUCENE-3016","RFE","RFE","Analyzer for Latvian","Less aggressive form of Kreslins' phd thesis: A stemming algorithm for Latvian.
"
"LUCENE-2781","CLEANUP","TASK","Drop deprecations from trunk","subj.
Also, to each remaining deprecation add release version when it first appeared.

Patch incoming."
"LUCENE-2110","REFACTORING","IMPROVEMENT","Refactoring of FilteredTermsEnum and MultiTermQuery","FilteredTermsEnum is confusing as it is initially positioned to the first term. It should instead work like an uninitialized TermsEnum for a field before the first call to next() or seek().
FilteredTermsEnums cannot implement seek() as eg. NRQ or Automaton are not able to support this. Seeking is also not needed for MTQ at all, so seek can just throw UOE.
This issue changes some of the internal behaviour of MTQ and FilteredTermsEnum to allow also seeking in NRQ and Automaton (see comments below)."
"LUCENE-2473","DOCUMENTATION","BUG","Clicking on the ""More Results"" link in luceneweb.war demo results in ArrayIndexOutOfBoundsException","Summary says it all."
"LUCENE-2513","BUG","BUG","IndexReader overwrites future commits when you open it on a past commit","Hit this on trying to build up a test index for perf testing...

IndexReader (and Writer) accept an IndexCommit on open.

This is quite powerful, because, if you use a deletion policy that keeps multiple commits around, you can open a not-current commit, make some changes, write a new commit, all without altering the ""future"" commits.

I use this to first build up a big wikipedia index, including one commit w/ multiple segments, then another commit after optimize(), and then I open an writable IR to perform deletions off of both those commits.  This gives me a single test index that has all four combinations (single vs multi segment; deletions vs no deletions).

But IndexReader has a bug whereby it overwrites the segments_N file.  (IndexWriter works correctly)."
"LUCENE-1682","TEST","IMPROVEMENT","unit tests should use private directories","This only affects our unit tests...

I run ""ant test"" and ""ant test-tag"" concurrently, but some tests have false failures (eg TestPayloads) because they use a fixed test directory in the filesystem for testing.

I've added a simple method to _TestUtil to get a temp dir, and switched over those tests that I've hit false failures on."
"LUCENE-869","REFACTORING","IMPROVEMENT","Make FSIndexInput and FSIndexOutput inner classes of FSDirectory","I would like make FSIndexInput and FSIndexOutput protected, static, inner classes of FSDirectory. Currently these classes are located in the same source file as FSDirectory, which means that classes outside the store package can not extend them.

I don't see any performance impacts or other side effects of this trivial patch. All unit tests pass."
"LUCENE-1374","BUG","BUG","Merging of compressed string Fields may hit NPE","This bug was introduced with LUCENE-1219 (only present on 2.4).

The bug happens when merging compressed string fields, but only if bulk-merging code does not apply because the FieldInfos for the segment being merged are not congruent.  This test shows the bug:

{code}
  public void testMergeCompressedFields() throws IOException {
    File indexDir = new File(System.getProperty(""tempDir""), ""mergecompressedfields"");
    Directory dir = FSDirectory.getDirectory(indexDir);
    try {
      for(int i=0;i<5;i++) {
        // Must make a new writer & doc each time, w/
        // different fields, so bulk merge of stored fields
        // cannot run:
        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), i==0, IndexWriter.MaxFieldLength.UNLIMITED);
        w.setMergeFactor(5);
        w.setMergeScheduler(new SerialMergeScheduler());
        Document doc = new Document();
        doc.add(new Field(""test1"", ""this is some data that will be compressed this this this"", Field.Store.COMPRESS, Field.Index.NO));
        doc.add(new Field(""test2"", new byte[20], Field.Store.COMPRESS));
        doc.add(new Field(""field"" + i, ""random field"", Field.Store.NO, Field.Index.TOKENIZED));
        w.addDocument(doc);
        w.close();
      }

      byte[] cmp = new byte[20];

      IndexReader r = IndexReader.open(dir);
      for(int i=0;i<5;i++) {
        Document doc = r.document(i);
        assertEquals(""this is some data that will be compressed this this this"", doc.getField(""test1"").stringValue());
        byte[] b = doc.getField(""test2"").binaryValue();
        assertTrue(Arrays.equals(b, cmp));
      }
    } finally {
      dir.close();
      _TestUtil.rmDir(indexDir);
    }
  }
{code}

It's because in FieldsReader, when we load a field ""for merge"" we create a FieldForMerge instance which subsequently does not return the right values for getBinary{Value,Length,Offset}."
"LUCENE-3176","BUG","BUG","TestNRTThreads test failure","hit a fail in TestNRTThreads running tests over and over:
"
"LUCENE-1384","BUG","BUG","addIndexesNoOptimize intermittantly throws incorrect ""segment exists in external directory..."" exception","Spinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200809.mbox/%3Cba72f77f0809111418l29cf215dnd45bf679832d7d42%40mail.gmail.com%3E

Here's my response on that thread:

The bug only happens when you call addIndexesNoOptimize, and one
simple workaround would be to use SerialMergeScheduler.

I think this is already fixed in trunk (soonish to be 2.4) as a side
effect of https://issues.apache.org/jira/browse/LUCENE-1335.

In 2.3, merges that involve external segments (which are segments
folded in by addIndexesNoOptimize) are not supposed to run in a BG
thread.  This is to prevent addIndexesNoOptimize from returning until
after all external segments have been carried over (merged or copied)
into the index, so that if there is an exception (eg disk full),
addIndexesNoOptimize is able to rollback to the index to the starting
point.

The primary merge() method of CMS indeed does not BG any external
merges, but the bug is that when a BG merge finishes it then selects a
new merge to kick off and that selection is happy to pick an external
segment."
"LUCENE-1790","RFE","RFE","Add Boosting Function Term Query and Some Payload Query refactorings","Similar to the BoostingTermQuery, the BoostingFunctionTermQuery is a SpanTermQuery, but the difference is the payload score for a doc is not the average of all the payloads, but applies a function to them instead.  BoostingTermQuery becomes a BoostingFunctionTermQuery with an AveragePayloadFunction applied to it.

Also add marker interface to indicate PayloadQuery types.  Refactor Similarity.scorePayload to also take in the doc id."
"LUCENE-1611","BUG","BUG","Do not launch new merges if IndexWriter has hit OOME","if IndexWriter has hit OOME, it defends itself by refusing to commit changes to the index, including merges.  But this can lead to infinite merge attempts because we fail to prevent starting a merge.

Spinoff from http://www.nabble.com/semi-infinite-loop-during-merging-td23036156.html."
"LUCENE-3273","TEST","","Convert Lucene Core tests over to a simple MockQueryParser","Most tests use Lucene Core's QueryParser for convenience.  We want to consolidate it into a QP module which we can't have as a dependency.  We should add a simple MockQueryParser which does String.split() on the query string, analyzers the terms and builds a BooleanQuery if necessary.  Any more complex Queries (such as phrases) should be done programmatically. "
"LUCENE-1382","RFE","IMPROVEMENT","Allow storing user data when IndexWriter.commit() is called","Spinoff from here:

    http://www.mail-archive.com/java-user@lucene.apache.org/msg22303.html

The idea is to allow optionally passing an opaque String commitUserData to the IndexWriter.commit method.  This String would be stored in the segments_N file, and would be retrievable by an IndexReader.  Applications could then use this to assign meaning to each commit.

It would be nice to get this done for 2.4, but I don't think we should hold the release for it."
"LUCENE-920","DOCUMENTATION","","IndexModifier has incomplete Javadocs","A lot of public and protected members of org.apache.lucene.index.IndexModifier 
don't have javadocs."
"LUCENE-3043","BUG","BUG","o.a.l.analysis.de.GermanStemmer crashes on some inputs","See the tests from LUCENE-2560. 

GermanAnalyzer no longer uses this stemmer by default, but we should fix it."
"LUCENE-3502","CLEANUP","IMPROVEMENT","Packed ints: move .getArray into Reader API","This is a simple code cleanup... it's messy that a consumer of
PackedInts.Reader must check whether the impl is Direct8/16/32/64 in
order to get an array; it's better to move up the .getArray into the
Reader interface and then make the DirectN impls package private.
"
"LUCENE-3517","IMPROVEMENT","IMPROVEMENT","Fix pulsingcodec to reuse its enums","PulsingCodec currently doesnt always reuse its enums, which could lead to behavior like LUCENE-3515.

The problem is sometimes it returns the 'wrapped' enum, but other times it returns its 'pulsingenum' depending upon
whether terms are pulsed...

we can use the fact that these enums allow attributes to keep the reuse information for both so it can reuse when stepping through terms.
"
"LUCENE-2164","IMPROVEMENT","IMPROVEMENT","Make CMS smarter about thread priorities","Spinoff from LUCENE-2161...

The hard throttling CMS does (blocking the incoming thread that wants
to launch a new merge) can be devastating when it strikes during NRT
reopen.

It can easily happen if a huge merge is off and running, but then a
tiny merge is needed to clean up recently created segments due to
frequent reopens.

I think a small change to CMS, whereby it assigns a higher thread
priority to tiny merges than big merges, should allow us to increase
the max merge thread count again, and greatly reduce the chance that
NRT's reopen would hit this.
"
"LUCENE-1854","BUILD_SYSTEM","IMPROVEMENT","build.xml's tar task should use longfile=""gnu""","The default (used now) is the same, but we get all those nasty false warnings filling the screen."
"LUCENE-678","RFE","RFE","[PATCH] LockFactory implementation based on OS native locks (java.nio.*)","The current default locking for FSDirectory is SimpleFSLockFactory.
It uses java.io.File.createNewFile for its locking, which has this
spooky warning in Sun's javadocs:

    Note: this method should not be used for file-locking, as the
    resulting protocol cannot be made to work reliably. The FileLock
    facility should be used instead.

So, this patch provides a LockFactory implementation based on FileLock
(using java.nio.*).

All unit tests pass with this patch, on OS X (10.4.8), Linux (Ubuntu
6.06), and Windows XP SP2.

Another benefit of native locks is the OS automatically frees them if
the JVM exits before Lucene can free its locks.  Many people seem to
hit this (old lock files still on disk) now.

I've created this new class:

  org.apache.lucene.store.NativeFSLockFactory

and added a couple test cases to the existing TestLockFactory.

I've left SimpleFSLockFactory as the default locking for FSDirectory
for now.  I think we should get some usage / experience with
NativeFSLockFactory and then later on make it the default locking
implementation?

I also tested changing FSDirectory's default locking to
NativeFSLockFactory and all unit tests still pass (on the above
platforms).

One important note about locking over NFS: some NFS servers and/or
clients do not support it, or, it's a configuration option or mode
that must be explicitly enabled.  When it's misconfigured it's able to
take a long time (35 seconds in my case) before throwing an exception.
To handle this, I acquire & release a random test lock on creating the
NativeFSLockFactory to verify locking is configured properly.

A few other small changes in the patch:

    - Added a ""failure reason"" to Lock.java so that in
      obtain(lockWaitTimeout), if there is a persistent IOException
      in trying to obtain the lock, this can be messaged & included in
      the ""Lock obtain timed out"" that's raised.

    - Corrected javadoc in SimpleFSLockFactory: it previously said the
      wrong system property for overriding lock class via system
      properties

    - Fixed unhandled IOException when opening an IndexWriter for
      create, if the locks dir does not exist (just added
      lockDir.exists() check in clearAllLocks method of
      SimpleFSLockFactory & NativeFSLockFactory.

    - Fixed a few small unrelated issues with TestLockFactory, and
      also fixed tests to accept NativeFSLockFactory as the default
      locking implementation for FSDirectory.

    - Fixed a typo in javadoc in FieldsReader.java

    - Added some more javadoc for the LockFactory.setLockPrefix
"
"LUCENE-2836","RFE","RFE","FieldCache rewrite method for MultiTermQueries","For some MultiTermQueries, like RangeQuery we have a FieldCacheRangeFilter etc (in this case its particularly optimized).

But in the general case, since LUCENE-2784 we can now have a rewrite method to rewrite any MultiTermQuery 
using the FieldCache, because MultiTermQuery's getEnum no longer takes IndexReader but Terms, and all the 
FilteredTermsEnums are now just real TermsEnum decorators.

In cases like low frequency queries this is actually slower (I think this has been shown for numeric ranges before too),
but for the really high-frequency cases like especially ugly wildcards, regexes, fuzzies, etc, this can be several times faster 
using the FieldCache instead, since all the terms are in RAM and automaton can blast through them quicker.
"
"LUCENE-2904","BUG","BUG","non-contiguous LogMergePolicy should be careful to not select merges already running","Now that LogMP can do non-contiguous merges, the fact that it disregards which segments are already being merged is more problematic since it could result in it returning conflicting merges and thus failing to run multiple merges concurrently.
"
"LUCENE-3358","BUG","BUG","StandardTokenizer disposes of Hiragana combining mark dakuten instead of attaching it to the character it belongs to","Lucene 3.3 (possibly 3.1 onwards) exhibits less than great behaviour for tokenising hiragana, if combining marks are in use.

Here's a unit test:

{code}
    @Test
    public void testHiraganaWithCombiningMarkDakuten() throws Exception
    {
        // Hiragana 'S' following by the combining mark dakuten
        TokenStream stream = new StandardTokenizer(Version.LUCENE_33, new StringReader(""\u3055\u3099""));

        // Should be kept together.
        List<String> expectedTokens = Arrays.asList(""\u3055\u3099"");
        List<String> actualTokens = new LinkedList<String>();
        CharTermAttribute term = stream.addAttribute(CharTermAttribute.class);
        while (stream.incrementToken())
        {
            actualTokens.add(term.toString());
        }

        assertEquals(""Wrong tokens"", expectedTokens, actualTokens);

    }
{code}

This code fails with:
{noformat}
java.lang.AssertionError: Wrong tokens expected:<[]> but was:<[]>
{noformat}

It seems as if the tokeniser is throwing away the combining mark entirely.

3.0's behaviour was also undesirable:
{noformat}
java.lang.AssertionError: Wrong tokens expected:<[]> but was:<[, ]>
{noformat}

But at least the token was there, so it was possible to write a filter to work around the issue.

Katakana seems to be avoiding this particular problem, because all katakana and combining marks found in a single run seem to be lumped into a single token (this is a problem in its own right, but I'm not sure if it's really a bug.)
"
"LUCENE-3207","BUG","BUG","CustomScoreQuery calls weight() where it should call createWeight()","Thanks to Uwe for helping me track down this bug after I pulled my hair out for hours on LUCENE-3174."
"LUCENE-2584","BUG","BUG","Concurrency issues in SegmentInfo.files() could lead to ConcurrentModificationException","The multi-threaded call of the files() in SegmentInfo could lead to the ConcurrentModificationException if one thread is not finished additions to the ArrayList (files) yet while the other thread already obtained it as cached (see below). This is a rare exception, but it would be nice to fix. I see the code is no longer problematic in the trunk (and others ported from flex_1458), looks it was fixed while implementing post 3.x features. The fix to 3.x and 2.9.x branches could be the same - create the files set first and populate it, and then assign to the member variable at the end of the method. This will resolve the issue. I could prepare the patch for 2.9.4 and 3.x, if needed.

--

INFO: [19] webapp= path=/replication params={command=fetchindex&wt=javabin} status=0 QTime=1
Jul 30, 2010 9:13:05 AM org.apache.solr.core.SolrCore execute
INFO: [19] webapp= path=/replication params={command=details&wt=javabin} status=0 QTime=24
Jul 30, 2010 9:13:05 AM org.apache.solr.handler.ReplicationHandler doFetch
SEVERE: SnapPull failed
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at java.util.AbstractCollection.addAll(AbstractCollection.java:305)
        at org.apache.lucene.index.SegmentInfos.files(SegmentInfos.java:826)
        at org.apache.lucene.index.DirectoryReader$ReaderCommit.<init>(DirectoryReader.java:916)
        at org.apache.lucene.index.DirectoryReader.getIndexCommit(DirectoryReader.java:856)
        at org.apache.solr.search.SolrIndexReader.getIndexCommit(SolrIndexReader.java:454)
        at org.apache.solr.handler.SnapPuller.fetchLatestIndex(SnapPuller.java:261)
        at org.apache.solr.handler.ReplicationHandler.doFetch(ReplicationHandler.java:264)
        at org.apache.solr.handler.ReplicationHandler$1.run(ReplicationHandler.java:146)
"
"LUCENE-3767","REFACTORING","IMPROVEMENT","Explore streaming Viterbi search in Kuromoji","I've been playing with the idea of changing the Kuromoji viterbi
search to be 2 passes (intersect, backtrace) instead of 4 passes
(break into sentences, intersect, score, backtrace)... this is very
much a work in progress, so I'm just getting my current state up.
It's got tons of nocommits, doesn't properly handle the user dict nor
extended modes yet, etc.

One thing I'm playing with is to add a double backtrace for the long
compound tokens, ie, instead of penalizing these tokens so that
shorter tokens are picked, leave the scores unchanged but on backtrace
take that penalty and use it as a threshold for a 2nd best
segmentation...
"
"LUCENE-1828","BUG","BUG","MemoryIndex doesn't call TokenStream.reset() and TokenStream.end()","MemoryIndex from contrib/memory does not honor the contract for a consumer of a TokenStream

will work up a patch right quick"
"LUCENE-432","REFACTORING","BUG","Make FieldSortedHitQueue public","Currently, those who utilize the ""advanced"" search API cannot sort results using
the handy FieldSortedHitQueue. I suggest making this class public to facilitate
this use, as I can't think of a reason not to."
"LUCENE-1267","RFE","IMPROVEMENT","add numDocs() and maxDoc() methods to IndexWriter; deprecate docCount()","Spinoff from here:

  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200804.mbox/%3c405706.11550.qm@web65411.mail.ac4.yahoo.com%3e

I think we should add maxDoc() and numDocs() methods to IndexWriter,
and deprecate docCount() in favor of maxDoc().  To do this I think we
should cache the deletion count of each segment in the segments file.

"
"LUCENE-3396","RFE","IMPROVEMENT","Make TokenStream Reuse Mandatory for Analyzers","In LUCENE-2309 it became clear that we'd benefit a lot from Analyzer having to return reusable TokenStreams.  This is a big chunk of work, but its time to bite the bullet.

I plan to attack this in the following way:

- Collapse the logic of ReusableAnalyzerBase into Analyzer
- Add a ReuseStrategy abstraction to Analyzer which controls whether the TokenStreamComponents are reused globally (as they are today) or per-field.
- Convert all Analyzers over to using TokenStreamComponents.  I've already seen that some of the TokenStreams created in tests need some work to be reusable (even if they aren't reused).
- Remove Analyzer.reusableTokenStream and convert everything over to using .tokenStream (which will now be returning reusable TokenStreams)."
"LUCENE-2595","TEST","TEST","most tests should use MockRAMDirectory not RAMDirectory",""
"LUCENE-2657","BUILD_SYSTEM","IMPROVEMENT","Replace Maven POM templates with full POMs, and change documentation accordingly","The current Maven POM templates only contain dependency information, the bare bones necessary for uploading artifacts to the Maven repository.

The full Maven POMs in the attached patch include the information necessary to run a multi-module Maven build, in addition to serving the same purpose as the current POM templates.

Several dependencies are not available through public maven repositories.  A profile in the top-level POM can be activated to install these dependencies from the various {{lib/}} directories into your local repository.  From the top-level directory:

{code}
mvn -N -Pbootstrap install
{code}

Once these non-Maven dependencies have been installed, to run all Lucene/Solr tests via Maven's surefire plugin, and populate your local repository with all artifacts, from the top level directory, run:

{code}
mvn install
{code}

When one Lucene/Solr module depends on another, the dependency is declared on the *artifact(s)* produced by the other module and deposited in your local repository, rather than on the other module's un-jarred compiler output in the {{build/}} directory, so you must run {{mvn install}} on the other module before its changes are visible to the module that depends on it.

To create all the artifacts without running tests:

{code}
mvn -DskipTests install
{code}

I almost always include the {{clean}} phase when I do a build, e.g.:

{code}
mvn -DskipTests clean install
{code}
"
"LUCENE-1585","RFE","RFE","Allow to control how payloads are merged","Lucene handles backwards-compatibility of its data structures by
converting them from the old into the new formats during segment
merging. 

Payloads are simply byte arrays in which users can store arbitrary
data. Applications that use payloads might want to convert the format
of their payloads in a similar fashion. Otherwise it's not easily
possible to ever change the encoding of a payload without reindexing.

So I propose to introduce a PayloadMerger class that the SegmentMerger
invokes to merge the payloads from multiple segments. Users can then
implement their own PayloadMerger to convert payloads from an old into
a new format.

In the future we need this kind of flexibility also for column-stride
fields (LUCENE-1231) and flexible indexing codecs.

In addition to that it would be nice if users could store version
information in the segments file. E.g. they could store ""in segment _2
the term a:b uses payloads of format x.y"".
"
"LUCENE-2468","BUG","BUG","reopen on NRT reader should share readers w/ unchanged segments","A repoen on an NRT reader doesn't seem to share readers for those segments that are unchanged.
http://search.lucidimagination.com/search/document/9f0335d480d2e637/nrt_and_caching_based_on_indexreader"
"LUCENE-1494","RFE","RFE","masking field of span for cross searching across multiple fields (many-to-one style)","This issue is to cover the changes required to do a search across multiple fields with the same name in a fashion similar to a many-to-one database. Below is my post on java-dev on the topic, which details the changes we need:

---

We have an interesting situation where we are effectively indexing two 'entities' in our system, which share a one-to-many relationship (imagine 'User' and 'Delivery Address' for demonstration purposes). At the moment, we index one Lucene Document per 'many' end, duplicating the 'one' end data, like so:

    userid: 1
    userfirstname: fred
    addresscountry: au
    addressphone: 1234

    userid: 1
    userfirstname: fred
    addresscountry: nz
    addressphone: 5678

    userid: 2
    userfirstname: mary
    addresscountry: au
    addressphone: 5678

(note: 2 Documents indexed for user 1). This is somewhat annoying for us, because when we search in Lucene the results we want back (conceptually) are at the 'user' level, so we have to collapse the results by distinct user id, etc. etc (let alone that it blows out the size of our index enormously). So why do we do it? It would make more sense to use multiple fields:
    userid: 1
    userfirstname: fred
    addresscountry: au
    addressphone: 1234
    addresscountry: nz
    addressphone: 5678

    userid: 2
    userfirstname: mary
    addresscountry: au
    addressphone: 5678

But imagine the search ""+addresscountry:au +addressphone:5678"". We'd like this to match ONLY Mary, but of course it matches Fred also because he matches both those terms (just for different addresses).

There are two aspects to the approach we've (more or less) got working but I'd like to run them past the group and see if they're worth trying to get them into Lucene proper (if so, I'll create a JIRA issue for them)

1) Use a modified SpanNearQuery. If we assume that country + phone will always be one token, we can rely on the fact that the positions of 'au' and '5678' in Fred's document will be different.

   SpanQuery q1 = new SpanTermQuery(new Term(""addresscountry"", ""au""));
   SpanQuery q2 = new SpanTermQuery(new Term(""addressphone"", ""5678""));
   SpanQuery snq = new SpanNearQuery(new SpanQuery[]{q1, q2}, 0, false);

the slop of 0 means that we'll only return those where the two terms are in the same position in their respective fields. This works brilliantly, BUT requires a change to SpanNearQuery's constructor (which checks that all the clauses are against the same field). Are people amenable to perhaps adding another constructor to SNQ which doesn't do the check, or subclassing it to do the same (give it a protected non-checking constructor for the subclass to call)?

2) (snipped ... see LUCENE-1626 for second idea)"
"LUCENE-945","BUILD_SYSTEM","BUG","contrib/benchmark tests fail find data dirs","This was exposed by LUCENE-940 - a test was added that uses the Reuters collection. Then tests succeed when ran from contrib/benchmark (e.g. by IDE) but fail when running as part of ""ant test-contrib"" because the test expects to find the Reuters data under trunk/work. 
"
"LUCENE-1872","DOCUMENTATION","IMPROVEMENT","Improve javadocs for Numeric*","I'm working on improving Numeric* javadocs."
"LUCENE-2564","IMPROVEMENT","BUG","wordlistloader is inefficient","WordListLoader is basically used for loading up stopwords lists, stem dictionaries, etc.
Unfortunately the api returns Set<String> and sometimes even HashSet<String> or HashMap<String,String>

I think we should break it and return CharArraySets and CharArrayMaps (but leave the return value as generic Set,Map).

If someone objects to breaking it in 3.1, then we can do this only in 4.0, but i think it would be good to fix it both places.
The reason is that if someone does new FooAnalyzer() a lot (probably not uncommon) i think its doing a bunch of useless copying.

I think we should slap @lucene.internal on this API too, since thats mostly how its being used.
"
"LUCENE-1684","RFE","IMPROVEMENT","Add matchVersion to StandardAnalyzer","I think we should add a matchVersion arg to StandardAnalyzer.  This
allows us to fix bugs (for new users) while keeping precise back
compat (for users who upgrade).

We've discussed this on java-dev, but I'd like to now make it concrete
(patch attached).  I think it actually works very well, and is a
simple tool to help us carry out our back-compat policy.

I coded up an example with StandardAnalyzer:

  * The ctor now takes a required arg (Version matchVersion).  You
    pass Version.LUCENE_CURRENT to always get lates & greatest, or eg
    Version.LUCENE_24 to match 2.4's bugs/settings/behavior.

  * StandardAalyzer conditionalizes the ""replace invalid acronym"" and
    ""enable position increment in StopFilter"" based on matchVersion.

  * It also prevents creating zillions of ctors, over time, as we need
    to change settings in the class.  EG StandardAnalyzer now has 2
    settings that are version dependent, and there's at least another
    2 issues open on fixing some more of its bugs.

The migration is also very clean: we'd only add this to classes on an
""as needed"" basis.  On the first release that adds the arg, the
default remains back compatible with the prior release.  Then, going
forward, we are free to fix issues on that class and conditionalize by
matchVersion.

The javadoc at the top of StandardAnalyzer clearly calls out what
version specific behavior is done:

{code}
 * <p>You must specify the required {@link Version}
 * compatibility when creating StandardAnalyzer:
 * <ul>
 *   <li> As of 2.9, StopFilter preserves position
 *        increments by default
 *   <li> As of 2.9, Tokens incorrectly idenfied as acronyms
 *        are corrected (see <a href=""https://issues.apache.org/jira/browse/LUCENE-1068"">LUCENE-1608</a>
 * </ul>
 *
{code}
"
"LUCENE-3108","RFE","TASK","Land DocValues on trunk","Its time to move another feature from branch to trunk. I want to start this process now while still a couple of issues remain on the branch. Currently I am down to a single nocommit (javadocs on DocValues.java) and a couple of testing TODOs (explicit multithreaded tests and unoptimized with deletions) but I think those are not worth separate issues so we can resolve them as we go. 
The already created issues (LUCENE-3075 and LUCENE-3074) should not block this process here IMO, we can fix them once we are on trunk. 

Here is a quick feature overview of what has been implemented:
 * DocValues implementations for Ints (based on PackedInts), Float 32 / 64, Bytes (fixed / variable size each in sorted, straight and deref variations)
 * Integration into Flex-API, Codec provides a PerDocConsumer->DocValuesConsumer (write) / PerDocValues->DocValues (read) 
 * By-Default enabled in all codecs except of PreFlex
 * Follows other flex-API patterns like non-segment reader throw UOE forcing MultiPerDocValues if on DirReader etc.
 * Integration into IndexWriter, FieldInfos etc.
 * Random-testing enabled via RandomIW - injecting random DocValues into documents
 * Basic checks in CheckIndex (which runs after each test)
 * FieldComparator for int and float variants (Sorting, currently directly integrated into SortField, this might go into a separate DocValuesSortField eventually)
 * Extended TestSort for DocValues
 * RAM-Resident random access API plus on-disk DocValuesEnum (currently only sequential access) -> Source.java / DocValuesEnum.java
 * Extensible Cache implementation for RAM-Resident DocValues (by-default loaded into RAM only once and freed once IR is closed) -> SourceCache.java
 
PS: Currently the RAM resident API is named Source (Source.java) which seems too generic. I think we should rename it into RamDocValues or something like that, suggestion welcome!   


Any comments, questions (rants :)) are very much appreciated."
"LUCENE-1189","BUG","BUG","QueryParser does not correctly handle escaped characters within quoted strings","The Lucene query parser incorrectly handles escaped characters inside quoted strings; specifically, a quoted string that ends with an (escaped) backslash followed by any additional quoted string will not be properly tokenized. Consider the following example:

bq. {{(name:""///mike\\\\\\"") or (name:""alphonse"")}}

This is not a contrived example -- it derives from an actual bug we've encountered in our system. Running this query will throw an exception, but removing the second clause resolves the problem. After some digging I've found that the problem is with the way quoted strings are processed by the lexer: you'll notice that Mike's name is followed by three escaped backslashes right before the ending quote; looking at the JavaCC code for the query parser highlights the problem:

{code:title=QueryParser.jj|borderStyle=solid}
<DEFAULT> TOKEN : {
  <AND:       (""AND"" | ""&&"") >
| <OR:        (""OR"" | ""||"") >
| <NOT:       (""NOT"" | ""!"") >
| <PLUS:      ""+"" >
| <MINUS:     ""-"" >
| <LPAREN:    ""("" >
| <RPAREN:    "")"" >
| <COLON:     "":"" >
| <STAR:      ""*"" >
| <CARAT:     ""^"" > : Boost
| <QUOTED:     ""\"""" (~[""\""""] | ""\\\"""")* ""\"""">
...
{code}

Take a look at the way the QUOTED token is constructed -- there is no lexical processing of the escaped characters within the quoted string itself. In the above query the lexer matches everything from the first quote through all the backslashes, _treating the end quote as an escaped character_, thus also matching the starting quote of the second term. This causes a lexer error, because the last quote is then considered the start of a new match.

I've come to understand that the Lucene query handler is supposed to be able to handle unsanitized human input; indeed the lexer above would handle a query like {{""blah\""}} without complaining, but that's a ""best-guess"" approach that results in bugs with legal, automatically generated queries. I've attached a patch that fixes the erroneous behavior but does not maintain leniency with malformed queries; I believe this is the correct approach because the two design goals are fundamentally at odds. I'd appreciate any comments."
"LUCENE-2653","BUG","BUG","ThaiAnalyzer assumes things about your jre","The ThaiAnalyzer/ThaiWordFilter depends on the fact that BreakIterator.getWordInstance(new Locale(""th"")) returns a dictionary-based break iterator that can segment thai phrases into words (it does not use whitespace).

But this is non-standard that the JRE will specialize this locale in this way, its nice, but you can't depend on it.
For example, if you are running on IBM JRE, this analyzer/wordfilter is completely ""broken"" in the sense it won't do what it claims to do.

At the minimum, we need to document this and suggest users look at ICUTokenizer for thai, which always has this breakiterator and is not jre-dependent.

Better, would be to check statically that the thing actually works.
when creating a new ThaiWordFilter we could clone() the BreakIterator, which is often cheaper than making a new one anyway.
we could throw an exception, if its not supported, and add a boolean so the user knows it works.
and we could refer to this boolean with Assert.assume in its tests.
"
"LUCENE-931","DOCUMENTATION","","Some files are missing the license headers","Jukka provided the following list of files that are missing the license headers.
In addition there might be other files (like build scripts) that don't have the headers.

src/java/org/apache/lucene/document/MapFieldSelector.java
src/java/org/apache/lucene/search/PrefixFilter.java
src/test/org/apache/lucene/TestHitIterator.java
src/test/org/apache/lucene/analysis/TestISOLatin1AccentFilter.java
src/test/org/apache/lucene/index/TestAddIndexesNoOptimize.java
src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
src/test/org/apache/lucene/index/TestFieldInfos.java
src/test/org/apache/lucene/index/TestIndexFileDeleter.java
src/test/org/apache/lucene/index/TestIndexWriter.java
src/test/org/apache/lucene/index/TestIndexWriterDelete.java
src/test/org/apache/lucene/index/TestIndexWriterLockRelease.java
src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java
src/test/org/apache/lucene/index/TestNorms.java
src/test/org/apache/lucene/index/TestParallelTermEnum.java
src/test/org/apache/lucene/index/TestSegmentTermEnum.java
src/test/org/apache/lucene/index/TestTerm.java
src/test/org/apache/lucene/index/TestTermVectorsReader.java
src/test/org/apache/lucene/search/TestRangeQuery.java
src/test/org/apache/lucene/search/TestTermScorer.java
src/test/org/apache/lucene/store/TestBufferedIndexInput.java
src/test/org/apache/lucene/store/TestWindowsMMap.java
src/test/org/apache/lucene/store/_TestHelper.java
src/test/org/apache/lucene/util/_TestUtil.java
contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/server/FeedNotFoundException.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/server/registry/ComponentType.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/server/registry/RegistryException.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/storage/lucenestorage/StorageAccountWrapper.java
contrib/gdata-server/src/core/src/test/org/apache/lucene/gdata/storage/lucenestorage/TestModifiedEntryFilter.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/AtomUriElementTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMEntryImplTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMFeedImplTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMGenereatorImplTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMSourceImplTest.java
contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java
contrib/javascript/queryConstructor/luceneQueryConstructor.js
contrib/javascript/queryEscaper/luceneQueryEscaper.js
contrib/javascript/queryValidator/luceneQueryValidator.js
contrib/queries/src/java/org/apache/lucene/search/BooleanFilter.java
contrib/queries/src/java/org/apache/lucene/search/BoostingQuery.java
contrib/queries/src/java/org/apache/lucene/search/FilterClause.java
contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery.java
contrib/queries/src/java/org/apache/lucene/search/TermsFilter.java
contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThisQuery.java
contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java
contrib/regex/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java
contrib/snowball/src/java/net/sf/snowball/Among.java
contrib/snowball/src/java/net/sf/snowball/SnowballProgram.java
contrib/snowball/src/java/net/sf/snowball/TestApp.java
contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/BooleanQueryTst.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/ExceptionQueryTst.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/SingleFieldTestDb.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/Test01Exceptions.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/Test02Boolean.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/Test03Distance.java
contrib/wordnet/src/java/org/apache/lucene/wordnet/SynExpand.java
contrib/wordnet/src/java/org/apache/lucene/wordnet/SynLookup.java
contrib/wordnet/src/java/org/apache/lucene/wordnet/Syns2Index.java
"
"LUCENE-1154","DOCUMENTATION","BUG","System Reqs page should be release specific","The System Requirements page, currently under the Main->Resources section of the website should be part of a given version's documentation, since it will be changing for a given release.  

I will ""deprecate"" the existing one, but leave it in place(with a message) to cover the existing releases that don't have this, but will also add it to the release docs for future releases."
"LUCENE-361","DESIGN_DEFECT","BUG","FieldSortedHitQueue.lessThan() should not be final","The final seems to provide little benefit and it takes away the ability to
specialize this method (which I need to do, forcing a customization of Lucene to
remove the final)."
"LUCENE-1310","BUG","BUG","Phrase query with term repeated 3 times requires more slop than expected","Consider a document with the text ""A A A"".
The phrase query ""A A A"" (exact match) succeeds.
The query ""A A A""~1 (same document and query, just increasing the slop value by one) fails.
""A A A""~2 succeeds again.

If the exact match succeeds, I wouldn't expect the same query but with more slop to fail.  The fault seems to require some term to be repeated at least three times in the query, but the three occurrences do not need to be adjacent.  I will attach a file that contains a set of JUnit tests that demonstrate what I mean."
"LUCENE-963","IMPROVEMENT","IMPROVEMENT","Add setters to Field to allow re-use of Field instances during indexing","If we add setters to Field it makes it possible to re-use Field
instances during indexing which is a sizable performance gain for
small documents.  See here for some discussion:

    http://www.gossamer-threads.com/lists/lucene/java-dev/51041
"
"LUCENE-2882","REFACTORING","TASK","Cut over SpanQuery#getSpans to AtomicReaderContext","Followup from LUCENE-2831 - SpanQuery#getSpans(IR) seems to be the last remaining artifact that doesn't enforce per-segments context while it should really work on AtomicReaderContext (SpanQuery#getSpans(AtomicReaderContext) instead of a naked IR."
"LUCENE-3880","IMPROVEMENT","BUG","UAX29URLEmailTokenizer fails to recognize emails as such when the mailto: scheme is prepended","As [reported by Kai Glzau on solr-user|http://markmail.org/message/n32kji3okqm2c5qn]:

UAX29URLEmailTokenizer seems to split at the wrong place:

{noformat}mailto:test@example.org{noformat} ->
{noformat}mailto:test{noformat}
{noformat}example.org{noformat}

As a workaround I use

{code:xml}
<charFilter class=""solr.PatternReplaceCharFilterFactory"" pattern=""mailto:"" replacement=""mailto: ""/>
{code}"
"LUCENE-1199","BUG","BUG","NullPointerException in IndexModifier.close()","We upgraded from Lucene 2.0.0. to 2.3.1 hoping this would resolve this issue.

http://jira.codehaus.org/browse/MRM-715

Trace is as below for Lucene 2.3.1:
java.lang.NullPointerException
at org.apache.lucene.index.IndexModifier.close(IndexModifier.java:576)
at org.apache.maven.archiva.indexer.lucene.LuceneRepositoryContentIndex.closeQuietly(LuceneRepositoryContentIndex.java:416)
at org.apache.maven.archiva.indexer.lucene.LuceneRepositoryContentIndex.modifyRecord(LuceneRepositoryContentIndex.java:152)
at org.apache.maven.archiva.consumers.lucene.IndexContentConsumer.processFile(IndexContentConsumer.java:169)
at org.apache.maven.archiva.repository.scanner.functors.ConsumerProcessFileClosure.execute(ConsumerProcessFileClosure.java:51)
at org.apache.commons.collections.functors.IfClosure.execute(IfClosure.java:117)
at org.apache.commons.collections.CollectionUtils.forAllDo(CollectionUtils.java:388)
at org.apache.maven.archiva.repository.scanner.RepositoryContentConsumers.executeConsumers(RepositoryContentConsumers.java:283)
at org.apache.maven.archiva.proxy.DefaultRepositoryProxyConnectors.transferFile(DefaultRepositoryProxyConnectors.java:597)
at org.apache.maven.archiva.proxy.DefaultRepositoryProxyConnectors.fetchFromProxies(DefaultRepositoryProxyConnectors.java:157)
at org.apache.maven.archiva.web.repository.ProxiedDavServer.applyServerSideRelocation(ProxiedDavServer.java:447)
at org.apache.maven.archiva.web.repository.ProxiedDavServer.fetchContentFromProxies(ProxiedDavServer.java:354)
at org.apache.maven.archiva.web.repository.ProxiedDavServer.process(ProxiedDavServer.java:189)
at org.codehaus.plexus.webdav.servlet.multiplexed.MultiplexedWebDavServlet.service(MultiplexedWebDavServlet.java:119)
at org.apache.maven.archiva.web.repository.RepositoryServlet.service(RepositoryServlet.java:155)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)"
"LUCENE-554","BUG","BUG","Possible index corruption if crashing while replacing segments file","Lucene's indexing is expected to be reasonably tolerant to computer crashes or the indexing process being killed. By reasonably tolerant, I mean that it is ok to lose a few documents (those currently buffered in memory), or have to repeat some work (e.g., a long merge that was in progress) - but it is not ok for the entire index, or large chunks of it, to become irreversebly corrupt.

The fact that Lucene works by repeated merging of several small segments into a new larger segments, solves most of the crash problems, because until the new segment is fully created, the old segments are still there and fully functional. However, one possibility for corruption remains in the segment replacement code:

After a new segment is created, a new segments file is written as a new file ""segments.new"", and then this file is renamed to ""segments"". The problem is that this renaming is done using Directory.renameFile(), and FSDirectory.renameFile is *NOT* atomic: it first deletes the old file, and then renames the new file. A crash between these stages (or perhaps during Java's rename which also isn't guaranteed to be atomic) will potentially leave us without a working ""segments"" file.

I will post here a patch for this bug shortly.

The patch will also include a change to Directory.renameFile()'s Javadoc. It currently claims ""This replacement should be atomic."", which is false in FSDirectory. Instead it should make a weaker claim, for example
   ""This replacement does not have to be atomic, but must at least obey a weaker guarantee: at any time during the replacement, either the ""from"" file is still available, or the ""to"" file is available with either the new or old content.""
(or, we can just drop the guaranteee altogether, like Java's File.renameTo() provides no atomic-ness guarantees)."
"LUCENE-3222","BUG","BUG","Buffered deletes under count RAM","I found this while working on LUCENE-2548: when we freeze the deletes (create FrozenBufferedDeletes), when we set the bytesUsed we are failing to account for RAM required for the term bytes (and now term field)."
"LUCENE-3014","RFE","TASK","comparator API for segment versions","See LUCENE-3012 for an example.

Things get ugly if you want to use SegmentInfo.getVersion()

For example, what if we committed my patch, release 3.2, but later released 3.1.1 (will ""3.1.1"" this be whats written and returned by this function?)
Then suddenly we broke the index format because we are using Strings here without a reasonable comparator API.

In this case one should be able to compute if the version is < 3.2 safely.

If we don't do this, and we rely upon this version information internally in lucene, I think we are going to break something."
"LUCENE-1651","IMPROVEMENT","TASK","Make IndexReader.open() always return MSR to simplify (re-)opens.","As per discussion in mailing list, I'm making DirectoryIndexReader.open() always return MSR, even for single-segment indexes.
While theoretically valid in the past (if you make sure to keep your index constantly optimized) this feature is made practically obsolete by per-segment collection.

The patch somewhat de-hairies (re-)open logic for MSR/SR.
SR no longer needs an ability to pose as toplevel directory-owning IR.
All related logic is moved from DIR to MSR.
DIR becomes almost empty, and copying two or three remaining fields over to MSR/SR, I remove it.
Lots of tests fail, as they rely on SR returned from IR.open(), I fix by introducing SR.getOnlySegmentReader static package-private method.
Some previous bugs are uncovered, one is fixed in LUCENE-1645, another (partially fixed in LUCENE-1648) is fixed in this patch. "
"LUCENE-3684","RFE","IMPROVEMENT","Add offsets to postings (D&PEnum)","I think should explore making start/end offsets a first-class attr in the
postings APIs, and fixing the indexer to index them into postings.

This will make term vector access cleaner (we now have to jump through
hoops w/ non-first-class offset attr).  It can also enable efficient
highlighting without term vectors / reanalyzing, if the app indexes
offsets into the postings.
"
"LUCENE-1226","BUG","BUG","IndexWriter.addIndexes(IndexReader[]) fails to create compound files","Even if no exception is thrown while writing the compound file at the end of the 
addIndexes() call, the transaction is rolled back and the successfully written cfs 
file deleted. The fix is simple: There is just the 
{code:java}
success = true;
{code}
statement missing at the end of the try{} clause.

All tests pass. I'll commit this soon to trunk and 2.3.2."
"LUCENE-1736","IMPROVEMENT","IMPROVEMENT","DateTools.java general improvements","Applying the attached patch shows the improvements to DateTools.java that I think should be done. All logic that does anything at all is moved to instance methods of the inner class Resolution. I argue this is more object-oriented.

1. In cases where Resolution is an argument to the method, I can simply invoke the appropriate call on the Resolution object. Formerly there was a big branch if/else.
2. Instead of ""synchronized"" being used seemingly everywhere, synchronized is used to sync on the object that is not threadsafe, be it a DateFormat or Calendar instance.
3. Since different DateFormat and Calendar instances are created per-Resolution, there is now less lock contention since threads using different resolutions will not use the same locks.
4. The old implementation of timeToString rounded the time before formatting it. That's unnecessary since the format only includes the resolution desired.
5. round() now uses a switch statement that benefits from fall-through (no break).

Another debatable improvement that could be made is putting the resolution instances into an array indexed by format length. This would mean I could remove the switch in lookupResolutionByLength() and avoid the length constants there. Maybe that would be a bit too over-engineered when the switch is fine."
"LUCENE-2041","RFE","IMPROVEMENT","Complete parallelizaton of ParallelMultiSearcher","ParallelMultiSearcher is parallel only for the method signatures of 'search'.

Part of a query process calls the method docFreq(). There was a TODO comment to parallelize this. Parallelizing this method actually increases the performance of a query on multiple indexes, especially remotely.

"
"LUCENE-1805","BUG","BUG","CloseableThreadLocal should allow null Objects","CloseableThreadLocal does not allow null Objects in its get() method, but does nothing to prevent them in set(Object). The comment in get() before assert v != null is irrelevant - the application might have passed null.

Null is an important value for Analyzers. Since tokenStreams (a ThreadLocal private member in Analyzer) is not accessible by extending classes, the only way for an Analyzer to reset the tokenStreams is by calling setPreviousTokenStream(null).

I will post a patch w/ a test"
"LUCENE-3480","REFACTORING","TASK","refactoring of docvalues params in Codec.java","While working on LUCENE-2621 I am trying to do some cleanup of the Codec APIs, currently Codec.java has a boolean for getDocValuesUseCFS()

I think this is an impl detail that should not be in Codec.java: e.g. i might make a SimpleText impl that uses only 1 file and then the param
is awkward.

So, instead I created Sep impls that dont use CFS (use separate files) and placed them under the sep package, if you don't want to use
CFS you can just use these implementations in your codec."
"LUCENE-3231","RFE","IMPROVEMENT","Add fixed size DocValues int variants & expose Arrays where possible","currently we only have variable bit packed ints implementation. for flexible scoring or loading field caches it is desirable to have fixed int implementations for 8, 16, 32 and 64 bit. "
"LUCENE-3032","BUG","BUG","TestIndexWriterException fails with NPE on realtime","{noformat}
   [junit] Testsuite: org.apache.lucene.index.TestIndexWriterExceptions
    [junit] Testcase: testRandomExceptionsThreads(org.apache.lucene.index.TestIndexWriterExceptions):	Caused an ERROR
    [junit] (null)
    [junit] java.lang.NullPointerException
    [junit] 	at org.apache.lucene.index.DocumentsWriterPerThread.prepareFlush(DocumentsWriterPerThread.java:329)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:378)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:512)
    [junit] 	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:2619)
    [junit] 	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:2594)
    [junit] 	at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2464)
    [junit] 	at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2537)
    [junit] 	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2519)
    [junit] 	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2503)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:230)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1226)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1154)
    [junit] 
    [junit] 
    [junit] Tests run: 18, Failures: 0, Errors: 1, Time elapsed: 22.548 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testRandomExceptionsThreads -Dtests.seed=-5079747362001734044:1572064802119081373
    [junit] WARNING: test method: 'testRandomExceptionsThreads' left thread running: merge thread: _25(4.0):cv2/1 _29(4.0):cv2/1 _20(4.0):cv3/1 into _2m
    [junit] RESOURCE LEAK: test method: 'testRandomExceptionsThreads' left 1 thread(s) running
    [junit] NOTE: test params are: codec=RandomCodecProvider: {content=Pulsing(freqCutoff=2), field=MockSep, id=Pulsing(freqCutoff=2), other=MockSep, contents=SimpleText, content1=MockSep, content2=SimpleText, content4=MockRandom, content5=MockRandom, content6=MockVariableIntBlock(baseBlockSize=41), crash=Standard, content7=MockFixedIntBlock(blockSize=1633)}, locale=en_GB, timezone=Europe/Vaduz
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestToken, TestDateTools, Test2BTerms, TestAddIndexes, TestFilterIndexReader, TestIndexWriterExceptions]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=155417240,total=292945920
    [junit] ------------- ---------------- ---------------
{noformat}"
"LUCENE-3562","IMPROVEMENT","IMPROVEMENT","Stop storing TermsEnum in CloseableThreadLocal inside Terms instance","We have sugar methods in Terms.java (docFreq, totalTermFreq, docs,
docsAndPositions) that use a saved thread-private TermsEnum to do the
lookups.

But on apps that send many threads through Lucene, and/or have many
segments, this can add up to a lot of RAM, especially if the codecs
impl holds onto stuff.

Also, Terms has a close method (closes the CloseableThreadLocal) which
must be called, but we fail to do so in some places.

These saved enums are the cause of the recent OOME in TestNRTManager
(TestNRTManager.testNRTManager -seed
2aa27e1aec20c4a2:-4a5a5ecf46837d0e:-7c4f651f1f0b75d7 -mult 3
-nightly).

Really sharing these enums is a holdover from before Lucene queries
would share state (ie, save the TermState from the first pass, and use
it later to pull enums, get docFreq, etc.).  It's not helpful anymore,
and it can use gobbs of RAM, so I'd like to remove it.
"
"LUCENE-1450","BUG","BUG","RangeQuery & RangeFilter used with collation seek to lowerTerm using compareTo()","The constructor for RangeTermEnum initializes a TermEnum starting with lowerTermText, but when a collator is defined, all terms in the given field need to be checked, since collation can introduce non-Unicode orderings.  Instead, the RangeTermEnum constructor should test for a non-null collator, and if there is one, point the TermEnum at the first term in the given field.

LUCENE-1424 introduced this bug."
"LUCENE-2190","RFE","BUG","CustomScoreQuery (function query) is broken (due to per-segment searching)","Spinoff from here:

  http://lucene.markmail.org/message/psw2m3adzibaixbq

With the cutover to per-segment searching, CustomScoreQuery is not really usable anymore, because the per-doc custom scoring method (customScore) receives a per-segment docID, yet there is no way to figure out which segment you are currently searching.

I think to fix this we must also notify the subclass whenever a new segment is switched to.  I think if we copy Collector.setNextReader, that would be sufficient.  It would by default do nothing in CustomScoreQuery, but a subclass could override."
"LUCENE-1679","DESIGN_DEFECT","IMPROVEMENT","Make WildcardTermEnum#difference() non-final","The method WildcardTermEnum#difference() is declared final. I found it very useful to subclass WildcardTermEnum to implement different scoring for exact vs. partial matches. The change is rather trivial (attached)  but I guess it could make life easier for a couple of users.

I attached two patches:
 - one which contains the single change to make difference() non-final (WildcardTermEnum.patch)
 - one which does also contain some minor cleanup of WildcardTermEnum. I removed unnecessary member initialization and made those final. ( WildcardTermEnum_cleanup.patch)

Thanks simon"
"LUCENE-1933","RFE","IMPROVEMENT","Provide an convenience AttributeFactory that implements all default attributes with Token","I found some places in contrib tests, where the Token.class was added using addAttributeImpl(). The problem here is, that you cannot be sure, that the attribute is really added and you may fail later (because you only update your local instance). The tests in contrib will partially fail with 3.0 without backwards layer (because the backwards layer uses Token/TokenWrapper internally and copyTo() will work.

The correct way to achieve this is using an AttributeFactory. The AttributeFactory is currently private in SingleTokenTokenStream. I want to move it to Token.java as a static class / static member. In this case the tests can be rewritten.

I also want to mark addAttributeImpl() as EXPERT, because you must really know whats happening and what are the traps."
"LUCENE-734","BUILD_SYSTEM","TASK","Upload Lucene 2.0 artifacts in the Maven 1 repository","The Lucene 2.0 artifacts can be found in the Maven 2 repository, but not in the Maven 1 repository. There are still projects using Maven 1 who might be interested in upgrading to Lucene 2, so having the artifacts also in the Maven 1 repository would be very helpful."
"LUCENE-2717","BUG","BUG","BasicOperations.concatenate creates invariants","I started writing a test for LUCENE-2716, and i found a problem with BasicOperations.concatenate(Automaton, Automaton):
it creates automata with invariant representation (which should never happen, unless you manipulate states/transitions manually).

strangely enough the BasicOperations.concatenate(List<Automaton>) does not have this problem.
"
"LUCENE-2819","BUG","BUG","LuceneTestCase's check for uncaught exceptions in threads causes collateral damage?","Eg see these failures:

    https://hudson.apache.org/hudson/job/Lucene-3.x/214/

Multiple test methods failed in TestIndexWriterOnDiskFull, but, I think only 1 test had a real failure but somehow our ""thread hit exc"" tracking incorrectly blames the other 3 cases?

I'm not sure about this but it seems like something like that is going on...

So, one problem is that LuceneTestCase.tearDown fails on any thread excs, but if CMS had also hit a failure, then fails to clear CMS's thread failures.  I think we should just remove CMS's thread failure tracking?  (It's static so it can definitely bleed across tests).  Ie, just rely on LuceneTestCase's tracking."
"LUCENE-2315","DESIGN_DEFECT","BUG","AttributeSource's methods for accessing attributes should be final, else its easy to corrupt the internal states","The methods that operate and modify the internal maps of AttributeSource should be final, which is a backwards break. But anybody that overrides such methods simply creates a buggy AS either case.

I want to makeall impls final (in general the class should be final at all, but it is made for extension in TokenStream). So its important that the implementations are final!"
"LUCENE-1448","DESIGN_DEFECT","BUG","add getFinalOffset() to TokenStream","If you add multiple Fieldable instances for the same field name to a document, and you then index those fields with TermVectors storing offsets, it's very likely the offsets for all but the first field instance will be wrong.

This is because IndexWriter under the hood adds a cumulative base to the offsets of each field instance, where that base is 1 + the endOffset of the last token it saw when analyzing that field.

But this logic is overly simplistic.  For example, if the WhitespaceAnalyzer is being used, and the text being analyzed ended in 3 whitespace characters, then that information is lost and then next field's offsets are then all 3 too small.  Similarly, if a StopFilter appears in the chain, and the last N tokens were stop words, then the base will be 1 + the endOffset of the last non-stopword token.

To fix this, I'd like to add a new getFinalOffset() to TokenStream.  I'm thinking by default it returns -1, which means ""I don't know so you figure it out"", meaning we fallback to the faulty logic we have today.

This has come up several times on the user's list."
"LUCENE-3378","TEST","BUG","Some contribs depend on core tests to be compiled and fail when ant clean was done before","If you do ""ant clean"" on the root level of Lucene and then go to e.g. contrib/queryparser (3.x only) or contrib/misc (3.x and trunk) and call ""ant test"", the build of tests fails:
- contrib/queryparser's ExtendedableQPTests extend a core TestQueryParser (3.x only, in module this works, of course)
- contrib/misc/TestIndexSplitter uses a core class to build its index

To find the root cause: We should first remove the core tests from contrib classpath, so the issue gets visible even without ""ant clean"" before. Then we can fix this."
"LUCENE-2541","BUG","BUG","NumericRangeQuery errors with endpoints near long min and max values","This problem first reported in Solr:

http://lucene.472066.n3.nabble.com/range-query-on-TrieLongField-strange-result-tt970974.html#a970974"
"LUCENE-1027","IMPROVEMENT","BUG","contrib/benchmark config does not play nice with doubles with the flush.by.ram value","In the o.a.l.benchmark.byTask.utils.Config.java file, the nextRound and various other methods do not handle doubles in the ""round"" property configuration syntax.

To replicate this, copy the micro-standard.alg and replace 
merge.factor=mrg:10:100:10:100
max.buffered=buf:10:10:100:100

with

ram.flush.mb=ram:32:40:48:56

and you will get various ClassCastExceptions in Config (one in newRound() and, when that is fixed, in getColsValuesForValsByRound.

The fix seems to be to just to mirror the handling of int[].

The fix seems relatively minor.  Patch shortly and will plan to commit tomorrow evening."
"LUCENE-896","RFE","IMPROVEMENT","Let users set Similarity for MoreLikeThis","Let users set Similarity used for MoreLikeThis

For discussion, see:
http://www.nabble.com/MoreLikeThis-API-changes--tf3838535.html"
"LUCENE-2722","IMPROVEMENT","IMPROVEMENT","Sep codec should store less in terms dict","I'm working on improving Lucene's performance with int block codecs
(FOR/PFOR), but in early perf testing I found that these codecs cause
a big perf hit to those MTQs that need to scan many terms but don't
end up accepting many of those terms (eg fuzzy, wildcard, regexp).

This is because sep codec stores much more in the terms dict, since
each file is separate, ie seek points for each of doc, frq, pos, pyl,
skp files.

So I'd like to shift these seek points to instead be stored in the doc
file, except for the doc seek point itself.  Since a given query will
always need to seek to the doc file, this does not add an extra seek.
But it saves tons of vInt decodes for the next/seke intensive MTQs...
"
"LUCENE-996","RFE","IMPROVEMENT","Parsing mixed inclusive/exclusive range queries","The current query parser doesn't handle parsing a range query (i.e. ConstantScoreRangeQuery) with mixed inclusive/exclusive bounds."
"LUCENE-714","IMPROVEMENT","IMPROVEMENT","Use a System.arraycopy more than a for","In org.apache.lucene.index.DocumentWriter. The patch will explain by itself. I didn't make any performance test, but I think it is obvious that it will be faster.
All tests passed."
"LUCENE-1957","CLEANUP","TASK","Remove deprecated Filter.bits() and make Filter.getDocIdSet() abstract.",""
"LUCENE-3722","IMPROVEMENT","IMPROVEMENT","make similarities/term/collectionstats take long (for > 2B docs)","As noted by Yonik and Andrzej on SOLR-1632, this would be useful for distributed scoring.

we can also add a sugar method add() to both of these to make it easier to sum."
"LUCENE-3495","BUG","BUG","BlockJoinQuery doesn't implement boost","After reviewing LUCENE-3494, i checked other queries and noticed that BlockJoinQuery currently throws UOE for getBoost and setBoost:
{noformat}
throw new UnsupportedOperationException(""this query cannot support boosting; please use childQuery.setBoost instead"");
{noformat}

I don't think we can safely do that in queries, because other parts of lucene rely upon this working... for example BQs rewrite when
it has a single clause and erases itself.

So I think we should just pass down the boost to the inner weight.
"
"LUCENE-2675","RFE","IMPROVEMENT","Add support for 3.0 indexes in 2.9 branch","There was a lot of user requests to be able to read Lucene 3.0 indexes also with 2.9. This would make the migration easier. There is no problem in doing that, as the new stored fields version in Lucene 3.0 is only used to mark a segment's stored fields file as no longer containing compressed fields. But index format did not really change. This patch simply allows FieldsReader to pass a Lucene 3.0 version number, but still writes segments in 2.9 format (as you could suddenly turn on compression for added documents).

I added ZIP files for 3.0 indexes for TestBackwards. Without the patch it does not pass, as FieldsReader complains about incorrect version number (although it could read the file easily). If we would release maybe a 2.9.4 release of Lucene we should include that patch."
"LUCENE-3867","BUG","BUG","RamUsageEstimator.NUM_BYTES_ARRAY_HEADER and other constants are incorrect","RamUsageEstimator.NUM_BYTES_ARRAY_HEADER is computed like that: NUM_BYTES_OBJECT_HEADER + NUM_BYTES_INT + NUM_BYTES_OBJECT_REF. The NUM_BYTES_OBJECT_REF part should not be included, at least not according to this page: http://www.javamex.com/tutorials/memory/array_memory_usage.shtml

{quote}
A single-dimension array is a single object. As expected, the array has the usual object header. However, this object head is 12 bytes to accommodate a four-byte array length. Then comes the actual array data which, as you might expect, consists of the number of elements multiplied by the number of bytes required for one element, depending on its type. The memory usage for one element is 4 bytes for an object reference ...
{quote}

While on it, I wrote a sizeOf(String) impl, and I wonder how do people feel about including such helper methods in RUE, as static, stateless, methods? It's not perfect, there's some room for improvement I'm sure, here it is:

{code}
	/**
	 * Computes the approximate size of a String object. Note that if this object
	 * is also referenced by another object, you should add
	 * {@link RamUsageEstimator#NUM_BYTES_OBJECT_REF} to the result of this
	 * method.
	 */
	public static int sizeOf(String str) {
		return 2 * str.length() + 6 // chars + additional safeness for arrays alignment
				+ 3 * RamUsageEstimator.NUM_BYTES_INT // String maintains 3 integers
				+ RamUsageEstimator.NUM_BYTES_ARRAY_HEADER // char[] array
				+ RamUsageEstimator.NUM_BYTES_OBJECT_HEADER; // String object
	}
{code}

If people are not against it, I'd like to also add sizeOf(int[] / byte[] / long[] / double[] ... and String[])."
"LUCENE-2901","BUG","BUG","KeywordMarkerFilter resets keyword attribute state to false for tokens not in protwords.txt","KeywordMarkerFilter sets true or false for the KeywordAttribute on all tokens. This erases previous state established further up the filter chain, for example in the case where a custom filter wants to prevent a token from being stemmed. 

If a token is already marked as a keyword (KeywordAttribute.isKeyword() == true), perhaps the KeywordMarkerFilterFactory should not re-set the state to false."
"LUCENE-580","RFE","IMPROVEMENT","Pre-analyzed fields","Adds the possibility to set a TokenStream at Field constrution time, available as tokenStreamValue in addition to stringValue, readerValue and binaryValue.

There might be some problems with mixing stored fields with the same name as a field with tokenStreamValue."
"LUCENE-1708","IMPROVEMENT","IMPROVEMENT","Improve the use of isDeleted in the indexing code","A spin off from here: http://www.nabble.com/Some-thoughts-around-the-use-of-reader.isDeleted-and-hasDeletions-td23931216.html.
Two changes:
# Optimize SegmentMerger work when a reader has no deletions.
# IndexReader.document() will no longer check if the document is deleted.

Will post a patch shortly"
"LUCENE-1129","BUG","BUG","ReadTask ignores traversalSize","The ReadTask doLogic() method ignores the value of the traversalSize and loops over hits.length() instead, thus falsely reporting the desired number of iterations through the hit list.

The fix is relatively trivial since we already calculate 
{code}
int traversalSize = Math.min(hits.length(), traversalSize());
{code}
so we just need to use this value in the loop condition."
"LUCENE-752","TEST","BUG","Some tests fail due to common use of java.io.tmpdir","Some tests use java.io.tmpdir, while others use tempDir (which is defined in common-build.xml).  Those that rely on java.io.tmpdir can fail when being run on the same machine as someone else who is running tests (this came up in testing the new nightly build scripts on lucene.zones.a.o)

Proposed fix is to map java.io.tmpdir in the ANT Junit task to be the same value as tempDir."
"LUCENE-593","BUG","BUG","Spellchecker's dictionary iterator misbehaves","In LuceneDictionary, the LuceneIterator.hasNext() method has two issues that makes it misbehave:

1) If hasNext is called more than once, items are skipped
2) Much more seriously, when comparing fieldnames it is done with != rather than .equals() with the potential result that nothing is indexed
"
"LUCENE-2424","IMPROVEMENT","BUG","FieldDoc.toString only returns super.toString","The FieldDoc.toString method very carefully builds a StringBuffer sb containing the information for the FieldDoc instance and then just returns super.toString() instead of sb.toString()"
"LUCENE-2288","IMPROVEMENT","IMPROVEMENT","Create EMPTY_ARGS constsant in SnowballProgram instead of allocating new Object[0]","Instead of allocating new Object[0] create a proper constant in SnowballProgram. The same (for new Class[0]) is created in Among, although it's less critical because Among is called from static initializers ... Patch will follow shortly."
"LUCENE-2702","RFE","IMPROVEMENT","BytesRefHash#get() should expect a BytesRef instances for consistency","BytesRefHash#get should use a provided BytesRef instances instead of the internally used scratch. This is how all other APIs currently work and we should be consistent."
"LUCENE-2906","RFE","RFE","Filter to process output of ICUTokenizer and create overlapping bigrams for CJK ","The ICUTokenizer produces unigrams for CJK. We would like to use the ICUTokenizer but have overlapping bigrams created for CJK as in the CJK Analyzer.  This filter would take the output of the ICUtokenizer, read the ScriptAttribute and for selected scripts (Han, Kana), would produce overlapping bigrams."
"LUCENE-2953","BUG","BUG","PriorityQueue is inheriently broken if subclass attempts to use ""heap"" w/generic T bound to anything other then ""Object""","as discovered in SOLR-2410 the fact that the protected ""heap"" variable in PriorityQueue is initialized using an Object[] makes it impossible for subclasses of PriorityQueue to exist and access the ""heap"" array unless they bind the generic to Object."
"LUCENE-2121","RFE","TASK","add UnicodeUtil.nextValidUTF16String ","In flex branch, TermRef must not contain unpaired surrogates, etc.
But in trunk/previous releases, people could (and do) seek to these.
Also some lucene multitermqueries will generate these invalid seek locations, even now (which we should separately fix)
I think the common case is already handled with a hack in SegmentReader.LegacyTermEnum, but we should clean up this hack and handle all cases.

I would also like to use this nextValidUTF16String in LUCENE-1606, and there might be other places it could be used for better bw compat.
"
"LUCENE-3159","CLEANUP","TASK","lucene benchmark has some unnecessary files","lucene/contrib/benchmark/.rsync-filter is only in the source pack (and in SVN), I was not aware of this file, though it was added long ago in https://issues.apache.org/jira/browse/LUCENE-848?focusedCommentId=12491404&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12491404
Not a blocker for this RC, just interesting to note.

maybe this is related to LUCENE-3155 too, in that we could consider this one for automatic exclusion (like DS_Store), but we should fix it if its committed in SVN too.
"
"LUCENE-1522","RFE","IMPROVEMENT","another highlighter","I've written this highlighter for my project to support bi-gram token stream (general token stream (e.g. WhitespaceTokenizer) also supported. see test code in patch). The idea was inherited from my previous project with my colleague and LUCENE-644. This approach needs highlight fields to be TermVector.WITH_POSITIONS_OFFSETS, but is fast and can support N-grams. This depends on LUCENE-1448 to get refined term offsets.

usage:
{code:java}
TopDocs docs = searcher.search( query, 10 );
Highlighter h = new Highlighter();
FieldQuery fq = h.getFieldQuery( query );
for( ScoreDoc scoreDoc : docs.scoreDocs ){
  // fieldName=""content"", fragCharSize=100, numFragments=3
  String[] fragments = h.getBestFragments( fq, reader, scoreDoc.doc, ""content"", 100, 3 );
  if( fragments != null ){
    for( String fragment : fragments )
      System.out.println( fragment );
  }
}
{code}

features:
- fast for large docs
- supports not only whitespace-based token stream, but also ""fixed size"" N-gram (e.g. (2,2), not (1,3)) (can solve LUCENE-1489)
- supports PhraseQuery, phrase-unit highlighting with slops
{noformat}
q=""w1 w2""
<b>w1 w2</b>
---------------
q=""w1 w2""~1
<b>w1</b> w3 <b>w2</b> w3 <b>w1 w2</b>
{noformat}
- highlight fields need to be TermVector.WITH_POSITIONS_OFFSETS
- easy to apply patch due to independent package (contrib/highlighter2)
- uses Java 1.5
- looks query boost to score fragments (currently doesn't see idf, but it should be possible)
- pluggable FragListBuilder
- pluggable FragmentsBuilder

to do:
- term positions can be unnecessary when phraseHighlight==false
- collects performance numbers
"
"LUCENE-1320","RFE","RFE","ShingleMatrixFilter, a three dimensional permutating shingle filter","Backed by a column focused matrix that creates all permutations of shingle tokens in three dimensions. I.e. it handles multi token synonyms.

Could for instance in some cases be used to replaces 0-slop phrase queries with something speedier.

{code:java}
Token[][][]{
  {{hello}, {greetings, and, salutations}},
  {{world}, {earth}, {tellus}}
}
{code}

passes the following test  with 2-3 grams:

{code:java}
assertNext(ts, ""hello_world"");
assertNext(ts, ""greetings_and"");
assertNext(ts, ""greetings_and_salutations"");
assertNext(ts, ""and_salutations"");
assertNext(ts, ""and_salutations_world"");
assertNext(ts, ""salutations_world"");
assertNext(ts, ""hello_earth"");
assertNext(ts, ""and_salutations_earth"");
assertNext(ts, ""salutations_earth"");
assertNext(ts, ""hello_tellus"");
assertNext(ts, ""and_salutations_tellus"");
assertNext(ts, ""salutations_tellus"");
{code}

Contains more and less complex tests that demonstrate offsets, posincr, payload boosts calculation and construction of a matrix from a token stream.

The matrix attempts to hog as little memory as possible by seeking no more than maximumShingleSize columns forward in the stream and clearing up unused resources (columns and unique token sets). Can still be optimized quite a bit though."
"LUCENE-3224","BUG","BUG","bugs in ByteArrayDataInput","ByteArrayDataInput has a byte[] ctor, but it doesn't actually work (some things like readVint will work, others will fail due to asserts).

The problem is it doesnt set things like limit in the ctor... I think the ctor should call reset()
Most code using this passes null to the ctor to initialize it, then uses reset(), instead they could just call ByteArrayInput(BytesRef.EMPTY_BYTES) if they want to do that.
finally, reset()'s limit looks like it should be offset + len"
"LUCENE-1657","IMPROVEMENT","IMPROVEMENT","Make ""boolean readOnly"" a required arg to IndexReader.open","Most apps don't need read/write IndexReader, and, a readOnly
IndexReader has better concurrent performance.

I'd love to simply default readOnly to true, and you'd have to specify
""false"" if you want a read/write reader (I think that's the natural
default), but I think that'd break too many back-compat cases.

So the workaround is to make the parameter explicit, in 2.9.

I think even for IndexSearcher's methods that open an IndexReader
under the hood, we should also make the parameter explicit.
"
"LUCENE-909","BUILD_SYSTEM","TASK","Demo targets for running the demo","Now that the demo build targets are working and build the jar/war, it may be useful for users to also be able to run the demo with something like 'ant run-demo'. This complements existing docs/demo.html."
"LUCENE-1836","BUG","BUG","Flexible QueryParser fails with local different from en_US","I get the following error during the mentioned testcases on my computer, if I use the Locale de_DE (windows 32):

{code}
    [junit] Testsuite: org.apache.lucene.queryParser.standard.TestQPHelper
    [junit] Tests run: 29, Failures: 1, Errors: 0, Time elapsed: 1,156 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] Result: (fieldX:xxxxx fieldy:xxxxxxxx)^2.0
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testLocalDateFormat(org.apache.lucene.queryParser.standard.TestQPHelper): FAILED
    [junit] expected:<1> but was:<0>
    [junit] junit.framework.AssertionFailedError: expected:<1> but was:<0>
    [junit]     at org.apache.lucene.queryParser.standard.TestQPHelper.assertHits(TestQPHelper.java:1148)
    [junit]     at org.apache.lucene.queryParser.standard.TestQPHelper.testLocalDateFormat(TestQPHelper.java:1005)
    [junit]     at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:201)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.queryParser.standard.TestQPHelper FAILED
    [junit] Testsuite: org.apache.lucene.queryParser.standard.TestQueryParserWrapper
    [junit] Tests run: 27, Failures: 1, Errors: 0, Time elapsed: 1,219 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] Result: (fieldX:xxxxx fieldy:xxxxxxxx)^2.0
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testLocalDateFormat(org.apache.lucene.queryParser.standard.TestQueryParserWrapper):       FAILED
    [junit] expected:<1> but was:<0>
    [junit] junit.framework.AssertionFailedError: expected:<1> but was:<0>
    [junit]     at org.apache.lucene.queryParser.standard.TestQueryParserWrapper.assertHits(TestQueryParserWrapper.java:1120)
    [junit]     at org.apache.lucene.queryParser.standard.TestQueryParserWrapper.testLocalDateFormat(TestQueryParserWrapper.java:985)
    [junit]     at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:201)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.queryParser.standard.TestQueryParserWrapper FAILED
{code}

With en_US as locale it works."
"LUCENE-406","RFE","RFE","sort missing string fields last","A SortComparatorSource for string fields that orders documents with the sort
field missing after documents with the field.  This is the reverse of the
default Lucene implementation.

The concept and first-pass implementation was done by Chris Hostetter."
"LUCENE-3796","DESIGN_DEFECT","BUG","Disallow setBoost() on StringField, throw exception if boosts are set if norms are omitted","Occasionally users are confused why index-time boosts are not applied to their norms-omitted fields.

This is because we silently discard the boost: there is no reason for this!

The most absurd part: in 4.0 you can make a StringField and call setBoost and nothing complains... (more reasons to remove StringField totally in my opinion)"
"LUCENE-2523","BUG","BUG","if index is too old you should hit an exception saying so","If you create an index in 2.3.x (I used demo's IndexFiles) and then try to read it in 4.0.x (I used CheckIndex), you hit a confusing exception like this:
{noformat}
java.io.IOException: read past EOF
        at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:154)
        at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)
        at org.apache.lucene.store.ChecksumIndexInput.readByte(ChecksumIndexInput.java:40)
        at org.apache.lucene.store.DataInput.readInt(DataInput.java:76)
        at org.apache.lucene.index.SegmentInfo.<init>(SegmentInfo.java:171)
        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:230)
        at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:269)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:649)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:484)
        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:265)
        at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:308)
        at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:287)
        at org.apache.lucene.index.CheckIndex.main(CheckIndex.java:930)
{noformat}

I think instead we should throw an IndexTooOldException or something like that?"
"LUCENE-3123","BUG","BUG","TestIndexWriter.testBackgroundOptimize fails with too many open files","Recreate with this line:

ant test -Dtestcase=TestIndexWriter -Dtestmethod=testBackgroundOptimize -Dtests.seed=-3981504507637360146:51354004663342240

Might be related to LUCENE-2873 ?"
"LUCENE-3414","RFE","RFE","Bring Hunspell for Lucene into analysis module","Some time ago I along with Robert and Uwe, wrote an Stemmer which uses the Hunspell algorithm.  It has the benefit of supporting dictionaries for a wide array of languages.   

It seems to still be being used but has fallen out of date.  I think it would benefit from being inside the analysis module where additional features such as decompounding support, could be added."
"LUCENE-3630","DESIGN_DEFECT","BUG","MultiReader and ParallelReader accidently override doOpenIfChanged(boolean readOnly) with doOpenIfChanged(boolean doClone)","I found this during adding deprecations for RW access in LUCENE-3606:

the base class defines doOpenIfChanged(boolean readOnly), but MultiReader and ParallelReader ""override"" this method with a signature doOpenIfChanged(doClone) and missing @Override. This makes consumers calling IR.openIfChanged(boolean readOnly) do the wrong thing. Instead they should get UOE like for the other unimplemented doOpenIfChanged methods in MR and PR.

Easy fix is to rename and hide this internal ""reopen"" method, like DirectoryReader,..."
"LUCENE-3896","TEST","BUG","CharTokenizer has bugs for large documents.","Initially found by hudson from additional testing added in LUCENE-3894, but 
currently not reproducable (see LUCENE-3895).

But its easy to reproduce for a simple single-threaded case in TestDuelingAnalyzers."
"LUCENE-1067","BUG","BUG","TestStressIndexing has intermittent failures","See http://www.gossamer-threads.com/lists/lucene/java-dev/55092 copied below:

 OK, I have seen this twice in the last two days:
Testsuite: org.apache.lucene.index.TestStressIndexing
[junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 18.58
sec
[junit]
[junit] ------------- Standard Output ---------------
[junit] java.lang.NullPointerException
[junit] at
org.apache.lucene.store.RAMInputStream.readByte(RAMInputStream.java:67)
[junit] at
org.apache.lucene.store.IndexInput.readInt(IndexInput.java:66)
[junit] at org.apache.lucene.index.SegmentInfos
$FindSegmentsFile.run(SegmentInfos.java:544)
[junit] at
org
.apache
.lucene.index.DirectoryIndexReader.open(DirectoryIndexReader.java:63)
[junit] at
org.apache.lucene.index.IndexReader.open(IndexReader.java:209)
[junit] at
org.apache.lucene.index.IndexReader.open(IndexReader.java:192)
[junit] at
org.apache.lucene.search.IndexSearcher.<init>(IndexSearcher.java:56)
[junit] at org.apache.lucene.index.TestStressIndexing
$SearcherThread.doWork(TestStressIndexing.java:111)
[junit] at org.apache.lucene.index.TestStressIndexing
$TimedThread.run(TestStressIndexing.java:55)
[junit] ------------- ---------------- ---------------
[junit] Testcase:
testStressIndexAndSearching
(org.apache.lucene.index.TestStressIndexing): FAILED
[junit] hit unexpected exception in search1
[junit] junit.framework.AssertionFailedError: hit unexpected
exception in search1
[junit] at
org
.apache
.lucene.index.TestStressIndexing.runStressTest(TestStressIndexing.java:
159)
[junit] at
org
.apache
.lucene
.index
.TestStressIndexing
.testStressIndexAndSearching(TestStressIndexing.java:187)
[junit]
[junit]
[junit] Test org.apache.lucene.index.TestStressIndexing FAILED

Subsequent runs have, however passed. Has anyone else hit this on
trunk?

I am running using ""ant clean test""

I'm on a Mac Pro 4 core, 4GB machine, if that helps at all. Not sure
how to reproduce at this point, but strikes me as a threading issue.
Oh joy!

I'll try to investigate more tomorrow to see if I can dream up a test
case.

-Grant 

"
"LUCENE-2246","BUG","BUG","While indexing Turkish web pages, ""Parse Aborted: Lexical error...."" occurs","When I try to index Turkish page if there is a Turkish specific character in the HTML specific tag HTML parser gives ""Parse Aborted: Lexical error.on ... line"" error.
For this case ""<IMG SRC=""../images/head.jpg"" WIDTH=570 HEIGHT=47 BORDER=0 ALT="""">"" exception address """" character (which has 351 ascii value) as an error. OR  character in title tag.
<a title=""()"">

Turkish character in the content do not create any problem."
"LUCENE-1789","IMPROVEMENT","IMPROVEMENT","getDocValues should provide a MultiReader DocValues abstraction","When scoring a ValueSourceQuery, the scoring code calls ValueSource.getValues(reader) on *each* leaf level subreader -- so DocValue instances are backed by the individual FieldCache entries of the subreaders -- but if Client code were to inadvertently  called getValues() on a MultiReader (or DirectoryReader) they would wind up using the ""outer"" FieldCache.

Since getValues(IndexReader) returns DocValues, we have an advantage here that we don't have with FieldCache API (which is required to provide direct array access). getValues(IndexReader) could be implimented so that *IF* some a caller inadvertently passes in a reader with non-null subReaders, getValues could generate a DocValues instance for each of the subReaders, and then wrap them in a composite ""MultiDocValues"".


"
"LUCENE-2755","IMPROVEMENT","IMPROVEMENT","Some improvements to CMS","While running optimize on a large index, I've noticed several things that got me to read CMS code more carefully, and find these issues:

* CMS may hold onto a merge if maxMergeCount is hit. That results in the MergeThreads taking merges from the IndexWriter until they are exhausted, and only then that blocked merge will run. I think it's unnecessary that that merge will be blocked.

* CMS sorts merges by segments size, doc-based and not bytes-based. Since the default MP is LogByteSizeMP, and I hardly believe people care about doc-based size segments anymore, I think we should switch the default impl. There are two ways to make it extensible, if we want:
** Have an overridable member/method in CMS that you can extend and override - easy.
** Have OneMerge be comparable and let the MP determine the order (e.g. by bytes, docs, calibrate deletes etc.). Better, but will need to tap into several places in the code, so more risky and complicated.

On the go, I'd like to add some documentation to CMS - it's not very easy to read and follow.

I'll work on a patch."
"LUCENE-610","BUG","BUG","BooleanScorer2 does not compile with ecj","BooleanScorer2, derived from scorer, has two inner classes both derived, ultimately, from Scorer.
As such they all define doc() or inherit it.
ecj produces an error when doc() is called from score in the inner classes in the methods
        countingDisjunctionSumScorer
    and
        countingConjunctionSumScorer

The error message is:
    The method doc is defined in an inherited type and in an enclosing scope.

The affected lines are: 160, 161, 178, and 179


I have run the junit test TestBoolean2 (as well as all the others) with
        doc()
    changed to
        BooleanScorer2.this.doc()
    and also to:
        this.doc();
The result was that the tests passed for both.

I added debug statements to all the doc methods and the score methods in the affected classes, but I could not determine what it should be.
"
"LUCENE-3520","BUG","BUG","If the NRT reader hasn't changed then IndexReader.openIfChanged should return null","I hit a failure in TestSearcherManager (NOTE: doesn't always fail):

{noformat}
  ant test -Dtestcase=TestSearcherManager -Dtestmethod=testSearcherManager -Dtests.seed=459ac99a4256789c:-29b8a7f52497c3b4:145ae632ae9e1ecf
{noformat}

It was tripping the assert inside SearcherLifetimeManager.record,
because two different IndexSearcher instances had different IR
instances sharing the same version.  This was happening because
IW.getReader always returns a new reader even when there are no
changes.  I think we should fix that...

Separately I found a deadlock in
TestSearcherManager.testIntermediateClose, if the test gets
SerialMergeScheduler and needs to merge during the second commit.
"
"LUCENE-2486","BUG","BUG","when opening the merged SegmentReader, IW attempts to open store files that were deleted","The issue happens when a merge runs that does not merge the doc stores, those doc stores are still being written to, IW is using CFS, and while the merge is running the doc stores get closed and turned into a cfx file.

When we then try to open the reader (for warming), which as of LUCENE-2311 will now [correctly] open the doc stores, we hit FNFE because the SegmentInfo for the merge does not realize that the doc stores were turned into  a cfx.

This issue does affect trunk; if you crank up the #docs in the test, it happens consistently (I will tie this to _TestUtil.getRandomMultiplier!)."
"LUCENE-491","BUG","BUG","DateTools needs to use UTC for correct collation,","If your local timezone is Europe/London then the times Sun, 30 Oct 2005 00:00:00 +0000 and exactly one hour later are both converted to 200530010000 by DateTools.dateToString() with minute resolution.   The Linux date command is useful in seeing why:

    $ date --date ""Sun, 30 Oct 2005 00:00:00 +0000""
    Sun Oct 30 01:00:00 BST 2005

    $ date --date ""Sun, 30 Oct 2005 01:00:00 +0000""
    Sun Oct 30 01:00:00 GMT 2005

Both times are 1am in the morning, but one is when DST is in force, the other isn't.   Of course, these are actually different times!

Of course, if dates are stored in the index with implicit timezone information then not only do we get problems when the clocks go back at the end of summer, but we also have problems crossing timezones.   If a database is created in California and used in Paris then the times are going to be badly skewed (there's a nine hour time difference most of the year).
"
"LUCENE-3487","BUG","BUG","TestBooleanMinShouldMatch test failure","ant test -Dtestcase=TestBooleanMinShouldMatch -Dtestmethod=testRandomQueries -Dtests.seed=505d62a62e9f90d0:-60daa428161b404b:-406411290a98f416

I think its an absolute/relative epsilon issue"
"LUCENE-2180","CLEANUP","TASK","Deprecated API called in o.a.l.store Directories","just ran into NIOFSDirectory and others still call getFile instead of getDirectory"
"LUCENE-2845","BUILD_SYSTEM","TASK","move contrib/benchmark to modules/benchmark","I think we should move lucene/contrib/benchmark to a shared modules/benchmark, so you can easily benchmark anything (lucene, solr, other modules like analysis or whatever).

For example, if you want to do some benchmarking of something in solr (LUCENE-2844) you should be able to do this.
Another example is simply being able to benchmark an analyzer definition from a schema.xml, its more convenient than writing the equivalent java analyzer just for benchmarking.

"
"LUCENE-3551","BUG","BUG","Yet another race in IW#nrtIsCurrent","In IW#nrtIsCurrent looks like this:

{code}
  synchronized boolean nrtIsCurrent(SegmentInfos infos) {
    ensureOpen();
    return infos.version == segmentInfos.version && !docWriter.anyChanges() && !bufferedDeletesStream.any();
  }
{code}

* the version changes once we checkpoint the IW
* docWriter has changes if there are any docs in ram or any deletes in the delQueue
* bufferedDeletes contain all frozen del packages from the delQueue

yet, what happens is 1. we decrement the numDocsInRam in DWPT#doAfterFlush (which is executed during DWPT#flush) but before we checkpoint. 2. if we freeze deletes (empty the delQueue) we put them in the flushQueue to maintain the order.  This means they are not yet in the bufferedDeleteStream.

Bottom line, there is a window where we could see IW#nrtIsCurrent returning true if we check within this particular window. Phew, I am not 100% sure if that is the reason for our latest failure in SOLR-2861 but from what the logs look like this could be what happens. If we randomly hit low values for maxBufferedDocs & maxBufferedDeleteTerms this is absolutely possible."
"LUCENE-2066","TEST","IMPROVEMENT","Add Highlighter test for RegexQuery",""
"LUCENE-2107","BUILD_SYSTEM","TASK","Add contrib/fast-vector-highlighter to Maven central repo","I'm not at all familiar with the Lucene build/deployment process, but it would be very nice if releases of the fast vector highlighter were pushed to the maven central repository, as is done with other contrib modules.

(Issue filed at the request of Grant Ingersoll.)"
"LUCENE-2210","BUG","BUG","trectopicsreader doesn't properly read descriptions or narratives","TrecTopicsReader does not read these fields correctly, as demonstrated by the test case.
"
"LUCENE-1871","IMPROVEMENT","BUG","Highlighter wraps caching token filters that are not CachingTokenFilter in CachingTokenFilter","I figured this was fine and a rare case that you would have another caching tokenstream to feed the highlighter with - but I guess if its happening to you, especially depending on what you are doing - its not an ideal situation."
"LUCENE-1685","IMPROVEMENT","IMPROVEMENT","Make the Highlighter use SpanScorer by default","I've always thought this made sense, but frankly, it took me a year to get the SpanScorer included with Lucene at all, so I was pretty much ready to move on after I it got in, rather than push for it as a default.

I think it makes sense as the default in Solr as well, and I mentioned that back when it was put in, but alas, its an option there as well.

The Highlighter package has no back compat req, but custom has been conservative - one reason I havn't pushed for this change before. Might be best to actually make the switch in 3? I could go either way - as is, I know a bunch of people use it, but I'm betting its the large minority. It has never been listed in a changes entry and its not in LIA 1, so you pretty much have to stumble upon it, and figure out what its for.

I'll point out again that its just as fast as the standard scorer for any clause of a query that is not position sensitive. Position sensitive query clauses will obviously be somewhat slower to highlight, but that is because they will be highlighted correctly rather than ignoring position."
"LUCENE-3208","REFACTORING","BUG","Move Query.weight() to IndexSearcher as protected method","We had this issue several times, latest in LUCENE-3207.

The method Query.weight() was left in Query for backwards reasons in Lucene 2.9 when we changed Weight class. This method is only to be called on top-level queries - and this is done by IndexSearcher. This method is just a utility method, that has nothing to do with the query itsself (it just combines the createWeight method and calls the normalization afterwards). 

The problem we have is that any query that wraps other queries (like CustomScore, ConstantScore, Boolean) calls Query.weight() instead of Query.createWeight(), it will do normalization two times, leading to strange bugs.

For 3.3 I will make Query.weight() simply delegate to IndexSearcher's replacement method with a big deprecation warning, so user sees this. In IndexSearcher itsself the method will be protected to only be called by itsself or subclasses of IndexSearcher. Delegation for backwards is no problem, as protected is accessible by classes in same package.

I would suggest the method name to be IndexSearcher.createNormalizedWeight(Query q)"
"LUCENE-3326","BUG","BUG","MoreLikeThis reuses a reader after it has already closed it","MoreLikeThis has a fatal bug whereby it tries to reuse a reader for multiple fields:

{code}
    Map<String,Int> words = new HashMap<String,Int>();
    for (int i = 0; i < fieldNames.length; i++) {
        String fieldName = fieldNames[i];
        addTermFrequencies(r, words, fieldName);
    }
{code}

However, addTermFrequencies() is creating a TokenStream for this reader:

{code}
    TokenStream ts = analyzer.reusableTokenStream(fieldName, r);
    int tokenCount=0;
    // for every token
    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
    ts.reset();
    while (ts.incrementToken()) {
        /* body omitted */
    }
    ts.end();
    ts.close();
{code}

When it closes this analyser, it closes the underlying reader.  Then the second time around the loop, you get:

{noformat}
Caused by: java.io.IOException: Stream closed
	at sun.nio.cs.StreamDecoder.ensureOpen(StreamDecoder.java:27)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:128)
	at java.io.InputStreamReader.read(InputStreamReader.java:167)
	at com.acme.util.CompositeReader.read(CompositeReader.java:101)
	at org.apache.lucene.analysis.standard.StandardTokenizerImpl.zzRefill(StandardTokenizerImpl.java:803)
	at org.apache.lucene.analysis.standard.StandardTokenizerImpl.getNextToken(StandardTokenizerImpl.java:1010)
	at org.apache.lucene.analysis.standard.StandardTokenizer.incrementToken(StandardTokenizer.java:178)
	at org.apache.lucene.analysis.standard.StandardFilter.incrementTokenClassic(StandardFilter.java:61)
	at org.apache.lucene.analysis.standard.StandardFilter.incrementToken(StandardFilter.java:57)
	at com.acme.storage.index.analyser.NormaliseFilter.incrementToken(NormaliseFilter.java:51)
	at org.apache.lucene.analysis.LowerCaseFilter.incrementToken(LowerCaseFilter.java:60)
	at org.apache.lucene.search.similar.MoreLikeThis.addTermFrequencies(MoreLikeThis.java:931)
	at org.apache.lucene.search.similar.MoreLikeThis.retrieveTerms(MoreLikeThis.java:1003)
	at org.apache.lucene.search.similar.MoreLikeThis.retrieveInterestingTerms(MoreLikeThis.java:1036)
{noformat}

My first thought was that it seems like a ""ReaderFactory"" of sorts should be passed in so that a new Reader can be created for the second field (maybe the factory could be passed the field name, so that if someone wanted to pass a different reader to each, they could.)

Interestingly, the methods taking File and URL exhibit the same issue.  I'm not sure what to do about those (and we're not using them.)  The method taking File could open the file twice, but the method taking a URL probably shouldn't fetch the same URL twice.
"
"LUCENE-1045","BUG","BUG","SortField.AUTO doesn't work with long","This is actually the same as LUCENE-463 but I cannot find a way to re-open that issue. I'm attaching a test case by dragon-fly999 at hotmail com that shows the problem and a patch that seems to fix it.

The problem is that a long (as used for dates) cannot be parsed as an integer, and the next step is then to parse it as a float, which works but which is not correct. With the patch the following parsers are used in this order: int, long, float.
"
"LUCENE-2534","BUG","BUG","MultiTermsEnum over-shares between different Docs/AndPositionsEnum","Robert found this in working on LUCENE-2352.

MultiTermsEnum incorrectly shared sub-enums on two different invocation of .docs/AndPositionsEnum."
"LUCENE-1438","BUG","BUG","StandardTokenizer splits host names with hyphens into multiple tokens","
StandardTokenizer does not recognize host names with hyphens as a single HOST token. Specifically ""www.m-w.com"" is tokenized as ""www.m"" and ""w.com"", both of ""<HOST>"" type.

StandardTokenizer should instead output a single HOST token for ""www.m-w.com"", since hyphens are a legitimate character in DNS host names.

We've a local fix to the grammar file which also required us to significantly simplify the NUM type to get the behavior we needed for host names.

here's a junit test for the desired behavior;

	public void testWithHyphens() throws Exception {
		final String host = ""www.m-w.com"";
		final StandardTokenizer tokenizer = new StandardTokenizer(
				new StringReader(host));
		final Token token = new Token();
		tokenizer.next(token);
		assertEquals(""<HOST>"", token.type());
		assertEquals(""www.m-w.com"", token.term());
	}

"
"LUCENE-2838","RFE","IMPROVEMENT","ConstantScoreQuery should directly support wrapping Query and simply strip off scores","Especially in MultiTermQuery rewrite modes we often simply need to strip off scores from Queries and make them constant score. Currently the code to do this looks quite ugly: new ConstantScoreQuery(new QueryWrapperFilter(query))

As the name says, QueryWrapperFilter should make any other Query constant score, so why does it not take a Query as ctor param? This question was aldso asked quite often by my customers and is simply correct, if you think about it.

Looking closer into the code, it is clear that this would also speed up MTQs:
- One additional wrapping and method calls can be removed
- Maybe we can even deprecate QueryWrapperFilter in 3.1 now (it's now only used in tests and the use-case for this class is not really available) and LUCENE-2831 does not need the stupid hack to make Simon's assertions pass
- CSQ now supports out-of-order scoring and topLevel scoring, so a CSQ on top-level now directly feeds the Collector. For that a small trick is used: The score(Collector) calls are directly delegated and the scores are stripped by wrapping the setScorer() method in Collector

During that I found a visibility bug in Scorer (LUCENE-2839): The method ""boolean score(Collector collector, int max, int firstDocID)"" should be public not protected, as its not solely intended to be overridden by subclasses and is called from other classes, too! This leads to no compiler bugs as the other classes that calls it is mainly BooleanScorer(2) and thats in same package, but visibility is wrong. I will open an issue for that and fix it at least in trunk where we have no backwards-requirement."
"LUCENE-743","RFE","IMPROVEMENT","IndexReader.reopen()","This is Robert Engels' implementation of IndexReader.reopen() functionality, as a set of 3 new classes (this was easier for him to implement, but should probably be folded into the core, if this looks good).
"
"LUCENE-3081","IMPROVEMENT","IMPROVEMENT","Document Maven nightly builds, artifact generation, and using Maven to build Lucene/Solr","There should be documentation we can point people to when they ask how to use Maven with Lucene and Solr."
"LUCENE-3251","BUG","BUG","Directory#copy leaks file handles","Directory#copy doesn't close the target directories output stream if sourceDir.openInput(srcFile) throws an Exception. Before LUCENE-3218 Directory#copy wasn't used extensively so this wasn't likely to happen during tests. Today we had a failure on the 3.x branch that is likely caused by this bug:

{noformat}
[junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Testcase: testAddIndexesWithRollback(org.apache.lucene.index.TestAddIndexes):	Caused an ERROR
    [junit] MockDirectoryWrapper: cannot close: there are still open files: {_co.cfs=1}
    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_co.cfs=1}
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:483)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads.closeDir(TestAddIndexes.java:693)
    [junit] 	at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithRollback(TestAddIndexes.java:924)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1277)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1195)
    [junit] Caused by: java.lang.RuntimeException: unclosed IndexOutput: _co.cfs
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.addFileHandle(MockDirectoryWrapper.java:410)
    [junit] 	at org.apache.lucene.store.MockCompoundFileDirectoryWrapper.<init>(MockCompoundFileDirectoryWrapper.java:39)
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.createCompoundOutput(MockDirectoryWrapper.java:439)
    [junit] 	at org.apache.lucene.index.SegmentMerger.createCompoundFile(SegmentMerger.java:128)
    [junit] 	at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:3101)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:839)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:667)
    [junit] 
    [junit] 
    [junit] Tests run: 18, Failures: 0, Errors: 1, Time elapsed: 9.034 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] java.lang.IllegalStateException: CFS has pending open files
    [junit] 	at org.apache.lucene.store.CompoundFileWriter.close(CompoundFileWriter.java:143)
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.close(CompoundFileDirectory.java:181)
    [junit] 	at org.apache.lucene.store.DefaultCompoundFileDirectory.close(DefaultCompoundFileDirectory.java:58)
    [junit] 	at org.apache.lucene.store.MockCompoundFileDirectoryWrapper.close(MockCompoundFileDirectoryWrapper.java:55)
    [junit] 	at org.apache.lucene.index.SegmentMerger.createCompoundFile(SegmentMerger.java:139)
    [junit] 	at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:3101)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:839)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:667)
{noformat}"
"LUCENE-814","BUILD_SYSTEM","TASK","javacc on Win32 (cygwin) creates wrong line endings - fix them with 'ant replace'","""ant javacc"" in Windows/Cygwin generates files with wrong line endings (\r  or \r\n instead of *Nix's \n). 
I managed to get rid of those using    perl -p -e 's/(\r\n|\n|\r)/\n/g'
Some useful info on line ending issues is in http://en.wikipedia.org/wiki/Newline

After wasting some time to get rid of those, I modified javacc-QueryParser build.xml task to take care of that.
So now QueryParser files created with ""ant javacc"" are fixed (if required) to have \n as line ends.

Should probably do that also for the other javacc targets: javacc-HTMLParser and javacc-StandardAnalyzer(?)
"
"LUCENE-3822","TEST","BUG","Inner classes of FilterAtomicReader (trunk) / FilterIndexReader (3.x) do not override all methods to be filtered","This issue adds missing checks in the FilterReader test to also check overridden methods in the enum implementations (inner classes) similar to the checks added by Shai Erea."
"LUCENE-3144","BUG","BUG","FreqProxTermsWriter leaks file handles if exceptions are thrown during flush()","FreqProxTermsWriter leaks open file handles if exceptions are thrown during flush. Code needs to be protected by try-finally clauses."
"LUCENE-3599","REFACTORING","BUG","haversine() is broken / misdocumented","DistanceUtils.haversine() is coded in a way that is erroneous based on the documented order of the parameters.  The parameters are defined as (x1,y1,x2,y2,radius)  -- i.e. lon,lat order.  The code implementing the algorithm, however, is as if the meaning of x and y are transposed, which means that if you supply the arguments in lat,lon (y,x) order, you will get the correct behavior.  It turns out that all callers of this method do this!

FYI I found out about this bug since it is inherited code in LSP (lucene-spatial-playground) and I have been supplying parameters according to its documented order.  Apparently I shouldn't do that ;-)"
"LUCENE-664","DOCUMENTATION","IMPROVEMENT","[PATCH] small fixes to the new scoring.html doc","This is an awesome initiative.  We need more docs that cleanly explain the inner workings of Lucene in general... thanks Grant & Steve & others!

I have a few small initial proposed fixes, largely just adding some more description around the components of the formula.  But also a couple typos, another link out to Wikipedia, a missing closing ), etc.  I've only made it through the ""Understanding the Scoring Formula"" section so far."
"LUCENE-2103","DESIGN_DEFECT","BUG","NoLockFactory should have a private constructor","NoLockFactory documents in its javadocs that one should use the static getNoLockFactory() method. However the class does not declare a private empty constructor, which breaks its Singleton purpose. We cannot add the empty private constructor because that'd break break-compat (even though I think it's very low chance someone actually instantiates the class), therefore we'll add a @deprecated warning to the class about this, and add the method in 4.0. I personally prefer to add an empty constructor w/ the @deprecated method, but am fine either way.

Don't know if a patch is needed, as this is a trivial change. "
"LUCENE-2791","RFE","RFE","WindowsDirectory","We can use Windows' overlapped IO to do pread() and avoid the performance problems of SimpleFS/NIOFSDir.
"
"LUCENE-3515","IMPROVEMENT","BUG","Possible slowdown of indexing/merging on 3.x vs trunk","Opening an issue to pursue the possible slowdown Marc Sturlese uncovered."
"LUCENE-1355","BUG","BUG","Using the WeightedTerms option in the Highlighter can cause fragments to be supressed for indexes with deletes","An index with a few documents and many deletes can report a lower total docs than docFreq for a term - total docs will account for deletes while docFreq will not - this causes the idf to be negative and the fragment to score < 0."
"LUCENE-3020","TEST","TEST","better payload testing with mockanalyzer","MockAnalyzer currently always indexes some fixed-length payloads.

Instead it should decide for each field randomly (and remember it for that field):
* if the field should index no payloads at all
* field should index fixed length payloads
* field should index variable length payloads.
"
"LUCENE-2566","SPEC","BUG","+ - operators allow any amount of whitespace","As an example, (foo - bar) is treated like (foo -bar).
It seems like for +- to be treated as unary operators, they should be immediately followed by the operand."
"LUCENE-2463","IMPROVEMENT","IMPROVEMENT","Improve Greek analysis","* Changed tokenstreams to CharTermAttribute
* Moved stopwords out of private String[] to a txt file (for use by Solr, etc)
* Removed TODO / fixed unicode conformance of GreekLowerCaseFilter
* Reformatted touched files to normal indentation
* Added inflectional stemmer (Ntais algorithm)

all the changes are backwards compatible with Version."
"LUCENE-2481","IMPROVEMENT","IMPROVEMENT","Enhance SnapshotDeletionPolicy to allow taking multiple snapshots","A spin off from here: http://www.gossamer-threads.com/lists/lucene/java-dev/99161?do=post_view_threaded#99161

I will:
# Replace snapshot() with snapshot(String), so that one can name/identify the snapshot
# Add some supporting methods, like release(String), getSnapshots() etc.
# Some unit tests of course.

This is mostly written already - I want to contribute it. I've also written a PersistentSDP, which persists the snapshots on stable storage (a Lucene index in this case) to support opening an IW with existing snapshots already, so they don't get deleted. If it's interesting, I can contribute it as well.

Porting my patch to the new API. Should post it soon."
"LUCENE-2217","IMPROVEMENT","IMPROVEMENT","Remaining reallocation should use ArrayUtil.getNextSize()","See recent discussion on ArrayUtils.getNextSize()."
"LUCENE-2837","CLEANUP","IMPROVEMENT","Collapse Searcher/Searchable/IndexSearcher; remove contrib/remote; merge PMS into IndexSearcher","We've discussed cleaning up our *Searcher stack for some time... I
think we should try to do this before releasing 4.0.

So I'm attaching an initial patch which:

  * Removes Searcher, Searchable, absorbing all their methods into IndexSearcher

  * Removes contrib/remote

  * Removes MultiSearcher

  * Absorbs ParallelMultiSearcher into IndexSearcher (ie you can now
    pass useThreads=true, or a custom ES to the ctor)

The patch is rough -- I just ripped stuff out, did search/replace to
IndexSearcher, etc.  EG nothing is directly testing using threads with
IndexSearcher, but before committing I think we should add a
newSearcher to LuceneTestCase, which randomly chooses whether the
searcher uses threads, and cutover tests to use this instead of making
their own IndexSearcher.

I think MultiSearcher has a useful purpose, but as it is today it's
too low-level, eg it shouldn't be involved in rewriting queries: the
Query.combine method is scary.  Maybe in its place we make a higher
level class, with limited API, that's able to federate search across
multiple IndexSearchers?  It'd also be able to optionally use thread
per IndexSearcher.
"
"LUCENE-1927","BUILD_SYSTEM","BUG","Lucene-core 2.9.0 missing from Maven Central Repository","Sub-projects like lucene-demos, lucene-contrib, etc. exist in central, and depend on 2.9.0 of lucene-core. However, the lucene-core 2.9.0 artifact itself is missing."
"LUCENE-2270","BUG","BUG","queries with zero boosts don't work","Queries consisting of only zero boosts result in incorrect results."
"LUCENE-1971","CLEANUP","TASK","Remove deprecated RangeQuery classes","Remove deprecated RangeQuery classes"
"LUCENE-3185","BUG","BUG","NRTCachingDirectory.deleteFile always throws exception","Silly bug."
"LUCENE-1802","DOCUMENTATION","TASK","Un-deprecate QueryParser and remove documentation that says it will be replaced in 3.0","This looks like the consensus move at first blush. We can (of course) re-evaluate if things change."
"LUCENE-2601","RFE","IMPROVEMENT","Make getAttribute(Class attClass) Generic","org.apache.lucene.util.AttributeSource

current:
public Attribute getAttribute(Class attClass) {
    final Attribute att = (Attribute) this.attributes.get(attClass);
    if (att == null) {
      throw new IllegalArgumentException(""This AttributeSource does not have the attribute '"" + attClass.getName() + ""'."");
    }
    return att;
}
sample usage:
TermAttribute termAtt = (TermAttribute)ts.getAttribute(TermAttribute.class)


my improvment:
@SuppressWarnings(""unchecked"")
	public <T> T getAttribute2(Class<? extends Attribute> attClass) {
    final T att = (T) this.attributes.get(attClass);
    if (att == null) {
      throw new IllegalArgumentException(""This AttributeSource does not have the attribute '"" + attClass.getName() + ""'."");
    }
    return att;
 }
sample usage:
TermAttribute termAtt = ts.getAttribute(TermAttribute.class)"
"LUCENE-3082","RFE","RFE","Add tool to upgrade all segments of an index to last recent supported index format without optimizing","Currently if you want to upgrade an old index to the format of your current Lucene version, you have to optimize your index or use addIndexes(IndexReader...) [see LUCENE-2893] to copy to a new directory. The optimize() approach fails if your index is already optimized.

I propose to add a custom MergePolicy to upgrade all segments to the last format. This MergePolicy could simply also ignore all segments already up-to-date. All segments in prior formats would be merged to a new segment using another MergePolicy's optimize strategy.

This issue is different from LUCENE-2893, as it would only support upgrading indexes from previous Lucene versions in-place using the official path. Its a tool for the end user, not a developer tool.

This addition should also go to Lucene 3.x, as we need to make users with pre-3.0 indexes go the step through 3.x, else they would not be able to open their index with 4.0. With this tool in 3.x the users could safely upgrade their index without relying on optimize to work on already-optimized indexes."
"LUCENE-1200","BUG","BUG","IndexWriter.addIndexes* can deadlock in rare cases","In somewhat rare cases it's possible for addIndexes to deadlock
because it is a synchronized method.

Normally the merges that are necessary for addIndexes are done
serially (with the primary thread) because they involve segments from
an external directory.  However, if mergeFactor of these merges
complete then a merge becomes necessary for the merged segments, which
are not external, and so it can run in the background.  If too many BG
threads need to run (currently > 4) then the ""pause primary thread""
approach adopted in LUCENE-1164 will deadlock, because the addIndexes
method is holding a lock on IndexWriter.

This was appearing as a intermittant deadlock in the
TestIndexWriterMerging test case.

This issue is not present in 2.3 (it was caused by LUCENE-1164).

The solution is to shrink the scope of synchronization: don't
synchronize on the whole method & wrap synchronized(this) in the right
places inside the methods."
"LUCENE-3769","REFACTORING","IMPROVEMENT","Simplify NRTManager","NRTManager is hairy now, because the applyDeletes is separately passed
to ctor, passed to maybeReopen, passed to getSearcherManager, etc.

I think, instead, you should pass it only to the ctor, and if you have
some cases needing deletes and others not then you can make two
NRTManagers.  This should be no less efficient than we have today,
just simpler.

I think it will also enable NRTManager to subclass ThingyManager
(LUCENE-3761).
"
"LUCENE-2879","BUG","BUG","MultiPhraseQuery sums its own idf instead of Similarity.","MultiPhraseQuery is a generalized version of PhraseQuery, and computes IDF the same way by default (by summing across the terms).

The problem is it doesn't let the Similarity do this: PhraseQuery calls Similarity.idfExplain(Collection<Term> terms, IndexSearcher searcher),
but MultiPhraseQuery just sums itself, calling Similarity.idf(int, int) for each term.

"
"LUCENE-2365","BUG","BUG","Finding Newest Segment In Empty Index","While extending the index writer, I discovered that its newestSegment method does not check to see if there are any segments before accessing the segment infos vector. Specifically, if you call the IndexWriter#newestSegment method on a brand-new index which is essentially empty, then it throws an java.lang.ArrayIndexOutOfBoundsException exception.

The proposed fix is to return null if no segments exist, as shown below:

--- lucene/src/java/org/apache/lucene/index/IndexWriter.java	(revision 930788)
+++ lucene/src/java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -4587,7 +4587,7 @@
 
   // utility routines for tests
   SegmentInfo newestSegment() {
-    return segmentInfos.info(segmentInfos.size()-1);
+    return segmentInfos.size() > 0 ? segmentInfos.info(segmentInfos.size()-1) : null;
   }
"
"LUCENE-3731","REFACTORING","IMPROVEMENT","Create a analysis/uima module for UIMA based tokenizers/analyzers","As discussed in SOLR-3013 the UIMA Tokenizers/Analyzer should be refactored out in a separate module (modules/analysis/uima) as they can be used in plain Lucene. Then the solr/contrib/uima will contain only the related factories."
"LUCENE-2300","BUG","BUG","IndexWriter should never pool readers for external segments","EG when addIndexes is called, it enrolls external segment infos, which are then merged.  But merging will simply ask the pool for the readers, and if writer is pooling (NRT reader has been pooled) it incorrectly pools these readers.

It shouldn't break anything but it's a waste because these readers are only used for merging, once, and they are not opened by NRT reader."
"LUCENE-2963","IMPROVEMENT","IMPROVEMENT","Easier way to run benchmark","Move Benchmark.main to a more easily accessible method exec() that can be easily invoked by external programs.
"
"LUCENE-3017","RFE","IMPROVEMENT","FST should differentiate between final vs non-final stop nodes","I'm breaking out this one improvement from LUCENE-2948...

Currently, if a node has no outgoing edges (a ""stop node"") the FST
forcefully marks this as a final node, but it need not do this.  Ie,
whether that node is final or not should be orthogonal to whether it
has arcs leaving or not.
"
"LUCENE-2529","BUG","BUG","always apply position increment gap between values","I'm doing some fancy stuff with span queries that is very sensitive to term positions.  I discovered that the position increment gap on indexing is only applied between values when there are existing terms indexed for the document.  I suspect this logic wasn't deliberate, it's just how its always been for no particular reason.  I think it should always apply the gap between fields.  Reference DocInverterPerField.java line 82:

if (fieldState.length > 0)
          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);

This is checking fieldState.length.  I think the condition should simply be:  if (i > 0).
I don't think this change will affect anyone at all but it will certainly help me.  Presently, I can either change this line in Lucene, or I can put in a hack so that the first value for the document is some dummy value which is wasteful."
"LUCENE-2095","BUG","BUG","Document not guaranteed to be found after write and commit","after same email on developer list:
""I developed a stress test to assert that a new document containing a
specific term ""X"" is always found after a commit on the IndexWriter.
This works most of the time, but it fails under load in rare occasions.

I'm testing with 40 Threads, both with a SerialMergeScheduler and a
ConcurrentMergeScheduler, all sharing a common IndexWriter.
Attached testcase is using a RAMDirectory only, but I verified a
FSDirectory behaves in the same way so I don't believe it's the
Directory implementation or the MergeScheduler.
This test is slow, so I don't consider it a functional or unit test.
It might give false positives: it doesn't always fail, sorry I
couldn't find out how to make it more likely to happen, besides
scheduling it to run for a longer time.""

I tested this to affect versions 2.4.1 and 2.9.1;
"
"LUCENE-974","DOCUMENTATION","","Remove Author tags from code","Remove all author tags from the code."
"LUCENE-2140","RFE","IMPROVEMENT","TopTermsScoringBooleanQueryRewrite minscore","when using the TopTermsScoringBooleanQueryRewrite (LUCENE-2123), it would be nice if MultiTermQuery could set an attribute specifying the minimum required score once the Priority Queue is filled. 

This way, FilteredTermsEnums could adjust their behavior accordingly based on the minimal score needed to actually be a useful term (i.e. not just pass thru the pq)

An example is FuzzyTermsEnum: at some point the bottom of the priority queue contains words with edit distance of 1 and enumerating any further terms is simply a waste of time.
This is because terms are compared by score, then termtext. So in this case FuzzyTermsEnum could simply seek to the exact match, then end.

This behavior could be also generalized for all n, for a different impl of fuzzyquery where it is only looking in the term dictionary for words within edit distance of n' which is the lowest scoring term in the pq (they adjust their behavior during enumeration of the terms depending upon this attribute).

Other FilteredTermsEnums could make use of this minimal score in their own way, to drive the most efficient behavior so that they do not waste time enumerating useless terms.
"
"LUCENE-1406","RFE","RFE","new Arabic Analyzer (Apache license)","I've noticed there is no Arabic analyzer for Lucene, most likely because Tim Buckwalter's morphological dictionary is GPL.

However, it is not necessary  to have full morphological analysis engine for a quality arabic search. 
This implementation implements the light-8s algorithm present in the following paper: http://ciir.cs.umass.edu/pubfiles/ir-249.pdf

As you can see from the paper, improvement via this method over searching surface forms (as lucene currently does) is significant, with almost 100% improvement in average precision.

While I personally don't think all the choices were the best, and some easily improvements are still possible, the major motivation for implementing it exactly the way it is presented in the paper is that the algorithm is TREC-tested, so the precision/recall improvements to lucene are already documented.

For a stopword list, I used a list present at http://members.unine.ch/jacques.savoy/clef/index.html simply because the creator of this list documents the data as BSD-licensed.

This implementation (Analyzer) consists of above mentioned stopword list plus two filters:
 ArabicNormalizationFilter: performs orthographic normalization (such as hamza seated on alif, alif maksura, teh marbuta, removal of harakat, tatweel, etc)
 ArabicStemFilter: performs arabic light stemming

Both filters operate directly on termbuffer for maximum performance. There is no object creation in this Analyzer.

There are no external dependencies. I've indexed about half a billion words of arabic text and tested against that.

If there are any issues with this implementation I am willing to fix them. I use lucene on a daily basis and would like to give something back. Thanks.
"
"LUCENE-2954","CLEANUP","BUG","CheckIndex prints wrong version number on 3.1 indexes (and posibly also in trunk)","When you run CheckIndex on an index created/updated with 3.1, it prints about the SegmentInfos:

{noformat}
Segments file=segments_g19 numSegments=5 version=-11 [Lucene 1.3 or prior]
{noformat}

We should fix CheckIndex and also verify other cases where version numbers are printed out. In trunk the issue may be more complicated!"
"LUCENE-863","CLEANUP","TASK","Deprecate StandardBenchmarker and ""old"" benchmarker code in favor of the Task based approach","We should deprecate the StandardBechmarker code that was the start of the benchmark contribution in favor of the much easier to use/extend byTask benchmark code"
"LUCENE-3385","IMPROVEMENT","","EasySimilarity to interpret document length as float",""
"LUCENE-979","CLEANUP","IMPROVEMENT","Remove Deprecated Benchmarking Utilities from contrib/benchmark","The old Benchmark utilities in contrib/benchmark have been deprecated and should be removed in 2.9 of Lucene."
"LUCENE-1680","DESIGN_DEFECT","IMPROVEMENT","Make prefixLength accessible to PrefixTermEnum subclasses","PrefixTermEnum#difference() offers a way to influence scoring based on the difference between the prefix Term and a term in the enumeration. To effectively use this facility the length of the prefix should be accessible to subclasses. Currently the prefix term is private to PrefixTermEnum. I added a getter for the prefix length and made PrefixTermEnum#endEnum(), PrefixTermEnum#termCompare() final for consistency with other TermEnum subclasses.

Patch is attached.

Simon"
"LUCENE-1661","IMPROVEMENT","IMPROVEMENT","Change visibility of getComparator method in SortField from protected to public","Hi,

Currently I'm using SortField for the creation of FieldComparators, but I ran into an issue.
I cannot invoke SortField.getComparator(...) directly from my code, which forces me to use a  workaround. (subclass SortField and override the getComparator method with visiblity public)
I'm proposing to make this method public. Currently I do not see any problems changing the visibility to public, I do not know if there are any (and the reason why this method is currently protected)
I think that this is a cleaner solution then the workaround I used and also other developers can benefit from it. I will also attach a patch to this issue based on the code in the trunk (26th of May). place). 
Please let me know your thoughts about this.

Cheers,

Martijn

 "
"LUCENE-2218","IMPROVEMENT","IMPROVEMENT","ShingleFilter improvements","ShingleFilter should allow configuration of minimum shingle size (in addition to maximum shingle size), so that it's possible to (e.g.) output only trigrams instead of bigrams mixed with trigrams.  The token separator used in composing shingles should be configurable too."
"LUCENE-1057","BUG","BUG","indexing doesn't reset token state","IndexWriter (DocumentsWriter) forgets to reset the token state resulting in incorrect positionIncrements, payloads, and token types."
"LUCENE-2990","IMPROVEMENT","IMPROVEMENT","Improve ArrayUtil/CollectionUtil.*Sort() methods to early-reaturn on empty or one-element lists/arrays","It might be a good idea to make CollectionUtil or ArrayUtil return early if the passed-in list or array's length <= 1 because sorting is unneeded then. This improves maybe automaton or other places, as for empty or one-element lists no SorterTermplate is created."
"LUCENE-2739","REFACTORING","TEST","Refactor TestIndexWriter","TestIndexWriter is getting a bit unwieldy:
* 5,315 lines of code
* 116 test methods
* runtimes frequently between 1 and 2 minutes.

It starts to be pretty scary to merge changes because its so massive.

A lot of the tests arguably belong somewhere else (e.g. the addIndex* tests belong in TestAddIndexes)

But here is a start:
# Pulls out the *OnDiskFull tests into TestIndexWriterOnDiskFull
# Pulls out the multithreaded tests into TestIndexWriterWithThreads
"
"LUCENE-573","BUG","BUG","Escaped quotes inside a phrase cause a ParseException","QueryParser cannot handle escaped quotes when inside a phrase. Escaped quotes not in a phrase are not a problem. This can be added to TestQueryParser.testEscaped() to demonstrate the issue - the second assert throws an exception:

assertQueryEquals(""a \\\""b c\\\"" d"", a, ""a \""b c\"" d"");
assertQueryEquals(""\""a \\\""b c\\\"" d\"""", a, ""\""a \""b c\"" d\"""");

See also this thread:
http://www.nabble.com/ParseException-with-escaped-quotes-in-a-phrase-t1647115.html
"
"LUCENE-557","BUG","BUG","search vs explain - score discrepancies","I'm on a mission to demonstrate (and then hopefully fix) any inconsistencies between the score you get for a doc when executing a search, and the score you get when asking for an explanation of the query for that doc."
"LUCENE-1947","DOCUMENTATION","TASK","Snowball package contains BSD licensed code with ASL header","All classes in org.tartarus.snowball (but not in org.tartarus.snowball.ext) has for some reason been given an ASL header. These classes are licensed with BSD. Thus the ASL header should be removed. I suppose this a misstake or possible due to the ASL header automation tool."
"LUCENE-3100","BUG","BUG","IW.commit() writes but fails to fsync the N.fnx file","In making a unit test for NRTCachingDir (LUCENE-3092) I hit this surprising bug!

Because the new N.fnx file is written at the ""last minute"" along with the segments file, it's not included in the sis.files() that IW uses to figure out which files to sync.

This bug means one could call IW.commit(), successfully, return, and then the machine could crash and when it comes back up your index could be corrupted.

We should hopefully first fix TestCrash so that it hits this bug (maybe it needs more/better randomization?), then fix the bug...."
"LUCENE-3443","BACKPORT","IMPROVEMENT","Port 3.x FieldCache.getDocsWithField() to trunk","[Spinoff from LUCENE-3390]

I think the approach in 3.x for handling un-valued docs, and making it
possible to specify how such docs are sorted, is better than the
solution we have in trunk.

I like that FC has a dedicated method to get the Bits for docs with field
-- easy for apps to directly use.  And I like that the
bits have their own entry in the FC.

One downside is that it's 2 passes to get values and valid bits, but
I think we can fix this by passing optional bool to FC.getXXX methods
indicating you want the bits, and the populate the FC entry for the
missing bits as well.  (We can do that for 3.x and trunk). Then it's
single pass.
"
"LUCENE-2757","REFACTORING","IMPROVEMENT","Refactor RewriteMethods out of MultiTermQuery","Policeman work :-) - as usual"
"LUCENE-3461","RFE","BUG","Adding same IndexDocValuesField twice trips assert","Doc values fields are single-valued by design, ie a given field name can only occur once in the document.

But if you accidentally add it more than once, you get an assert error, which is spooky because if you run w/o asserts maybe something eviler happens.

I think we should explicitly check for this and throw clear exc since user could easily do this by accident."
"LUCENE-2168","BUG","BUG","benchmark alg can't handle negative relative priority","We can now set thread relative priority when we run BG tasks... but if you use a negative number it's parsing it as 0."
"LUCENE-1796","IMPROVEMENT","IMPROVEMENT","Speed up repeated TokenStream init"," by caching isMethodOverridden results"
"LUCENE-2064","IMPROVEMENT","IMPROVEMENT","Highlighter should support all MultiTermQuery subclasses without casts","In order to support MultiTermQuery subclasses the Highlighter component applies instanceof checks for concrete classes from the lucene core. This prevents classes like RegexQuery in contrib from being supported. Introducing dependencies on other contribs is not feasible just for being supported by the highlighter.

While the instanceof checks and subsequent casts might hopefully go somehow away  in the future but for supporting more multterm queries I have a alternative approach using a fake IndexReader that uses a RewriteMethod to force the MTQ to pass the field name to the given reader without doing any real work. It is easier to explain once you see the patch - I will upload shortly.
"
"LUCENE-711","IMPROVEMENT","IMPROVEMENT","BooleanWeight should size the weights Vector correctly","The weights field on BooleanWeight uses a Vector that will always be sized exactly the same as the outer class' clauses Vector, therefore can be sized correctly in the constructor. This is a trivial memory saving enhancement."
"LUCENE-3546","BUG","BUG","IW#nrtIsCurrent retruns true if changes are in del queue but not in bufferedDeleteStream yet","spinnoff from SOLR-2861 - since the delete queue is not necessarily applied entirely on each request there is a chance that there are changes in the delete queue but not yet in buffered deletes. this can prevent NRT readers from reopen when they should... this shows the problematic code:

{code}
Index: java/org/apache/lucene/index/IndexWriter.java
===================================================================
--- java/org/apache/lucene/index/IndexWriter.java	(revision 1195214)
+++ java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -4074,7 +4074,7 @@
   synchronized boolean nrtIsCurrent(SegmentInfos infos) {
     //System.out.println(""IW.nrtIsCurrent "" + (infos.version == segmentInfos.version && !docWriter.anyChanges() && !bufferedDeletesStream.any()));
     ensureOpen();
-    return infos.version == segmentInfos.version && !docWriter.anyChanges() && !bufferedDeletesStream.any();
+    return infos.version == segmentInfos.version && !docWriter.anyChanges() && !docWriter.deleteQueue.anyChanges();
   }
{code}"
"LUCENE-2006","IMPROVEMENT","IMPROVEMENT","Optimization for FieldDocSortedHitQueue","When updating core for generics,  I found the following as a optimization of FieldDocSortedHitQueue:

All FieldDoc values are Compareables (also the score or docid, if they
appear as SortField in a MultiSearcher or ParallelMultiSearcher). The code
of lessThan seems very ineffective, as it has a big switch statement on the
SortField type, then casts the value to the underlying numeric type Object,
calls Number.xxxValue() & co for it and then compares manually. As
j.l.Number is itself Comparable, I see no reason to do this. Just call
compareTo on the Comparable interface and we are happy. The big deal is that
it prevents casting and the two method calls xxxValue(), as Number.compareTo
works more efficient internally.

The only special cases are String sort, where the Locale may be used and the
score sorting which is backwards. But these are two if statements instead of
the whole switch.

I had not tested it now for performance, but in my opinion it should be
faster for MultiSearchers. All tests still pass (because they should).
"
"LUCENE-732","RFE","IMPROVEMENT","Support DateTools in QueryParser","The QueryParser currently uses the deprecated class DateField to create RangeQueries with date values. However, most users probably use DateTools to store date values in their indexes, because this is the recommended way since DateField has been deprecated. In that case RangeQueries with date values produced by the QueryParser won't work with those indexes.

This patch replaces the use of DateField in QueryParser by DateTools. Because DateTools can produce date values with different resolutions, this patch adds the following methods to QueryParser:

  /**
   * Sets the default date resolution used by RangeQueries for fields for which no
   * specific date resolutions has been set. Field specific resolutions can be set
   * with {@link #setDateResolution(String, DateTools.Resolution)}.
   *  
   * @param dateResolution the default date resolution to set
   */
  public void setDateResolution(DateTools.Resolution dateResolution);
  
  /**
   * Sets the date resolution used by RangeQueries for a specific field.
   *  
   * @param field field for which the date resolution is to be set 
   * @param dateResolution date resolution to set
   */
  public void setDateResolution(String fieldName, DateTools.Resolution dateResolution);

(I also added the corresponding getter methods).

Now the user can set a default date resolution used for all fields or, with the second method, field specific date resolutions.
The initial default resolution, which is used if the user does not set a different resolution, is DateTools.Resolution.DAY. 

Please let me know if you think we should use a different resolution as default.

I extended TestQueryParser to test this new feature.

All unit tests pass.
"
"LUCENE-1240","IMPROVEMENT","IMPROVEMENT","TermsFilter: reuse TermDocs","TermsFilter currently calls termDocs(Term) once per term in the TermsFilter.  If we sort the terms it's filtering on, this can be optimised to call termDocs() once and then skip(Term) once per term, which should significantly speed up this filter.
"
"LUCENE-1357","BUG","BUG","SpanScorer does not respect ConstantScoreRangeQuery setting","ConstantScoreRangeQuery is actually on and can't be disabled when it should default to off with the option to turn it on."
"LUCENE-2268","BUILD_SYSTEM","TEST","Add test to check maven artifacts and their poms","As release manager it is hard to find out if the maven artifacts work correct. It would be good to have an ant task that executes maven with a .pom file that requires all contrib/core artifacts (or one for each contrib) that ""downloads"" the artifacts from the local dist/maven folder and builds that test project. This would require maven to execute the build script. Also it should pass the ${version} ANT property to this pom.xml"
"LUCENE-3831","BUG","BUG","Passing a null fieldname to MemoryFields#terms in MemoryIndex throws a NPE","I found this when querying a MemoryIndex using a RegexpQuery wrapped by a SpanMultiTermQueryWrapper.  If the regexp doesn't match anything in the index, it gets rewritten to an empty SpanOrQuery with a null field value, which then triggers the NPE."
"LUCENE-485","IMPROVEMENT","IMPROVEMENT","IndexWriter.mergeSegments should not hold the commit lock while cleaning up.","Same happens in IndexWriter.addIndexes(IndexReader[] readers).

The commit lock should be obtained whenever the Index structure/version is read or written.  It should be kept for as short a period as possible.

The write lock is needed to make sure only one IndexWriter or IndexReader instance can update the index (multiple IndexReaders can of course use the index for searching).

The list of files that can be deleted is stored in the file ""deletable"".  It is only read or written by the IndexWriter instance that holds the write lock, so there's no need to have the commit lock to to update it.

On my production system deleting the obsolete segment files after a mergeSegments() happens can occasionally take several seconds(!) and the commit lock blocks the searcher machines from updating their IndexReader instance.
Even on a standalone machine, the time to update the segments file is about 3ms, the time to delete the obsolete segments about 30ms.
"
"LUCENE-2944","BUG","BUG","BytesRef reuse bugs in QueryParser and analysis.jsp","Some code uses BytesRef as if it were a ""String"", in this case consumers of TermToBytesRefAttribute.
The thing is, while our general implementation works on char[] and then populates the consumers BytesRef,
not all TermToBytesRefAttribute implementations do this, specifically ICU collation, it reuses the bytes and simply sets the pointers:
{noformat}
  @Override
  public int toBytesRef(BytesRef target) {
    collator.getRawCollationKey(toString(), key);
    target.bytes = key.bytes;
    target.offset = 0;
    target.length = key.size;
    return target.hashCode();
  }
{noformat}

Most of the blame falls on me as I added this to the queryparser in LUCENE-2514.

Attached is a patch so that these consumers re-use a 'spare' and copy the bytes when they are going to make a long lasting object such as a Term.
"
"LUCENE-2972","BUG","BUG","Intermittent failure in TestFieldCacheTermsFilter.testMissingTerms","Running tests in while(1) I hit this:

{noformat}

NOTE: reproduce with: ant test -Dtestcase=TestFieldCacheTermsFilter -Dtestmethod=testMissingTerms -Dtests.seed=-1046382732738729184:5855929314778232889

1) testMissingTerms(org.apache.lucene.search.TestFieldCacheTermsFilter)
java.lang.AssertionError: Must match 1 expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:91)
	at org.junit.Assert.failNotEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:126)
	at org.junit.Assert.assertEquals(Assert.java:470)
	at org.apache.lucene.search.TestFieldCacheTermsFilter.testMissingTerms(TestFieldCacheTermsFilter.java:63)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1214)
	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1146)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:157)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:136)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at org.junit.runner.JUnitCore.runMain(JUnitCore.java:98)
	at org.junit.runner.JUnitCore.runMainAndExit(JUnitCore.java:53)
	at org.junit.runner.JUnitCore.main(JUnitCore.java:45)
{noformat}

Unfortunately the seed doesn't [consistently] repro for me..."
"LUCENE-1610","DOCUMENTATION","IMPROVEMENT","Preserve whitespace in <code> sections in the Changes.html generated from CHANGES.txt by changes2html.pl","The Trunk section of CHANGES.txt sports use of a new feature: <code> sections, for the two mentions of LUCENE-1575.

This looks fine in the text rendering, but looks crappy in the HTML version, since changes2html.pl escapes HTML metacharacters to appear as-is in the HTML rendering, but the newlines in the code are converted to a single space. 

I think this should be fixed by modifying changes2html.pl to convert <code> and </code> into (unescaped) <code><pre> and </pre></code>, respectively, since just passing through <code> and </code>, without </?pre>, while changing the font to monospaced (nice), still collapses whitespace (not nice). 

See the java-dev thread that spawned this issue here: http://www.nabble.com/CHANGES.txt-td23102627.html"
"LUCENE-1222","BUG","BUG","IndexWriter.doAfterFlush not being called when there are no deletions flushed","It should be called when flushing either added docs or deletions.  The fix is trivial.  I'll commit shortly to trunk & 2.3.2."
"LUCENE-2834","IMPROVEMENT","BUG","don't spawn thread statically in FSDirectory on Mac OS X","on the Mac, creating the digester starts up a PKCS11 thread.

I do not think threads should be created statically (I have this same issue with TimeLimitedCollector and also FilterManager).

Uwe discussed removing this md5 digester, I don't care if we remove it or not, just as long as it doesn't create a thread,
and just as long as it doesn't use the system default locale."
"LUCENE-1795","IMPROVEMENT","BUG","New QueryParser should not allow leading wildcard by default","The current QueryParser disallows leading wildcard characters by default."
"LUCENE-3639","TEST","IMPROVEMENT","Add test case support for shard searching","New test case that helps stress test the APIs to support sharding...."
"LUCENE-339","RFE","IMPROVEMENT","date encoding limitation removing","currently there is some limitation to date encoding in lucene. I think it's 
because dates should preserve lexicografical ordering, i.e. if one date precedes 
another date then encoded values should keep same ordering.

I know that it can be difficult to integrate it into existing version but there 
is way to remove this limitation.
Date milliseconds can be encoded as unsigned values with prefix that indicates 
positive or negative value.

In more details:
I used hex encoding and prefix &#8216;p&#8217; and &#8216;n&#8217; for positive and negative values. I 
got following results:

Value -10000 is encoded with nffffffffffffd8f0, 
-100	- nffffffffffffff9c
0	- p0000000000000000
100	- p0000000000000064
10000	- p0000000000002710

This preserves ordering between values and theirs encoding.

Also hex encoding can be replaced with Character.MAX_RADIX encoding.

Part of code that do this work:
   final static char[] digits = {
	'0' , '1' , '2' , '3' , '4' , '5' ,
	'6' , '7' , '8' , '9' , 'a' , 'b' ,
	'c' , 'd' , 'e' , 'f' , 'g' , 'h' ,
	'i' , 'j' , 'k' , 'l' , 'm' , 'n' ,
	'o' , 'p' , 'q' , 'r' , 's' , 't' ,
	'u' , 'v' , 'w' , 'x' , 'y' , 'z'
    };


    char prefix;
    if (time >= 0) {
      prefix = 'p';
    } else {
      prefix = 'n';
    }

    char[] chars = new char[DATE_LEN + 1];
    int index = DATE_LEN;
    while (time != 0) {
      int b = (int) (time & 0x0F);
      chars[index--] = digits[b];
      time = time >>> 4;
    }

    while (index >= 0) {
      chars[index--] = '0';
    }
    chars[0] = prefix;

    return new String(chars);"
"LUCENE-762","IMPROVEMENT","RFE","[PATCH] Efficiently retrieve sizes of field values","Sometimes an application would like to know how large a document is before retrieving it.  This can be important for memory management or choosing between algorithms, especially in cases where documents might be very large.

This patch extends the existing FieldSelector mechanism with two new FieldSelectorResults:  SIZE and SIZE_AND_BREAK.  SIZE creates fields on the retrieved document that store field sizes instead of actual values.  SIZE_AND_BREAK is especially efficient if one field comprises the bulk of the document size (e.g., the body field) and can thus be used as a reasonable size approximation.
"
"LUCENE-2123","RFE","BUG","Move FuzzyQuery rewrite as separate RewriteMode into MTQ, was: Highlighter fails to highlight FuzzyQuery","As FuzzyQuery does not allow to change the rewrite mode, highlighter fails with UOE in flex since LUCENE-2110, because it changes the rewrite mode to Boolean query. The fix is: Allow MTQ to change rewrite method and make FUZZY_REWRITE public for that.

The rewrite mode will live in MTQ as TOP_TERMS_SCORING_BOOLEAN_REWRITE. Also the code will be refactored to make heavy reuse of term enumeration code and only plug in the PQ for filtering the top terms."
"LUCENE-369","RFE","BUG","Cannot use Lucene in an unsigned applet due to Java security restrictions","A few of the Lucene source files call System.getProperty and perform a couple of
other operations within static initializers that assume full Java 2 permissions.
This prevents Lucene from being used from an unsigned applet embedded in a
browser page, since the default security permissions for an applet will prohibit
reading of most properties.

I would suggest wrapping the initialization of the properties in try/catch
blocks. This does mean that a couple properties would need to be made non-final,
and in some cases, getter and setter methods might be desirable to allow the
applet programmer to change the property values at runtime (some variables are
public static and could be changed directly without accessors).

This problem occurs with the shipping 1.4.3 version as well as the latest (as of
 07-apr-2005) source code fetched from CVS.

Currently, the files that are affected are org.apache.lucene.index.IndexWriter,
org.apache.lucene.index.SegmentReader, org.apache.lucene.search.BooleanQuery,
and org.apache.lucene.store.FSDirectory.

I have modified versions of these files with some suggested changes, plus a
simple test applet and associated files that demonstrate the situation. The
sample applet can be launched in a browser either by double-clicking the file
locally or by putting it on a web server and launching it from an http URL. As
soon as I can figure out how to attach to a bug report, I'll do that.

P.S. This topic came up in August, 2004 in lucene dev mailing list but as far as
I can tell, has not yet been resolved."
"LUCENE-921","DOCUMENTATION","","IndexReader.FieldOption has incomplete Javadocs","IndexReader.FieldOption has no javadocs at all."
"LUCENE-1397","BUG","BUG","When BG merge hits an exception, optimize sometimes throws an IOException missing the root cause","
When IndexWriter.optimize() is called, ConcurrentMergeScheduler will
run the requested merges with background threads and optimize() will
wait for these merges to complete.

If a merge hits an exception, it records the root cause exception such
that optimize can then retrieve this root cause and throw its own
exception, with the root cause.

But there is a bug: sometimes, the fact that an exception occurred on
a merge is recorded, but the root cause is missing.  In this cause,
optimize() still throws an exception (correctly indicating that the
optimize() has not finished successfully), but it's not helpful
because it's missing the root cause.  You must then go find the root
cause in the JRE's stderr logs.

This has hit a few users on this lists, most recently:

  http://www.nabble.com/Background-merge-hit-exception-td19540409.html#a19540409

I found the isssue, and finally got a unit test to intermittently show
it.  It's a simple thread safety issue: in a finally clause in
IndexWriter.merge we record the fact that the merge hit an exception
before actually setting the root cause, and then only in
ConcurrentMergeScheduler's exception handler do we set the root
cause.  If the optimize thread is scheduled in between these two, it
can throw an exception missing its root cause.

The fix is straightforward.  I plan to commit to 2.4 & 2.9.
"
"LUCENE-1086","BUG","BUG","DocMakers setup for the ""docs.dir"" property fails when passing an absolute path.","setConfig in TrecDocMaker assumes docs.dir is a relative path. Therefore it create new File(workDir, docs.dir). However, if docs.dir is an absolute path, this works incorrectly and results in No txt files in dataDir exception."
"LUCENE-1156","RFE","BUG","Wikipedia Document Generation Changes","The EnwikiDocMaker currently produces a fair number of documents that are in the download, but are of dubious use in terms of both benchmarking and indexing.  

These issues are:

# Redirect (it currently only handles REDIRECT and redirect, but there are documents as Redirect
# Template files appear to be useless.  These are marked by the term Template: at the beginning of the body.  See for example: http://en.wikipedia.org/wiki/Template:=)
# Image only pages, as in http://en.wikipedia.org/wiki/Image:Sciencefieldnewark.jpg.jpg  These are about as useful as the Redirects and Templates
# Files pending deletion:  This one is a bit trickier to handle, but they are generally marked by ""Wikipedia:Votes for deletion"" or some variation of that depending where along it is in being deleted

I think I can implement this such that it is backward compatible, if there is such a need when it comes to the contrib/benchmark suite.



"
"LUCENE-790","OTHER","IMPROVEMENT","contrib/benchmark - few improvements and a bug fix","Benchmark byTask was slightly improved:

1. fixed a bug in the ""child-should-not-report"" mechanism. If a task sequence contained only simple tasks it worked as expected (i.e. child tasks did not report times/memory) but if a child was a task sequence, then its children would report - they should not - this was fixed, so this property is now ""penetrating/inherited"" all the way down.

2. doc size control now possible also for the Reuters doc maker. (allowing to index N docs of size C characters each.)

3. TrecDocMaker was added - it reads as input the .gz files used in Trec - e.g. .gov data - this can be handy to benchmark Lucene on these large collections.  Similar to the Reuters collection, the doc-maker scans the input directory for all the files and extracts documents from the files.  Here there are multiple documents in each input file. Unlike the Reuters collection, we cannot provide a 'loader' for these collections - they are available from http://trec.nist.gov - for research purposes.

4. a new BasicDocMaker abstract class handles most of doc-maker tasks, including creating docs with specific size, so adding new doc-makers for other data is now much simpler."
"LUCENE-2531","IMPROVEMENT","IMPROVEMENT","FieldComparator.TermOrdValComparator compares by value unnecessarily","Digging on LUCENE-2504, I noticed that TermOrdValComparator's compareBottom method falls back on compare-by-value when it needn't.

Specifically, if we know the current bottom ord ""matches"" the current segment, we can skip the value comparison when the ords are the same (ie, return 0) because the ords are exactly comparable.

This is hurting string sort perf especially for optimized indices (and also unoptimized indices), and especially for highly redundant (not many unique values) fields.  This affects all releases >= 2.9.x, but trunk is likely more severely affected since looking up a value is more costly."
"LUCENE-3057","BUG","BUG","LuceneTestCase#newFSDirectoryImpl misses to set LockFactory if ctor call throws exception","selckin reported on IRC that if you run ant test -Dtestcase=TestLockFactory -Dtestmethod=testNativeFSLockFactoryPrefix -Dtests.directory=FSDirectory the test fails. Since FSDirectory is an abstract class it can not be instantiated so our code falls back to FSDirector.open. yet we miss to set the given lockFactory though."
"LUCENE-1670","DOCUMENTATION","IMPROVEMENT","Cosmetic JavaDoc updates","I've taken the liberty of making a few cosmetic updates to various JavaDocs:

* MergePolicy (minor cosmetic change)
* LogMergePolicy (minor cosmetic change)
* IndexWriter (major cleanup in class description, changed anchors to JavaDoc links [now works in Eclipse], no content change)

Attached diff from SVN r780545.

I would appreciate if whomever goes over this can let me know if my issue parameter choices were correct (yeah, blame my OCD), and if there's a more practical/convenient way to send these in, please let me know :-)
"
"LUCENE-1852","TEST","BUG","Fix remaining localization test failures in lucene","see also LUCENE-1836 and LUCENE-1846

all tests should pass under different locales.
the fix is to run 'ant test' under different locales, look and fix problems, and use the LocalizedTestCase from LUCENE-1836 to keep them from coming back.

the same approach as LUCENE-1836 fixes the core queryparser, but I am running ant test under a few locales to look for more problems.
"
"LUCENE-2872","RFE","IMPROVEMENT","Terms dict should block-encode terms","With PrefixCodedTermsReader/Writer we now encode each term standalone,
ie its bytes, metadata, details for postings (frq/prox file pointers),
etc.

But, this is costly when something wants to visit many terms but pull
metadata for only few (eg respelling, certain MTQs).  This is
particularly costly for sep codec because it has more metadata to
store, per term.

So instead I think we should block-encode all terms between indexed
term, so that the metadata is stored ""column stride"" instead.  This
makes it faster to enum just terms.
"
"LUCENE-1619","IMPROVEMENT","IMPROVEMENT","TermAttribute.termLength() optimization","
   public int termLength() {
     initTermBuffer(); // This patch removes this method call 
     return termLength;
   }

I see no reason to initTermBuffer() in termLength()... all tests pass, but I could be wrong?

"
"LUCENE-3410","RFE","IMPROVEMENT","Make WordDelimiterFilter's instantiation more readable","Currently WordDelimiterFilter's constructor is:

{code}
public WordDelimiterFilter(TokenStream in,
	                             byte[] charTypeTable,
	                             int generateWordParts,
	                             int generateNumberParts,
	                             int catenateWords,
	                             int catenateNumbers,
	                             int catenateAll,
	                             int splitOnCaseChange,
	                             int preserveOriginal,
	                             int splitOnNumerics,
	                             int stemEnglishPossessive,
	                             CharArraySet protWords) {
{code}

which means its instantiation is an unreadable combination of 1s and 0s.  

We should improve this by either using a Builder, 'int flags' or an EnumSet."
"LUCENE-1955","DOCUMENTATION","BUG","Fix Hits deprecation notice","Just needs to be committed to 2.9 branch since hits is now removed."
"LUCENE-1042","SPEC","BUG","discrepancy in getTermFreqVector-methods ","getTermFreqVector(int, TermVectorMapper) never calls the mapper if there is no term vector, consitent with all the other getTermFreqVector methods that returns null. 

getTermFreqVector(int, String, TermVectorMapper) throws an IOException when a field does not contain the term vector.

My suggestion:

{code}
Index: src/java/org/apache/lucene/index/SegmentReader.java
===================================================================
--- src/java/org/apache/lucene/index/SegmentReader.java (revision 590149)
+++ src/java/org/apache/lucene/index/SegmentReader.java (working copy)
@@ -648,7 +648,7 @@
     ensureOpen();
     FieldInfo fi = fieldInfos.fieldInfo(field);
     if (fi == null || !fi.storeTermVector || termVectorsReaderOrig == null)
-      throw new IOException(""field does not contain term vectors"");
+      return; 
{code}"
"LUCENE-2352","TEST","TEST","Add a new TestBackwardsCompatibility index for flex backwards (a 3.0 one with also numeric fields)","In flex we change also the encode/decoder for numeric fields (NumericTokenSteam) using BytesRef and also the collation filters. We should add a test index from 3.0 that contains these fields and do some validation, that field contents did not change when read with flex."
"LUCENE-1287","RFE","RFE","Allow usage of HyphenationCompoundWordTokenFilter without dictionary","We should allow to use the HyphenationCompoundWordTokenFilter without a dictionary. This produces a lot of ""nonword"" tokens but might be useful sometimes."
"LUCENE-2400","IMPROVEMENT","IMPROVEMENT","ShingleFilter: don't output all-filler shingles/unigrams; also, convert from TermAttribute to CharTermAttribute","When the input token stream to ShingleFilter has position increments greater than one, filler tokens are inserted for each position for which there is no token in the input token stream.  As a result, unigrams (if configured) and shingles can be filler-only.  Filler-only output tokens make no sense - these should be removed.

Also, because TermAttribute has been deprecated in favor of CharTermAttribute, the patch will also convert TermAttribute usages to CharTermAttribute in ShingleFilter."
"LUCENE-3177","REFACTORING","IMPROVEMENT","Decouple indexer from Document/Field impls","I think we should define minimal iterator interfaces,
IndexableDocument/Field, that indexer requires to index documents.

Indexer would consume only these bare minimum interfaces, not the
concrete Document/Field/FieldType classes from oal.document package.

Then, the Document/Field/FieldType hierarchy is one concrete impl of
these interfaces. Apps are free to make their own impls as well.
Maybe eventually we make another impl that enforces a global schema,
eg factored out of Solr's impl.

I think this frees design pressure on our Document/Field/FieldType
hierarchy, ie, these classes are free to become concrete
fully-featured ""user-space"" classes with all sorts of friendly sugar
APIs for adding/removing fields, getting/setting values, types, etc.,
but they don't need substantial extensibility/hierarchy. Ie, the
extensibility point shifts to IndexableDocument/Field interface.

I think this means we can collapse the three classes we now have for a
Field (Fieldable/AbstracField/Field) down to a single concrete class
(well, except for LUCENE-2308 where we want to break out dedicated
classes for different field types...).
"
"LUCENE-582","IMPROVEMENT","IMPROVEMENT","Don't throw TooManyClauses exception","I wonder if it would make sense to fall back to a ConstantScoreQuery instead of throwing a TooManyClauses exception?"
"LUCENE-2184","BUG","BUG","CartesianPolyFilterBuilder doesn't properly account for which tiers actually exist in the index ","In the CartesianShapeFilterBuilder, there is logic that determines the ""best fit"" tier to create the Filter against.  However, it does not account for which fields actually exist in the index when doing so.  For instance, if you index tiers 1 through 10, but then choose a very small radius to restrict the space to, it will likely choose a tier like 15 or 16, which of course does not exist."
"LUCENE-638","RFE","BUG","Can't put non-index files (e.g. CVS, SVN directories) in a Lucene index directory","Lucene won't tolerate foreign files in its index directories.  This makes it impossible to keep an index in a CVS or Subversion repository.

For instance, this exception appears when creating a RAMDirectory from a java.io.File that contains a subdirectory called "".svn"".

java.io.FileNotFoundException: /home/local/ejj/ic/.caches/.search/.index/.svn
(Is a directory)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
        at
org.apache.lucene.store.FSIndexInput$Descriptor.<init>(FSDirectory.java:425)
        at org.apache.lucene.store.FSIndexInput.<init>(FSDirectory.java:434)
        at org.apache.lucene.store.FSDirectory.openInput(FSDirectory.java:324)
        at org.apache.lucene.store.RAMDirectory.<init>(RAMDirectory.java:61)
        at org.apache.lucene.store.RAMDirectory.<init>(RAMDirectory.java:86)"
"LUCENE-721","BUILD_SYSTEM","RFE","Code coverage reports","Hi all,

We should be able to measure the code coverage of our unit testcases. I believe it would be very helpful for the committers, if they could verify before committing a patch if it does not reduce the coverage. 

Furthermore people could take a look in the code coverage reports to figure out where work needs to be done, i. e. where additional testcases are neccessary. It would be nice if we could add a page to the Lucene website showing the report, generated by the nightly build. Maybe you could add that to your preview page (LUCENE-707), Grant?

I attach a patch here that uses the tool EMMA to generate the code coverage reports. EMMA is a very nice open-source tool released under the CPL (same license as junit). The patch adds three targets to common-build.xml: 
- emma-check: verifys if both emma.jar and emma_ant.jar are in the ant classpath 
- emma-instrument: instruments the compiled code 
- generate-emma-report: generates an html code coverage report 

The following steps are neccessary in order to generate a code coverage report:
- add emma.jar and emma_ant.jar to your ant classpath (download emma from http://emma.sourceforge.net/)
- execute ant target 'emma-instrument' (depends on compile-test, so it will compile all core and test classes)
- execute ant target 'test' to run the unit tests
- execute ant target 'generate-emma-report'

To view the emma report open build/test/emma/index.html"
"LUCENE-2652","TEST","TEST","Rethink LocalizedTestCaseRunner with JUnit 4 - Clover OOM","As a spinn off from this [conversation|http://www.lucidimagination.com/search/document/ae20885bf5baedc5/build_failed_in_hudson_lucene_3_x_116#7ed351341152ee2d] we should rethink the way how we execute testcases with different locals since glover reports appears to throw OOM errors b/c Junit treats each local as a single test case run.

Here are some options:
* select the local at random only run the test with a single local
* set the local via system property -Dtest.locale=en.EN
* run with the default locale only -Dtest.skiplocale=true
* one from the above but only if instrumented with clover (let common tests run all the locale)

"
"LUCENE-3442","BUG","BUG","QueryWrapperFilter gets null DocIdSetIterator when wrapping TermQuery","If you try to get the iterator for the DocIdSet returned by a QueryWrapperFilter which wraps a TermQuery you get null instead of an iterator that returns the same documents as the search on the TermQuery.

Code demonstrating the issue:

{code:java}
import java.io.IOException;
import org.apache.lucene.analysis.WhitespaceAnalyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.document.Field;
import org.apache.lucene.document.Field.Index;
import org.apache.lucene.document.Field.Store;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.index.IndexWriterConfig;
import org.apache.lucene.index.Term;
import org.apache.lucene.store.RAMDirectory;
import org.apache.lucene.util.Version;
import org.apache.lucene.search.DocIdSet;
import org.apache.lucene.search.DocIdSetIterator;
import org.apache.lucene.search.Filter;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.QueryWrapperFilter;
import org.apache.lucene.search.TermQuery;
import org.apache.lucene.search.TopDocs;

public class TestQueryWrapperFilterIterator {
   public static void main(String[] args) {
		try {
			IndexWriterConfig iwconfig = new IndexWriterConfig(Version.LUCENE_34, new WhitespaceAnalyzer(Version.LUCENE_34));
			iwconfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE);
			RAMDirectory dir = new RAMDirectory();
		
			IndexWriter writer = new IndexWriter(dir, iwconfig);
			Document d = new Document();
			d.add(new Field(""id"", ""1001"", Store.YES, Index.NOT_ANALYZED));
			d.add(new Field(""text"", ""headline one group one"", Store.YES, Index.ANALYZED));
			d.add(new Field(""group"", ""grp1"", Store.YES, Index.NOT_ANALYZED));
		    writer.addDocument(d);
			writer.commit();
			writer.close();
			
			IndexReader rdr = IndexReader.open(dir);
			IndexSearcher searcher = new IndexSearcher(rdr);
			
			TermQuery tq = new TermQuery(new Term(""text"", ""headline""));
			
			TopDocs results = searcher.search(tq, 5);
			System.out.println(""Number of search results: "" + results.totalHits);
			
			Filter f = new QueryWrapperFilter(tq);
			
			DocIdSet dis = f.getDocIdSet(rdr);
			
			DocIdSetIterator it = dis.iterator();
			if (it != null) {
				int docId = it.nextDoc();
				while (docId != DocIdSetIterator.NO_MORE_DOCS) {
					Document doc = rdr.document(docId);
					System.out.println(""Iterator doc: "" + doc.get(""id""));
					docId = it.nextDoc();
				}
			} else {
				System.out.println(""Iterator was null: "");
			}
			
			searcher.close();
			rdr.close();
		} catch (IOException ioe) {
			ioe.printStackTrace();
		}

	}
}
{code}"
"LUCENE-2490","BUILD_SYSTEM","TASK","'ant generate-maven-artifacts' should work for lucene+solr 3.x+","The maven build scripts need to be updated so that solr uses the artifacts from lucene.

For consistency, we should be able to have a different 'maven_version' then the 'version'  That is, we want to build: 3.1-SNAPSHOT with a jar file: 3.1-dev"
"LUCENE-1893","DESIGN_DEFECT","BUG","All spatial contrib shape classes implement equals but not hashCode","violates contract - at a min, need to implement return constant."
"LUCENE-868","RFE","RFE","Making Term Vectors more accessible","One of the big issues with term vector usage is that the information is loaded into parallel arrays as it is loaded, which are then often times manipulated again to use in the application (for instance, they are sorted by frequency).

Adding a callback mechanism that allows the vector loading to be handled by the application would make this a lot more efficient.

I propose to add to IndexReader:
abstract public void getTermFreqVector(int docNumber, String field, TermVectorMapper mapper) throws IOException;
and a similar one for the all fields version

Where TermVectorMapper is an interface with a single method:
void map(String term, int frequency, int offset, int position);

The TermVectorReader will be modified to just call the TermVectorMapper.  The existing getTermFreqVectors will be reimplemented to use an implementation of TermVectorMapper that creates the parallel arrays.  Additionally, some simple implementations that automatically sort vectors will also be created.

This is my first draft of this API and is subject to change.  I hope to have a patch soon.

See http://www.gossamer-threads.com/lists/lucene/java-user/48003?search_string=get%20the%20total%20term%20frequency;#48003 for related information."
"LUCENE-2736","BUG","BUG","Wrong implementation of DocIdSetIterator.advance ","Implementations of {{DocIdSetIterator}} behave differently when advanced is called. Taking the following test for {{OpenBitSet}}, {{DocIdBitSet}} and {{SortedVIntList}} only {{SortedVIntList}} passes the test:
{code:title=org.apache.lucene.search.TestDocIdSet.java|borderStyle=solid}
...
	public void testAdvanceWithOpenBitSet() throws IOException {
		DocIdSet idSet = new OpenBitSet( new long[] { 1121 }, 1 );  // bits 0, 5, 6, 10
		assertAdvance( idSet );
	}

	public void testAdvanceDocIdBitSet() throws IOException {
		BitSet bitSet = new BitSet();
		bitSet.set( 0 );
		bitSet.set( 5 );
		bitSet.set( 6 );
		bitSet.set( 10 );
		DocIdSet idSet = new DocIdBitSet(bitSet);
		assertAdvance( idSet );
	}

	public void testAdvanceWithSortedVIntList() throws IOException {
		DocIdSet idSet = new SortedVIntList( 0, 5, 6, 10 );
		assertAdvance( idSet );
	}	

	private void assertAdvance(DocIdSet idSet) throws IOException {
		DocIdSetIterator iter = idSet.iterator();
		int docId = iter.nextDoc();
		assertEquals( ""First doc id should be 0"", 0, docId );

		docId = iter.nextDoc();
		assertEquals( ""Second doc id should be 5"", 5, docId );

		docId = iter.advance( 5 );
		assertEquals( ""Advancing iterator should return the next doc id"", 6, docId );
	}
{code}

The javadoc for {{advance}} says:
{quote}
Advances to the first *beyond* the current whose document number is greater than or equal to _target_.
{quote}
This seems to indicate that {{SortedVIntList}} behaves correctly, whereas the other two don't. 
Just looking at the {{DocIdBitSet}} implementation advance is implemented as:
{code}
bitSet.nextSetBit(target);
{code}
where the docs of {{nextSetBit}} say:
{quote}
Returns the index of the first bit that is set to true that occurs *on or after* the specified starting index
{quote}
"
"LUCENE-1595","REFACTORING","IMPROVEMENT","Split DocMaker into ContentSource and DocMaker","This issue proposes some refactoring to the benchmark package. Today, DocMaker has two roles: collecting documents from a collection and preparing a Document object. These two should actually be split up to ContentSource and DocMaker, which will use a ContentSource instance.

ContentSource will implement all the methods of DocMaker, like getNextDocData, raw size in bytes tracking etc. This can actually fit well w/ 1591, by having a basic ContentSource that offers input stream services, and wraps a file (for example) with a bzip or gzip streams etc.

DocMaker will implement the makeDocument methods, reusing DocState etc.

The idea is that collecting the Enwiki documents, for example, should be the same whether I create documents using DocState, add payloads or index additional metadata. Same goes for Trec and Reuters collections, as well as LineDocMaker.
In fact, if one inspects EnwikiDocMaker and LineDocMaker closely, they are 99% the same and 99% different. Most of their differences lie in the way they read the data, while most of the similarity lies in the way they create documents (using DocState).
That led to a somehwat bizzare extension of LineDocMaker by EnwikiDocMaker (just the reuse of DocState). Also, other DocMakers do not use that DocState today, something they could have gotten for free with this refactoring proposed.

So by having a EnwikiContentSource, ReutersContentSource and others (TREC, Line, Simple), I can write several DocMakers, such as DocStateMaker, ConfigurableDocMaker (one which accpets all kinds of config options) and custom DocMakers (payload, facets, sorting), passing to them a ContentSource instance and reuse the same DocMaking algorithm with many content sources, as well as the same ContentSource algorithm with many DocMaker implementations.

This will also give us the opportunity to perf test content sources alone (i.e., compare bzip, gzip and regular input streams), w/o the overhead of creating a Document object.

I've already done so in my code environment (I extend the benchmark package for my application's purposes) and I like the flexibility I have. I think this can be a nice contribution to the benchmark package, which can result in some code cleanup as well."
"LUCENE-1545","BUG","BUG","Standard analyzer does not correctly tokenize combining character U+0364 COMBINING LATIN SMALL LETTRE E","Standard analyzer does not correctly tokenize combining character U+0364 COMBINING LATIN SMALL LETTRE E.
The word ""mochte"" is incorrectly tokenized into ""mo"" ""chte"", the combining character is lost.
Expected result is only on token ""mochte""."
"LUCENE-3586","RFE","IMPROVEMENT","Choose a specific Directory implementation running the CheckIndex main","It should be possible to choose a specific Directory implementation to use during the CheckIndex process when we run it from its main.
What about an additional main parameter?
In fact, I'm experiencing some problems with MMapDirectory working with a big segment, and after some failed attempts playing with maxChunkSize, I decided to switch to another FSDirectory implementation but I needed to do that on my own main.
Should we also consider to use a FileSwitchDirectory?
I'm willing to contribute, could you please let me know your thoughts about it?"
"LUCENE-384","BUG","BUG","Range Query works only with lower case terms","I am performing a range query that returns results if the terms are lower 
case, but does not return result when the terms are mixed case.

In my collection, I have terms alpha, beta, delta, gamma.  I am using the 
StandardAnalyzer for both indexing and searching.

The query [alpha TO gamma] returns all four terms.  When I perform the query 
[Alpha TO Gamma], no results are returned.

It appears the lowerCaseFilter(), which is a part of the StandardAnalyzer, 
does not work properly on the search terms.  I've used Luke to peek at my 
collection, and the terms are all lower case in the collection.

I'm fairly new to Lucene, so I hope I'm not making a ""common mistake""."
"LUCENE-2323","REFACTORING","IMPROVEMENT","reorganize contrib modules","it would be nice to reorganize contrib modules, so that they are bundled together by functionality.

For example:
* the wikipedia contrib is a tokenizer, i think really belongs in contrib/analyzers
* there are two highlighters, i think could be one highlighters package.
* there are many queryparsers and queries in different places in contrib
"
"LUCENE-2057","DESIGN_DEFECT","BUG","TopDocsCollector should have bounded generic <T extends ScoreDoc>","TopDocsCollector was changed to be TopDocsCollector<T>. However it has methods which specifically assume the PQ stores ScoreDoc. Therefore, if someone extends it and defines a type which is not ScoreDoc, things will break.

We shouldn't put <T> on TopDocsCollector at all, but rather change its ctor to *protected TopDocsCollector(PriorityQueue<? extends ScoreDoc> pq)*. TopDocsCollector should handle ScoreDoc types. If we do this, we'll need to change FieldValueHitQueue's Entry to extend ScoreDoc as well."
"LUCENE-423","IMPROVEMENT","IMPROVEMENT","thread pool implementation of parallel queries","This component is a replacement for ParallelMultiQuery that runs a thread pool
with queue instead of starting threads for every query execution (so its
performance is better)."
"LUCENE-2694","IMPROVEMENT","IMPROVEMENT","MTQ rewrite + weight/scorer init should be single pass","Spinoff of LUCENE-2690 (see the hacked patch on that issue)...

Once we fix MTQ rewrite to be per-segment, we should take it further and make weight/scorer init also run in the same single pass as rewrite."
"LUCENE-3289","RFE","IMPROVEMENT","FST should allow controlling how hard builder tries to share suffixes","Today we have a boolean option to the FST builder telling it whether
it should share suffixes.

If you turn this off, building is much faster, uses much less RAM, and
the resulting FST is a prefix trie.  But, the FST is larger than it
needs to be.  When it's on, the builder maintains a node hash holding
every node seen so far in the FST -- this uses up RAM and slows things
down.

On a dataset that Elmer (see java-user thread ""Autocompletion on large
index"" on Jul 6 2011) provided (thank you!), which is 1.32 M titles
avg 67.3 chars per title, building with suffix sharing on took 22.5
seconds, required 1.25 GB heap, and produced 91.6 MB FST.  With suffix
sharing off, it was 8.2 seconds, 450 MB heap and 129 MB FST.

I think we should allow this boolean to be shade-of-gray instead:
usually, how well suffixes can share is a function of how far they are
from the end of the string, so, by adding a tunable N to only share
when suffix length < N, we can let caller make reasonable tradeoffs. 
"
"LUCENE-2939","IMPROVEMENT","BUG","Highlighter should try and use maxDocCharsToAnalyze in WeightedSpanTermExtractor when adding a new field to MemoryIndex as well as when using CachingTokenStream","huge documents can be drastically slower than need be because the entire field is added to the memory index
this cost can be greatly reduced in many cases if we try and respect maxDocCharsToAnalyze

things can be improved even further by respecting this setting with CachingTokenStream

"
"LUCENE-3676","RFE","IMPROVEMENT","Support SortedSource in MultiDocValues","MultiDocValues doesn't support Sorted variant ie. SortedSource but throws UnsupportedOperationException. This forces users to work per segment. For consistency we should support sortedsource also if we wrap the DocValues in MDV."
"LUCENE-3770","REFACTORING","","Rename FilterIndexReader to FilterAtomicReader","This class has to be renamed to be consistent with the new naming."
"LUCENE-3513","RFE","IMPROVEMENT","Add SimpleFragListBuilder constructor with margin parameter","{{SimpleFragListBuilder}} would benefit from an additional constructor that takes in {{margin}}. Currently, the margin is defined as a constant, so to ""implement"" a {{FragListBuilder}} with a different margin, one has no choice but to copy and paste {{SimpleFragListBuilder}} into a new class that must be placed in the {{org.apache.lucene.search.vectorhighlight}} package due to accesses of package-protected fields in other classes.

If this change were made, the precondition check of the constructor's {{fragCharSize}} should probably be altered to ensure that it's less than {{max(1, margin*3)}} to allow for a margin of 0."
"LUCENE-3901","RFE","RFE","Add katakana stem filter to better deal with certain katakana spelling variants","Many Japanese katakana words end in a long sound that is sometimes optional.

For example,  and  are both perfectly valid for ""party"".  Similarly we have  and  that are variants of ""center"" as well as  and  for ""server"".

I'm proposing that we add a katakana stemmer that removes this long sound if the terms are longer than a configurable length.  It's also possible to add the variant as a synonym, but I think stemming is preferred from a ranking point of view.

"
"LUCENE-1643","IMPROVEMENT","IMPROVEMENT","use reusable collation keys in ICUCollationFilter","ICUCollationFilter need not create a new CollationKey object for each token.
In ICU there is a mechanism to use a reusable key.
"
"LUCENE-1908","DOCUMENTATION","IMPROVEMENT","Similarity javadocs for scoring function to relate more tightly to scoring models in effect","See discussion in the related issue."
"LUCENE-2681","CLEANUP","BUG","fix generics violations in contrib/modules","There are some generics violations... we should fix them."
"LUCENE-2106","BUG","BUG","Benchmark does not close its Reader when OpenReader/CloseReader are not used","Only the Searcher is closed, but because the reader is passed to the Searcher, the Searcher does not close the Reader, causing a resource leak."
"LUCENE-1712","IMPROVEMENT","IMPROVEMENT","Set default precisionStep for NumericField and NumericRangeFilter","This is a spinoff from LUCENE-1701.

A user using Numeric* should not need to understand what's
""under the hood"" in order to do their indexing & searching.

They should be able to simply:
{code}
doc.add(new NumericField(""price"", 15.50);
{code}

And have a decent default precisionStep selected for them.

Actually, if we add ctors to NumericField for each of the supported
types (so the above code works), we can set the default per-type.  I
think we should do that?

4 for int and 6 for long was proposed as good defaults.

The default need not be ""perfect"", as advanced users can always
optimize their precisionStep, and for users experiencing slow
RangeQuery performance, NumericRangeQuery with any of the defaults we
are discussing will be much faster.
"
"LUCENE-418","RFE","BUG","[PATCH] Contribution: A QueryParser which passes wildcard and prefix queries to analyzer","Lucenes built-in QueryParser class does not analyze prefix nor wildcard queries.
Attached is a subclass which passes these queries to the analyzer as well."
"LUCENE-994","IMPROVEMENT","IMPROVEMENT","Change defaults in IndexWriter to maximize ""out of the box"" performance","This is follow-through from LUCENE-845, LUCENE-847 and LUCENE-870;
I'll commit this once those three are committed.

Out of the box performance of IndexWriter is maximized when flushing
by RAM instead of a fixed document count (the default today) because
documents can vary greatly in size.

Likewise, merging performance should be faster when merging by net
segment size since, to minimize the net IO cost of merging segments
over time, you want to merge segments of equal byte size.

Finally, ConcurrentMergeScheduler improves indexing speed
substantially (25% in a simple initial test in LUCENE-870) because it
runs the merges in the backround and doesn't block
add/update/deleteDocument calls.  Most machines have concurrency
between CPU and IO and so it makes sense to default to this
MergeScheduler.

Note that these changes will break users of ParallelReader because the
parallel indices will no longer have matching docIDs.  Such users need
to switch IndexWriter back to flushing by doc count, and switch the
MergePolicy back to LogDocMergePolicy.  It's likely also necessary to
switch the MergeScheduler back to SerialMergeScheduler to ensure
deterministic docID assignment.

I think the combination of these three default changes, plus other
performance improvements for indexing (LUCENE-966, LUCENE-843,
LUCENE-963, LUCENE-969, LUCENE-871, etc.) should make for some sizable
performance gains Lucene 2.3!"
"LUCENE-465","TEST","BUG","surround test code is incompatible with *Test pattern in test target.","Attachments to follow:
renamed BooleanQueryTest to BooleanQueryTst,
renamed ExceptionQueryTest to ExceptionQueryTst,
patch for the remainder of the test code to use the renamed classes."
"LUCENE-2153","RFE","BUG","IndexReader.open should take Codecs","Need to make this public... it's private now."
"LUCENE-318","DOCUMENTATION","BUG","""System Properties"" doc lists ""lockDir"" instead of ""lockdir""","The ""System Properties"" documentation page states that the lock file directory
can be set with the system property ""org.apache.lucene.lockDir"".  However, as
implemented in org.apache.lucene.store.FSDirectory, line 56, the property name
is actually ""org.apache.lucene.lockdir"" (lower case ""d"" in ""lockdir""). 
Recommend changing documentation to match code."
"LUCENE-1771","IMPROVEMENT","BUG","Using explain may double ram reqs for fieldcaches when using ValueSourceQuery/CustomScoreQuery or for ConstantScoreQuerys that use a caching Filter.",""
"LUCENE-1140","BUG","BUG","NPE in StopFilter caused by StandardAnalyzer(boolean replaceInvalidAcronym) constructor","I think that I found a problem with the new code (https://issues.apache.org/jira/browse/LUCENE-1068).
Usage of the new constructor StandardAnalyzer(boolean replaceInvalidAcronym) causes NPE in
StopFilter:

java.lang.NullPointerException
        at org.apache.lucene.analysis.StopFilter.<init>(StopFilter.java:74)
        at org.apache.lucene.analysis.StopFilter.<init>(StopFilter.java:86)
        at
org.apache.lucene.analysis.standard.StandardAnalyzer.tokenStream(StandardAnalyzer.java:151)
        at
org.apache.lucene.queryParser.QueryParser.getFieldQuery(QueryParser.java:452)
        at
org.apache.lucene.queryParser.QueryParser.Term(QueryParser.java:1133)
        at
org.apache.lucene.queryParser.QueryParser.Clause(QueryParser.java:1020)
        at
org.apache.lucene.queryParser.QueryParser.Query(QueryParser.java:948)
        at
org.apache.lucene.queryParser.QueryParser.Clause(QueryParser.java:1024)
        at
org.apache.lucene.queryParser.QueryParser.Query(QueryParser.java:948)
        at
org.apache.lucene.queryParser.QueryParser.TopLevelQuery(QueryParser.java:937)
        at
org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:147)

The reason is that new constructor forgets to initialize the stopSet field:
  public StandardAnalyzer(boolean replaceInvalidAcronym) {
    this.replaceInvalidAcronym = replaceInvalidAcronym;
  }

I guess this should be changed to something like this:
  public StandardAnalyzer(boolean replaceInvalidAcronym) {
    this(STOP_WORDS);
    this.replaceInvalidAcronym = replaceInvalidAcronym;
  }

The bug is present in RC3. Fix is one line, it'll be great to have it in 2.3
release.
"
"LUCENE-2281","IMPROVEMENT","IMPROVEMENT","Add doBeforeFlush to IndexWriter","IndexWriter has doAfterFlush which can be overridden by extensions in order to perform operations after flush has been called. Since flush is final, one can only override doAfterFlush. This issue will handle two things:
# Make doAfterFlush protected, instead of package-private, to allow for easier extendability of IW.
# Add doBeforeFlush which will be called by flush before it starts, to allow extensions to perform any operations before flush begings.

Will post a patch shortly.

BTW, any chance to get it out in 3.0.1?"
"LUCENE-1556","BUG","BUG","some valid email address characters not correctly recognized","the EMAIL expression in StandardTokenizerImpl.jflex misses some unusual but valid characters in the left-hand-side of the email address. This causes an address to be broken into several tokens, for example:

somename+site@gmail.com gets broken into ""somename"" and ""site@gmail.com""
husband&wife@talktalk.net gets broken into ""husband"" and ""wife@talktalk.net""

These seem to be occurring more often. The first seems to be because of an anti-spam trick you can use with google (see: http://labnol.blogspot.com/2007/08/gmail-plus-smart-trick-to-find-block.html). I see the second in several domains but a disproportionate amount are from talktalk.net, so I expect it's a signup suggestion from the service.

Perhaps a fix would be to change line 102 of StandardTokenizerImpl.jflex from:
EMAIL      =  {ALPHANUM} (("".""|""-""|""_"") {ALPHANUM})* ""@"" {ALPHANUM} (("".""|""-"") {ALPHANUM})+

to 

EMAIL      =  {ALPHANUM} (("".""|""-""|""_""|""+""|""&"") {ALPHANUM})* ""@"" {ALPHANUM} (("".""|""-"") {ALPHANUM})+

I'm aware that the StandardTokenizer is meant to be more of a basic implementation rather than an implementation the full standard, but it is quite useful in places and hopefully this would improve it slightly."
"LUCENE-884","DOCUMENTATION","IMPROVEMENT","Query Syntax page does not make it clear that wildcard searches are not allowed in Phrase Queries","The queryparsersyntax page which is where I expect most novices (such as myself) start with lucene seems to indicate that wildcards can be used in phrase terms

Quoting:
'Terms: A query is broken up into terms and operators. There are two types of terms: Single Terms and Phrases.
A Single Term is a single word such as ""test"" or ""hello"".
A Phrase is a group of words surrounded by double quotes such as ""hello dolly"".

....

Wildcard Searches
Lucene supports single and multiple character wildcard searches.
To perform a multiple character wildcard search use the ""*"" symbol.
Multiple character wildcard searches looks for 0 or more characters. For example, to search for test, tests or tester, you can use the search:

test*
You can also use the wildcard searches in the middle of a term.

'
there is nothing to indicate in the section on Wildcard Searches that it can be performed only on Single word terms not Phrase terms.

Chris  argues 'that there is nothing in the description of a Phrase to indicate that it can be anything other then what it says ""a group of words surrounded by double quotes"" .. at no point does it
suggest that other types of queries or syntax can be used inside the quotes.  likewise the discussion of Wildcards makes no mention of phrases to suggest that wildcard characters can be used in a phrase.'
but I don't accept this because there is nothing in the description of a Single Term either to indicate it can use wildcards either. Wildcards are only mentioned in the Wildcard section and there it says thay can be used in a term, it does not restrict the type of term


I Propose a simple solution modify:

Lucene supports single and multiple character wildcard searches.

to 

Lucene supports single and multiple character wildcard searches within single terms.

(Chris asked for a patch, but Im not sure how to do this, but the change is simple enough)



"
"LUCENE-3801","IMPROVEMENT","IMPROVEMENT","Generify FST shortestPaths() to take a comparator","Not sure we should do this, it costs 5-10% performance for WFSTSuggester.
But maybe we can optimize something here, or maybe its just no big deal to us.

Because in general, this could be pretty powerful, e.g. if you needed to store 
some custom stuff in the suggester, you could use pairoutputs, or whatever.

And the possibility we might need shortestPaths for other cool things... at the
least I just wanted to have the patch up here.

I haven't tested this on pairoutputs... but i've tested it with e.g. FloatOutputs
and other things and it works fine.

I tried to minimize the generics violations, there is only 1 (cannot create generic array).
"
"LUCENE-1516","REFACTORING","IMPROVEMENT","Integrate IndexReader with IndexWriter ","The current problem is an IndexReader and IndexWriter cannot be open
at the same time and perform updates as they both require a write
lock to the index. While methods such as IW.deleteDocuments enables
deleting from IW, methods such as IR.deleteDocument(int doc) and
norms updating are not available from IW. This limits the
capabilities of performing updates to the index dynamically or in
realtime without closing the IW and opening an IR, deleting or
updating norms, flushing, then opening the IW again, a process which
can be detrimental to realtime updates. 

This patch will expose an IndexWriter.getReader method that returns
the currently flushed state of the index as a class that implements
IndexReader. The new IR implementation will differ from existing IR
implementations such as MultiSegmentReader in that flushing will
synchronize updates with IW in part by sharing the write lock. All
methods of IR will be usable including reopen and clone. 
"
"LUCENE-1938","RFE","RFE","Precedence query parser using the contrib/queryparser framework","Extend the current StandardQueryParser on contrib so it supports boolean precedence"
"LUCENE-2570","TEST","TEST","Some improvements to _TestUtil and its usage","I've started this issue because I've noticed that _TestUtil.getRandomMultiplier() is called from many loops' condition check, sometimes hundreds and thousands of times. Each time it does Integer.parseInt after calling System.getProperty. This really can become a constant IMO, either in LuceneTestCase(J4) or _TestUtil, as it's not expected to change while tests are running ...

I then reviewed the class and spotted some more things that I think can be fixed/improved:
# getTestCodec() can become a constant as well
# arrayToString is marked deprecated. I've checked an no one calls them, so I'll delete them. This is a 4.0 code branch + a test-only class. No need to deprecate anything.
# getTempDir calls new Random(), instead of newRandom() in LuceneTestCaseJ4, which means that if something fails, we won't know the random seed used ...
#* In that regard, we might want to output all the classes that obtained a static seed in reportAdditionalFailures(), instead of just the class that ran the test.
# rmDir(String) can be removed IMO, and leave only rmDir(File)
# I suggest we include some recursion in rmDir(File) to handle the deletion of nested directories.
#* Also, it does not check whether the dir deletion itself succeeds (but it does so for the files). This can bite us on Windows, if some test did not close things properly.

I'll work out a patch."
"LUCENE-3479","BUG","BUG","TestGrouping failure","{noformat}
ant test -Dtestcase=TestGrouping -Dtestmethod=testRandom -Dtests.seed=295cdb78b4a442d4:-4c5d64ef4d698c27:-425d4c1eb87211ba
{noformat}

fails with this on current trunk:

{noformat}

    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestGrouping -Dtestmethod=testRandom -Dtests.seed=295cdb78b4a442d4:-4c5d64ef4d698c27:-425d4c1eb87211ba
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=MockRandom, content=MockSep, sort2=SimpleText, groupend=Pulsing(freqCutoff=3 minBlockSize=65 maxBlockSize=132), sort1=Memory, group=Memory}, sim=RandomSimilarityProvider(queryNorm=true,coord=false): {id=DFR I(F)L2, content=DFR BeL3(800.0), sort2=DFR GL3(800.0), groupend=DFR G2, sort1=DFR GB3(800.0), group=LM Jelinek-Mercer(0.700000)}, locale=zh_TW, timezone=America/Indiana/Indianapolis
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestGrouping]
    [junit] NOTE: Linux 2.6.33.6-147.fc13.x86_64 amd64/Sun Microsystems Inc. 1.6.0_21 (64-bit)/cpus=24,threads=1,free=143246344,total=281804800
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testRandom(org.apache.lucene.search.grouping.TestGrouping):	FAILED
    [junit] expected:<11> but was:<7>
    [junit] junit.framework.AssertionFailedError: expected:<11> but was:<7>
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
    [junit] 	at org.apache.lucene.search.grouping.TestGrouping.assertEquals(TestGrouping.java:980)
    [junit] 	at org.apache.lucene.search.grouping.TestGrouping.testRandom(TestGrouping.java:865)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
    [junit] 
    [junit] 
{noformat}

I dug for a while... the test is a bit sneaky because it compares sorted docs (by score) across 2 indexes.  Index #1 has no deletions; Index #2 has same docs, but organized into doc blocks by group, and has some deletions.  In theory (I think) even though the deletions will cause scores to differ across the two indices, it should not alter the sort order of the docs.  Here is the explain output of the docs that sorted differently:

{noformat}
#1: top hit in the ""has deletes doc-block"" index (id=239):

explain: 2.394486 = (MATCH) weight(content:real1 in 292)
[DFRSimilarity], result of:
 2.394486 = score(DFRSimilarity, doc=292, freq=1.0), computed from:
   1.0 = termFreq=1
   41.944084 = NormalizationH3, computed from:
     1.0 = tf
     5.3102274 = avgFieldLength
     2.56 = len
   102.829 = BasicModelBE, computed from:
     41.944084 = tfn
     880.0 = numberOfDocuments
     239.0 = totalTermFreq
   0.023286095 = AfterEffectL, computed from:
     41.944084 = tfn


#2: hit in the ""no deletes normal index"" (id=229)

ID=229 explain=2.382285 = (MATCH) weight(content:real1 in 225)
[DFRSimilarity], result of:
 2.382285 = score(DFRSimilarity, doc=225, freq=1.0), computed from:
   1.0 = termFreq=1
   41.765594 = NormalizationH3, computed from:
     1.0 = tf
     5.3218827 = avgFieldLength
     10.24 = len
   101.879845 = BasicModelBE, computed from:
     41.765594 = tfn
     786.0 = numberOfDocuments
     215.0 = totalTermFreq
   0.023383282 = AfterEffectL, computed from:
     41.765594 = tfn

Then I went and called explain on the ""no deletes normal index"" for
the top doc (id=239):

explain: 2.3822558 = (MATCH) weight(content:real1 in 17)
[DFRSimilarity], result of:
 2.3822558 = score(DFRSimilarity, doc=17, freq=1.0), computed from:
   1.0 = termFreq=1
   42.165264 = NormalizationH3, computed from:
     1.0 = tf
     5.3218827 = avgFieldLength
     2.56 = len
   102.8307 = BasicModelBE, computed from:
     42.165264 = tfn
     786.0 = numberOfDocuments
     215.0 = totalTermFreq
   0.023166776 = AfterEffectL, computed from:
     42.165264 = tfn
{noformat}"
"LUCENE-2378","CLEANUP","IMPROVEMENT","Cutover remaining usage of pre-flex APIs","A number of places still use the pre-flex APIs.

This is actually healthy, since it gives us ongoing testing of the back compat emulation layer.

But we should at some point cut them all over to flex.  Latest we can do this is 4.0, but I'm not sure we should do them all for 3.1... still marking this as 3.1 to ""remind us"" :)"
"LUCENE-3256","REFACTORING","","Consolidate CustomScoreQuery, ValueSourceQuery and BoostedQuery ","Lucene's CustomScoreQuery and Solr's BoostedQuery do essentially the same thing: they boost the scores of Documents by the value from a ValueSource.  BoostedQuery does this in a direct fashion, by accepting a ValueSource. CustomScoreQuery on the other hand, accepts a series of ValueSourceQuerys.  ValueSourceQuery seems to do exactly the same thing as FunctionQuery.

With Lucene's ValueSource being deprecated / removed, we need to resolve these dependencies and simplify the code.

Therefore I recommend we do the following things:

- Move CustomScoreQuery (and CustomScoreProvider) to the new Queries module and change it over to use FunctionQuerys instead of ValueSourceQuerys.  
- Deprecate Solr's BoostedQuery in favour of the new CustomScoreQuery.  CSQ provides a lot of support for customizing the scoring process.
- Move and consolidate all tests of CSQ and BoostedQuery, to the Queries module and have them test CSQ instead."
"LUCENE-1745","RFE","IMPROVEMENT","Add ability to specify compilation/matching flags to RegexCapabiltiies implementations","The Jakarta Regexp and Java Util Regex packages both support the ability to provides flags that alter the matching behavior of a given regular expression. While the java.util.regex.Pattern implementation supports providing these flags as part of the regular expression string, the Jakarta Regexp implementation does not.  Therefore, this improvement request is to add the capability to provide those modification flags to either implementation. 

I've developed a working implementation that makes minor additions to the existing code. The default constructor is explicitly defined with no arguments, and then a new constructor with an additional ""int flags"" argument is provided. This provides complete backwards compatibility. For each RegexCapabilties implementation, the appropriate flags from the regular expression package is defined as  FLAGS_XXX static fields. These are pass through to the underlying implementation. They are re-defined to avoid bleeding the actual implementation classes into the caller namespace.

Proposed changes:

For the JavaUtilRegexCapabilities.java, the following is the changes made.

  private int flags = 0;
  
  // Define the optional flags from Pattern that can be used.
  // Do this here to keep Pattern contained within this class.
  
  public final int FLAG_CANON_EQ = Pattern.CANON_EQ;
  public final int FLAG_CASE_INSENSATIVE = Pattern.CASE_INSENSATIVE;
  public final int FLAG_COMMENTS = Pattern.COMMENTS;
  public final int FLAG_DOTALL = Pattern.DOTALL;
  public final int FLAG_LITERAL = Pattern.LITERAL;
  public final int FLAG_MULTILINE = Pattern.MULTILINE;
  public final int FLAG_UNICODE_CASE = Pattern.UNICODE_CASE;
  public final int FLAG_UNIX_LINES = Pattern.UNIX_LINES;
  
  /**
   * Default constructor that uses java.util.regex.Pattern 
   * with its default flags.
   */
  public JavaUtilRegexCapabilities()  {
    this.flags = 0;
  }
  
  /**
   * Constructor that allows for the modification of the flags that
   * the java.util.regex.Pattern will use to compile the regular expression.
   * This gives the user the ability to fine-tune how the regular expression 
   * to match the functionlity that they need. 
   * The {@link java.util.regex.Pattern Pattern} class supports specifying 
   * these fields via the regular expression text itself, but this gives the caller
   * another option to modify the behavior. Useful in cases where the regular expression text
   * cannot be modified, or if doing so is undesired.
   * 
   * @flags The flags that are ORed together.
   */
  public JavaUtilRegexCapabilities(int flags) {
    this.flags = flags;
  }
  
  public void compile(String pattern) {
    this.pattern = Pattern.compile(pattern, this.flags);
  }


For the JakartaRegexpCapabilties.java, the following is changed:

  private int flags = RE.MATCH_NORMAL;

  /**
   * Flag to specify normal, case-sensitive matching behaviour. This is the default.
   */
  public static final int FLAG_MATCH_NORMAL = RE.MATCH_NORMAL;
  
  /**
   * Flag to specify that matching should be case-independent (folded)
   */
  public static final int FLAG_MATCH_CASEINDEPENDENT = RE.MATCH_CASEINDEPENDENT;
 
  /**
   * Contructs a RegexCapabilities with the default MATCH_NORMAL match style.
   */
  public JakartaRegexpCapabilities() {}
  
  /**
   * Constructs a RegexCapabilities with the provided match flags.
   * Multiple flags should be ORed together.
   * 
   * @param flags The matching style
   */
  public JakartaRegexpCapabilities(int flags)
  {
    this.flags = flags;
  }
  
  public void compile(String pattern) {
    regexp = new RE(pattern, this.flags);
  }
"
"LUCENE-3588","IMPROVEMENT","IMPROVEMENT","Try harder to prevent SIGSEGV on cloned MMapIndexInputs","We are unmapping mmapped byte buffers which is disallowed by the JDK, because it has the risk of SIGSEGV when you access the mapped byte buffer after unmapping.

We currently prevent this for the main IndexInput by setting its buffer to null, so we NPE if somebody tries to access the underlying buffer. I recently fixed also the stupid curBuf (LUCENE-3200) by setting to null.

The big problem are cloned IndexInputs which are generally not closed. Those still contain references to the unmapped ByteBuffer, which lead to SIGSEGV easily. The patch from Mike in LUCENE-3439 prevents most of this in Lucene 3.5, but its still not 100% safe (as it uses non-volatiles).

This patch will fix the remaining issues by also setting the buffers of clones to null when the original is closed. The trick is to record weak references of all clones created and close them together with the original. This uses a ConcurrentHashMap<WeakReference<MMapIndexInput>,?> as store with the logic borrowed from WeakHashMap to cleanup the GCed references (using ReferenceQueue).

If we respin 3.5, we should maybe also get this in."
"LUCENE-3444","RFE","RFE","Distinct field value count per group","Support a second pass collector that counts unique field values of a field per group.
This is just one example of group statistics that one might want."
"LUCENE-3388","BUG","BUG","TestTermsEnum.testIntersectRandom fail","{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestTermsEnum
    [junit] Testcase: testIntersectRandom(org.apache.lucene.index.TestTermsEnum):	FAILED
    [junit] (null)
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1530)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1432)
    [junit] 	at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$IntersectEnum.getState(BlockTreeTermsReader.java:894)
    [junit] 	at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$IntersectEnum.seekToStartTerm(BlockTreeTermsReader.java:969)
    [junit] 	at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$IntersectEnum.<init>(BlockTreeTermsReader.java:786)
    [junit] 	at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader.intersect(BlockTreeTermsReader.java:483)
    [junit] 	at org.apache.lucene.index.TestTermsEnum.testIntersectRandom(TestTermsEnum.java:293)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1530)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1432)
    [junit] 
    [junit] 
    [junit] Tests run: 6, Failures: 1, Errors: 0, Time elapsed: 14.762 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestTermsEnum -Dtestmethod=testIntersectRandom -Dtests.seed=320d0949741fc6b1:3cabdb9b04d0d243:-4b7c80572775ed92 -Dtests.multiplier=3
{noformat}"
"LUCENE-2219","TEST","BUG","improve BaseTokenStreamTestCase to test end()","If offsetAtt/end() is not implemented correctly, then there can be problems with highlighting: see LUCENE-2207 for an example with CJKTokenizer.

In my opinion you currently have to write too much code to test this.

This patch does the following:
* adds optional Integer finalOffset (can be null for no checking) to assertTokenStreamContents
* in assertAnalyzesTo, automatically fill this with the String length()

In my opinion this is correct, for assertTokenStreamContents the behavior should be optional, it may not even have a Tokenizer. If you are using assertTokenStreamContents with a Tokenizer then simply provide the extra expected value to check it.

for assertAnalyzesTo then it is implied there is a tokenizer so it should be checked.

the tests pass for core but there are failures in contrib even besides CJKTokenizer (apply Koji's patch from LUCENE-2207, it is correct). Specifically ChineseTokenizer has a similar problem.
"
"LUCENE-2008","DOCUMENTATION","IMPROVEMENT","TokenStream/Tokenizer/TokenFilter/Token javadoc improvements","Some of the javadoc for the new TokenStream/Tokenizer/TokenFilter/Token APIs had javadoc errors.  To the best of my knowledge, I corrected these and refined the copy a bit."
"LUCENE-1806","TEST","IMPROVEMENT","Add args to test-macro","Add passing args to JUnit.  (Like Solr and mainly for debugging).  "
"LUCENE-3720","TEST","TEST","OOM in TestBeiderMorseFilter.testRandom","This has been OOM'ing a lot... we should see why, its likely a real bug.

ant test -Dtestcase=TestBeiderMorseFilter -Dtestmethod=testRandom -Dtests.seed=2e18f456e714be89:310bba5e8404100d:-3bd11277c22f4591 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1"""
"LUCENE-1639","BUG","BUG","intermittent failure in TestIndexWriter. testRandomIWReader","Rarely, this test (which was added with LUCENE-1516) fails in MockRAMDirectory.close because some files were not closed, eg:
{code}
   [junit] NOTE: random seed of testcase 'testRandomIWReader' was: -5001333286299627079
   [junit] ------------- ---------------- ---------------
   [junit] Testcase: testRandomIWReader(org.apache.lucene.index.TestStressIndexing2):        Caused an ERROR
   [junit] MockRAMDirectory: cannot close: there are still open files: {_cq.tvx=3, _cq.fdx=3, _cq.tvf=3, _cq.tvd=3, _cq.fdt=3}
   [junit] java.lang.RuntimeException: MockRAMDirectory: cannot close: there are still open files: {_cq.tvx=3, _cq.fdx=3, _cq.tvf=3, _cq.tvd=3, _cq.fdt=3}
   [junit]     at org.apache.lucene.store.MockRAMDirectory.close(MockRAMDirectory.java:292)
   [junit]     at org.apache.lucene.index.TestStressIndexing2.testRandomIWReader(TestStressIndexing2.java:66)
   [junit]     at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)
{code}"
"LUCENE-1254","BUG","BUG","addIndexesNoOptimize should not enforce maxMergeDocs/maxMergeSize limit","If you pass an index that has a segment > maxMergeDocs or maxMergeSize
to addIndexesNoOptimize, it throws an IllegalArgumentException.

But this check isn't reasonable because segment merging can easily
produce segments over these sizes since those limits apply to each
segment being merged, not to the final size of the segment produced.

So if you set maxMergeDocs to X, build up and index, then try to add
that index to another index that also has maxMergeDocs X, you can
easily hit the exception.

I think it's being too pedantic; I plan to just remove the checks for
sizes."
"LUCENE-1896","REFACTORING","IMPROVEMENT","Modify confusing javadoc for queryNorm","See http://markmail.org/message/arai6silfiktwcer

The javadoc confuses me as well."
"LUCENE-3559","CLEANUP","IMPROVEMENT","remove IndexSearcher.docFreq/maxDoc","As pointed out by Mark on SOLR-1632, these are no longer used by the scoring system.

We've added new stats to Lucene, so having these methods on indexsearcher makes no sense.
Its confusing to people upgrading if they subclassed IndexSearcher to provide distributed stats,
only to find these are not used (LUCENE-3555 has a correct API for them to do this).

So I think we should remove these in 4.0."
"LUCENE-813","BUG","BUG","leading wildcard's don't work with trailing wildcard","As reported by Antony Bowesman, leading wildcards don't work when there is a trailing wildcard character -- instead a PrefixQuery is constructed.


http://www.nabble.com/QueryParser-bug--tf3270956.html"
"LUCENE-2986","REFACTORING","TASK","divorce defaultsimilarityprovider from defaultsimilarity","In LUCENE-2236 as a start, we made DefaultSimilarity which implements the factory interface (SimilarityProvider), and also extends Similarity.

Its factory interface just returns itself always by default.

Doron mentioned it would be cleaner to split the two, and I thought it would be good to revisit it later.

Today as I was looking at SOLR-2338, it became pretty clear that we should do this, it makes things a lot cleaner. I think currently its confusing to users to see the two apis mixed if they are trying to subclass.
"
"LUCENE-3594","BACKPORT","IMPROVEMENT","Backport FieldCacheTermsFilter code duplication removal to 3.x","In trunk I already cleaned up FieldCacheTermsFilter to not duplicate code of FieldCacheRangeFilter. This issue simply backports this."
"LUCENE-2817","BUG","BUG","SimpleText has a bulk enum buffer reuse bug","testBulkPostingsBufferReuse fails with SimpleText codec."
"LUCENE-2354","IMPROVEMENT","IMPROVEMENT","Convert NumericUtils and NumericTokenStream to use BytesRef instead of Strings/char[]","After LUCENE-2302, we should use TermToBytesRefAttribute to index using NumericTokenStream. This also should convert the whole NumericUtils to use BytesRef when converting numerics."
"LUCENE-2016","BUG","BUG","replace invalid U+FFFF character during indexing","If the invalid U+FFFF character is embedded in a token, it actually causes indexing to silently corrupt the index by writing duplicate terms into the terms dict.  CheckIndex will catch the error, and merging will hit exceptions (I think).

We already replace invalid surrogate pairs with the replacement character U+FFFD, so I'll just do the same with U+FFFF."
"LUCENE-2843","RFE","IMPROVEMENT","Add variable-gap terms index impl.","PrefixCodedTermsReader/Writer (used by all ""real"" core codecs) already
supports pluggable terms index impls.

The only impl we have now is FixedGapTermsIndexReader/Writer, which
picks every Nth (default 32) term and holds it in efficient packed
int/byte arrays in RAM.  This is already an enormous improvement (RAM
reduction, init time) over 3.x.

This patch adds another impl, VariableGapTermsIndexReader/Writer,
which lets you specify an arbitrary IndexTermSelector to pick which
terms are indexed, and then uses an FST to hold the indexed terms.
This is typically even more memory efficient than packed int/byte
arrays, though, it does not support ord() so it's not quite a fair
comparison.

I had to relax the terms index plugin api for
PrefixCodedTermsReader/Writer to not assume that the terms index impl
supports ord.

I also did some cleanup of the FST/FSTEnum APIs and impls, and broke
out separate seekCeil and seekFloor in FSTEnum.  Eg we need seekFloor
when the FST is used as a terms index but seekCeil when it's holding
all terms in the index (ie which SimpleText uses FSTs for).
"
"LUCENE-2248","TEST","TEST","Tests using Version.LUCENE_CURRENT will produce problems in backwards branch, when development for 3.2 starts","A lot of tests for the most-recent functionality in Lucene use Version.LUCENE_CURRENT, which is fine in trunk, as we use the most recent version without hassle and changing this in later versions.

The problem is, if we copy these tests to backwards branch after 3.1 is out and then start to improve analyzers, we then will have the maintenance hell for backwards tests. And we loose backward compatibility testing for older versions. If we would specify a specific version like LUCENE_31 in our tests, after moving to backwards they must work without any changes!

To not always modify all tests after a new version comes out (e.g. after switching to 3.2 dev), I propose to do the following:
- declare a static final Version TEST_VERSION = Version.LUCENE_CURRENT (or better) Version.LUCENE_31 in LuceneTestCase(4J).
- change all tests that use Version.LUCENE_CURRENT using eclipse refactor to use this constant and remove unneeded import statements.

When we then move the tests to backward we must only change one line, depending on how we define this constant:
- If in trunk LuceneTestCase it's Version.LUCENE_CURRENT, we just change the backwards branch to use the version numer of the released thing.
- If trunk already uses the LUCENE_31 constant (I prefer this), we do not need to change backwards, but instead when switching version numbers we just move trunk forward to the next major version (after added to Version enum)."
"LUCENE-1446","BUILD_SYSTEM","IMPROVEMENT","Run 'test-tag' in nightly build","Changes in this trivial patch:
- ant target 'nightly' now also depends on 'test-tag'
- adds property 'compatibility.tag' to common-build.xml that should always point to the last tagged release; its unit tests will be downloaded unless -Dtag="""" is used to override
- 'download-tag' does not fail if the svn checkout wasn't successful; instead 'test-tag' checks if the specified tag is checked-out and available, if not it fails "
"LUCENE-1151","IMPROVEMENT","IMPROVEMENT","Fix StandardAnalyzer to not mis-identify HOST as ACRONYM by default","Coming out of the discussion around back compatibility, it seems best to default StandardAnalyzer to properly fix LUCENE-1068, while preserving the ability to get the back-compatible behavior in the rare event that it's desired.

This just means changing the replaceInvalidAcronym = false with = true, and, adding a clear entry to CHANGES.txt that this very slight non back compatible change took place.

Spinoff from here:

    http://www.gossamer-threads.com/lists/lucene/java-dev/57517#57517

I'll commit that change in a day or two."
"LUCENE-292","DOCUMENTATION","IMPROVEMENT","[PATCH] Comment corrections in MMapDirectory.java","These comments ended up on the wrong lines after the last changes"
"LUCENE-1525","BUILD_SYSTEM","BUG","Missing dependencies in two generated maven pom files ","There are some missing dependencies in generated maven pom.xml files (benchmark and highlighter)"
"LUCENE-2592","RFE","IMPROVEMENT","Add static readSnapshotsInfo to PersistentSnapshotDeletionPolicy","PSDP persists the snapshots information in a Directory. When you open PSDP, it obtains a write lock on the snapshots dir (by keeping an open IndexWriter), and updates the directory when snapshots are created/released.

This causes problem in the following scenario -- you have two processes, one updates the 'content' index and keeps PSDP open (because it also takes snapshots). Another process wants to read the existing snapshots information and open a read-only IndexReader on the 'content' index. The other process cannot read the existing snapshots information, because p1 keeps a write lock on the snapshots directory.

There are two possible solutions:
# Have PSDP open the IndexWriter over the directory for each snapshot/release. A bit expensive, and unnecessary.
# Introduce a static readSnapshotsInfo on PSDP which accepts a Directory and returns the snapshots information. IMO it's cleaner, and won't have the performance overhead of opening/closing the IW as before.

I'll post a patch (implementing the 2nd approach) shortly. I'd appreciate any comments."
"LUCENE-494","RFE","RFE","Analyzer for preventing overload of search service by queries with common terms in large indexes","An analyzer used primarily at query time to wrap another analyzer and provide a layer of protection
which prevents very common words from being passed into queries. For very large indexes the cost
of reading TermDocs for a very common word can be  high. This analyzer was created after experience with
a 38 million doc index which had a term in around 50% of docs and was causing TermQueries for 
this term to take 2 seconds.

Use the various ""addStopWords"" methods in this class to automate the identification and addition of 
stop words found in an already existing index."
"LUCENE-781","BUG","BUG","NPE in MultiReader.isCurrent() and getVersion()","I'm attaching a fix for the NPE in MultiReader.isCurrent() plus a testcase. For getVersion(), we should throw a better exception that NPE. I will commit unless someone objects or has a better idea."
"LUCENE-2482","RFE","RFE","Index sorter","A tool to sort index according to a float document weight. Documents with high weight are given low document numbers, which means that they will be first evaluated. When using a strategy of ""early termination"" of queries (see TimeLimitedCollector) such sorting significantly improves the quality of partial results.

(Originally this tool was created by Doug Cutting in Nutch, and used norms as document weights - thus the ordering was limited by the limited resolution of norms. This is a pure Lucene version of the tool, and it uses arbitrary floats from a specified stored field)."
"LUCENE-720","TEST","BUG","Unit tests TestBackwardsCompatibility and TestIndexFileDeleter might fail depending on JVM","In the two units tests TestBackwardsCompatibility and TestIndexFileDeleter several index file names are hardcoded. For example, in TestBackwardsCompatibility.testExactFileNames() it is tested if the index directory contains exactly the expected files after several operations like addDocument(), deleteDocument() and setNorm() have been performed. Apparently the unit tests pass on the nightly build machine, but in my environment (Windows XP, IBM JVM 1.5) they fail for the following reason:

When IndexReader.setNorm() is called a new norm file for the specified field is created with the file  ending .sx, where x is the number of the field. The problem is that the SegmentMerger can not guarantee to keep the order of the fields, in other words after a merge took place a field can have a different field number. This specific testcase fails, because it expects the file ending .s0, but the file has the ending .s1.

The reason why the field numbers can be different on different JVMs is the use of HashSet in SegmentReader.getFieldNames(). Depending on the HashSet implementation an iterator might not iterate over the entries in insertion order. When I change HashSet to LinkedHashSet, the two testcases pass.

However, even with a LinkedHashSet the order of the field numbers might change during a merge, because the order in which the SegmentMerger merges the FieldInfos depends on the field options like TERMVECTOR, INDEXED... (see SegmentMerger.mergeFields() for details). 

So I think we should not use LinkedHashSet but rather change the problematic testcases. Furthermore I'm not sure if we should have hardcoded filenames in the tests anyway, because if we change the index format or file names in the future these test cases would fail without modification."
"LUCENE-854","RFE","RFE","Create merge policy that doesn't periodically inadvertently optimize","The current merge policy, at every maxBufferedDocs *
power-of-mergeFactor docs added, will do a fully cascaded merge, which
is the same as an optimize.

I think this is not good because at that ""optimization poin"", the
particular addDocument call is [surprisingly] very expensive.  While,
amortized over all addDocument calls, the cost is low, the cost is
paid ""up front"" and in a very ""bunched up"" manner.

I think of this as ""pay it forward"": you are paying the full cost of
an optimize right now on the expectation / hope that you will be
adding a great many more docs.  But, if you don't add that many more
docs, then, the amortized cost for your index is in fact far higher
than it should have been.  Better to ""pay as you go"" instead.

So we could make a small change to the policy by only merging the
first mergeFactor segments once we hit 2X the merge factor.  With
mergeFactor=10, when we have created the 20th level 0 (just flushed)
segment, we merge the first 10 into a level 1 segment.  Then on
creating another 10 level 0 segments, we merge the second set of 10
level 0 segments into a level 1 segment, etc.

With this new merge policy, an index that's a bit bigger than a
current ""optimization point"" would then have a lower amortized cost
per document.  Plus the merge cost is less ""bunched up"" and less ""pay
it forward"": instead you pay for what you are actually using.

We can start by creating this merge policy (probably, combined with
with the ""by size not by doc count"" segment level computation from
LUCENE-845) and then later decide whether we should make it the
default merge policy.
"
"LUCENE-2266","BUG","BUG","problem with edgengramtokenfilter and highlighter","i ran into a problem while using the edgengramtokenfilter, it seems to report incorrect offsets when generating tokens, more specifically all the tokens have offset 0 and term length as start and end, this leads to goofy highlighting behavior when creating edge grams for tokens beyond the first one, i created a small patch that takes into account the start of the original token and adds that to the reported start/end offsets.

"
"LUCENE-2920","CLEANUP","TASK","Deprecate and remove ShingleMatrixFilter","Spin-off from LUCENE-1391: This filter is unmainatined and no longer up-to-date, has bugs nobody understands and does not work with attributes.

This issue deprecates it as of Lucene 3.1 and removes it from trunk."
"LUCENE-1773","RFE","RFE","Add benchmark task for FastVectorHighlighter",""
"LUCENE-2712","BUG","BUG","FieldBoostMapAttribute in contrib/qp is broken.","While looking for more SuppressWarnings in lucene, i came across two of them in contrib/queryparser.

even worse, i found these revolved around using maps with CharSequence as key.

From the javadocs for CharSequence:

This interface does not refine the general contracts of the equals and hashCode methods. The result of comparing two objects that implement CharSequence is therefore, in general, undefined. Each object may be implemented by a different class, and there is no guarantee that each class will be capable of testing its instances for equality with those of the other. {color:red} It is therefore inappropriate to use arbitrary CharSequence instances as elements in a set or as keys in a map. {color}

"
"LUCENE-2005","DOCUMENTATION","TASK","Add LuSql project to ""Apache Lucene - Contributions"" wiki page","Add [LuSql|http://lab.cisti-icist.nrc-cnrc.gc.ca/cistilabswiki/index.php/LuSql] to the Apache Lucene - Contributions page [http://lucene.apache.org/java/2_9_0/contributions.html]
I am the author of LuSql. I can supply any text needed. 

Perhaps a new heading is needed to capture Database/JDBC oriented Lucene tools (there are others out there)?"
"LUCENE-2548","CLEANUP","IMPROVEMENT","Remove all interning of field names from flex API","In previous versions of Lucene, interning of fields was important to minimize string comparison cost when iterating TermEnums, to detect changes in field name. As we separated field names from terms in flex, no query compares field names anymore, so the whole performance problematic interning can be removed. I will start with doing this, but we need to carefully review some places e.g. in preflex codec.

Maybe before this issue we should remove the Term class completely. :-) Robert?"
"LUCENE-1980","DOCUMENTATION","TASK","Fix javadocs after deprecation removal","There are a lot of @links in Javadocs to methods/classes that no longer exist. javadoc target prints tons of warnings. We should fix that."
"LUCENE-1051","DOCUMENTATION","IMPROVEMENT","Separate javadocs for core and contribs","A while ago we had a discussion on java-dev about separating the javadocs
for the contrib modules instead of having only one big javadoc containing 
the core and contrib classes.

This patch:
* Adds new targets to build.xml: 
  ** ""javadocs-all"" Generates Javadocs for the core, demo, and contrib 
    classes
  ** ""javadocs-core"" Generates Javadocs for the core classes
  ** ""javadocs-demo"" Generates Javadocs for the demo classes
  ** ""javadocs-contrib"" Using contrib-crawl it generates the Javadocs for 
    all contrib modules, except ""similarity"" (currently empty) and gdata.
* Adds submenues to the Javadocs link on the Lucene site with links to
  the different javadocs
* Includes the javadocs in the maven artifacts

Remarks:
- I removed the ant target ""javadocs-internal"", because I didn't want to
  add corresponding targets for all new javadocs target. Instead I 
  defined a new property ""javadoc.access"", so now  
  ""ant -Djavadoc.access=package"" can be used in combination with any of
  the javadocs targets. Is this ok?
- I didn't include gdata (yet) because it uses build files that don't 
  extend Lucenes standard build files.
  
Here's a preview:
http://people.apache.org/~buschmi/site-preview/index.html

Please let me know what you think about these changes!"
"LUCENE-1148","RFE","RFE","Create a new sub-class of SpanQuery to enable use of a RangeQuery within a SpanQuery","Our users express queries using a syntax which enables them to embed various query types within SpanQuery instances.  One feature they've been asking for is the ability to embed a numeric range query so they could, for example, find documents matching ""[2.0 2.75]MHz"".  The attached patch adds the capability and I hope others will find it useful.
"
"LUCENE-1801","IMPROVEMENT","TASK","Tokenizers (which are the source of Tokens) should call AttributeSource.clearAttributes() first","This is a followup for LUCENE-1796:
{quote}
Token.clear() used to be called by the consumer... but then it was switched to the producer here: LUCENE-1101 
I don't know if all of the Tokenizers in lucene were ever changed, but in any case it looks like at least some of these bugs were introduced with the switch to the attribute API - for example StandardTokenizer did clear it's reusableToken... and now it doesn't.
{quote}

As alternative to changing all core/contrib Tokenizers to call clearAttributes first, we could do this in the indexer, what would be a overhead for old token streams that itsself clear their reusable token. This issue should also update the Javadocs, to clearly state inside Tokenizer.java, that the source TokenStream (normally the Tokenizer) should clear *all* Attributes. If it does not do it and e.g. the positionIncrement is changed to 0 by any TokenFilter, but the filter does not change it back to 1, the TokenStream would stay with 0. If the TokenFilter would call PositionIncrementAttribute.clear() (because he is responsible), it could also break the TokenStream, because clear() is a general method for the whole attribute instance. If e.g. Token is used as AttributeImpl, a call to clear() would also clear offsets and termLength, which is not wanted. So the source of the Tokenization should rest the attributes to default values.

LUCENE-1796 removed the iterator creation cost, so clearAttributes should run fast, but is an additional cost during Tokenization, as it was not done consistently before, so a small speed degradion is caused by this, but has nothing to do with the new TokenStream API."
"LUCENE-3486","RFE","RFE","Add SearcherLifetimeManager, so you can retrieve the same searcher you previously used","The idea is similar to SOLR-2809 (adding searcher leases to Solr).

This utility class sits above whatever your source is for ""the
current"" searcher (eg NRTManager, SearcherManager, etc.), and records
(holds a reference to) each searcher in recent history.

The idea is to ensure that when a user does a follow-on action (clicks
next page, drills down/up), or when two or more searcher invocations
within a single user search need to happen against the same searcher
(eg in distributed search), you can retrieve the same searcher you
used ""last time"".

I think with the new searchAfter API (LUCENE-2215), doing follow-on
searches on the same searcher is more important, since the ""bottom""
(score/docID) held for that API can easily shift when a new searcher
is opened.

When you do a ""new"" search, you record the searcher you used with the
manager, and it returns to you a long token (currently just the
IR.getVersion()), which you can later use to retrieve the same
searcher.

Separately you must periodically call prune(), to prune the old
searchers, ideally from the same thread / at the same time that
you open a new searcher."
"LUCENE-1951","IMPROVEMENT","IMPROVEMENT","wildcardquery rewrite improvements","wildcardquery has logic to rewrite to termquery if there is no wildcard character, but
* it needs to pass along the boost if it does this
* if the user asked for a 'constant score' rewriteMethod, it should rewrite to a constant score query for consistency.

additionally, if the query is really a prefixquery, it would be nice to rewrite to prefix query.
both will enumerate the same number of terms, but prefixquery has a simpler comparison function."
"LUCENE-3899","TEST","TEST","Evil up MockDirectoryWrapper.checkIndexOnClose","MockDirectoryWrapper checks any indexes tests create on close(), if they exist.

The problem is the logic it uses to determine if an index exists could mask real bugs (e.g. segments file corrumption):
{code}
if (DirectoryReader.indexExists(this) {
  ...
  // evil stuff like crash()
  ...
  _TestUtil.checkIndex(this)
}
{code}

and for reference DirectoryReader.indexExists is:
{code}
try {
  new SegmentInfos().read(directory);
  return true;
} catch (IOException ioe) {
  return false;
}
{code}

So if there are segments file problems, we just silently do no checkIndex.
"
"LUCENE-2770","IMPROVEMENT","IMPROVEMENT","Optimize SegmentMerger to work on atomic (Segment)Readers where possible","This is a spin-off from LUCENE-2769:

Currently SegmentMerger has some optimizations when it merges segments that are SegmentReaders (e.g. when doing normal indexing or optimizing). But when you do IndexWriter.addIndexes(IndexReader...) the listed IndexReaders may not really be per-segment. SegmentMerger should track down all passed in reads down to the lowest level (Segment)Reader (or other atomic readers like SlowMultiReaderWrapper) and then merge. We can then remove most MultiFields usage (except term merging itsself) and clean up the code.

This especially saves lots of memory for merging norms, as no longer the duplicate norms arrays are created when MultiReaders are used!"
"LUCENE-3309","RFE","IMPROVEMENT","Add narrow API for loading stored fields, to replace FieldSelector","I think we should ""invert"" the FieldSelector API, with a ""push"" API
whereby FieldsReader invokes this API once per field in the document
being visited.

Implementations of the API can then do arbitrary things like save away
the field's size, load the field, clone the IndexInput for later lazy
loading, etc.

This very thin API would be a mirror image of the very thin index time
API we now have (IndexableField) and, importantly, it would have no
dependence on our ""user space"" Document/Field/FieldType impl, so apps
are free to do something totally custom.

After we have this, we should build the ""sugar"" API that rebuilds a
Document instance (ie IR.document(int docID)) on top of this new thin
API.  This'll also be a good test that the API is sufficient.

Relevant discussions from IRC this morning at
http://colabti.org/irclogger/irclogger_log/lucene-dev?date=2011-07-13#l76
"
"LUCENE-252","BUG","BUG","[PATCH] Problem with Sort logic on tokenized fields","When you set s SortField to a Text field which gets tokenized
FieldCacheImpl uses the term to do the sort, but then sorting is off 
especially with more then one word in the field. I think it is much 
more logical to sort by field's string value if the sort field is Tokenized and
stored. This way you'll get the CORRECT sort order"
"LUCENE-3617","TEST","TASK","Graduate appendingcodec from contrib/misc","* All tests pass with this codec (at least once, maybe we don't test that two-phase commit stuff very well!)
* It doesn't require special client side configuration anymore to work (just set it on indexwriter and go)
* it now works with the compound file format.

I don't think it needs to live in contrib anymore."
"LUCENE-1644","IMPROVEMENT","IMPROVEMENT","Enable MultiTermQuery's constant score mode to also use BooleanQuery under the hood","When MultiTermQuery is used (via one of its subclasses, eg
WildcardQuery, PrefixQuery, FuzzyQuery, etc.), you can ask it to use
""constant score mode"", which pre-builds a filter and then wraps that
filter as a ConstantScoreQuery.

If you don't set that, it instead builds a [potentially massive]
BooleanQuery with one SHOULD clause per term.

There are some limitations of this approach:

  * The scores returned by the BooleanQuery are often quite
    meaningless to the app, so, one should be able to use a
    BooleanQuery yet get constant scores back.  (Though I vaguely
    remember at least one example someone raised where the scores were
    useful...).

  * The resulting BooleanQuery can easily have too many clauses,
    throwing an extremely confusing exception to newish users.

  * It'd be better to have the freedom to pick ""build filter up front""
    vs ""build massive BooleanQuery"", when constant scoring is enabled,
    because they have different performance tradeoffs.

  * In constant score mode, an OpenBitSet is always used, yet for
    sparse bit sets this does not give good performance.

I think we could address these issues by giving BooleanQuery a
constant score mode, then empower MultiTermQuery (when in constant
score mode) to pick & choose whether to use BooleanQuery vs up-front
filter, and finally empower MultiTermQuery to pick the best (sparse vs
dense) bit set impl.
"
"LUCENE-1779","CLEANUP","IMPROVEMENT","Remove unused ""numSlotsFull"" from FieldComparator.setNextReader","This param is a relic from older optimizations that we've since turned off, and it's quite confusing.  I don't think we need it, and we haven't released the API yet so we're free to remove it now."
"LUCENE-676","REFACTORING","IMPROVEMENT","Promote solr's PrefixFilter into Java Lucene's core","Solr's PrefixFilter class is not specific to Solr and seems to be of interest to core lucene users (PyLucene in this case).
Promoting it into the Lucene core would be helpful."
"LUCENE-1578","RFE","IMPROVEMENT","InstantiatedIndex supports non-optimized IndexReaders","InstantiatedIndex does not currently support non-optimized IndexReaders.  "
"LUCENE-1844","TEST","IMPROVEMENT","Speed up junit tests","As Lucene grows, so does the number of JUnit tests. This is obviously a good thing, but it comes with longer and longer test times. Now that we also run back compat tests in a standard test run, this problem is essentially doubled.

There are some ways this may get better, including running parallel tests. You will need the hardware to fully take advantage, but it should be a nice gain. There is already an issue for this, and Junit 4.6, 4.7 have the beginnings of something we might be able to count on soon. 4.6 was buggy, and 4.7 still doesn't come with nice ant integration. Parallel tests will come though.

Beyond parallel testing, I think we also need to concentrate on keeping our tests lean. We don't want to sacrifice coverage or quality, but I'm sure there is plenty of fat to skim.

I've started making a list of some of the longer tests - I think with some work we can make our tests much faster - and then with parallelization, I think we could see some really great gains."
"LUCENE-3102","BUG","BUG","Few issues with CachingCollector","CachingCollector (introduced in LUCENE-1421) has few issues:
# Since the wrapped Collector may support out-of-order collection, the document IDs cached may be out-of-order (depends on the Query) and thus replay(Collector) will forward document IDs out-of-order to a Collector that may not support it.
# It does not clear cachedScores + cachedSegs upon exceeding RAM limits
# I think that instead of comparing curScores to null, in order to determine if scores are requested, we should have a specific boolean - for clarity
# This check ""if (base + nextLength > maxDocsToCache)"" (line 168) can be relaxed? E.g., what if nextLength is, say, 512K, and I cannot satisfy the maxDocsToCache constraint, but if it was 10K I would? Wouldn't we still want to try and cache them?

Also:
* The TODO in line 64 (having Collector specify needsScores()) -- why do we need that if CachingCollector ctor already takes a boolean ""cacheScores""? I think it's better defined explicitly than implicitly?

* Let's introduce a factory method for creating a specialized version if scoring is requested / not (i.e., impl the TODO in line 189)

* I think it's a useful collector, which stands on its own and not specific to grouping. Can we move it to core?

* How about using OpenBitSet instead of int[] for doc IDs?
** If the number of hits is big, we'd gain some RAM back, and be able to cache more entries
** NOTE: OpenBitSet can only be used for in-order collection only. So we can use that if the wrapped Collector does not support out-of-order

* Do you think we can modify this Collector to not necessarily wrap another Collector? We have such Collector which stores (in-memory) all matching doc IDs + scores (if required). Those are later fed into several processes that operate on them (e.g. fetch more info from the index etc.). I am thinking, we can make CachingCollector *optionally* wrap another Collector and then someone can reuse it by setting RAM limit to unlimited (we should have a constant for that) in order to simply collect all matching docs + scores.

* I think a set of dedicated unit tests for this class alone would be good.

That's it so far. Perhaps, if we do all of the above, more things will pop up."
"LUCENE-1071","BUG","BUG","SegmentMerger doesn't set payload bit in new optimized code","In the new optimized code in SegmentMerger the payload bit is not set correctly
in the merged segment. This means that we loose all payloads during a merge!

The Payloads unit test doesn't catch this. Now that we have the new
DocumentsWriter we buffer much more docs by default then before. This means
that the test cases can't assume anymore that the DocsWriter flushes after 10
docs by default. TestPayloads however falsely assumed this, which means that no
merges happen anymore in TestPayloads. We should check whether there are
other testcases that rely on this.

The fixes for TestPayloads and SegmentMerger are very simple, I'll attach a patch
soon."
"LUCENE-1120","IMPROVEMENT","IMPROVEMENT","Use bulk-byte-copy when merging term vectors","Indexing all of Wikipedia, with term vectors on, under the YourKit
profiler, shows that 26% of the time (!!) was spent merging the
vectors.  This was without offsets & positions, which would make
matters even worse.

Depressingly, merging, even with ConcurrentMergeScheduler, cannot in
fact keep up with the flushing of new segments in this test, and this
is on a strong IO system (Mac Pro with 4 drive RAID 0 array, 4 CPU
cores).

So, just like Robert's idea to merge stored fields with bulk copying
whenever the field name->number mapping is ""congruent"" (LUCENE-1043),
we can do the same with term vectors.

It's a little trickier because the term vectors format doesn't quite
make it easy to bulk-copy because it doesn't directly encode the
offset into the tvf file.

I worked out a patch that changes the tvx format slightly, by storing
the absolute position in the tvf file for the start of each document
into the tvx file, just like it does for tvd now.  This adds an extra
8 bytes (long) in the tvx file, per document.

Then, I removed a vLong (the first ""position"" stored inside the tvd
file), which makes tvd contents fully position independent (so you can
just copy the bytes).

This adds up to 7 bytes per document (less for larger indices) that
have term vectors enabled, but I think this small increase in index
size is acceptable for the gains in indexing performance?

With this change, the time spent merging term vectors dropped from 26%
to 3%.  Of course, this only applies if your documents are ""regular"".
I think in the future we could have Lucene try hard to assign the same
field number for a given field name, if it had been seen before in the
index...

Merging terms now dominates the merge cost (~20% over overall time
building the Wikipedia index).

I also beefed up TestBackwardsCompatibility unit test: test a non-CFS
and a CFS of versions 1.9, 2.0, 2.1, 2.2 index formats, and added some
term vector fields to these indices.
"
"LUCENE-705","IMPROVEMENT","IMPROVEMENT","CompoundFileWriter should pre-set its file length","I've read that if you are writing a large file, it's best to pre-set
the size of the file in advance before you write all of its contents.
This in general minimizes fragmentation and improves IO performance
against the file in the future.

I think this makes sense (intuitively) but I haven't done any real
performance testing to verify.

Java has the java.io.File.setLength() method (since 1.2) for this.

We can easily fix CompoundFileWriter to call setLength() on the file
it's writing (and add setLength() method to IndexOutput).  The
CompoundFileWriter knows exactly how large its file will be.

Another good thing is: if you are going run out of disk space, then,
the setLength call should fail up front instead of failing when the
compound file is actually written.  This has two benefits: first, you
find out sooner that you will run out of disk space, and, second, you
don't fill up the disk down to 0 bytes left (always a frustrating
experience!).  Instead you leave what space was available
and throw an IOException.

My one hesitation here is: what if out there there exists a filesystem
that can't handle this call, and it throws an IOException on that
platform?  But this is balanced against possible easy-win improvement
in performance.

Does anyone have any feedback / thoughts / experience relevant to
this?
"
"LUCENE-323","RFE","BUG","[PATCH] MultiFieldQueryParser and BooleanQuery do not provide adequate support for queries across multiple fields","The attached test case demonstrates this problem and provides a fix:
  1.  Use a custom similarity to eliminate all tf and idf effects, just to 
isolate what is being tested.
  2.  Create two documents doc1 and doc2, each with two fields title and 
description.  doc1 has ""elephant"" in title and ""elephant"" in description.  
doc2 has ""elephant"" in title and ""albino"" in description.
  3.  Express query for ""albino elephant"" against both fields.
Problems:
      a.  MultiFieldQueryParser won't recognize either document as containing 
both terms, due to the way it expands the query across fields.
      b.  Expressing query as ""title:albino description:albino title:elephant 
description:elephant"" will score both documents equivalently, since each 
matches two query terms.
  4.  Comparison to MaxDisjunctionQuery and my method for expanding queries 
across fields.  Using notation that () represents a BooleanQuery and ( | ) 
represents a MaxDisjunctionQuery, ""albino elephant"" expands to:
        ( (title:albino | description:albino)
          (title:elephant | description:elephant) )
This will recognize that doc2 has both terms matched while doc1 only has 1 
term matched, score doc2 over doc1.

Refinement note:  the actual expansion for ""albino query"" that I use is:
        ( (title:albino | description:albino)~0.1
          (title:elephant | description:elephant)~0.1 )
This causes the score of each MaxDisjunctionQuery to be the score of highest 
scoring MDQ subclause plus 0.1 times the sum of the scores of the other MDQ 
subclauses.  Thus, doc1 gets some credit for also having ""elephant"" in the 
description but only 1/10 as much as doc2 gets for covering another query term 
in its description.  If doc3 has ""elephant"" in title and both ""albino"" 
and ""elephant"" in the description, then with the actual refined expansion, it 
gets the highest score of all (whereas with pure max, without the 0.1, it 
would get the same score as doc2).

In real apps, tf's and idf's also come into play of course, but can affect 
these either way (i.e., mitigate this fundamental problem or exacerbate it)."
"LUCENE-3677","CLEANUP","TASK","Remove old byte[] norms api from IndexReader","Followup to LUCENE-3628.

We should remove this api and just use docvalues everywhere, to allow for norms of arbitrary size in the future (not just byte[])"
"LUCENE-2177","DESIGN_DEFECT","IMPROVEMENT","The Field ctors that take byte[] shouldn't take Store, since it must be YES","API silliness.  Makes you think you can set Store.NO for binary fields.  This used to be meaningful when we also accepted COMPRESS, but now it's an orphan."
"LUCENE-471","BUILD_SYSTEM","BUG","gcj ant target doesn't work on windows","In order to fix it I made two changes, both really simple.

First I added to org/apache/lucene/store/GCJIndexInput.cc some code to use windows memory-mapped I/O instead than unix mmap().

Then I had to rearrange the link order in the Makefile in order to avoid unresolved symbol errors. Also to build repeatedly I had to instruct make to ignore the return code for the mkdir command as on windows it fails if the directory already exists.

I'm attaching two patches corresponding to the changes; please note that with the patches applied, the gcj target still works on linux. Both patches apply cleanly to the current svn head."
"LUCENE-2410","IMPROVEMENT","IMPROVEMENT","Optimize PhraseQuery","Looking the scorers for PhraseQuery, I think there are some speedups
we could do:

  * The AND part of the scorer (which advances to the next doc that
    has all the terms), in PhraseScorer.doNext, should do the same
    optimizing as BooleanQuery's ConjunctionScorer, ie sort terms from
    rarest to most frequent.  I don't think it should use a linked
    list/firstToLast() that it does today.

  * We do way too much work now when .score() is not called, because
    we go and find all occurrences of the phrase in the doc, whereas
    we should stop only after finding the first and then go and count
    the rest if .score() is called.

  * For the exact case, I think we can use two int arrays to find the
    matches.  The first array holds the count of how many times a term
    in the phrase ""matched"" a phrase starting at that position.  When
    that count == the number of terms in the phrase, it's a match.
    The 2nd is a ""gen"" array (holds docID when that count was last
    touched), to avoid clearing.  Ie when incrementing the count, if
    the docID != gen, we reset count to 0.  I think this'd be faster
    than the PQ we now use.  Downside of this is if you have immense
    docs (position gets very large) we'd need 2 immense arrays.

It'd be great to do LUCENE-1252 along with this, ie factor
PhraseScorer into two AND'd sub-scorers (LUCENE-1252 is open for
this).  The first one should be ConjunctionScorer, and the 2nd one
checks the positions (ie, either the exact or sloppy scorers).  This
would mean if the PhraseQuery is AND'd w/ other clauses (or, a filter
is applied) we would save CPU by not checking the positions for a doc
unless all other AND'd clauses accepted the doc.
"
"LUCENE-2001","BUG","BUG","wordnet parsing bug","A user reported that wordnet parses the prolog file incorrectly.

Also need to check the wordnet parser in the memory contrib for this problem.

If this is a false alarm, i'm not worried, because the test will be the first unit test wordnet package ever had.

{noformat}
For example, looking up the synsets for the
word ""king"", we get:

java SynLookup wnindex king
baron
magnate
mogul
power
queen
rex
scrofula
struma
tycoon

Here, ""scrofula"" and ""struma"" are extraneous. This happens because, the line
parser code in Syns2Index.java interpretes the two consecutive single quotes
in entry s(114144247,3,'king''s evil',n,1,1) in  wn_s.pl file, as
termination
of the string and separates into ""king"". This entry concerns
synset of words ""scrofula"" and ""struma"", and thus they get inserted in the
synset of ""king"". *There 1382 such entries, in wn_s.pl* and more in other
WordNet
Prolog data-base files, where such use of two consecutive single quotes
appears.

We have resolved this by adding a statement in the line parsing portion of
Syns2Index.java, as follows:

           // parse line
           line = line.substring(2);
          * line = line.replaceAll(""\'\'"", ""`""); // added statement*
           int comma = line.indexOf(',');
           String num = line.substring(0, comma);  ... ... etc.
In short we replace ""''"" by ""`"" (a back-quote). Then on recreating the
index, we get:

java SynLookup zwnindex king
baron
magnate
mogul
power
queen
rex
tycoon
{noformat}"
"LUCENE-2390","TEST","BUG","contrib/remote tests fail randomly","The contrib/remote tests will fail randomly.

This is because they use this _TestUtil.getRandomSocketPort() which
simply generates a random number, but if this is already in use, it will fail.

Additionally there is duplicate RMI logic across all 3 test classes."
"LUCENE-3061","RFE","IMPROVEMENT","Open IndexWriter API to allow custom MergeScheduler implementation","IndexWriter's getNextMerge() and merge(OneMerge) are package-private, which makes it impossible for someone to implement his own MergeScheduler. We should open up these API, as well as any other that can be useful for custom MS implementations."
"LUCENE-1596","IMPROVEMENT","IMPROVEMENT","optimize MultiTermEnum/MultiTermDocs","Optimize MultiTermEnum and MultiTermDocs to avoid seeks on TermDocs that don't match the term."
"LUCENE-3078","RFE","TASK","Add generics to DocumentsWriterDeleteQueue.Node","DocumentsWriterDeleteQueue.Note should be generic as the subclasses hold different types of items. This generification is a little bit tricks, but the generics policeman can't wait to fix this *g*."
"LUCENE-890","DOCUMENTATION","TASK","Fix for small syntax omission in TermQuery documentation","A coding example, which could be cut'n'paste by a user, has unbalanced parenthesis.

This fix corrects the documentation, making no changes to functionality, only readability."
"LUCENE-2762","BUG","BUG","Don't leak deleted open file handles with pooled readers","If you have CFS enabled today, and pooling is enabled (either directly
or because you've pulled an NRT reader), IndexWriter will hold open
SegmentReaders against the non-CFS format of each merged segment.

So even if you close all NRT readers you've pulled from the writer,
you'll still see file handles open against files that have been
deleted.

This count will not grow unbounded, since it's limited by the number
of segments in the index, but it's still a serious problem since the
app had turned off CFS in the first place presumably to avoid risk of
too-many-open-files.  It's also bad because it ties up disk space
since these files would otherwise be deleted.
"
"LUCENE-3143","BUG","BUG","SegmentMerger should assert .del and .s* files are not passed to createCompoundFile","Spinoff from LUCENE-3126. SegmentMerger.createCompoundFile does not document that it should not receive files that are not included in the .cfs, such as .del and .s* (separate norms). Today, that method is called from code which ensures that, but we should:
# Add some documentation to clarify that.
# Add some asserts so that if a test (or other code, running w/ -ea) does that, we catch it.

Will post a patch soon"
"LUCENE-248","RFE","IMPROVEMENT","[PATCH] Add StopFilter ignoreCase option","Wanted to have the ability to ignore case in the stop filter.  In some cases, I
don't want to have to lower case before passing through the stop filter, b/c I
may need case preserved for other analysis further down the stream, yet I don't
need the stopwords and I don't want to have to apply stopword filters twice."
"LUCENE-300","REFACTORING","IMPROVEMENT","[PATCH] Refactoring of SpanScorer","Refactored some common code in next() and skipTo(). 
Removed dependency on score value for next() and skipTo(). 
Passes all current tests at just about the same speed 
as the current version. Added minimal javadoc. 
 
Iirc, there has been some discussion on the dependency of next() 
and skipTo() on the score value, but I don't remember the conclusion. 
In case that dependency should stay in, it can be adapted 
in the refactored code."
"LUCENE-2326","BUILD_SYSTEM","IMPROVEMENT","Remove SVN.exe and revision numbers from build.xml by svn-copy the backwards branch and linking snowball tests by svn:externals","As we often need to update backwards tests together with trunk and always have to update the branch first, record rev no, and update build xml, I would simply like to do a svn copy/move of the backwards branch.

After a release, this is simply also done:
{code}
svn rm backwards
svn cp releasebranch backwards
{code}

By this we can simply commit in one pass, create patches in one pass.

The snowball tests are currently downloaded by svn.exe, too. These need a fixed version for checkout. I would like to change this to use svn:externals. Will provide patch, soon."
"LUCENE-984","CLEANUP","BUG","remove TermVectorsWriter (it's no longer used)","We should remove TermVectorsWriter: it's no longer used now that
DocumentsWriter writes the term vectors directly to the index."
"LUCENE-2015","IMPROVEMENT","IMPROVEMENT","ASCIIFoldingFilter: expose folding logic + small improvements to ISOLatin1AccentFilter","This patch adds a couple of non-ascii chars to ISOLatin1AccentFilter (namely: left & right single quotation marks, en dash, em dash) which we very frequently encounter in our projects. I know that this class is now deprecated; this improvement is for legacy code that hasn't migrated yet.

It also enables easy access to the ascii folding technique use in ASCIIFoldingFilter for potential re-use in non-Lucene-related code."
"LUCENE-588","BUG","BUG","Escaped wildcard character in wildcard term not handled correctly","If an escaped wildcard character is specified in a wildcard query, it is treated as a wildcard instead of a literal.
e.g., t\??t is converted by the QueryParser to t??t - the escape character is discarded."
"LUCENE-2304","IMPROVEMENT","IMPROVEMENT","FuzzyLikeThisQuery should set MaxNonCompetitiveBoost for faster speed","FuzzyLikeThisQuery uses FuzzyTermsEnum directly, and maintains 
a priority queue for its purposes.

Just like TopTermsRewrite method, it should set the 
MaxNonCompetitiveBoost attribute, so that FuzzyTermsEnum can
run faster. Its already tracking the minScore, just not updating
the attribute.

This would be especially nice as it appears to have nice defaults
already (pq size of 50)
"
"LUCENE-1333","IMPROVEMENT","IMPROVEMENT","Token implementation needs improvements","This was discussed in the thread (not sure which place is best to reference so here are two):
http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200805.mbox/%3C21F67CC2-EBB4-48A0-894E-FBA4AECC0D50@gmail.com%3E
or to see it all at once:
http://www.gossamer-threads.com/lists/lucene/java-dev/62851

Issues:
1. JavaDoc is insufficient, leading one to read the code to figure out how to use the class.
2. Deprecations are incomplete. The constructors that take String as an argument and the methods that take and/or return String should *all* be deprecated.
3. The allocation policy is too aggressive. With large tokens the resulting buffer can be over-allocated. A less aggressive algorithm would be better. In the thread, the Python example is good as it is computationally simple.
4. The parts of the code that currently use Token's deprecated methods can be upgraded now rather than waiting for 3.0. As it stands, filter chains that alternate between char[] and String are sub-optimal. Currently, it is used in core by Query classes. The rest are in contrib, mostly in analyzers.
5. Some internal optimizations can be done with regard to char[] allocation.
6. TokenStream has next() and next(Token), next() should be deprecated, so that reuse is maximized and descendant classes should be rewritten to over-ride next(Token)
7. Tokens are often stored as a String in a Term. It would be good to add constructors that took a Token. This would simplify the use of the two together.
"
"LUCENE-660","RFE","RFE","GData html render preview","GData output is usually ATOM / RSS e.g plain xml. This feature enables users or admins to preview the server output as html transformed by user defined xsl stylesheet. Stylesheet is configurable per service.

That's just a cool feature for developing and for users wanna use the server for simple blog or feed server.

regards simon"
"LUCENE-351","DOCUMENTATION","BUG","More javadocs for Weight","From Doug's reply of 21 Feb 2005 in bug 31841"
"LUCENE-1878","CLEANUP","TASK","remove deprecated classes from spatial","spatial has not been released, so we can remove the deprecated classes"
"LUCENE-2238","CLEANUP","TASK","deprecate ChineseAnalyzer","The ChineseAnalyzer, ChineseTokenizer, and ChineseFilter (not the smart one, or CJK) indexes chinese text as individual characters and removes english stopwords, etc.

In my opinion we should simply deprecate all of this in favor of StandardAnalyzer, StandardTokenizer, and StopFilter, which does the same thing."
"LUCENE-1095","RFE","IMPROVEMENT","StopFilter should have option to incr positionIncrement after stop word","I've seen this come up on the mailing list a few times in the last month, so i'm filing a known bug/improvement arround it...

StopFilter should have an option that if set, records how many stop words are ""skipped"" in a row, and then sets that value as the positionIncrement on the ""next"" token that StopFilter does return."
"LUCENE-3354","RFE","IMPROVEMENT","Extend FieldCache architecture to multiple Values","I would consider this a bug. It appears lots of people are working around this limitation, 
why don't we just change the underlying data structures to natively support multiValued fields in the FieldCache architecture?

Then functions() will work properly, and we can do things like easily geodist() on a multiValued field.

Thoughts?"
"LUCENE-3421","BUG","BUG","PayloadTermQuery's explain is broken when span score is not included","When setting includeSpanScore to false with PayloadTermQuery, the explain is broken."
"LUCENE-332","BUILD_SYSTEM","IMPROVEMENT","nightly build/javadocs for sandbox","This isn't something i think is crucial, but since i've been on the lucene-users
mailing list (less then 2 months) I've seen several people post questions asking
where they can find documentation on some module available in the sandbox.

the answer of course is usually that they should download the source and build
the javadocs themselves, but since it keeps coming up, I figured i'd suggest
setting up a nightly ""build"" of the whole sandbox, including the javadocs.

if nothing else, it will cut down on the number of questions -- but i think it
may also have an added benefit to the size of the user base.  In my experience,
people tend to be more willing to download/install something and try it out
after they've read the docs online."
"LUCENE-3217","IMPROVEMENT","IMPROVEMENT","Improve DocValues merging","Some DocValues impl. still load all values from merged segments into memory during merge. For efficiency we should merge them on the fly without buffering in memory"
"LUCENE-3452","TEST","BUG","The native FS lock used in test-framework's o.a.l.util.LuceneJUnitResultFormatter prohibits testing on a multi-user system","{{LuceneJUnitResultFormatter}} uses a lock to buffer test suites' output, so that when run in parallel, they don't interrupt each other when they are displayed on the console.

The current implementation uses a fixed directory ({{lucene_junit_lock/}} in {{java.io.tmpdir}} (by default {{/tmp/}} on Unix/Linux systems) as the location of this lock.  This functionality was introduced on SOLR-1835.

As Shawn Heisey reported on SOLR-2739, some tests fail when run as root, but succeed when run as a non-root user.  

On #lucene IRC today, Shawn wrote:
{quote}
(2:06:07 PM) elyograg: Now that I know I can't run the tests as root, I have discovered /tmp/lucene_junit_lock.  Once you run the tests as user A, you cannot run them again as user B until that directory is deleted, and only root or the original user can do so.
{quote}
"
"LUCENE-1440","RFE","RFE","Add ability to run backwards-compatibility tests automatically","This is an idea Doug mentioned on LUCENE-1422.

This patch adds new targets to build.xml to automatically download the junit tests from a previous Lucene release and run them against the current core.
Execute tests like this:
ant -Dtag=lucene_2_4_0 test-tag

It will create a new directory tags/lucene_2_4_0 and fetch the tests from the svn repository and run them."
"LUCENE-2070","DOCUMENTATION","IMPROVEMENT","document LengthFilter wrt Unicode 4.0","LengthFilter calculates its min/max length from TermAttribute.termLength()
This is not characters, but instead UTF-16 code units.

In my opinion this should not be changed, merely documented.
If we changed it, it would have an adverse performance impact because we would have to actually calculate Character.codePointCount() on the text.

If you feel strongly otherwise, fixing it to count codepoints would be a trivial patch, but I'd rather not hurt performance.
I admit I don't fully understand all the use cases for this filter.
"
"LUCENE-1647","BUG","BUG","IndexReader.undeleteAll can mess up the deletion count stored in the segments file","Spinoff from LUCENE-1474.  I'll attach a test case showing the issue."
"LUCENE-3886","IMPROVEMENT","BUG","MemoryIndex memory estimation in toString inconsistent with getMemorySize()","After LUCENE-3867 was committed, there are some more minor problems with MemoryIndex's estimates. This patch will fix those and also add verbose test output of RAM needed for MemoryIndex vs. RAMDirectory.

Interestingly, the RAMDirectory always takes (according to estimates, so even with buffer overheads) only 2/3 of the MemoryIndex (excluding IndexReaders)."
"LUCENE-1098","IMPROVEMENT","IMPROVEMENT","Small performance enhancement for StandardAnalyzer","The class StandardAnalyzer has an inner class, SavedStreams, which is used internally for maintaining some state. This class doesn't use the implicit reference to the enclosing class, so it can be made static and reduce some memory requirements. A patch will be attached shortly."
"LUCENE-3623","BUG","BUG","SegmentReader.getFieldNames ignores FieldOption.DOC_VALUES","we use this getFieldNames api in segmentmerger if we merge something that isn't a SegmentReader (e.g. FilterIndexReader)

it looks to me that if you use a FilterIndexReader, call addIndexes(Reader...) the docvalues will be simply dropped.

I dont think its enough to just note that the field has docvalues either right? We need to also set the type 
correctly in the merged field infos? This would imply that instead of FieldOption.DOCVALUES, we need to have a 
FieldOption for each ValueType so that we correctly update the type.

But looking at FI.update/setDocValues, it doesn't look like we 'type-promote' here anyway?
"
"LUCENE-1588","RFE","IMPROVEMENT","Update Spatial Lucene sort to use FieldComparatorSource","Update distance sorting to use FieldComparator sorting as opposed to SortComparator"
"LUCENE-1464","BUG","BUG","FSDirectory.getDirectory always creates index path","This was reported to me as a Luke bug, but going deeper it proved to be a non-intuitive (broken?) behavior of FSDirectory.

If you use FSDirectory.getDirectory(File nonexistent) on a nonexistent path, but one that is located under some existing parent path, then FSDirectory:174 uses file.mkdirs() to create this directory. One would expect a variant of the method with a boolean flag to decide whether or not to create the output path. However, the API with ""create"" flag is now deprecated, with a comment that points to IndexWriter's ""create"" flag. This comment is misleading, because the indicated path is created anyway in the file system just by calling FSDirectory.getDirectory().

I propose to do one of the following:

* reinstate the variant of the method with ""create"" flag. In case if this flag is false, and the index directory is missing, either return null or throw an IOException,

* keep the API as it is now, but either return null or throw IOException if the index dir is missing. This breaks the backwards compatibility, because now users are required to do file.mkdirs() themselves prior to calling FSDirectory.getDirectory()."
"LUCENE-2785","BUG","BUG","TopFieldCollector throws AIOOBE if numHits is 0","See solr-user thread ""ArrayIndexOutOfBoundsException for query with rows=0 and sort param"".

I think we should just create a null collector (only tallies up totalHits) if numHits is 0?"
"LUCENE-506","IMPROVEMENT","IMPROVEMENT","Optimize Memory Use for Short-Lived Indexes (Do not load TermInfoIndex if you know the queries ahead of time)","Summary: Provide a way to avoid loading the TermInfoIndex into memory if you know all the terms you are ever going to query.

In our search environment, we have a large number of indexes (many thousands), any of which may be queried by any number of hosts.  These indexes may be very large (~1M document), but since we have a low term/doc ratio, we have 7-11M terms.  With an index interval of 128, that means ~70-90K terms.  On loading the index, it instantiates a Term, a TermInfo, a String, and a char[].  When the document is long lived, this makes some sense because you can quickly search the list of terms using binary search.  However, since we throw away the Indexes very often, a lot of garbage is created per query

Here's an example where we load a large index 10 times.  This corresponds to 7MB of garbage per query.
          percent          live          alloc'ed  stack class
 rank   self  accum     bytes objs     bytes  objs trace name
    1  4.48%  4.48%   4678736 128946  23393680 644730 387749 char[]
    3  3.95% 12.61%   4126272 128946  20631360 644730 387751 org.apache.lucene.index.TermInfo
    6  2.96% 22.71%   3094704 128946  15473520 644730 387748 java.lang.String
    8  1.98% 26.97%   2063136 128946  10315680 644730 387750 org.apache.lucene.index.Term

This adds up after a while.  Since we know exactly which Terms we're going to search for before even opening the index, there's no need to allocate this much memory.  Upon opening the index, we can go through the TII in sequential order and retrieve the entries into the main term dictionary and reduce the storage requirements dramatically.  This reduces the amount of garbage generated by querying by about 60% if you only make 1 query/index with a 77% increase in throughput.

This is accomplished by factoring out the ""index loading"" aspects of TermInfosReader into a new file, SegmentTermInfosReader.  TermInfosReader becomes a base class to allow access to terms.  A new class, PrefetchedTermInfosReader will, upon startup, sort the passed in terms and retrieve the IndexEntries for those terms.  IndexReader and SegmentReader are modified to take new constructor methods that take a Collection of Terms that correspond to the total set of terms that will ever be searched in the life of the index.

In order to support the ""skipping"" behavior, some changes need to be made to SegmentTermEnum: specifically, we need to be able to go back an entry in order to retrieve the previous TermInfo and IndexPointer.  This is because, unlike the normal case, with the index  we want to return the value right before the intended field (so that we can be behind the desired termin the main dictionary).   For example, if we're looking for  ""apple"" in the index,  and the two adjacent values are ""abba"" and ""argon"", we want to return ""abba"" instead of ""argon"".  That way we won't miss any terms in the real index.   This code is confusing; it should probably be moved to an subclass of TermBuffer, but that required more code.  Not wanting to modify TermBuffer to keep it small, also lead to the odd NPE catch in SegmentTermEnum.java.  Stickler for contracts may want to rename SegmentTermEnum.skipTo() to a different name because it implements a different contract: but it would be useful for anyone trying to skip around in the TII, so I figured it was the right thing to do."
"LUCENE-1420","IMPROVEMENT","IMPROVEMENT","Similarity.lengthNorm and positionIncrement=0","Calculation of lengthNorm factor should in some cases take into account the number of tokens with positionIncrement=0. This should be made optional, to support two different scenarios:

* when analyzers insert artificially constructed tokens into TokenStream (e.g. ASCII-fied versions of accented terms, stemmed terms), and it's unlikely that users submit queries containing both versions of tokens: in this case lengthNorm calculation should ignore the tokens with positionIncrement=0.

* when analyzers insert synonyms, and it's likely that users may submit queries that contain multiple synonymous terms: in this case the lengthNorm should be calculated as it is now, i.e. it should take into account all terms no matter what is their positionIncrement.

The default should be backward-compatible, i.e. it should count all tokens.

(See also the discussion here: http://markmail.org/message/vfvmzrzhr6pya22h )"
"LUCENE-622","BUILD_SYSTEM","TASK","Provide More of Lucene For Maven","Please provide javadoc & source jars for lucene-core.  Also, please provide the rest of lucene (the jars inside of ""contrib"" in the download bundle) if possible."
"LUCENE-231","BUILD_SYSTEM","BUG","Can't build lucene 06/13/2004 CVS under jdk 1.5.0","[root@shilo jakarta-lucene]# /usr/local/ant/bin/ant build
Buildfile: build.xml

BUILD FAILED
Target `build' does not exist in this project. 

Total time: 1 second
[root@shilo jakarta-lucene]# /usr/local/ant/bin/ant 
Buildfile: build.xml

init:
    [mkdir] Created dir: /usr/src/jakarta-lucene/build
    [mkdir] Created dir: /usr/src/jakarta-lucene/dist

compile-core:
    [mkdir] Created dir: /usr/src/jakarta-lucene/build/classes/java
    [javac] Compiling 160 source files to /usr/src/jakarta-
lucene/build/classes/java
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/FilterIndexReader.java:42: as of 
release 1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     public void seek(TermEnum enum) throws IOException { in.seek
(enum); }
    [javac]                               ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/FilterIndexReader.java:42: as of 
release 1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     public void seek(TermEnum enum) throws IOException { in.seek
(enum); }
    [javac]                                                                  ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:55: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]   public void seek(TermEnum enum) throws IOException {
    [javac]                             ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:59: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     if (enum instanceof SegmentTermEnum && ((SegmentTermEnum) 
enum).fieldInfos == parent.fieldInfos)          // optimized case
    [javac]         ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:59: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     if (enum instanceof SegmentTermEnum && ((SegmentTermEnum) 
enum).fieldInfos == parent.fieldInfos)          // optimized case
    [javac]                                                               ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:60: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]       ti = ((SegmentTermEnum) enum).termInfo();
    [javac]                               ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:62: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]       ti = parent.tis.get(enum.term());
    [javac]                           ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:63: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     SegmentTermEnum enum = (SegmentTermEnum)enumerators.get();
    [javac]                     ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:64: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     if (enum == null) {
    [javac]         ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:65: enum types 
must not be local
    [javac]       enum = terms();
    [javac]       ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:65: <identifier> 
expected
    [javac]       enum = terms();
    [javac]            ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:65: '{' expected
    [javac]       enum = terms();
    [javac]                     ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:66: <identifier> 
expected
    [javac]       enumerators.set(enum);
    [javac]                      ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:66: ';' expected
    [javac]       enumerators.set(enum);
    [javac]                       ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:66: <identifier> 
expected
    [javac]       enumerators.set(enum);
    [javac]                           ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:66: '{' expected
    [javac]       enumerators.set(enum);
    [javac]                            ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:68: illegal start 
of type
    [javac]     return enum;
    [javac]     ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:68: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     return enum;
    [javac]            ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:75: illegal start 
of expression
    [javac]   private final void readIndex() throws IOException {
    [javac]   ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:199: ';' expected
    [javac] }
    [javac] ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:200: '}' expected
    [javac] ^
    [javac] 21 errors

BUILD FAILED
/usr/src/jakarta-lucene/build.xml:140: Compile failed; see the compiler error 
output for details.

Total time: 4 seconds
[root@shilo jakarta-lucene]#"
"LUCENE-2020","CLEANUP","TASK","Remove unused imports","With all of the churn recently, now seems like the opportune time to do some import cleanup"
"LUCENE-605","IMPROVEMENT","IMPROVEMENT","Make Explanation include information about match/non-match","As discussed, I'm looking into the possibility of improving the Explanation class to include some basic info about the ""match"" status of the Explanation -- independent of the value...

http://www.nabble.com/BooleanWeight.normalize%28float%29-doesn%27t-normalize-prohibited-clauses--t1596471.html#a4347644

This is neccesary to deal with things like LUCENE-451"
"LUCENE-1353","BUILD_SYSTEM","TASK","javacc ant task for contrib/misc precedence query parser","Add a javacc task in contrib/misc for the precedence query parser."
"LUCENE-1274","RFE","RFE","Expose explicit 2-phase commit in IndexWriter","Currently when IndexWriter commits, it does so with a two-phase
commit, internally: first it prepares all the new index files, syncs
them; then it writes a new segments_N file and syncs that, and only if
that is successful does it remove any now un-referenced index files.

However, these two phases are done privately, internal to the commit()
method.

But when Lucene is involved in a transaction with external resources
(eg a database), it's very useful to explicitly break out the prepare
phase from the commit phase.

Spinoff from this thread:

  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200804.mbox/%3C16627610.post@talk.nabble.com%3E

"
"LUCENE-315","DOCUMENTATION","BUG","Documentation Error for FilteredTermEnum","As pointed out in 
http://nagoya.apache.org/eyebrowse/ReadMsg?listName=lucene-user@jakarta.apache.org&msgNo=11034
the documentation of FilteredTermEnum.term() is wrong:
it says 
'Returns the current Term in the enumeration. Initially invalid, valid after
next() called for the first time.'
but the implementation of the constructors of the two derived classes
(FuzzyTermEnum and WildcardTermEnum) already initializes the object to point to
the first match. So calling next() before accessing terms will leave out the
first match.

So I suggest to replace the second sentance by something like
'Returns null if no Term matches or all terms have been enumerated.'
(I checked that for WildcardTermEnum only).
Further one might add some note to the docs of the constructors of FuzzyTermEnum
and WildcardTermEnum that they will point to the first element of the
enumeration (if any)."
"LUCENE-565","RFE","BUG","Supporting deleteDocuments in IndexWriter (Code and Performance Results Provided)","Today, applications have to open/close an IndexWriter and open/close an
IndexReader directly or indirectly (via IndexModifier) in order to handle a
mix of inserts and deletes. This performs well when inserts and deletes
come in fairly large batches. However, the performance can degrade
dramatically when inserts and deletes are interleaved in small batches.
This is because the ramDirectory is flushed to disk whenever an IndexWriter
is closed, causing a lot of small segments to be created on disk, which
eventually need to be merged.

We would like to propose a small API change to eliminate this problem. We
are aware that this kind change has come up in discusions before. See
http://www.gossamer-threads.com/lists/lucene/java-dev/23049?search_string=indexwriter%20delete;#23049
. The difference this time is that we have implemented the change and
tested its performance, as described below.

API Changes
-----------
We propose adding a ""deleteDocuments(Term term)"" method to IndexWriter.
Using this method, inserts and deletes can be interleaved using the same
IndexWriter.

Note that, with this change it would be very easy to add another method to
IndexWriter for updating documents, allowing applications to avoid a
separate delete and insert to update a document.

Also note that this change can co-exist with the existing APIs for deleting
documents using an IndexReader. But if our proposal is accepted, we think
those APIs should probably be deprecated.

Coding Changes
--------------
Coding changes are localized to IndexWriter. Internally, the new
deleteDocuments() method works by buffering the terms to be deleted.
Deletes are deferred until the ramDirectory is flushed to disk, either
because it becomes full or because the IndexWriter is closed. Using Java
synchronization, care is taken to ensure that an interleaved sequence of
inserts and deletes for the same document are properly serialized.

We have attached a modified version of IndexWriter in Release 1.9.1 with
these changes. Only a few hundred lines of coding changes are needed. All
changes are commented by ""CHANGE"". We have also attached a modified version
of an example from Chapter 2.2 of Lucene in Action.

Performance Results
-------------------
To test the performance our proposed changes, we ran some experiments using
the TREC WT 10G dataset. The experiments were run on a dual 2.4 Ghz Intel
Xeon server running Linux. The disk storage was configured as RAID0 array
with 5 drives. Before indexes were built, the input documents were parsed
to remove the HTML from them (i.e., only the text was indexed). This was
done to minimize the impact of parsing on performance. A simple
WhitespaceAnalyzer was used during index build.

We experimented with three workloads:
  - Insert only. 1.6M documents were inserted and the final
    index size was 2.3GB.
  - Insert/delete (big batches). The same documents were
    inserted, but 25% were deleted. 1000 documents were
    deleted for every 4000 inserted.
  - Insert/delete (small batches). In this case, 5 documents
    were deleted for every 20 inserted.

                                current       current          new
Workload                      IndexWriter  IndexModifier   IndexWriter
-----------------------------------------------------------------------
Insert only                     116 min       119 min        116 min
Insert/delete (big batches)       --          135 min        125 min
Insert/delete (small batches)     --          338 min        134 min

As the experiments show, with the proposed changes, the performance
improved by 60% when inserts and deletes were interleaved in small batches.


Regards,
Ning


Ning Li
Search Technologies
IBM Almaden Research Center
650 Harry Road
San Jose, CA 95120"
"LUCENE-2484","CLEANUP","TASK","Remove deprecated TermAttribute from tokenattributes and legacy support in indexer","The title says it:
- Remove interface TermAttribute
- Remove empty fake implementation TermAttributeImpl extends CharTermAttributeImpl
- Remove methods from CharTermAttributeImpl (and indirect from Token)
- Remove sophisticated backwards Layer in TermsHash*
- Remove IAE from NumericTokenStream, if TA is available in AS
- Fix rest of core tests (TestToken)"
"LUCENE-2045","BUG","BUG","FNFE hit when creating an empty index and infoStream is on","Shai just reported this on the dev list.  Simple test:
{code}
Directory dir = new RAMDirectory();
IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), MaxFieldLength.UNLIMITED);
writer.setInfoStream(System.out);
writer.addDocument(new Document());
writer.commit();
writer.close();
{code}

hits this:

{code}
Exception in thread ""main"" java.io.FileNotFoundException: _0.prx
    at org.apache.lucene.store.RAMDirectory.fileLength(RAMDirectory.java:149)
    at org.apache.lucene.index.DocumentsWriter.segmentSize(DocumentsWriter.java:1150)
    at org.apache.lucene.index.DocumentsWriter.flush(DocumentsWriter.java:587)
    at org.apache.lucene.index.IndexWriter.doFlushInternal(IndexWriter.java:3572)
    at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3483)
    at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3474)
    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1940)
    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1894)
{code}

Turns out it's just silly -- this is actually an issue I've already fixed on the flex (LUCENE-1458) branch.  DocumentsWriter has its own method to enumerate the flushed files and compute their size, but really it shouldn't do that -- it should use SegmentInfo's method, instead."
"LUCENE-543","BUG","IMPROVEMENT","Wildcard query with no wildcard characters in the term throws StringIndexOutOfBounds exception","
Query q1 = new WildcardQuery(new Term(""Text"", ""a""));
Hits hits = searcher.search(q1);


Caught Exception
java.lang.StringIndexOutOfBoundsException : String index out of range: -1
    at java.lang.String.substring(Unknown Source)
    at org.apache.lucene.search.WildcardTermEnum.<init>(WildcardTermEnum.java:65)
    at org.apache.lucene.search.WildcardQuery.getEnum (WildcardQuery.java:38)
    at org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:54)
    at org.apache.lucene.search.IndexSearcher.rewrite(IndexSearcher.java:137)
    at org.apache.lucene.search.Query.weight (Query.java:92)
    at org.apache.lucene.search.Hits.<init>(Hits.java:41)
    at org.apache.lucene.search.Searcher.search(Searcher.java:44)
    at org.apache.lucene.search.Searcher.search(Searcher.java:36)
    at QuickTest.main(QuickTest.java:45)


From Erik Hatcher

Feel free to log this as a bug report in our JIRA issue tracker.  It
seems like a reasonable change to make, such that a WildcardQuery
without a wildcard character would behave like TermQuery."
"LUCENE-1763","DESIGN_DEFECT","IMPROVEMENT","MergePolicy should require an IndexWriter upon construction","MergePolicy does not require an IW upon construction, but requires one to be passed as method arg to various methods. This gives the impression as if a single MP instance can be shared across various IW instances, which is not true for all MPs (if at all). In addition, LogMergePolicy uses the IW instance passed to these methods incosistently, and is currently exposed to potential NPEs.

This issue will change MP to require an IW instance, however for back-compat reasons the following changes will be made:
# A new MP ctor w/ IW as arg will be introduced. Additionally, for back-compat a default ctor will also be declared which will assign null to the member IW.
# Methods that require IW will be deprecated, and new ones will be declared.
#* For back-compat, the new ones will not be made abstract, but will throw UOE, with a comment that they will become abstract in 3.0.
# All current MP impls will move to use the member instance.
# The code which calls MP methods will continue to use the deprecated methods, passing an IW even that it won't be necessary --> this is strictly for back-compat.

In 3.0, we'll remove the deprecated default ctor and methods, and change the code to not call the IW method variants anymore.

I hope that I didn't leave anything out. I'm sure I'll find out when I work on the patch :)."
"LUCENE-538","BUG","BUG","Using WildcardQuery with MultiSearcher, and Boolean MUST_NOT clause","We are searching across multiple indices using a MultiSearcher. There seems to be a problem when we use a WildcardQuery to exclude documents from the result set. I attach a set of unit tests illustrating the problem.

In these tests, we have two indices. Each index contains a set of documents with fields for 'title',  'section' and 'index'. The final aim is to do a keyword search, across both indices, on the title field and be able to exclude documents from certain sections (and their subsections) using a
WildcardQuery on the section field.
 
 e.g. return documents from both indices which have the string 'xyzpqr' in their title but which do not lie
 in the news section or its subsections (section = /news/*).
 
The first unit test (testExcludeSectionsWildCard) fails trying to do this.
 If we relax any of the constraints made above, tests pass:
 
* Don't use WildcardQuery, but pass in the news section and it's child section to exclude explicitly (testExcludeSectionsExplicit)</li>
* Exclude results from just one section, not it's children too i.e. don't use WildcardQuery(testExcludeSingleSection)</li>
* Do use WildcardQuery, and exclude a section and its children, but just use one index thereby using the simple
   IndexReader and IndexSearcher objects (testExcludeSectionsOneIndex).
* Try the boolean MUST clause rather than MUST_NOT using the WildcardQuery i.e. only include results from the /news/ section
   and its children."
"LUCENE-753","RFE","RFE","Use NIO positional read to avoid synchronization in FSIndexInput","As suggested by Doug, we could use NIO pread to avoid synchronization on the underlying file.
This could mitigate any MT performance drop caused by reducing the number of files in the index format."
"LUCENE-3824","DESIGN_DEFECT","IMPROVEMENT","TermOrdVal/DocValuesComparator does too much work in compareBottom","We now have logic to fall back to by-value comparison, when the bottom
slot is not from the current reader.

But this is silly, because if the bottom slot is from a different
reader, it means the tie-break case is not possible (since the current
reader didn't have the bottom value), so when the incoming ord equals
the bottom ord we should always return x > 0.

I added a new random string sort test case to TestSort...

I also renamed DocValues.SortedSource.getByValue -> getOrdByValue and
cleaned up some whitespace.
"
"LUCENE-2022","CLEANUP","TASK","remove contrib deprecations","there aren't too many deprecations in contrib to remove for 3.0, but we should get rid of them."
"LUCENE-1228","BUG","BUG","IndexWriter.commit()  does not update the index version","IndexWriter.commit() can update the index *version* and *generation* but the update of *version* is lost.
As result added documents are not seen by IndexReader.reopen().
(There might be other side effects that I am not aware of).
The fix is 1 line - update also the version in SegmentsInfo.updateGeneration().
(Finding this line involved more lines though... :-) )
"
"LUCENE-2896","IMPROVEMENT","IMPROVEMENT","in advance(), don't try to skip if there is evidence it will fail","There are TODO's about this in the code everywhere, and this was part of
Mike speeding up ExactPhraseScorer.

I think the codec should do this."
"LUCENE-2240","RFE","TASK","SimpleAnalyzer and WhitespaceAnalyzer should have Version ctors","Due to the Changes to CharTokenizer ( LUCENE-2183 ) WhitespaceAnalyzer and SimpleAnalyzer need a Version ctor. Default ctors must be deprecated"
"LUCENE-1972","CLEANUP","TASK","Remove (deprecated) ExtendedFieldCache and Auto/Custom caches and lot's of deprecated sort logic","Remove (deprecated) ExtendedFieldCache and Auto/Custom caches and sort"
"LUCENE-2959","RFE","RFE","[GSoC] Implementing State of the Art Ranking for Lucene","Lucene employs the Vector Space Model (VSM) to rank documents, which compares
unfavorably to state of the art algorithms, such as BM25. Moreover, the architecture is
tailored specically to VSM, which makes the addition of new ranking functions a non-
trivial task.

This project aims to bring state of the art ranking methods to Lucene and to implement a
query architecture with pluggable ranking functions.

The wiki page for the project can be found at http://wiki.apache.org/lucene-java/SummerOfCode2011ProjectRanking."
"LUCENE-1694","DESIGN_DEFECT","IMPROVEMENT","Query#mergeBooleanQueries argument should be of type BooleanQuery[] instead of Query[]","The method #mergeBooleanQueries accepts Query[] and casts elements to BooleanQuery without checking. This will guarantee a ClassCastException if it is not a boolean query. We should enforce this by changing the signature. This won't really break back compat. as it only works with instances of BooleanQuery.

"
"LUCENE-264","DOCUMENTATION","IMPROVEMENT","[PATCH] Improved javadoc for maxClauseCount","As discussed on lucene-dev before, queries with lots of terms can use 
up a lot of unused buffer space for their TermDocs, because most terms 
have few documents."
"LUCENE-3457","BUG","BUG","Upgrade to commons-compress 1.2","Commons Compress bug COMPRESS-127 was fixed in 1.2, so the workaround in benchmark's StreamUtils is no longer required. Compress is also used in solr. Replace with new jar in both benchmark and solr and get rid of that workaround."
"LUCENE-1677","CLEANUP","TASK","Remove GCJ IndexReader specializations","These specializations are outdated, unsupported, most probably pointless due to the speed of modern JVMs and, I bet, nobody uses them (Mike, you said you are going to ask people on java-user, anybody replied that they need it?). While giving nothing, they make SegmentReader instantiation code look real ugly.

If nobody objects, I'm going to post a patch that removes these from Lucene."
"LUCENE-1818","BUG","BUG","contrib-spatial java.lang.UnsupportedOperationException on QueryWrapperFilter.getDocIdSet","We use in our Project (which is in the devel phase) the latest Snapshot release of lucene. After i updated to the latest Snapshot a few days ago one of our JUnit tests fails and throws the following error:

java.lang.UnsupportedOperationException
	at org.apache.lucene.search.Query.createWeight(Query.java:91)
	at org.apache.lucene.search.QueryWrapperFilter.getDocIdSet(QueryWrapperFilter.java:72)
	at org.apache.lucene.misc.ChainedFilter.getDISI(ChainedFilter.java:150)
	at org.apache.lucene.misc.ChainedFilter.initialResult(ChainedFilter.java:173)
	at org.apache.lucene.misc.ChainedFilter.getDocIdSet(ChainedFilter.java:211)
	at org.apache.lucene.misc.ChainedFilter.getDocIdSet(ChainedFilter.java:141)
	at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.<init>(ConstantScoreQuery.java:116)
	at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:81)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:244)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:172)
	at org.apache.lucene.search.Searcher.search(Searcher.java:183)
	at org.hibernate.search.query.QueryHits.updateTopDocs(QueryHits.java:100)
	at org.hibernate.search.query.QueryHits.<init>(QueryHits.java:61)
	at org.hibernate.search.query.QueryHits.<init>(QueryHits.java:51)
	at org.hibernate.search.query.FullTextQueryImpl.getQueryHits(FullTextQueryImpl.java:373)
	at org.hibernate.search.query.FullTextQueryImpl.list(FullTextQueryImpl.java:293)
	...

I think it appeared after the Hudson build 917... and the following commit of the Query.java http://hudson.zones.apache.org/hudson/job/Lucene-trunk/917/changes#detail4 and is in connection with this JIRA issue: LUCENE-1771
I hope i'm at the right place and that you can fix it. Thanks!"
"LUCENE-2887","CLEANUP","IMPROVEMENT","Remove/deprecate IndexReader.undeleteAll","This API is rather dangerous in that it's ""best effort"" since it can only un-delete docs that have not yet been merged away, or, dropped (as of LUCENE-2010).

Given that it exposes impl details of how Lucene prunes deleted docs, I think we should remove this API.

Are there legitimate use cases....?"
"LUCENE-3643","IMPROVEMENT","IMPROVEMENT","Improve FilteredQuery to shortcut on wrapped MatchAllDocsQuery","Since the rewrite of Lucene trunk to delegate all Filter logic to FilteredQuery, by simply wrapping in IndexSearcher.wrapFilter(), we can do more short circuits and improve query execution. A common use case it to pass MatchAllDocsQuery as query to IndexSearcher and a filter. For the underlying hit collection this is stupid and slow, as MatchAllDocsQuery simply increments the docID and checks acceptDocs. If the filter is sparse, this is a big waste. This patch changes FilteredQuery.rewrite() to short circuit and return ConstantScoreQuery, if the query is MatchAllDocs."
"LUCENE-3694","REFACTORING","BUG","DocValuesField should not overload setInt/setFloat etc","See my description on LUCENE-3687. In general we should avoid this for primitive types and give them each unique names.

So I think instead of setInt(byte), setInt(short), setInt(int), setInt(long), setFloat(float) and setFloat(double),
we should have setByte(byte), setShort(short), setInt(int), setLong(long), setFloat(float) and setDouble(double)."
"LUCENE-3253","TEST","TEST","TestIndexwriterWithThreads#testCloseWithThreads hangs if a thread hit an exception before indexing its first document","in TestIndexwriterWithThreads#testCloseWithThreads we loop until all threads have indexed a single document but if one or more threads fail on before they index the first doc the test hangs forever. We should check if the thread is still alive unless it has indexed a document and fail if it already died."
"LUCENE-3311","CLEANUP","","Cleanup XML QueryParser Code","Before I move the XML QueryParser to the queryparser module, I want to pass over it and bring it up to module standards."
"LUCENE-2223","RFE","RFE","ShingleFilter benchmark","Spawned from LUCENE-2218: a benchmark for ShingleFilter, along with a new task to instantiate (non-default-constructor) ShingleAnalyzerWrapper: NewShingleAnalyzerTask.

The included shingle.alg runs ShingleAnalyzerWrapper, wrapping the default StandardAnalyzer, with 4 different configurations over 10,000 Reuters documents each.  To allow ShingleFilter timings to be isolated from the rest of the pipeline, StandardAnalyzer is also run over the same set of Reuters documents.  This set of 5 runs is then run 5 times.

The patch includes two perl scripts, the first to output JIRA table formatted timing information, with the minimum elapsed time for each of the 4 ShingleAnalyzerWrapper runs and the StandardAnalyzer run, and the second to compare two runs' JIRA output, producing another JIRA table showing % improvement."
"LUCENE-3748","BUG","IMPROVEMENT","EnglishPossessiveFilter should work with Unicode right single quotation mark","The current EnglishPossessiveFilter (used in EnglishAnalyzer) removes possessives using only the '\'' character (plus 's' or 'S'), but some common systems (German?) insert the Unicode ""\u2019"" (RIGHT SINGLE QUOTATION MARK) instead and this is not removed when processing UTF-8 text. I propose to change EnglishPossesiveFilter to support '\u2019' as an alternative to '\''."
"LUCENE-727","BUG","BUG","MMapDirectory can't create new index on Windows","When I set the system property to request the use of the mmap directory, and start building a large index, the process dies with an IOException trying to delete a file. Apparently, Lucene isn't closing down the memory map before deleting the file.
"
"LUCENE-1344","BUILD_SYSTEM","IMPROVEMENT","Make the Lucene jar an OSGi bundle","In order to use Lucene in an OSGi environment, some additional headers are needed in the manifest of the jar. As Lucene has no dependency, it is pretty straight forward and it ill be easy to maintain I think."
"LUCENE-3095","BUG","BUG","TestIndexWriter#testThreadInterruptDeadlock fails with OOM ","Selckin reported a repeatedly failing test that throws OOM Exceptions. According to the heapdump the MockDirectoryWrapper#createdFiles HashSet takes about 400MB heapspace containing 4194304 entries. Seems kind of way too many though :)

{noformat}
 [junit] java.lang.OutOfMemoryError: Java heap space
    [junit] Dumping heap to /tmp/java_pid25990.hprof ...
    [junit] Heap dump file created [520807744 bytes in 4.250 secs]
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
    [junit] Testcase: testThreadInterruptDeadlock(org.apache.lucene.index.TestIndexWriter):	FAILED
    [junit] 
    [junit] junit.framework.AssertionFailedError: 
    [junit] 	at org.apache.lucene.index.TestIndexWriter.testThreadInterruptDeadlock(TestIndexWriter.java:2249)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1282)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1211)
    [junit] 
    [junit] 
    [junit] Testcase: testThreadInterruptDeadlock(org.apache.lucene.index.TestIndexWriter):	FAILED
    [junit] Some threads threw uncaught exceptions!
    [junit] junit.framework.AssertionFailedError: Some threads threw uncaught exceptions!
    [junit] 	at org.apache.lucene.util.LuceneTestCase.tearDown(LuceneTestCase.java:557)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1282)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1211)
    [junit] 
    [junit] 
    [junit] Tests run: 67, Failures: 2, Errors: 0, Time elapsed: 3,254.884 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] FAILED; unexpected exception
    [junit] java.lang.OutOfMemoryError: Java heap space
    [junit] 	at org.apache.lucene.store.RAMFile.newBuffer(RAMFile.java:85)
    [junit] 	at org.apache.lucene.store.RAMFile.addBuffer(RAMFile.java:58)
    [junit] 	at org.apache.lucene.store.RAMOutputStream.switchCurrentBuffer(RAMOutputStream.java:132)
    [junit] 	at org.apache.lucene.store.RAMOutputStream.copyBytes(RAMOutputStream.java:171)
    [junit] 	at org.apache.lucene.store.MockIndexOutputWrapper.copyBytes(MockIndexOutputWrapper.java:155)
    [junit] 	at org.apache.lucene.index.CompoundFileWriter.copyFile(CompoundFileWriter.java:223)
    [junit] 	at org.apache.lucene.index.CompoundFileWriter.close(CompoundFileWriter.java:189)
    [junit] 	at org.apache.lucene.index.SegmentMerger.createCompoundFile(SegmentMerger.java:138)
    [junit] 	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3344)
    [junit] 	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2959)
    [junit] 	at org.apache.lucene.index.SerialMergeScheduler.merge(SerialMergeScheduler.java:37)
    [junit] 	at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1763)
    [junit] 	at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1758)
    [junit] 	at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1754)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1373)
    [junit] 	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1230)
    [junit] 	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1211)
    [junit] 	at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:2154)
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testThreadInterruptDeadlock -Dtests.seed=7183538093651149:3431510331342554160
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testThreadInterruptDeadlock -Dtests.seed=7183538093651149:3431510331342554160
    [junit] The following exceptions were thrown by threads:
    [junit] *** Thread: Thread-379 ***
    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_3r1n_0.tib=1, _3r1n_0.frq=1, _3r1n_0.pos=1, _3r1m.cfs=1, _3r1n_0.doc=1, _3r1n.tvf=1, _3r1n.tvd=1, _3r1n.tvx=1, _3r1n.fdx=1, _3r1n.fdt=1, _3r1q.cfs=1, _3r1o.cfs=1, _3r1n_0.skp=1, _3r1n_0.pyl=1}
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:448)
    [junit] 	at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:2217)
    [junit] Caused by: java.lang.RuntimeException: unclosed IndexOutput
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.createOutput(MockDirectoryWrapper.java:367)
    [junit] 	at org.apache.lucene.index.FieldInfos.write(FieldInfos.java:563)
    [junit] 	at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:82)
    [junit] 	at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:381)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:378)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:505)
    [junit] 	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:2621)
    [junit] 	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:2598)
    [junit] 	at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2464)
    [junit] 	at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2537)
    [junit] 	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2519)
    [junit] 	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2503)
    [junit] 	at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:2156)
    [junit] NOTE: test params are: codec=RandomCodecProvider: {=MockVariableIntBlock(baseBlockSize=105), f6=MockFixedIntBlock(blockSize=1372), f7=Pulsing(freqCutoff=11), f8=MockRandom, f9=MockVariableIntBlock(baseBlockSize=105), f1=MockSep, f0=Pulsing(freqCutoff=11), f3=Pulsing(freqCutoff=11), f2=MockFixedIntBlock(blockSize=1372), f5=MockVariableIntBlock(baseBlockSize=105), f4=MockRandom, f=Standard, c=Pulsing(freqCutoff=11), termVector=MockFixedIntBlock(blockSize=1372), d9=MockVariableIntBlock(baseBlockSize=105), d8=MockRandom, d5=MockSep, d4=Pulsing(freqCutoff=11), d7=MockFixedIntBlock(blockSize=1372), d6=MockVariableIntBlock(baseBlockSize=105), d25=SimpleText, d0=Pulsing(freqCutoff=11), c29=SimpleText, d24=MockSep, d1=MockSep, c28=MockVariableIntBlock(baseBlockSize=105), d23=MockRandom, d2=MockVariableIntBlock(baseBlockSize=105), c27=MockRandom, d22=Standard, d3=MockFixedIntBlock(blockSize=1372), d21=MockVariableIntBlock(baseBlockSize=105), d20=MockRandom, c22=SimpleText, c21=MockSep, c20=MockRandom, d29=MockFixedIntBlock(blockSize=1372), c26=MockFixedIntBlock(blockSize=1372), d28=MockVariableIntBlock(baseBlockSize=105), c25=MockVariableIntBlock(baseBlockSize=105), d27=MockSep, c24=MockSep, d26=Pulsing(freqCutoff=11), c23=Pulsing(freqCutoff=11), e9=MockSep, e8=Standard, e7=SimpleText, e6=MockVariableIntBlock(baseBlockSize=105), e5=MockRandom, c17=MockSep, e3=SimpleText, d12=Pulsing(freqCutoff=11), c16=Pulsing(freqCutoff=11), e4=Standard, d11=MockFixedIntBlock(blockSize=1372), c19=MockFixedIntBlock(blockSize=1372), e1=MockRandom, d14=MockVariableIntBlock(baseBlockSize=105), c18=MockVariableIntBlock(baseBlockSize=105), e2=MockVariableIntBlock(baseBlockSize=105), d13=MockRandom, e0=MockFixedIntBlock(blockSize=1372), d10=MockSep, d19=Pulsing(freqCutoff=11), c11=MockVariableIntBlock(baseBlockSize=105), c10=MockRandom, d16=MockRandom, c13=MockRandom, c12=Standard, d15=Standard, d18=SimpleText, c15=SimpleText, d17=MockSep, c14=MockSep, b3=SimpleText, b2=MockSep, b5=Pulsing(freqCutoff=11), b4=MockFixedIntBlock(blockSize=1372), b7=MockFixedIntBlock(blockSize=1372), b6=MockVariableIntBlock(baseBlockSize=105), d50=Pulsing(freqCutoff=11), b9=MockRandom, b8=Standard, d43=MockRandom, d42=Standard, d41=MockFixedIntBlock(blockSize=1372), d40=MockVariableIntBlock(baseBlockSize=105), d47=MockSep, d46=Pulsing(freqCutoff=11), b0=MockFixedIntBlock(blockSize=1372), d45=Standard, b1=Pulsing(freqCutoff=11), d44=SimpleText, d49=Pulsing(freqCutoff=11), d48=MockFixedIntBlock(blockSize=1372), c6=MockRandom, c5=Standard, c4=MockFixedIntBlock(blockSize=1372), c3=MockVariableIntBlock(baseBlockSize=105), c9=Pulsing(freqCutoff=11), c8=Standard, c7=SimpleText, d30=SimpleText, d32=Pulsing(freqCutoff=11), d31=MockFixedIntBlock(blockSize=1372), c1=Standard, d34=MockFixedIntBlock(blockSize=1372), c2=MockRandom, d33=MockVariableIntBlock(baseBlockSize=105), d36=MockRandom, c0=MockFixedIntBlock(blockSize=1372), d35=Standard, d38=Standard, d37=SimpleText, d39=Pulsing(freqCutoff=11), e92=MockFixedIntBlock(blockSize=1372), e93=Pulsing(freqCutoff=11), e90=MockSep, e91=SimpleText, e89=Pulsing(freqCutoff=11), e88=Standard, e87=SimpleText, e86=MockRandom, e85=Standard, e84=MockFixedIntBlock(blockSize=1372), e83=MockVariableIntBlock(baseBlockSize=105), e80=MockVariableIntBlock(baseBlockSize=105), e81=SimpleText, e82=Standard, e77=MockFixedIntBlock(blockSize=1372), e76=MockVariableIntBlock(baseBlockSize=105), e79=MockRandom, e78=Standard, e73=SimpleText, e72=MockSep, e75=Pulsing(freqCutoff=11), e74=MockFixedIntBlock(blockSize=1372), binary=Pulsing(freqCutoff=11), f98=MockSep, f97=Pulsing(freqCutoff=11), f99=MockVariableIntBlock(baseBlockSize=105), f94=MockRandom, f93=Standard, f96=SimpleText, f95=MockSep, e95=MockSep, e94=Pulsing(freqCutoff=11), e97=MockFixedIntBlock(blockSize=1372), e96=MockVariableIntBlock(baseBlockSize=105), e99=MockVariableIntBlock(baseBlockSize=105), e98=MockRandom, id=MockRandom, f34=SimpleText, f33=MockSep, f32=MockRandom, f31=Standard, f30=MockVariableIntBlock(baseBlockSize=105), f39=MockRandom, f38=MockFixedIntBlock(blockSize=1372), f37=MockVariableIntBlock(baseBlockSize=105), f36=MockSep, f35=Pulsing(freqCutoff=11), f43=MockSep, f42=Pulsing(freqCutoff=11), f45=MockFixedIntBlock(blockSize=1372), f44=MockVariableIntBlock(baseBlockSize=105), f41=SimpleText, f40=MockSep, f47=MockVariableIntBlock(baseBlockSize=105), f46=MockRandom, f49=Standard, f48=SimpleText, content=MockFixedIntBlock(blockSize=1372), e19=Standard, e18=SimpleText, e17=MockRandom, f12=Standard, e16=Standard, f11=SimpleText, f10=MockVariableIntBlock(baseBlockSize=105), e15=MockFixedIntBlock(blockSize=1372), e14=MockVariableIntBlock(baseBlockSize=105), f16=Pulsing(freqCutoff=11), e13=Pulsing(freqCutoff=11), f15=MockFixedIntBlock(blockSize=1372), e12=MockFixedIntBlock(blockSize=1372), e11=SimpleText, f14=SimpleText, e10=MockSep, f13=MockSep, f19=Standard, f18=MockFixedIntBlock(blockSize=1372), f17=MockVariableIntBlock(baseBlockSize=105), e29=MockFixedIntBlock(blockSize=1372), e26=Standard, f21=SimpleText, e25=SimpleText, f20=MockSep, e28=MockSep, f23=Pulsing(freqCutoff=11), e27=Pulsing(freqCutoff=11), f22=MockFixedIntBlock(blockSize=1372), f25=MockFixedIntBlock(blockSize=1372), e22=MockFixedIntBlock(blockSize=1372), f24=MockVariableIntBlock(baseBlockSize=105), e21=MockVariableIntBlock(baseBlockSize=105), f27=MockRandom, e24=MockRandom, f26=Standard, e23=Standard, f29=Standard, f28=SimpleText, e20=Pulsing(freqCutoff=11), field=MockRandom, string=Pulsing(freqCutoff=11), e30=MockSep, e31=SimpleText, a98=MockRandom, e34=MockVariableIntBlock(baseBlockSize=105), a99=MockVariableIntBlock(baseBlockSize=105), e35=MockFixedIntBlock(blockSize=1372), f79=MockVariableIntBlock(baseBlockSize=105), e32=Pulsing(freqCutoff=11), e33=MockSep, b97=Pulsing(freqCutoff=11), f77=MockFixedIntBlock(blockSize=1372), e38=SimpleText, b98=MockSep, f78=Pulsing(freqCutoff=11), e39=Standard, b99=MockVariableIntBlock(baseBlockSize=105), f75=MockSep, e36=MockRandom, f76=SimpleText, e37=MockVariableIntBlock(baseBlockSize=105), f73=SimpleText, f74=Standard, f71=MockRandom, f72=MockVariableIntBlock(baseBlockSize=105), f81=MockFixedIntBlock(blockSize=1372), f80=MockVariableIntBlock(baseBlockSize=105), e40=MockSep, e41=MockVariableIntBlock(baseBlockSize=105), e42=MockFixedIntBlock(blockSize=1372), e43=MockRandom, e44=MockVariableIntBlock(baseBlockSize=105), e45=SimpleText, e46=Standard, f86=MockVariableIntBlock(baseBlockSize=105), e47=MockSep, f87=MockFixedIntBlock(blockSize=1372), e48=SimpleText, f88=Standard, e49=MockFixedIntBlock(blockSize=1372), f89=MockRandom, f82=MockSep, f83=SimpleText, f84=MockFixedIntBlock(blockSize=1372), f85=Pulsing(freqCutoff=11), f90=MockVariableIntBlock(baseBlockSize=105), f92=Standard, f91=SimpleText, str=MockFixedIntBlock(blockSize=1372), a76=MockVariableIntBlock(baseBlockSize=105), e56=MockRandom, f59=MockRandom, a77=MockFixedIntBlock(blockSize=1372), e57=MockVariableIntBlock(baseBlockSize=105), a78=Standard, e54=MockFixedIntBlock(blockSize=1372), f57=MockFixedIntBlock(blockSize=1372), a79=MockRandom, e55=Pulsing(freqCutoff=11), f58=Pulsing(freqCutoff=11), e52=Pulsing(freqCutoff=11), e53=MockSep, e50=SimpleText, e51=Standard, f51=Standard, f52=MockRandom, f50=MockFixedIntBlock(blockSize=1372), f55=Pulsing(freqCutoff=11), f56=MockSep, f53=SimpleText, e58=Standard, f54=Standard, e59=MockRandom, a80=MockVariableIntBlock(baseBlockSize=105), e60=MockRandom, a82=Standard, a81=SimpleText, a84=SimpleText, a83=MockSep, a86=Pulsing(freqCutoff=11), a85=MockFixedIntBlock(blockSize=1372), a89=Pulsing(freqCutoff=11), f68=Standard, e65=Standard, f69=MockRandom, e66=MockRandom, a87=SimpleText, e67=MockSep, a88=Standard, e68=SimpleText, e61=MockFixedIntBlock(blockSize=1372), e62=Pulsing(freqCutoff=11), e63=MockRandom, e64=MockVariableIntBlock(baseBlockSize=105), f60=SimpleText, f61=Standard, f62=Pulsing(freqCutoff=11), f63=MockSep, e69=Pulsing(freqCutoff=11), f64=MockFixedIntBlock(blockSize=1372), f65=Pulsing(freqCutoff=11), f66=MockRandom, f67=MockVariableIntBlock(baseBlockSize=105), f70=MockRandom, a93=Pulsing(freqCutoff=11), a92=MockFixedIntBlock(blockSize=1372), a91=SimpleText, e71=MockSep, a90=MockSep, e70=Pulsing(freqCutoff=11), a97=MockRandom, a96=Standard, a95=MockFixedIntBlock(blockSize=1372), a94=MockVariableIntBlock(baseBlockSize=105), c58=MockFixedIntBlock(blockSize=1372), a63=Pulsing(freqCutoff=11), a64=MockSep, c59=Pulsing(freqCutoff=11), c56=MockSep, d59=MockSep, a61=SimpleText, c57=SimpleText, a62=Standard, c54=SimpleText, c55=Standard, a60=MockRandom, c52=MockRandom, c53=MockVariableIntBlock(baseBlockSize=105), d53=MockVariableIntBlock(baseBlockSize=105), d54=MockFixedIntBlock(blockSize=1372), d51=Pulsing(freqCutoff=11), d52=MockSep, d57=SimpleText, b62=Standard, d58=Standard, b63=MockRandom, d55=MockRandom, b60=MockVariableIntBlock(baseBlockSize=105), d56=MockVariableIntBlock(baseBlockSize=105), b61=MockFixedIntBlock(blockSize=1372), b56=MockSep, b55=Pulsing(freqCutoff=11), b54=Standard, b53=SimpleText, d61=SimpleText, b59=MockRandom, d60=MockSep, b58=Pulsing(freqCutoff=11), b57=MockFixedIntBlock(blockSize=1372), c62=MockFixedIntBlock(blockSize=1372), c61=MockVariableIntBlock(baseBlockSize=105), a59=MockRandom, c60=MockSep, a58=Standard, a57=MockVariableIntBlock(baseBlockSize=105), a56=MockRandom, a55=Pulsing(freqCutoff=11), a54=MockFixedIntBlock(blockSize=1372), a72=MockFixedIntBlock(blockSize=1372), c67=MockVariableIntBlock(baseBlockSize=105), a73=Pulsing(freqCutoff=11), c68=MockFixedIntBlock(blockSize=1372), a74=MockRandom, c69=Standard, a75=MockVariableIntBlock(baseBlockSize=105), c63=MockSep, c64=SimpleText, a70=Pulsing(freqCutoff=11), c65=MockFixedIntBlock(blockSize=1372), a71=MockSep, c66=Pulsing(freqCutoff=11), d62=MockRandom, d63=MockVariableIntBlock(baseBlockSize=105), d64=SimpleText, b70=MockRandom, d65=Standard, b71=SimpleText, d66=MockSep, b72=Standard, d67=SimpleText, b73=Pulsing(freqCutoff=11), d68=MockFixedIntBlock(blockSize=1372), b74=MockSep, d69=Pulsing(freqCutoff=11), b65=Pulsing(freqCutoff=11), b64=MockFixedIntBlock(blockSize=1372), b67=MockVariableIntBlock(baseBlockSize=105), b66=MockRandom, d70=MockSep, b69=MockRandom, b68=Standard, d72=MockFixedIntBlock(blockSize=1372), d71=MockVariableIntBlock(baseBlockSize=105), c71=MockVariableIntBlock(baseBlockSize=105), c70=MockRandom, a69=Pulsing(freqCutoff=11), c73=Standard, c72=SimpleText, a66=MockRandom, a65=Standard, a68=SimpleText, a67=MockSep, c32=Standard, c33=MockRandom, c30=MockVariableIntBlock(baseBlockSize=105), c31=MockFixedIntBlock(blockSize=1372), c36=Pulsing(freqCutoff=11), a41=MockSep, c37=MockSep, a42=SimpleText, a0=MockSep, c34=SimpleText, c35=Standard, a40=MockRandom, b84=SimpleText, d79=MockSep, b85=Standard, b82=MockRandom, d77=Standard, c38=MockFixedIntBlock(blockSize=1372), b83=MockVariableIntBlock(baseBlockSize=105), d78=MockRandom, c39=Pulsing(freqCutoff=11), b80=MockVariableIntBlock(baseBlockSize=105), d75=MockRandom, b81=MockFixedIntBlock(blockSize=1372), d76=MockVariableIntBlock(baseBlockSize=105), d73=MockFixedIntBlock(blockSize=1372), d74=Pulsing(freqCutoff=11), d83=MockSep, a9=Standard, d82=Pulsing(freqCutoff=11), d81=Standard, d80=SimpleText, b79=MockVariableIntBlock(baseBlockSize=105), b78=Pulsing(freqCutoff=11), b77=MockFixedIntBlock(blockSize=1372), b76=SimpleText, b75=MockSep, a1=SimpleText, a35=MockFixedIntBlock(blockSize=1372), a2=Standard, a34=MockVariableIntBlock(baseBlockSize=105), a3=Pulsing(freqCutoff=11), a33=MockSep, a4=MockSep, a32=Pulsing(freqCutoff=11), a5=MockFixedIntBlock(blockSize=1372), a39=Standard, c40=Pulsing(freqCutoff=11), a6=Pulsing(freqCutoff=11), a38=SimpleText, a7=MockRandom, a37=MockVariableIntBlock(baseBlockSize=105), a8=MockVariableIntBlock(baseBlockSize=105), a36=MockRandom, c41=SimpleText, c42=Standard, c43=Pulsing(freqCutoff=11), c44=MockSep, c45=MockFixedIntBlock(blockSize=1372), a50=Pulsing(freqCutoff=11), c46=Pulsing(freqCutoff=11), a51=MockSep, c47=MockRandom, a52=MockVariableIntBlock(baseBlockSize=105), c48=MockVariableIntBlock(baseBlockSize=105), a53=MockFixedIntBlock(blockSize=1372), b93=MockSep, d88=Pulsing(freqCutoff=11), c49=Standard, b94=SimpleText, d89=MockSep, b95=MockFixedIntBlock(blockSize=1372), b96=Pulsing(freqCutoff=11), d84=Standard, b90=MockVariableIntBlock(baseBlockSize=105), d85=MockRandom, b91=SimpleText, d86=MockSep, b92=Standard, d87=SimpleText, d92=Pulsing(freqCutoff=11), d91=MockFixedIntBlock(blockSize=1372), d94=MockVariableIntBlock(baseBlockSize=105), d93=MockRandom, b87=MockFixedIntBlock(blockSize=1372), b86=MockVariableIntBlock(baseBlockSize=105), d90=MockSep, b89=MockRandom, b88=Standard, a44=MockVariableIntBlock(baseBlockSize=105), a43=MockRandom, a46=Standard, a45=SimpleText, a48=SimpleText, a47=MockSep, c51=MockRandom, a49=MockFixedIntBlock(blockSize=1372), c50=Standard, d98=MockRandom, d97=Standard, d96=MockFixedIntBlock(blockSize=1372), d95=MockVariableIntBlock(baseBlockSize=105), d99=SimpleText, a20=Standard, c99=MockSep, c98=Pulsing(freqCutoff=11), c97=Standard, c96=SimpleText, b19=Standard, a16=Standard, a17=MockRandom, b17=MockVariableIntBlock(baseBlockSize=105), a14=MockVariableIntBlock(baseBlockSize=105), b18=MockFixedIntBlock(blockSize=1372), a15=MockFixedIntBlock(blockSize=1372), a12=MockFixedIntBlock(blockSize=1372), a13=Pulsing(freqCutoff=11), a10=MockSep, a11=SimpleText, b11=SimpleText, b12=Standard, b10=MockVariableIntBlock(baseBlockSize=105), b15=MockFixedIntBlock(blockSize=1372), b16=Pulsing(freqCutoff=11), a18=SimpleText, b13=MockSep, a19=Standard, b14=SimpleText, b30=Standard, a31=Pulsing(freqCutoff=11), a30=MockFixedIntBlock(blockSize=1372), b28=SimpleText, a25=SimpleText, b29=Standard, a26=Standard, a27=Pulsing(freqCutoff=11), a28=MockSep, a21=MockVariableIntBlock(baseBlockSize=105), a22=MockFixedIntBlock(blockSize=1372), a23=Standard, a24=MockRandom, b20=MockSep, b21=SimpleText, b22=MockFixedIntBlock(blockSize=1372), b23=Pulsing(freqCutoff=11), a29=MockFixedIntBlock(blockSize=1372), b24=MockVariableIntBlock(baseBlockSize=105), b25=MockFixedIntBlock(blockSize=1372), b26=Standard, b27=MockRandom, b41=MockVariableIntBlock(baseBlockSize=105), b40=MockRandom, c77=SimpleText, c76=MockSep, c75=MockRandom, c74=Standard, c79=MockSep, c78=Pulsing(freqCutoff=11), c80=MockSep, c83=MockRandom, c84=MockVariableIntBlock(baseBlockSize=105), c81=MockFixedIntBlock(blockSize=1372), b39=MockRandom, c82=Pulsing(freqCutoff=11), b37=MockVariableIntBlock(baseBlockSize=105), b38=MockFixedIntBlock(blockSize=1372), b35=Pulsing(freqCutoff=11), b36=MockSep, b33=MockSep, b34=SimpleText, b31=Standard, b32=MockRandom, str2=Standard, b50=MockRandom, b52=SimpleText, str3=Pulsing(freqCutoff=11), b51=MockSep, c86=MockSep, tvtest=Pulsing(freqCutoff=11), c85=Pulsing(freqCutoff=11), c88=MockFixedIntBlock(blockSize=1372), c87=MockVariableIntBlock(baseBlockSize=105), c89=MockRandom, c90=MockRandom, c91=MockVariableIntBlock(baseBlockSize=105), c92=Standard, c93=MockRandom, c94=MockSep, c95=SimpleText, content1=SimpleText, b46=MockRandom, b47=MockVariableIntBlock(baseBlockSize=105), content3=MockRandom, b48=SimpleText, content4=Standard, b49=Standard, content5=MockVariableIntBlock(baseBlockSize=105), b42=Pulsing(freqCutoff=11), b43=MockSep, b44=MockVariableIntBlock(baseBlockSize=105), b45=MockFixedIntBlock(blockSize=1372)}, locale=it_CH, timezone=Europe/Chisinau
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestMergeSchedulerExternal, TestToken, TestCodecs, TestFieldInfos, TestFlushByRamOrCountsPolicy, TestIndexReaderReopen, TestIndexWriter]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=275548088,total=309395456
{noformat}"
"LUCENE-746","IMPROVEMENT","IMPROVEMENT","Incorrect error message in AnalyzingQueryParser.getPrefixQuery","The error message of  getPrefixQuery is incorrect when tokens were added, for example by a stemmer. The message is ""token was consumed"" even if tokens were added.
Attached is a patch, which when applied gives a better description of what actually happened."
"LUCENE-2955","RFE","IMPROVEMENT","Add utitily class to manage NRT reopening","I created a simple class, NRTManager, that tries to abstract away some
of the reopen logic when using NRT readers.

You give it your IW, tell it min and max nanoseconds staleness you can
tolerate, and it privately runs a reopen thread to periodically reopen
the searcher.

It subsumes the SearcherManager from LIA2.  Besides running the reopen
thread, it also adds the notion of a ""generation"" containing changes
you've made.  So eg it has addDocument, returning a long.  You can
then take that long value and pass it back to the getSearcher method
and getSearcher will return a searcher that reflects the changes made
in that generation.

This gives your app the freedom to force ""immediate"" consistency (ie
wait for the reopen) only for those searches that require it, like a
verifier that adds a doc and then immediately searches for it, but
also use ""eventual consistency"" for other searches.

I want to also add support for the new ""applyDeletions"" option when
pulling an NRT reader.

Also, this is very new and I'm sure buggy -- the concurrency is either
wrong over overly-locking.  But it's a start...
"
"LUCENE-3478","BUG","BUG","TestSimpleExplanations failure","{noformat}
ant test -Dtestcase=TestSimpleExplanations -Dtestmethod=testDMQ8 -Dtests.seed=-7e984babece66153:3e3298ae627b33a9:3093059db62bcc71
{noformat}

fails w/ this on current trunk... looks like silly floating point precision issue:

{noformat}

    [junit] Testsuite: org.apache.lucene.search.TestSimpleExplanations
    [junit]   1.4508595 = (MATCH) sum of:
    [junit]     1.4508595 = (MATCH) weight(field:yy in 2) [DefaultSimilarity], result of:
    [junit]       1.4508595 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]         1.287682 = queryWeight, product of:
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           1.0 = queryNorm
    [junit]         1.1267219 = fieldWeight in 2, product of:
    [junit]           1.0 = tf(freq=1.0), with freq of:
    [junit]             1.0 = termFreq=1
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           0.875 = fieldNorm(doc=2)
    [junit]   145085.95 = (MATCH) weight(field:xx^100000.0 in 2) [DefaultSimilarity], result of:
    [junit]     145085.95 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]       128768.2 = queryWeight, product of:
    [junit]         100000.0 = boost
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         1.0 = queryNorm
    [junit]       1.1267219 = fieldWeight in 2, product of:
    [junit]         1.0 = tf(freq=1.0), with freq of:
    [junit]           1.0 = termFreq=1
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         0.875 = fieldNorm(doc=2)
    [junit]  expected:<145086.66> but was:<145086.69>)
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.544 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestSimpleExplanations -Dtestmethod=testDMQ8 -Dtests.seed=144152895b276837:eb7ba4953db943f:33373b79a971db02
    [junit] NOTE: test params are: codec=PreFlex, sim=RandomSimilarityProvider(queryNorm=false,coord=false): {field=DefaultSimilarity, alt=DFR I(ne)LZ(0.3), KEY=IB LL-D2}, locale=en_IN, timezone=Pacific/Samoa
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestSimpleExplanations]
    [junit] NOTE: Linux 2.6.33.6-147.fc13.x86_64 amd64/Sun Microsystems Inc. 1.6.0_21 (64-bit)/cpus=24,threads=1,free=130426744,total=189988864
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testDMQ8(org.apache.lucene.search.TestSimpleExplanations):	FAILED
    [junit] ((field:yy field:w5^100.0) | field:xx^100000.0)~0.5: score(doc=2)=145086.66 != explanationScore=145086.69 Explanation: 145086.69 = (MATCH) max plus 0.5 times others of:
    [junit]   1.4508595 = (MATCH) sum of:
    [junit]     1.4508595 = (MATCH) weight(field:yy in 2) [DefaultSimilarity], result of:
    [junit]       1.4508595 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]         1.287682 = queryWeight, product of:
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           1.0 = queryNorm
    [junit]         1.1267219 = fieldWeight in 2, product of:
    [junit]           1.0 = tf(freq=1.0), with freq of:
    [junit]             1.0 = termFreq=1
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           0.875 = fieldNorm(doc=2)
    [junit]   145085.95 = (MATCH) weight(field:xx^100000.0 in 2) [DefaultSimilarity], result of:
    [junit]     145085.95 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]       128768.2 = queryWeight, product of:
    [junit]         100000.0 = boost
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         1.0 = queryNorm
    [junit]       1.1267219 = fieldWeight in 2, product of:
    [junit]         1.0 = tf(freq=1.0), with freq of:
    [junit]           1.0 = termFreq=1
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         0.875 = fieldNorm(doc=2)
    [junit]  expected:<145086.66> but was:<145086.69>
    [junit] junit.framework.AssertionFailedError: ((field:yy field:w5^100.0) | field:xx^100000.0)~0.5: score(doc=2)=145086.66 != explanationScore=145086.69 Explanation: 145086.69 = (MATCH) max plus 0.5 times others of:
    [junit]   1.4508595 = (MATCH) sum of:
    [junit]     1.4508595 = (MATCH) weight(field:yy in 2) [DefaultSimilarity], result of:
    [junit]       1.4508595 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]         1.287682 = queryWeight, product of:
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           1.0 = queryNorm
    [junit]         1.1267219 = fieldWeight in 2, product of:
    [junit]           1.0 = tf(freq=1.0), with freq of:
    [junit]             1.0 = termFreq=1
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           0.875 = fieldNorm(doc=2)
    [junit]   145085.95 = (MATCH) weight(field:xx^100000.0 in 2) [DefaultSimilarity], result of:
    [junit]     145085.95 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]       128768.2 = queryWeight, product of:
    [junit]         100000.0 = boost
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         1.0 = queryNorm
    [junit]       1.1267219 = fieldWeight in 2, product of:
    [junit]         1.0 = tf(freq=1.0), with freq of:
    [junit]           1.0 = termFreq=1
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         0.875 = fieldNorm(doc=2)
    [junit]  expected:<145086.66> but was:<145086.69>
    [junit] 	at org.apache.lucene.search.CheckHits.verifyExplanation(CheckHits.java:324)
    [junit] 	at org.apache.lucene.search.CheckHits$ExplanationAsserter.collect(CheckHits.java:494)
    [junit] 	at org.apache.lucene.search.Scorer.score(Scorer.java:60)
    [junit] 	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:580)
    [junit] 	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:363)
    [junit] 	at org.apache.lucene.search.CheckHits.checkExplanations(CheckHits.java:302)
    [junit] 	at org.apache.lucene.search.QueryUtils.checkExplanations(QueryUtils.java:92)
    [junit] 	at org.apache.lucene.search.QueryUtils.check(QueryUtils.java:126)
    [junit] 	at org.apache.lucene.search.QueryUtils.check(QueryUtils.java:122)
    [junit] 	at org.apache.lucene.search.QueryUtils.check(QueryUtils.java:106)
    [junit] 	at org.apache.lucene.search.CheckHits.checkHitCollector(CheckHits.java:89)
    [junit] 	at org.apache.lucene.search.TestExplanations.qtest(TestExplanations.java:99)
    [junit] 	at org.apache.lucene.search.TestSimpleExplanations.testDMQ8(TestSimpleExplanations.java:224)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.search.TestSimpleExplanations FAILED
{noformat}"
"LUCENE-935","BUILD_SYSTEM","IMPROVEMENT","Improve maven artifacts","There are a couple of things we can improve for the next release:
- ""*pom.xml"" files should be renamed to ""*pom.xml.template""
- artifacts ""lucene-parent"" should extend ""apache-parent""
- add source jars as artifacts
- update <generate-maven-artifacts> task to work with latest version of maven-ant-tasks.jar
- metadata filenames should not contain ""local"""
"LUCENE-703","IMPROVEMENT","IMPROVEMENT","Change QueryParser to use ConstantScoreRangeQuery in preference to RangeQuery by default","Change to QueryParser to default to using new ConstantScoreRangeQuery in preference to RangeQuery
for range queries. This implementation is generally preferable because it 
a) Runs faster 
b) Does not have the scarcity of range terms unduly influence score 
c) avoids any ""TooManyBooleanClauses"" exception.

However, if applications really need to use the old-fashioned RangeQuery and the above
points are not required then the  ""useOldRangeQuery"" property can be used to revert to old behaviour.

The patch includes extra Junit tests for this flag and all other Junit tests pass"
"LUCENE-2678","TEST","TEST","TestCachingSpanFilter sometimes fails","if I run 
{noformat} 
ant test -Dtestcase=TestCachingSpanFilter -Dtestmethod=testEnforceDeletions -Dtests.seed=5015158121350221714:-3342860915127740146 -Dtests.iter=100
{noformat} 

I get two failures on my machine against current trunk
{noformat} 

junit-sequential:
    [junit] Testsuite: org.apache.lucene.search.TestCachingSpanFilter
    [junit] Testcase: testEnforceDeletions(org.apache.lucene.search.TestCachingSpanFilter):	FAILED
    [junit] expected:<2> but was:<3>
    [junit] junit.framework.AssertionFailedError: expected:<2> but was:<3>
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit] 	at org.apache.lucene.search.TestCachingSpanFilter.testEnforceDeletions(TestCachingSpanFilter.java:101)
    [junit] 
    [junit] 
    [junit] Testcase: testEnforceDeletions(org.apache.lucene.search.TestCachingSpanFilter):	FAILED
    [junit] expected:<2> but was:<3>
    [junit] junit.framework.AssertionFailedError: expected:<2> but was:<3>
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit] 	at org.apache.lucene.search.TestCachingSpanFilter.testEnforceDeletions(TestCachingSpanFilter.java:101)
    [junit] 
    [junit] 
    [junit] Tests run: 100, Failures: 2, Errors: 0, Time elapsed: 2.297 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestCachingSpanFilter -Dtestmethod=testEnforceDeletions -Dtests.seed=5015158121350221714:-3342860915127740146
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestCachingSpanFilter -Dtestmethod=testEnforceDeletions -Dtests.seed=5015158121350221714:-3342860915127740146
    [junit] NOTE: test params are: codec=MockVariableIntBlock(baseBlockSize=43), locale=fr, timezone=Africa/Bangui
    [junit] ------------- ---------------- ---------------
    [junit] Test org.apache.lucene.search.TestCachingSpanFilter FAILED
{noformat}

not sure what it is but it seems likely to be a WeakRef / GC issue in the cache. "
"LUCENE-3146","BUG","BUG","IndexReader.setNorms is no op if one of the field instances omits norms","If I add two documents to an index w/ same field, and one of them omit norms, then IndexReader.setNorms is no-op. I'll attach a patch w/ test case"
"LUCENE-2351","IMPROVEMENT","IMPROVEMENT","optimize automatonquery","Mike found a few cases in flex where we have some bad behavior with automatonquery.
The problem is similar to a database query planner, where sometimes simply doing a full table scan is faster than using an index.

We can optimize automatonquery a little bit, and get better performance for fuzzy,wildcard,regex queries.

Here is a list of ideas:
* create commonSuffixRef for infinite automata, not just really-bad linear scan cases
* do a null check rather than populating an empty commonSuffixRef
* localize the 'linear' case to not seek, but instead scan, when ping-ponging against loops in the state machine
* add a mechanism to enable/disable the terms dict cache, e.g. we can disable it for infinite cases, and maybe fuzzy N>1 also.
* change the use of BitSet to OpenBitSet or long[] gen for path-tracking
* optimize the backtracking code where it says /* String is good to go as-is */, this need not be a full run(), I think...
"
"LUCENE-2858","REFACTORING","TASK","Separate SegmentReaders (and other atomic readers) from composite IndexReaders","With current trunk, whenever you open an IndexReader on a directory you get back a DirectoryReader which is a composite reader. The interface of IndexReader has now lots of methods that simply throw UOE (in fact more than 50% of all methods that are commonly used ones are unuseable now). This confuses users and makes the API hard to understand.

This issue should split ""atomic readers"" from ""reader collections"" with a separate API. After that, you are no longer able, to get TermsEnum without wrapping from those composite readers. We currently have helper classes for wrapping (SlowMultiReaderWrapper - please rename, the name is really ugly; or Multi*), those should be retrofitted to implement the correct classes (SlowMultiReaderWrapper would be an atomic reader but takes a composite reader as ctor param, maybe it could also simply take a List<AtomicReader>). In my opinion, maybe composite readers could implement some collection APIs and also have the ReaderUtil method directly built in (possibly as a ""view"" in the util.Collection sense). In general composite readers do not really need to look like the previous IndexReaders, they could simply be a ""collection"" of SegmentReaders with some functionality like reopen.

On the other side, atomic readers do not need reopen logic anymore? When a segment changes, you need a new atomic reader? - maybe because of deletions thats not the best idea, but we should investigate. Maybe make the whole reopen logic simplier to use (ast least on the collection reader level).

We should decide about good names, i have no preference at the moment."
"LUCENE-2811","BUG","BUG","SegmentInfo should explicitly track whether that segment wrote term vectors","Today SegmentInfo doesn't know if it has vectors, which means its files() method must check if the files exist.

This leads to subtle bugs, because Si.files() caches the files but then we fail to invalidate that later when the term vectors files are created.

It also leads to sloppy code, eg TermVectorsReader ""gracefully"" handles being opened when the files do not exist.  I don't like that; it should only be opened if they exist.

This also fixes these intermittent failures we've been seeing:

{noformat}
junit.framework.AssertionFailedError: IndexFileDeleter doesn't know about file _1e.tvx
       at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:979)
       at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:917)
       at org.apache.lucene.index.IndexWriter.filesExist(IndexWriter.java:3633)
       at org.apache.lucene.index.IndexWriter.startCommit(IndexWriter.java:3699)
       at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2407)
       at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2478)
       at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2460)
       at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2444)
       at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:213)
{noformat}"
"LUCENE-1499","REFACTORING","IMPROVEMENT","Minor refactoring to IndexFileNameFilter","IndexFileNameFilter looks like it's designed to be a singleton, however its constructor is public and its singleton member is package visible. The proposed patch changes the constructor and member to private. Since it already has a static getFilter() method, and no code in Lucene references those two, I don't think it creates any problems from an API perspective."
"LUCENE-991","BUG","BUG","BoostingTermQuery.explain() bugs","There are a couple of minor bugs in BoostingTermQuery.explain().

1. The computation of average payload score produces NaN if no payloads were found. It should probably be:
float avgPayloadScore = super.score() * (payloadsSeen > 0 ? (payloadScore / payloadsSeen) : 1);

2. If the average payload score is zero, the value of the explanation is 0:
result.setValue(nonPayloadExpl.getValue() * avgPayloadScore);
If the query is part of a BooleanClause, this results in:
""no match on required clause...""
""failure to meet condition(s) of required/prohibited clause(s)""

The average payload score can be zero if the field boost = 0.

I've attached a patch to 'TestBoostingTermQuery.java', however, the test 'testNoPayload' fails in 'SpanScorer.score()' because the doc = -1. It looks like 'setFreqCurrentDoc() should have been called before 'score()'. Maybe someone more knowledgable of spans could investigate this.
"
"LUCENE-1768","RFE","RFE","NumericRange support for new query parser","It would be good to specify some type of ""schema"" for the query parser in future, to automatically create NumericRangeQuery for different numeric types? It would then be possible to index a numeric value (double,float,long,int) using NumericField and then the query parser knows, which type of field this is and so it correctly creates a NumericRangeQuery for strings like ""[1.567..*]"" or ""(1.787..19.5]"".

There is currently no way to extract if a field is numeric from the index, so the user will have to configure the FieldConfig objects in the ConfigHandler. But if this is done, it will not be that difficult to implement the rest.

The only difference between the current handling of RangeQuery is then the instantiation of the correct Query type and conversion of the entered numeric values (simple Number.valueOf(...) cast of the user entered numbers). Evenerything else is identical, NumericRangeQuery also supports the MTQ rewrite modes (as it is a MTQ).

Another thing is a change in Date semantics. There are some strange flags in the current parser that tells it how to handle dates."
"LUCENE-3347","BUG","BUG","XMLparser drops user boosting","The lucene XML parser seems to convert user defined boosting back to default 1.0 and thus boosting value is dropped from the query...

e.g.

{code:xml}
<BooleanQuery>
	<Clause occurs=""must"">
		<BooleanQuery>
			<Clause occurs=""should"">
				<UserQuery fieldName=""Vehicle.Colour"">red^66 blue~^8</UserQuery>
			</Clause>
		</BooleanQuery>
	</Clause>
	<Clause occurs=""should"">
		<BooleanQuery>
			<Clause occurs=""should"">
				<UserQuery fieldName=""Vehicle.Colour"">black^0.01</UserQuery>
			</Clause>
		</BooleanQuery>
	</Clause>
</BooleanQuery>
{code}

produces a lucene query: +( ( Vehicle.Colour:red^66 Vehicle.Colour:blue~0.5^8 ) ) ( Vehicle.Colour:black )

The expected query : +( ( Vehicle.Colour:red^66 Vehicle.Colour:blue~0.5^8 ) ) ( Vehicle.Colour:black^0.01 )

I have developed a work around by modifying line 77 of UserInputQueryBuilder.java 

from:

{code:java}
q.setBoost(DOMUtils.getAttribute(e,""boost"",1.0f));
{code}

to:

{code:java}
q.setBoost( DOMUtils.getAttribute( e, ""boost"", q.getBoost() ) );
{code}

"
"LUCENE-1021","TEST","BUG","Unit tests do not fail if a ConcurrentMergeScheduler thread hits an exception","Now that CMS is the default, it's important to fail any unit test that
hits an exception in a CMS thread.  But they do not fail now.  The
preferred solution (thanks to Erik Hatcher) is to fix all Lucene unit
tests to subclass from a new LuceneTestCase (in o.a.l.util) base that
asserts that there were no such exceptions during the test.
"
"LUCENE-800","BUG","BUG","Incorrect parsing by QueryParser.parse() when it encounters backslashes (always eats one backslash.)","Test code and output follow. Tested  Lucene 1.9 version only. Affects hose who would index/search for Lucene's reserved characters.

Description: When an input search string has a sequence of N (java-escaped) backslashes, where N >= 2, the QueryParser will produce a query in which that sequence has N-1 backslashes.

TEST CODE:
    Analyzer analyzer = new WhitespaceAnalyzer();
    String[] queryStrs = {""item:\\\\"",
                          ""item:\\\\*"",
                          ""(item:\\\\ item:ABCD\\\\))"",
                          ""(item:\\\\ item:ABCD\\\\)""};
    for (String queryStr : queryStrs) {
      System.out.println(""--------------------------------------"");
      System.out.println(""String queryStr = "" + queryStr);
      Query luceneQuery = null;
      try {
        luceneQuery = new QueryParser(""_default_"", analyzer).parse(queryStr);
        System.out.println(""luceneQuery.toString() = "" + luceneQuery.toString());
      } catch (Exception e) {
        System.out.println(e.getClass().toString());
      }
    }

OUTPUT (with remarks in comment notation:) 
--------------------------------------
String queryStr = item:\\
luceneQuery.toString() = item:\             //One backslash has disappeared. Searcher will fail on this query.
--------------------------------------
String queryStr = item:\\*
luceneQuery.toString() = item:\*           //One backslash has disappeared. This query will search for something unintended.
--------------------------------------
String queryStr = (item:\\ item:ABCD\\))
luceneQuery.toString() = item:\ item:ABCD\)     //This should have thrown a ParseException because of an unescaped ')'. It did not.
--------------------------------------
String queryStr = (item:\\ item:ABCD\\)
class org.apache.lucene.queryParser.ParseException        //...and this one should not have, but it did.

"
"LUCENE-2203","TEST","TEST","improved snowball testing","Snowball project has test vocabulary files for each language in their svn repository, along with expected output.

We should use these tests to ensure all languages are working correctly, and it might be helpful in the future for identifying back breaks/changes if we ever want to upgrade snowball, etc.
"
"LUCENE-2680","IMPROVEMENT","IMPROVEMENT","Improve how IndexWriter flushes deletes against existing segments","IndexWriter buffers up all deletes (by Term and Query) and only
applies them if 1) commit or NRT getReader() is called, or 2) a merge
is about to kickoff.

We do this because, for a large index, it's very costly to open a
SegmentReader for every segment in the index.  So we defer as long as
we can.  We do it just before merge so that the merge can eliminate
the deleted docs.

But, most merges are small, yet in a big index we apply deletes to all
of the segments, which is really very wasteful.

Instead, we should only apply the buffered deletes to the segments
that are about to be merged, and keep the buffer around for the
remaining segments.

I think it's not so hard to do; we'd have to have generations of
pending deletions, because the newly merged segment doesn't need the
same buffered deletions applied again.  So every time a merge kicks
off, we pinch off the current set of buffered deletions, open a new
set (the next generation), and record which segment was created as of
which generation.

This should be a very sizable gain for large indices that mix
deletes, though, less so in flex since opening the terms index is much
faster.
"
"LUCENE-2054","DOCUMENTATION","TASK","Remove ""System Properties"" page from release specific docs","We no longer use system properties to configure Lucene in version 3.0, the page is obsolete and should be removed before release."
"LUCENE-938","BUG","BUG","I/O exceptions can cause loss of buffered deletes","Some I/O exceptions that result in segmentInfos rollback operations can cause buffered deletes that existed before the rollback creation point to be incorrectly lost when the IOException triggers a rollback."
"LUCENE-2774","BUILD_SYSTEM","BUG","ant generate-maven-artifacts target broken for contrib","When executing 'ant generate-maven-artifacts' from a pristine checkout of branch_3x/lucene or trunk/lucene the following error is encountered:

{code}
dist-maven:
     [copy] Copying 1 file to /home/drew/lucene/branch_3x/lucene/build/contrib/analyzers/common
[artifact:install-provider] Installing provider: org.apache.maven.wagon:wagon-ssh:jar:1.0-beta-2:runtime
[artifact:pom] An error has occurred while processing the Maven artifact tasks.
[artifact:pom]  Diagnosis:
[artifact:pom] 
[artifact:pom] Unable to initialize POM pom.xml.template: Cannot find parent: org.apache.lucene:lucene-contrib for project: org.apache.lucene:lucene-analyzers:jar:3.1-SNAPSHOT for project org.apache.lucene:lucene-analyzers:jar:3.1-SNAPSHOT
[artifact:pom] Unable to download the artifact from any repository
{code}


The contrib portion of the ant build is executed in a subant task which does not pick up the pom definitions for lucene-parent and lucene-contrib from the main build.xml, so the lucene-parent and lucene-controb poms must be loaded specifically as a part of the contrib build using the artifact:pom task.
"
"LUCENE-2212","TEST","TEST","add a test for PorterStemFilter","There are no tests for PorterStemFilter, yet svn history reveals some (very minor) cleanups, etc.
The only thing executing its code in tests is a test or two in SmartChinese tests.

This patch runs the StemFilter against Martin Porter's test data set for this stemmer, checking for expected output.

The zip file is 100KB added to src/test, if this is too large I can change it to download the data instead.
"
"LUCENE-1185","IMPROVEMENT","IMPROVEMENT","[PATCH] Avoid checking for TermBuffer in SegmentTermEnum#scanTo","It seems that SegmentTermEnum#scanTo is a critical method which is called very often, especially whenever we iterate over a sequence of terms in an index.

When that method is called, the first thing happens is that it checks whether a temporary TermBuffer ""scratch"" has already been initialized.

In fact, this is not necessary. We can simply declare and initialize the ""scratch""-Buffer at the class-level (right now, the initial value is _null_). Java's lazy-loading capabilities allow this without adding any memory footprint for cases where we do not need that buffer.

The attached patch takes care of this. We now save one comparison per term.
In addition to that, the patch renames ""scratch"" to ""scanBuffer"", which aligns with the naming of the other two buffers that are declared in the class."
"LUCENE-1092","BUG","BUG","KeywordTokenizer/Analyzer cannot be re-used","
The new reusableTokenStream API in KeywordAnalyzer fails to reset the tokenizer when it re-uses it.

This issue came from this thread:

    http://www.gossamer-threads.com/lists/lucene/java-dev/55929

Thanks to Hideaki Takahashi for finding this!"
"LUCENE-2058","RFE","IMPROVEMENT","benchmark pkg: specify trec_eval submission output from the command line","the QueryDriver for the trec benchmark currently requires 4 command line arguments.
the third argument is ignored (i typically populate this with ""bogus"")
Instead, allow the third argument to specify the submission.txt file for trec_eval.

while I am here, add a usage() documenting what the arguments to this driver program do."
"LUCENE-1716","RFE","IMPROVEMENT","Adding norms, properties indexing and writer.infoStream support to benchmark","I would like to add the following support in benchmark:
# Ability to specify whether norms should be stored in the index.
# Ability to specify whether norms should be stored for the body field (assuming norms are usually stored for that field in real life applications, make it explicit)
# Ability to specify an infoStream for IndexWriter
# Ability to specify whether to index the properties returned on DocData (for content sources like TREC, these may include arbitrary <meta> tags, which we may not want to index).

Patch to come shortly."
"LUCENE-3762","TEST","IMPROVEMENT","Upgrade JUnit to 4.10, refactor state-machine of detecting setUp/tearDown call chaining.","Both Lucene and Solr use JUnit 4.7. I suggest we move forward and upgrade to JUnit 4.10 which provides several infrastructural changes (serializable Description objects, class-level rules, various tweaks). JUnit 4.10 also changes (or fixes, depends how you look at it) the order in which @Before/@After hooks and @Rules are applied. This makes the old state-machine in LuceneTestCase fail (because the order is changed).

I rewrote the state machine and used a different, I think simpler, although Uwe may disagree :), mechanism in which the hook methods setUp/ tearDown are still there, but they are empty at the top level and serve only to detect whether subclasses chain super.setUp/tearDown properly (if they override anything).

In the long term, I would love to just get rid of public setup/teardown methods and make them private (so that they cannot be overriden or even seen by subclasses) but this will require changes to the runner itself."
"LUCENE-2852","BUG","BUG","RAMInputStream hits false EOF if you seek to EOF then seek back then readBytes","TestLazyLoadThreadSafety fails in hudson, possibly an issue with RAMDirectory.
If you hack lucene testcase to return another directory, the same seed will pass."
"LUCENE-2923","DOCUMENTATION","IMPROVEMENT","cleanup contrib/demo","I don't think we should include optimize in the demo; many people start from the demo and may think you must optimize to do searching, and that's clearly not the case.

I think we should also use a buffered reader in FileDocument?

And... I'm tempted to remove IndexHTML (and the html parser) entirely.  It's ancient, and we now have Tika to extract text from many doc formats."
"LUCENE-368","RFE","IMPROVEMENT","Surround query language","This is a copy of what I posted about a year ago. 
 
The whole thing is hereby licenced under the Apache Licence 2.0, 
copyright 2005 Apache Software Foundation. 
 
For inclusion in Lucene (sandbox perhaps?) it will need 
at least the following adaptations: 
- renaming of package names 
  (org.surround to somewhere org.apache.lucene ) 
- moves of the source files to corresponding directories 
 
Although it uses the identifier sncf in some places 
I'm not associated with French railroads, but I like the TGV. 
 
Regards, 
Paul Elschot"
"LUCENE-1144","BUG","BUG","NPE crash in case of out of memory","The attached class makes Lucene crash with an NPE when starting it with -Xmx10M, although there's probably an OutOfMemory problem. The stacktrace:

Exception in thread ""main"" java.lang.NullPointerException
	at java.util.Arrays.fill(Unknown Source)
	at org.apache.lucene.index.DocumentsWriter$ByteBlockPool.reset(DocumentsWriter.java:2873)
	at org.apache.lucene.index.DocumentsWriter$ThreadState.resetPostings(DocumentsWriter.java:637)
	at org.apache.lucene.index.DocumentsWriter.resetPostingsData(DocumentsWriter.java:458)
	at org.apache.lucene.index.DocumentsWriter.abort(DocumentsWriter.java:423)
	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:2433)
	at org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:2397)
	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1445)
	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1424)
	at LuceneCrash.myrun(LuceneCrash.java:32)
	at LuceneCrash.main(LuceneCrash.java:19)

The documents are quite big (some hundred KB each), I cannot attach them but I can send them via private mail if needed. The crash happens the first time reset() is called, after indexing 10 documents. I assume the bug is just that the error is misleading, there maybe should be an OOM error.
"
"LUCENE-715","BUG","BUG","IndexWriter does not release its write lock when trying to open an index which does not yet exist","In version 2.0.0, the private IndexWriter constructor does not properly remove its write lock in the event of an error. This can be seen when one attempts to open (not create) an index in a directory which exists, but in which there is no segments file. Here is the offending code:

    247   private IndexWriter(Directory d, Analyzer a, final boolean create, boolean closeDir)
    248     throws IOException {
    249       this.closeDir = closeDir;
    250       directory = d;
    251       analyzer = a;
    252 
    253       Lock writeLock = directory.makeLock(IndexWriter.WRITE_LOCK_NAME);
    254       if (!writeLock.obtain(writeLockTimeout)) // obtain write lock
    255         throw new IOException(""Index locked for write: "" + writeLock);
    256       this.writeLock = writeLock;                   // save it
    257 
    258       synchronized (directory) {        // in- & inter-process sync
    259         new Lock.With(directory.makeLock(IndexWriter.COMMIT_LOCK_NAME), commitLockTimeout) {
    260             public Object doBody() throws IOException {
    261               if (create)
    262                 segmentInfos.write(directory);
    263               else
    264                 segmentInfos.read(directory);
    265               return null;
    266             }
    267           }.run();
    268       }
    269   }

On line 254, a write lock is obtained by the constructor. If an exception is raised inside the doBody() method (on line 260), then that exception is propagated, the constructor will fail, but the lock is not released until the object is garbage collected. This is typically an issue except when using the IndexModifier class.

As of the filing of this bug, this has not yet been fixed in the trunk (IndexWriter.java#472959):

    251   private IndexWriter(Directory d, Analyzer a, final boolean create, boolean closeDir)
    252     throws IOException {
    253       this.closeDir = closeDir;
    254       directory = d;
    255       analyzer = a;
    256 
    257       Lock writeLock = directory.makeLock(IndexWriter.WRITE_LOCK_NAME);
    258       if (!writeLock.obtain(writeLockTimeout)) // obtain write lock
    259         throw new IOException(""Index locked for write: "" + writeLock);
    260       this.writeLock = writeLock;                   // save it
    261 
    262       synchronized (directory) {        // in- & inter-process sync
    263         new Lock.With(directory.makeLock(IndexWriter.COMMIT_LOCK_NAME), commitLockTimeout) {
    264             public Object doBody() throws IOException {
    265               if (create)
    266                 segmentInfos.write(directory);
    267               else
    268                 segmentInfos.read(directory);
    269               return null;
    270             }
    271           }.run();
    272       }
    273   }"
"LUCENE-2569","BUG","BUG","TestParallelTermEnum fails with Sep codec","reproduceable in the 'preflexfixes' branch (since we test all codecs there) with: ant test-core -Dtestcase=TestParallelTermEnum -Dtests.codec=Sep

But I think there are probably more tests like this that have only been run with Standard and we might find more like this.
I don't think this should block LUCENE-2554.

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestParallelTermEnum
    [junit] Testcase: test1(org.apache.lucene.index.TestParallelTermEnum):      Caused an ERROR
    [junit] read past EOF
    [junit] java.io.IOException: read past EOF
    [junit]     at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:154)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)
    [junit]     at org.apache.lucene.store.DataInput.readVInt(DataInput.java:86)
    [junit]     at org.apache.lucene.index.codecs.sep.SingleIntIndexInput$Reader.next(SingleIntIndexInput.java:64)
    [junit]     at org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl$SepDocsEnum.nextDoc(SepPostingsReaderImpl.java:316)
    [junit]     at org.apache.lucene.index.TestParallelTermEnum.test1(TestParallelTermEnum.java:188)
    [junit]     at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:316)
    [junit]
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.009 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: random codec of testcase 'test1' was: Sep
    [junit] ------------- ---------------- ---------------
{noformat}
"
"LUCENE-650","BUG","BUG","NPE doing local sensitive sorting when sort field is missing","If you do a local sensitive sort against a field that is missing from some documents in the index an NPE will get thrown.

Attached is a patch which resolved the issue and updates the sort test case to give coverage to this issue."
"LUCENE-1316","IMPROVEMENT","BUG","Avoidable synchronization bottleneck in MatchAlldocsQuery$MatchAllScorer","The isDeleted() method on IndexReader has been mentioned a number of times as a potential synchronization bottleneck. However, the reason this  bottleneck occurs is actually at a higher level that wasn't focused on (at least in the threads I read).

In every case I saw where a stack trace was provided to show the lock/block, higher in the stack you see the MatchAllScorer.next() method. In Solr paricularly, this scorer is used for ""NOT"" queries. We saw incredibly poor performance (order of magnitude) on our load tests for NOT queries, due to this bottleneck. The problem is that every single document is run through this isDeleted() method, which is synchronized. Having an optimized index exacerbates this issues, as there is only a single SegmentReader to synchronize on, causing a major thread pileup waiting for the lock.

By simply having the MatchAllScorer see if there have been any deletions in the reader, much of this can be avoided. Especially in a read-only environment for production where you have slaves doing all the high load searching.

I modified line 67 in the MatchAllDocsQuery
FROM:
  if (!reader.isDeleted(id)) {
TO:
  if (!reader.hasDeletions() || !reader.isDeleted(id)) {

In our micro load test for NOT queries only, this was a major performance improvement.  We also got the same query results. I don't believe this will improve the situation for indexes that have deletions. 

Please consider making this adjustment for a future bug fix release.




"
"LUCENE-1523","DESIGN_DEFECT","IMPROVEMENT","isOpen needs to be accessible by subclasses of Directory","The Directory abstract class has a member variable named isOpen which is package accessible. The usage of the variable is such that it should be readable and must be writable (in order to implement close())  by any concrete implementation of directory. Because of the current accessibility of this variable is is not possible to create a Directory implementation that is not also in the org.apache.lucene.store.

I propose that either the isOpen variable either needs to be declared protected or that there should be getter/setter methods that are protected."
"LUCENE-2478","BUG","BUG","CachingWrapperFilter throws NPE when Filter.getDocIdSet() returns null","Followup for [http://www.lucidimagination.com/search/document/1014ea92f15677bd/filter_getdocidset_returning_null_and_what_this_means_for_cachingwrapperfilter]:

Daniel Noll is seeing an exception like this:

{noformat}
java.lang.NullPointerException
    at org.apache.lucene.search.CachingWrapperFilter.docIdSetToCache(CachingWrapperFilter.java:84)
    at org.apache.lucene.search.CachingWrapperFilter.getDocIdSet(CachingWrapperFilter.java:112)
    at com.nuix.storage.search.LazyConstantScoreQuery$LazyFilterWrapper.getDocIdSet(SourceFile:91)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.<init>(ConstantScoreQuery.java:116)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:81)
    at org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:297)
    at org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:297)
    at org.apache.lucene.search.QueryWrapperFilter$2.iterator(QueryWrapperFilter.java:75)
{noformat}

The class of our own is just an intermediary which delays creating the Filter object...

{code}
@Override
public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
            if (delegate == null) {
                delegate = factory.createFilter();
            }
            return delegate.getDocIdSet(reader);
}
{code}

Tracing through the code in CachingWrapperFilter, I can see that this NPE would occur if getDocIdSet() were to return null.

The Javadoc on Filter says that null will be returned if no documents will be accepted by the filter, but it doesn't seem that Lucene itself is handling null return values correctly, so which is correct?  The code or the Javadoc?  Supposing that null really is OK, does this cause any problems with how CachingWrapperFilter is implementing the caching?  I notice it's calling get() and then comparing against null so it wouldn't appear that it can distinguish ""the entry isn't in the cache"" from ""the entry is in the cache but it's null""."
"LUCENE-1380","RFE","IMPROVEMENT","Patch for ShingleFilter.enablePositions (or PositionFilter)","Make it possible for *all* words and shingles to be placed at the same position, that is for _all_ shingles (and unigrams if included) to be treated as synonyms of each other.

Today the shingles generated are synonyms only to the first term in the shingle.
For example the query ""abcd efgh ijkl"" results in:
   (""abcd"" ""abcd efgh"" ""abcd efgh ijkl"") (""efgh"" efgh ijkl"") (""ijkl"")

where ""abcd efgh"" and ""abcd efgh ijkl"" are synonyms of ""abcd"", and ""efgh ijkl"" is a synonym of ""efgh"".

There exists no way today to alter which token a particular shingle is a synonym for.
This patch takes the first step in making it possible to make all shingles (and unigrams if included) synonyms of each other.

See http://comments.gmane.org/gmane.comp.jakarta.lucene.user/34746 for mailing list thread."
"LUCENE-3628","RFE","IMPROVEMENT","Cut Norms over to DocValues","since IR is now fully R/O and norms are inside codecs we can cut over to use a IDV impl for writing norms. LUCENE-3606 has some [ideas|https://issues.apache.org/jira/browse/LUCENE-3606?focusedCommentId=13160559&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13160559] about how this could be implemented"
"LUCENE-695","IMPROVEMENT","IMPROVEMENT","Improve BufferedIndexInput.readBytes() performance","During a profiling session, I discovered that BufferedIndexInput.readBytes(),
the function which reads a bunch of bytes from an index, is very inefficient
in many cases. It is efficient for one or two bytes, and also efficient
for a very large number of bytes (e.g., when the norms are read all at once);
But for anything in between (e.g., 100 bytes), it is a performance disaster.
It can easily be improved, though, and below I include a patch to do that.

The basic problem in the existing code was that if you ask it to read 100
bytes, readBytes() simply calls readByte() 100 times in a loop, which means
we check byte after byte if the buffer has another character, instead of just
checking once how many bytes we have left, and copy them all at once.

My version, attached below, copies these 100 bytes if they are available at
bulk (using System.arraycopy), and if less than 100 are available, whatever
is available gets copied, and then the rest. (as before, when a very large
number of bytes is requested, it is read directly into the final buffer).

In my profiling, this fix caused amazing performance
improvement: previously, BufferedIndexInput.readBytes() took as much as 25%
of the run time, and after the fix, this was down to 1% of the run time! However, my scenario is *not* the typical Lucene code, but rather a version of Lucene with added payloads, and these payloads average at 100 bytes, where the original readBytes() did worst. I expect that my fix will have less of an impact on ""vanilla"" Lucene, but it still can have an impact because it is used for things like reading fields. (I am not aware of a standard Lucene benchmark, so I can't provide benchmarks on a more typical case).

In addition to the change to readBytes(), my attached patch also adds a new
unit test to BufferedIndexInput (which previously did not have a unit test).
This test simulates a ""file"" which contains a predictable series of bytes, and
then tries to read from it with readByte() and readButes() with various
sizes (many thousands of combinations are tried) and see that exactly the
expected bytes are read. This test is independent of my new readBytes()
inplementation, and can be used to check the old implementation as well.

By the way, it's interesting that BufferedIndexOutput.writeBytes was already efficient, and wasn't simply a loop of writeByte(). Only the reading code was inefficient. I wonder why this happened."
"LUCENE-936","DOCUMENTATION","BUG","Typo on query parser syntax web page.","On the web page http://lucene.apache.org/java/docs/queryparsersyntax.html#N10126 the text says:

""To search for documents that must contain ""jakarta"" and may contain ""lucene"" use the query:""

The example says:

+jakarta apache

The problem:
The example uses apache where the text says lucene."
"LUCENE-3398","BUG","BUG","TestCompoundFile fails on windows","ant test-core -Dtestcase=TestCompoundFile -Dtestmethod=testReadNestedCFP -Dtests.seed=-61cb66ec0d71d1ac:-46685c36ec38fd32:568c63299214892c"
"LUCENE-2618","TEST","BUG","Intermittent failure in TestThreadedOptimize","Failure looks like this:

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestThreadedOptimize
    [junit] Testcase: testThreadedOptimize(org.apache.lucene.index.TestThreadedOptimize):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError: null
    [junit] 	at org.apache.lucene.index.TestThreadedOptimize.runTest(TestThreadedOptimize.java:125)
    [junit] 	at org.apache.lucene.index.TestThreadedOptimize.testThreadedOptimize(TestThreadedOptimize.java:149)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:253)
{noformat}

I just committed some verbosity so next time it strikes we'll have more details."
"LUCENE-2718","REFACTORING","IMPROVEMENT","separate java code from .jj file","It would make development easier to move most of the java code out from the .jj file and into a real java file."
"LUCENE-1678","CLEANUP","BUG","Deprecate Analyzer.tokenStream","The addition of reusableTokenStream to the core analyzers unfortunately broke back compat of external subclasses:

    http://www.nabble.com/Extending-StandardAnalyzer-considered-harmful-td23863822.html

On upgrading, such subclasses would silently not be used anymore, since Lucene's indexing invokes reusableTokenStream.

I think we should should at least deprecate Analyzer.tokenStream, today, so that users see deprecation warnings if their classes override this method.  But going forward when we want to change the API of core classes that are extended, I think we have to  introduce entirely new classes, to keep back compatibility."
"LUCENE-1249","BUG","BUG","Bugs in org.apache.lucene.index.TermVectorsReader.clone()","A couple of things:

- The implementation can return null which is not allowed.  It should throw a CloneNotSupportedException if that's the case.

- Part of the code reads:

    TermVectorsReader clone = null;
    try {
      clone = (TermVectorsReader) super.clone();
    } catch (CloneNotSupportedException e) {}

    clone.tvx = (IndexInput) tvx.clone();

If a CloneNotSupportedException is caught then ""clone"" will be null and the assignment to clone.tvx will fail with a null pointer exception."
"LUCENE-1889","RFE","","FastVectorHighlighter: support for additional queries","I am using fastvectorhighlighter for some strange languages and it is working well! 

One thing i noticed immediately is that many query types are not highlighted (multitermquery, multiphrasequery, etc)
Here is one thing Michael M posted in the original ticket:

{quote}
I think a nice [eventual] model would be if we could simply re-run the
scorer on the single document (using InstantiatedIndex maybe, or
simply some sort of wrapper on the term vectors which are already a
mini-inverted-index for a single doc), but extend the scorer API to
tell us the exact term occurrences that participated in a match (which
I don't think is exposed today).
{quote}

Due to strange requirements I am using something similar to this (but specialized to our case).
I am doing strange things like forcing multitermqueries to rewrite into boolean queries so they will be highlighted,
and flattening multiphrasequeries into boolean or'ed phrasequeries.
I do not think these things would be 'fast', but i had a few ideas that might help:

* looking at contrib/highlighter, you can support FilteredQuery in flatten() by calling getQuery() right?
* maybe as a last resort, try Query.extractTerms() ?
"
"LUCENE-2094","RFE","BUG","Prepare CharArraySet for Unicode 4.0","CharArraySet does lowercaseing if created with the correspondent flag. This causes that  String / char[] with uncode 4 chars which are in the set can not be retrieved in ""ignorecase"" mode.
"
"LUCENE-1171","IMPROVEMENT","BUG","Make DocumentsWriter more robust on hitting OOM","I've been stress testing DocumentsWriter by indexing wikipedia, but not
giving enough memory to the JVM, in varying heap sizes to tickle the
different interesting cases.  Sometimes DocumentsWriter can deadlock;
other times it will hit a subsequent NPE or AIOOBE or assertion
failure.

I've fixed all the cases I've found, and added some more asserts.  Now
it just produces plain OOM exceptions.  All changes are contained to
DocumentsWriter.java.

All tests pass.  I plan to commit in a day or two!"
"LUCENE-1604","IMPROVEMENT","IMPROVEMENT","Stop creating huge arrays to represent the absense of field norms","Creating and keeping around huge arrays that hold a constant value is very inefficient both from a heap usage standpoint and from a localility of reference standpoint. It would be much more efficient to use null to represent a missing norms table."
"LUCENE-235","BUG","BUG","[PATCH] Error in GermanStemmer.java,v 1.11","GermanStemmer.java,v 1.11 in  lucene-1.4-final
 at the end of a word is not replaced by ss"
"LUCENE-2941","BUG","BUG","custom sort broken if IS uses executorservice",""
"LUCENE-1423","BUG","BUG","InstantiatedTermEnum#skipTo(Term) throws ArrayIndexOutOfBoundsException on empty index","{code}
java.lang.ArrayIndexOutOfBoundsException: 0
	at org.apache.lucene.store.instantiated.InstantiatedTermEnum.skipTo(InstantiatedTermEnum.java:105)
	at org.apache.lucene.store.instantiated.TestEmptyIndex.termEnumTest(TestEmptyIndex.java:73)
	at org.apache.lucene.store.instantiated.TestEmptyIndex.testTermEnum(TestEmptyIndex.java:54)
{code}"
"LUCENE-3468","IMPROVEMENT","IMPROVEMENT","FirstPassGroupingCollector should use pollLast()","Currently FirstPassGroupingCollector uses last and remove method (TreeSet) for replacing a more relevant grouping during grouping.
This can be replaced by pollLast since Lucene trunk is now Java 6. TermFirstPassGroupingCollectorJava6 in Solr can be removed as well."
"LUCENE-2635","BUG","BUG","BQ provides an explanation on a non-match","Plug in seed -6336594106867842617L into TestExplanations then run TestSimpleExplanationsOfNonMatches and you'll hit this:
{noformat}
    [junit] Testsuite: org.apache.lucene.search.TestSimpleExplanationsOfNonMatches
    [junit] Testcase: testBQ2(org.apache.lucene.search.TestSimpleExplanationsOfNonMatches):	FAILED
    [junit] Explanation of [[+yy +w3]] for #0 doesn't indicate non-match: 0.08778467 = (MATCH) product of:
    [junit]   0.17556934 = (MATCH) sum of:
    [junit]     0.17556934 = (MATCH) weight(field:w3 in 0), product of:
    [junit]       0.5165708 = queryWeight(field:w3), product of:
    [junit]         0.7768564 = idf(docFreq=4, maxDocs=4)
    [junit]         0.6649502 = queryNorm
    [junit]       0.33987468 = (MATCH) fieldWeight(field:w3 in 0), product of:
    [junit]         1.0 = tf(termFreq(field:w3)=1)
    [junit]         0.7768564 = idf(docFreq=4, maxDocs=4)
    [junit]         0.4375 = fieldNorm(field=field, doc=0)
    [junit]   0.5 = coord(1/2)
    [junit]  expected:<0.0> but was:<0.08778467>
    [junit] junit.framework.AssertionFailedError: Explanation of [[+yy +w3]] for #0 doesn't indicate non-match: 0.08778467 = (MATCH) product of:
    [junit]   0.17556934 = (MATCH) sum of:
    [junit]     0.17556934 = (MATCH) weight(field:w3 in 0), product of:
    [junit]       0.5165708 = queryWeight(field:w3), product of:
    [junit]         0.7768564 = idf(docFreq=4, maxDocs=4)
    [junit]         0.6649502 = queryNorm
    [junit]       0.33987468 = (MATCH) fieldWeight(field:w3 in 0), product of:
    [junit]         1.0 = tf(termFreq(field:w3)=1)
    [junit]         0.7768564 = idf(docFreq=4, maxDocs=4)
    [junit]         0.4375 = fieldNorm(field=field, doc=0)
    [junit]   0.5 = coord(1/2)
    [junit]  expected:<0.0> but was:<0.08778467>
    [junit] 	at org.apache.lucene.search.CheckHits.checkNoMatchExplanations(CheckHits.java:60)
    [junit] 	at org.apache.lucene.search.TestSimpleExplanationsOfNonMatches.qtest(TestSimpleExplanationsOfNonMatches.java:36)
    [junit] 	at org.apache.lucene.search.TestExplanations.qtest(TestExplanations.java:101)
    [junit] 	at org.apache.lucene.search.TestSimpleExplanations.testBQ2(TestSimpleExplanations.java:235)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:397)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.run(LuceneTestCase.java:389)
{noformat}

The bug is real -- BQ's explain method fails to properly enforce required clauses when the sub-scorer is null.  Thank you random testing!
"
"LUCENE-3714","RFE","RFE","add suggester that uses shortest path/wFST instead of buckets","Currently the FST suggester (really an FSA) quantizes weights into buckets (e.g. single byte) and puts them in front of the word.
This makes it fast, but you lose granularity in your suggestions.

Lately the question was raised, if you build lucene's FST with positiveintoutputs, does it behave the same as a tropical semiring wFST?

In other words, after completing the word, we instead traverse min(output) at each node to find the 'shortest path' to the 
best suggestion (with the highest score).

This means we wouldnt need to quantize weights at all and it might make some operations (e.g. adding fuzzy matching etc) a lot easier."
"LUCENE-3823","RFE","IMPROVEMENT","Add a field-filtering FilterAtomicReader to 4.0 so ParallelReaders can be better tested (in LTC.maybeWrapReader)","In addition to the filters in contrib/misc for horizontally filtering (by doc-id) AtomicReader, it would be good to have the same vertically (by field). For now I will add this implementation to test-framework, as it cannot stay in contrib/misc, because LTC will need it for maybeWrapReader.

LTC will use this FilterAtomicReader to construct a ParallelAtomicReader out of two (or maybe more) FieldFilterAtomicReaders."
"LUCENE-240","DOCUMENTATION","BUG","bug form doesn't list latest version"," "
"LUCENE-1748","DESIGN_DEFECT","BUG","getPayloadSpans on org.apache.lucene.search.spans.SpanQuery should be abstract","I just spent a long time tracking down a bug resulting from upgrading to Lucene 2.4.1 on a project that implements some SpanQuerys of its own and was written against 2.3.  Since the project's SpanQuerys didn't implement getPayloadSpans, the call to that method went to SpanQuery.getPayloadSpans which returned null and caused a NullPointerException in the Lucene code, far away from the actual source of the problem.  

It would be much better for this kind of thing to show up at compile time, I think.

Thanks!"
"LUCENE-3223","BUG","BUG","SearchWithSortTask ignores sorting by Doc","During my work in LUCENE-3912, I found the following code:

{code}
if (field.equals(""doc"")) {
    sortField0 = SortField.FIELD_DOC;
} if (field.equals(""score"")) {
    sortField0 = SortField.FIELD_SCORE;
} ...
{code}

This means the setting of SortField.FIELD_DOC is ignored.  While I don't know much about this code, this seems like a valid setting and obviously just a bug."
"LUCENE-577","RFE","RFE","SweetSpotSimiliarity","This is a new Similarity implimention for the contrib/miscellaneous/ package, it provides a Similiarty designed for people who know the ""sweetspot"" of their data.  three major pieces of functionality are included:

1) a lengthNorm which creates a ""platuea"" of values.
2) a baseline tf that provides a fixed value for tf's up to a minimum, at which point it becomes a sqrt curve (this is used by the tf(int) function.
3) a hyperbolic tf function which is best explained by graphing the equation.  this isn't used by default, but is available for subclasses to call from their own tf functions.

All constants used in all functions are configurable.  In the case of lengthNorm, the constants are configurable per field, as well as allowing for defaults for unspecified fields."
"LUCENE-1077","RFE","RFE","New Analysis  Contributions","With the advent of the new TeeTokenFilter and SinkTokenizer, there now exists some interesting new things that can be done in the analysis phase of indexing.  See LUCENE-1058.

This patch provides some new implementations of SinkTokenizer that may be useful."
"LUCENE-1994","RFE","BUG","EnwikiConentSource does not work with parallel tasks",""
"LUCENE-372","BUG","BUG","Unmatched right parentheses truncates query","The query processor truncates a query when right parentheses are unmatched.
E.g.:

 secret AND illegal) AND access:confidential

will not result in a ParseException instead will run as:

 secret AND illegal"
"LUCENE-940","BUG","BUG","SimpleDateFormat used in a non thread safe manner","As Mike pointed out in http://www.mail-archive.com/java-dev@lucene.apache.org/msg10831.html SimpleDateFormat is not thread safe and hence DocMakers need to maintain it in a ThreadLocal."
"LUCENE-435","IMPROVEMENT","IMPROVEMENT","[PATCH] BufferedIndexOutput - optimized writeBytes() method","I have created a patch that optimize writeBytes metod:

  public void writeBytes(byte[] b, int length) throws IOException {
    if (bufferPosition > 0) // flush buffer
      flush();
 
    if (length < BUFFER_SIZE) {
      flushBuffer(b, length);
      bufferStart += length;
    } else {
      int pos = 0;
      int size;
      while (pos < length) {
        if (length - pos < BUFFER_SIZE) {
          size = length - pos;
        } else {
          size = BUFFER_SIZE;
        }
        System.arraycopy(b, pos, buffer, 0, size);
        pos += size;
        flushBuffer(buffer, size);
        bufferStart += size;
      }
    }
  }

Its a much more faster now. I know that for indexing this not help much, but for copying files in the IndexStore this is so big improvement. Its about 400% faster that old implementation.

The patch was tested with 300MB data, ""ant test"" sucessfuly finished with no errors."
"LUCENE-3467","IMPROVEMENT","IMPROVEMENT","Cut over numeric docvalues to fixed straight bytes","Currently numeric docvalues types are encoded and stored individually which creates massive duplication of writing / indexing code. Yet, almost all of them (except packed ints) are essentially a fixed straight bytes variant. "
"LUCENE-1195","IMPROVEMENT","IMPROVEMENT","Performance improvement for TermInfosReader","Currently we have a bottleneck for multi-term queries: the dictionary lookup is being done
twice for each term. The first time in Similarity.idf(), where searcher.docFreq() is called.
The second time when the posting list is opened (TermDocs or TermPositions).

The dictionary lookup is not cheap, that's why a significant performance improvement is
possible here if we avoid the second lookup. An easy way to do this is to add a small LRU 
cache to TermInfosReader. 

I ran some performance experiments with an LRU cache size of 20, and an mid-size index of
500,000 documents from wikipedia. Here are some test results:

50,000 AND queries with 3 terms each:
old:                  152 secs
new (with LRU cache): 112 secs (26% faster)

50,000 OR queries with 3 terms each:
old:                  175 secs
new (with LRU cache): 133 secs (24% faster)

For bigger indexes this patch will probably have less impact, for smaller once more.

I will attach a patch soon."
"LUCENE-970","BUG","BUG","FilterIndexReader should overwrite isOptimized()","A call of FilterIndexReader.isOptimized() results in a NPE because FilterIndexReader does not overwrite isOptimized()."
"LUCENE-1563","TEST","TEST","Add example test case for surround query language",""
"LUCENE-918","DOCUMENTATION","","Interface TermFreqVector has incomplete Javadocs","We should improve the Javadocs of org.apache.lucene.index.TermFreqVector"
"LUCENE-3147","TEST","TEST","MockDirectoryWrapper should track open file handles of IndexOutput too","MockDirectoryWrapper currently tracks open file handles of IndexInput only. Therefore IO files that are not closed do not fail our tests, which can then lead to test directories fail to delete on Windows. We should make sure all open files are tracked and if they are left open, fail the test. I'll attach a patch shortly."
"LUCENE-3835","BUG","BUG","MergeThread throws unchecked exceptions from toString()","This causes nearly all thread-dumping routines to fail and in the effect obscure the original problem. I think this
should return a string (always), possibly indicating the underlying writer has been closed or something."
"LUCENE-1715","IMPROVEMENT","BUG","DirectoryIndexReader finalize() holding TermInfosReader longer than necessary","DirectoryIndexReader has a finalize method, which causes the JDK to keep a reference to the object until it can be finalized.  SegmentReader and MultiSegmentReader are subclasses that contain references to, potentially, hundreds of megabytes of cached data in a TermInfosReader.

Some options would be removing finalize() from DirectoryIndexReader (it releases a write lock at the moment) or possibly nulling out references in various close() and doClose() methods throughout the class hierarchy so that the finalizable object doesn't references the Term arrays.

Original mailing list message:
http://mail-archives.apache.org/mod_mbox/lucene-java-user/200906.mbox/%3C7A5CB4A7BBCE0C40B81C5145C326C31301A62971@NUMEVP06.na.imtn.com%3E"
"LUCENE-3850","CLEANUP","IMPROVEMENT","Fix rawtypes warnings for Java 7 compiler","Java 7 changed the warnings a little bit:
- Java 6 only knew ""unchecked"" warning type, applying for all types of generics violations, like missing generics (raw types)
- Java 7 still knows ""unchecked"" but only emits warning if the call is really unchecked. Declaration of variables/fields or constructing instances without type param now emits ""rawtypes"" warning.

The changes above causes the Java 7 compile now emit lots of ""rawtypes"" warnings, where Java 6 is silent. The easy fix is to suppres both warning types: @SuppressWarnings({""unchecked"",""rawtypes""}) for all those places. Changes are easy to do, will provide patch later!"
"LUCENE-1719","DOCUMENTATION","IMPROVEMENT","Add javadoc notes about ICUCollationKeyFilter's advantages over CollationKeyFilter","contrib/collation's ICUCollationKeyFilter, which uses ICU4J collation, is faster than CollationKeyFilter, the JVM-provided java.text.Collator implementation in the same package.  The javadocs of these classes should be modified to add a note to this effect.

My curiosity was piqued by [Robert Muir's comment|https://issues.apache.org/jira/browse/LUCENE-1581?focusedCommentId=12720300#action_12720300] on LUCENE-1581, in which he states that ICUCollationKeyFilter is up to 30x faster than CollationKeyFilter.

I timed the operation of these two classes, with Sun JVM versions 1.4.2/32-bit, 1.5.0/32- and 64-bit, and 1.6.0/64-bit, using 90k word lists of 4 languages (taken from the corresponding Debian wordlist packages and truncated to the first 90k words after a fixed random shuffling), using Collators at the default strength, on a Windows Vista 64-bit machine.  I used an analysis pipeline consisting of WhitespaceTokenizer chained to the collation key filter, so to isolate the time taken by the collation key filters, I also timed WhitespaceTokenizer operating alone for each combination.  The rightmost column represents the performance advantage of the ICU4J implemtation (ICU) over the java.text.Collator implementation (JVM), after discounting the WhitespaceTokenizer time (WST): (JVM-ICU) / (ICU-WST). The best times out of 5 runs for each combination, in milliseconds, are as follows:

||Sun JVM||Language||java.text||ICU4J||WhitespaceTokenizer||ICU4J Improvement||
|1.4.2_17 (32 bit)|English|522|212|13|156%|
|1.4.2_17 (32 bit)|French|716|243|14|207%|
|1.4.2_17 (32 bit)|German|669|264|16|163%|
|1.4.2_17 (32 bit)|Ukranian|931|474|25|102%|
|1.5.0_15 (32 bit)|English|604|176|16|268%|
|1.5.0_15 (32 bit)|French|817|209|17|317%|
|1.5.0_15 (32 bit)|German|799|225|20|280%|
|1.5.0_15 (32 bit)|Ukranian|1029|436|26|145%|
|1.5.0_15 (64 bit)|English|431|89|10|433%|
|1.5.0_15 (64 bit)|French|562|112|11|446%|
|1.5.0_15 (64 bit)|German|567|116|13|438%|
|1.5.0_15 (64 bit)|Ukranian|734|281|21|174%|
|1.6.0_13 (64 bit)|English|162|81|9|113%|
|1.6.0_13 (64 bit)|French|192|92|10|122%|
|1.6.0_13 (64 bit)|German|204|99|14|124%|
|1.6.0_13 (64 bit)|Ukranian|273|202|21|39%|
"
"LUCENE-265","IMPROVEMENT","IMPROVEMENT","[PATCH] to remove synchronized code from TermVectorsReader","Otis,

here the latest and last patch to get rid of all synchronized code from
TermVectorsReader. It should include at least 3 files, TermVectorsReader.diff,
SegmentReader.diff and the new junit test case TestMultiThreadTermVectors.java.
The patch was generated against the current CVS version of TermVectorsReader and
SegmentReader. All lucene related junit tests pass fine.

best regards
Bernhard"
"LUCENE-2594","TEST","IMPROVEMENT","cutover oal.index.* tests to use a random IWC to tease out bugs",""
"LUCENE-3864","RFE","IMPROVEMENT","support offsets in MemoryPostings","Really we should add this for Sep & Pulsing too... but this is one more"
"LUCENE-2267","RFE","IMPROVEMENT","Add solr's artifact signing scripts into lucene's build.xml/common-build.xml","Solr has nice artifact signing scripts in its common-build.xml and build.xml.

For me as release manager of 3.0 it would have be good to have them also when building lucene artifacts. I will investigate how to add them to src artifacts and maven artifacts"
"LUCENE-3023","RFE","TASK","Land DWPT on trunk","With LUCENE-2956 we have resolved the last remaining issue for LUCENE-2324 so we can proceed landing the DWPT development on trunk soon. I think one of the bigger issues here is to make sure that all JavaDocs for IW etc. are still correct though. I will start going through that first."
"LUCENE-2891","BUG","BUG","IndexWriterConfig does not allow readerTermsIndexDivisor to be -1, while the latest indicates the terms index should not be loaded","While you can pass -1 to IR.open(), and it's documented, you cannot do the same for IndexWriter's readers (b/c IWC blocks it). Need to allow this setting as well as add support for it in our tests, e.g. we should randomly set it to -1. Robert also suggested RandomIW use -1 randomly when it opens readers.

I'll work on a patch"
"LUCENE-1101","BUG","BUG","TokenStream.next(Token) reuse 'policy': calling Token.clear() should be responsibility of producer.","Tokenizers which implement the reuse form of the next method:
    next(Token result) 
should reset the postionIncrement of the returned token to 1."
"LUCENE-1262","BUG","BUG","IndexOutOfBoundsException from FieldsReader after problem reading the index","There is a situation where there is an IOException reading from Hits, and then the next time you get a NullPointerException instead of an IOException.

Example stack traces:

java.io.IOException: The specified network name is no longer available
	at java.io.RandomAccessFile.readBytes(Native Method)
	at java.io.RandomAccessFile.read(RandomAccessFile.java:322)
	at org.apache.lucene.store.FSIndexInput.readInternal(FSDirectory.java:536)
	at org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:74)
	at org.apache.lucene.index.CompoundFileReader$CSIndexInput.readInternal(CompoundFileReader.java:220)
	at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:93)
	at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:34)
	at org.apache.lucene.store.IndexInput.readVInt(IndexInput.java:57)
	at org.apache.lucene.index.FieldsReader.doc(FieldsReader.java:88)
	at org.apache.lucene.index.SegmentReader.document(SegmentReader.java:344)
	at org.apache.lucene.index.IndexReader.document(IndexReader.java:368)
	at org.apache.lucene.search.IndexSearcher.doc(IndexSearcher.java:84)
	at org.apache.lucene.search.Hits.doc(Hits.java:104)

That error is fine.  The problem is the next call to doc generates:

java.lang.NullPointerException
	at org.apache.lucene.index.FieldsReader.getIndexType(FieldsReader.java:280)
	at org.apache.lucene.index.FieldsReader.addField(FieldsReader.java:216)
	at org.apache.lucene.index.FieldsReader.doc(FieldsReader.java:101)
	at org.apache.lucene.index.SegmentReader.document(SegmentReader.java:344)
	at org.apache.lucene.index.IndexReader.document(IndexReader.java:368)
	at org.apache.lucene.search.IndexSearcher.doc(IndexSearcher.java:84)
	at org.apache.lucene.search.Hits.doc(Hits.java:104)

Presumably FieldsReader is caching partially-initialised data somewhere.  I would normally expect the exact same IOException to be thrown for subsequent calls to the method.
"
"LUCENE-3518","RFE","RFE","Add sort-by-term with DocValues","There are two sorted byte[] types with DocValues (BYTES_VAR_SORTED,
BYTES_FIXED_SORTED), so you can index this type, but you can't yet
sort by it.

So I added a FieldComparator just like TermOrdValComparator, except it
pulls from the doc values instead.

There are some small diffs, eg with doc values there are never null
values (see LUCENE-3504).
"
"LUCENE-1609","IMPROVEMENT","IMPROVEMENT","Eliminate synchronization contention on initial index reading in TermInfosReader ensureIndexIsRead ","synchronized method ensureIndexIsRead in TermInfosReader causes contention under heavy load

Simple to reproduce: e.g. Under Solr, with all caches turned off, do a simple range search e.g. id:[0 TO 999999] on even a small index (in my case 28K docs) and under a load/stress test application, and later, examining the Thread dump (kill -3) , many threads are blocked on 'waiting for monitor entry' to this method.

Rather than using Double-Checked Locking which is known to have issues, this implementation uses a state pattern, where only one thread can move the object from IndexNotRead state to IndexRead, and in doing so alters the objects behavior, i.e. once the index is loaded, the index nolonger needs a synchronized method. 

In my particular test, this uncreased throughput at least 30 times.

"
"LUCENE-1348","TEST","TEST","TestTimeLimitedCollector  shuold not fail if the testing machine happens to be slow","The test fails on Hudson about once a month, like this:

{quote}
   [junit] Testcase: testTimeoutNotGreedy(org.apache.lucene.search.TestTimeLimitedCollector):  FAILED
   [junit] lastDoc=21 ,&& allowed=799 ,&& elapsed=900 >= 886 = ( 2*resolution +  TIME_ALLOWED + SLOW_DOWN = 2*20 + 799 + 47)
   [junit] junit.framework.AssertionFailedError: lastDoc=21 ,&& allowed=799 ,&& elapsed=900 >= 886 = ( 2*resolution +  TIME_ALLOWED + SLOW_DOWN = 2*20 + 799 + 47)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestTimeout(TestTimeLimitedCollector.java:189)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.testTimeoutNotGreedy(TestTimeLimitedCollector.java:150)
   [junit]
   [junit]
   [junit] Test org.apache.lucene.search.TestTimeLimitedCollector FAILED
{quote}

Modify the test to just print a warning in this case - but still verify that there was an early termination.
"
"LUCENE-2730","BUG","BUG","intermittent deadlock in TestAtomicUpdate,TestIndexWriterExceptions","While backporting issues for 2.9.x/3.0.x release I hit deadlocks in these two tests, under both test-core and test-tag."
"LUCENE-454","IMPROVEMENT","IMPROVEMENT","lazily create SegmentMergeInfo.docMap","Since creating the docMap is expensive, and it's only used during segment merging, not searching, defer creation until it is requested.

SegmentMergeInfo is also used in MultiTermEnum, the term enumerator for a MultiReader.  TermEnum is used by queries such as PrefixQuery, RangeQuery, WildcardQuery, as well as RangeFilter, DateFilter, and sorting the first time (filling the FieldCache).

Performance Results:
  A simple single field index with 555,555 documents, and 1000 random deletions was queried 1000 times with a PrefixQuery matching a single document.

Performance Before Patch:
  indexing time = 121,656 ms
  querying time = 58,812 ms

Performance After Patch:
  indexing time = 121,000 ms
  querying time =         598 ms

A 100 fold increase in query performance!

All lucene unit tests pass."
"LUCENE-1514","BUG","IMPROVEMENT","ShingleMatrixFilter eaily throws StackOverFlow as the complexity of a matrix grows","ShingleMatrixFilter#next makes a recursive function invocation when the current permutation iterator is exhausted or if the current state of the permutation iterator already has produced an identical shingle. In a not too complex matrix this will require a gigabyte sized stack per thread.

My solution is to avoid the recursive invocation by refactoring like this:

{code:java}
public Token next(final Token reusableToken) throws IOException {
    assert reusableToken != null;
    if (matrix == null) {
      matrix = new Matrix();
      // fill matrix with maximumShingleSize columns
      while (matrix.columns.size() < maximumShingleSize && readColumn()) {
        // this loop looks ugly
      }
    }

    // this loop exists in order to avoid recursive calls to the next method
    // as the complexity of a large matrix
    // then would require a multi gigabyte sized stack.
    Token token;
    do {
      token = produceNextToken(reusableToken);
    } while (token == request_next_token);
    return token;
  }

  
  private static final Token request_next_token = new Token();

  /**
   * This method exists in order to avoid reursive calls to the method
   * as the complexity of a fairlt small matrix then easily would require
   * a gigabyte sized stack per thread.
   *
   * @param reusableToken
   * @return null if exhausted, instance request_next_token if one more call is required for an answer, or instance parameter resuableToken.
   * @throws IOException
   */
  private Token produceNextToken(final Token reusableToken) throws IOException {

{code}

"
"LUCENE-1276","BUILD_SYSTEM","BUG","Build file for Highlighter contrib works when run in isolation, but not when core dist is run","Build.xml for Highlighter does not work when compilation is triggered by clean core dist call.

Patch has changes to fix this by updating build.xml to follow xml-query-parser build.xml"
"LUCENE-923","DESIGN_DEFECT","","Should SegmentTermPositionVector be public?","I'm wondering why SegmentTermPositionVector is public. It implements the public
interface TermPositionVector. Should we remove ""public""?"
"LUCENE-1877","DOCUMENTATION","IMPROVEMENT","Use NativeFSLockFactory as default for new API (direct ctors & FSDir.open)","A user requested we add a note in IndexWriter alerting the availability of NativeFSLockFactory (allowing you to avoid retaining locks on abnormal jvm exit). Seems reasonable to me - we want users to be able to easily stumble upon this class. The below code looks like a good spot to add a note - could also improve whats there a bit - opening an IndexWriter does not necessarily create a lock file - that would depend on the LockFactory used.


{code}  <p>Opening an <code>IndexWriter</code> creates a lock file for the directory in use. Trying to open
  another <code>IndexWriter</code> on the same directory will lead to a
  {@link LockObtainFailedException}. The {@link LockObtainFailedException}
  is also thrown if an IndexReader on the same directory is used to delete documents
  from the index.</p>{code}

Anyone remember why NativeFSLockFactory is not the default over SimpleFSLockFactory?"
"LUCENE-1887","REFACTORING","IMPROVEMENT","o.a.l.messages should be moved to core","contrib/queryParser contains an org.apache.lucene.messages package containing some generallized code that (claims in it's javadocs) is not specific to the queryParser.

If this is truely general purpose code, it should probably be moved out of hte queryParser contrib -- either into it's own contrib, or into the core (it's very small)

*EDIT:* alternate suggestion to rename package to fall under the o.a.l.queryParser namespace retracted due to comments in favor of (eventually) promoting to it's own contrib"
"LUCENE-3915","RFE","RFE","Add Japanese filter to replace term attribute with readings","Koji and Robert are working on LUCENE-3888 that allows spell-checkers to do their similarity matching using a different word than its surface form.

This approach is very useful for languages such as Japanese where the surface form and the form we'd like to use for similarity matching is very different.  For Japanese, it's useful to use readings for this -- probably with some normalization."
"LUCENE-3474","RFE","TASK","pass liveDocs Bits down in scorercontext, instead of Weights pulling from the reader ","Spinoff from LUCENE-1536, this would allow filters to work in a more flexible way (besides just cleaning up)"
"LUCENE-2135","IMPROVEMENT","BUG","IndexReader.close should forcefully evict entries from FieldCache","Spinoff of java-user thread ""heap memory issues when sorting by a string field"".

We rely on WeakHashMap to hold our FieldCache, keyed by reader.  But this lacks immediacy on releasing the reference, after a reader is closed.

WeakHashMap can't free the key until the reader is no longer referenced by the app. And, apparently, WeakHashMap has a further impl detail that requires invoking one of its methods for it to notice that a key has just become only weakly reachable.

To fix this, I think on IR.close we should evict entries from the FieldCache, as long as the sub-readers are truly closed (refCount dropped to 0)."
"LUCENE-1834","CLEANUP","TASK","remove unused code in SmartChineseAnalyzer hmm pkg","there is some unused code in the hmm package.

I would like to remove it before I supply a fix for LUCENE-1817.

only after this can we refactor any of this analyzer, otherwise we risk breaking custom dictionary support."
"LUCENE-3021","TEST","TEST","randomize skipInterval in tests","we probably don't test the multi-level skipping very well, but skipInterval etc is now private to the codec, so for better test coverage we should parameterize it to the postings writers, and randomize it via mockrandomcodec."
"LUCENE-3409","BUG","BUG","NRT reader/writer over RAMDirectory memory leak","with NRT reader/writer, emptying an index using:
writer.deleteAll()
writer.commit()
doesn't release all allocated memory.

for example the following code will generate a memory leak:

/**
	 * Reveals a memory leak in NRT reader/writer<br>
	 * 
	 * The following main() does 10K cycles of:
	 * <ul>
	 * <li>Add 10K empty documents to index writer</li>
	 * <li>commit()</li>
	 * <li>open NRT reader over the writer, and immediately close it</li>
	 * <li>delete all documents from the writer</li>
	 * <li>commit changes to the writer</li>
	 * </ul>
	 * 
	 * Running with -Xmx256M results in an OOME after ~2600 cycles
	 */
	public static void main(String[] args) throws Exception {
		RAMDirectory d = new RAMDirectory();
		IndexWriter w = new IndexWriter(d, new IndexWriterConfig(Version.LUCENE_33, new KeywordAnalyzer()));
		Document doc = new Document();
		
		for(int i = 0; i < 10000; i++) {
			for(int j = 0; j < 10000; ++j) {
				w.addDocument(doc);
			}
			w.commit();
			IndexReader.open(w, true).close();

			w.deleteAll();
			w.commit();
		}
		
		w.close();
		d.close();
	}	"
"LUCENE-3634","CLEANUP","IMPROVEMENT","remove old static main methods in core","We have a few random static main methods that I think are very rarely used... we should remove them (IndexReader, UTF32ToUTF8, English).

The IndexReader main lets you list / extract the sub-files from a CFS... I think we should move this to a new tool in contrib/misc."
"LUCENE-2671","RFE","RFE","Add sort missing first/last ability to SortField and ValueComparator","When SortField and ValueComparator use EntryCreators (from LUCENE-2649) they use a special sort value when the field is missing.

This enables lucene to implement 'sort missing last' or 'sort missing first' for numeric values from the FieldCache.
"
"LUCENE-3283","REFACTORING","","Move core QueryParsers to queryparser module","Move the contents of lucene/src/java/org/apache/lucene/queryParser to the queryparser module.

To differentiate these parsers from the others, they are going to be placed a 'classic' package.  We'll rename QueryParser to ClassicQueryParser as well."
"LUCENE-3582","BUG","BUG","NumericUtils.floatToSortableInt/doubleToSortableLong does not sort certain NaN ranges correctly and NumericRangeQuery produces wrong results for NaNs with half-open ranges","The current implementation of floatToSortableInt does not account for different NaN ranges which may result in NaNs sorted before -Infinity and after +Infinity. The default Java ordering is: all NaNs after Infinity.

A possible fix is to make all NaNs canonic ""quiet NaN"" as in:
{code}
// Canonicalize NaN ranges. I assume this check will be faster here than 
// (v == v) == false on the FPU? We don't distinguish between different
// flavors of NaNs here (see http://en.wikipedia.org/wiki/NaN). I guess
// in Java this doesn't matter much anyway.
if ((v & 0x7fffffff) > 0x7f800000) {
  // Apply the logic below to a canonical ""quiet NaN""
  return 0x7fc00000 ^ 0x80000000;
}
{code}

I don't commit because I don't know how much of the existing stuff relies on this (nobody should be keeping different NaNs  in their indexes, but who knows...)."
"LUCENE-1012","BUG","BUG","Problems with maxMergeDocs parameter","I found two possible problems regarding IndexWriter's maxMergeDocs value. I'm using the following code to test maxMergeDocs:

{code:java} 
  public void testMaxMergeDocs() throws IOException {
    final int maxMergeDocs = 50;
    final int numSegments = 40;
    
    MockRAMDirectory dir = new MockRAMDirectory();
    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);      
    writer.setMergePolicy(new LogDocMergePolicy());
    writer.setMaxMergeDocs(maxMergeDocs);

    Document doc = new Document();
    doc.add(new Field(""field"", ""aaa"", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
    for (int i = 0; i < numSegments * maxMergeDocs; i++) {
      writer.addDocument(doc);
      //writer.flush();      // uncomment to avoid the DocumentsWriter bug
    }
    writer.close();
    
    new SegmentInfos.FindSegmentsFile(dir) {

      protected Object doBody(String segmentFileName) throws CorruptIndexException, IOException {

        SegmentInfos infos = new SegmentInfos();
        infos.read(directory, segmentFileName);
        for (int i = 0; i < infos.size(); i++) {
          assertTrue(infos.info(i).docCount <= maxMergeDocs);
        }
        return null;
      }
    }.run();
  }
{code} 
  
- It seems that DocumentsWriter does not obey the maxMergeDocs parameter. If I don't flush manually, then the index only contains one segment at the end and the test fails.

- If I flush manually after each addDocument() call, then the index contains more segments. But still, there are segments that contain more docs than maxMergeDocs, e. g. 55 vs. 50. The javadoc in IndexWriter says:
{code:java}
   /**
   * Returns the largest number of documents allowed in a
   * single segment.
   *
   * @see #setMaxMergeDocs
   */
  public int getMaxMergeDocs() {
    return getLogDocMergePolicy().getMaxMergeDocs();
  }
{code}"
"LUCENE-3363","BUG","BUG","minimizeHopcroft OOMEs on smallish (2096 states, finite) automaton","Not sure what's up w/ this... if you check out the blocktree branch (LUCENE-3030) and comment out the @Ignore in TestTermsEnum2.testFiniteVersusInfinite then this should hit OOME: {[ant test-core -Dtestcase=TestTermsEnum2 -Dtestmethod=testFiniteVersusInfinite -Dtests.seed=-2577608857970454726:-2463580050179334504}}"
"LUCENE-3889","CLEANUP","TASK","Remove/Uncommit SegmentingTokenizerBase","I added this class in LUCENE-3305 to support analyzers like Kuromoji,
but Kuromoji no longer needs it as of LUCENE-3767. So now nothing uses it.

I think we should uncommit before releasing, svn doesn't forget so
we can add this back if we want to refactor something like Thai or Smartcn
to use it."
"LUCENE-3620","BUG","BUG","FilterIndexReader does not override all of IndexReader methods","FilterIndexReader does not override all of IndexReader methods. We've hit an error in LUCENE-3573 (and fixed it). So I thought to write a simple test which asserts that FIR overrides all methods of IR (and we can filter our methods that we don't think that it should override). The test is very simple (attached), and it currently fails over these methods:
{code}
getRefCount
incRef
tryIncRef
decRef
reopen
reopen
reopen
reopen
clone
numDeletedDocs
document
setNorm
setNorm
termPositions
deleteDocument
deleteDocuments
undeleteAll
getIndexCommit
getUniqueTermCount
getTermInfosIndexDivisor
{code}

I didn't yet fix anything in FIR -- if you spot a method that you think we should not override and delegate, please comment."
"LUCENE-607","BUG","BUG","ParallelTermEnum is BROKEN","ParallelTermEnum.next() fails to advance properly to new fields.  This is a serious bug. 

Christian Kohlschuetter diagnosed this as the root problem underlying LUCENE-398 and posted a first patch there.

I've addressed a couple issues in the patch (close skipped field TermEnum's, generate field iterator only once, integrated Christian's test case as a Lucene test) and packaged in all the revised patch here.

All Lucene tests pass, and I've further tested in this in my app, which makes extensive use of ParallelReader.
"
"LUCENE-2822","IMPROVEMENT","BUG","TimeLimitingCollector starts thread in static {} with no way to stop them","See the comment in LuceneTestCase.

If you even do Class.forName(""TimeLimitingCollector"") it starts up a thread in a static method, and there isn't a way to kill it.

This is broken."
"LUCENE-1268","DOCUMENTATION","IMPROVEMENT","Changes.html should be visible to users for closed releases","Changes.html is currently available only in the dev page, for trunk. 
See LUCENE-1157 for discussion on where exactly to expose this."
"LUCENE-3438","REFACTORING","IMPROVEMENT","Move some *TermsEnum.java from oal.search to oal.index","I think FilteredTermsEnum, SingleTermsEnum should move?

I left TermRangeTermsEnum and FuzzyTermsEnum and PrefixTermsEnum since they seemed search specific."
"LUCENE-1143","BUG","BUG","DocumentsWriter.abort fails to clear docStoreOffset","I hit this in working on LUCENE-1044.

If you disk full event during flush, then DocumentsWriter will abort
(clear all buffered docs).  Then, if you then add another doc or two,
and then close your writer, and this time succeed in flushing (say
because it's only a couple buffered docs so the resulting segment is
smaller), you can flush a corrupt segment (that incorrectly has a
non-zero docStoreOffset).

I modified the TestConcurrentMergeScheduler test to show this bug.
I'll attach a patch shortly.
"
"LUCENE-1517","REFACTORING","IMPROVEMENT","Change superclass of TrieRangeQuery","This patch changes the superclass of TrieRangeQuery to ConstantScoreQuery. The current implementation is using rewrite() and was copied from early RangeQueries. But this is not needed, the TrieRangeQuery can easily subclassed from ConstantScoreQuery.

If LUCENE-1345 is solved, the whole TrieRangeQuery can be removed, as TrieRangeFilter can be added to BooleanQueries. The whole TrieRangeQuery class is just a convenience class for easier usage of the trie contrib."
"LUCENE-1377","REFACTORING","IMPROVEMENT","Add HTMLStripReader and WordDelimiterFilter from SOLR","SOLR has two classes HTMLStripReader and WordDelimiterFilter which are very useful for a wide variety of use cases.  It would be good to place them into core Lucene."
"LUCENE-804","BUILD_SYSTEM","TASK","build.xml: result of ""dist-src"" should support ""build-contrib""","Currently the packed src distribution would fail to run ""ant build-contrib"".
It would be much nicer if that work.
In fact, would be nicer if you could even ""re-pack"" with it.

For now I marked this for 2.1, although I am not yet sure if this is a stopper."
"LUCENE-526","BUG","BUG","FieldSortedHitQueue - subsequent String sorts with different locales sort identically","From my own post to the java-user list. I have looked into this further and am sure it's a bug.

---

It seems to me that there's a possible bug in FieldSortedHitQueue, specifically in getCachedComparator(). This is showing up on our 1.4.3 install, but it seems from source code inspection that if it's a bug, it's in 1.9.1 also.

The issue shows up when you need to sort results from a given IndexReader multiple times, using different locales. On line 180 (all line numbers from the 1.9.1 code), we have this:

ScoreDocComparator comparator = lookup (reader, fieldname, type, factory);

Then, if no comparator is found in the cache, a new one is created (line 193) and then stored in the cache (line 202). HOWEVER, both the cache lookup() and store() do NOT take into account locale; if we, on the same index reader, try to do one search sorted by Locale.FRENCH and one by Locale.ITALIAN, the first one will result in a cache miss, a new French comparator will be created, and stored in the cache. Second time through, lookup() finds the cached French comparator -- even though this time, the locale parameter to getCachedComparator() is an Italian locale. Therefore, we don't create a new comparator and we use the wrong one to sort the results.

It looks to me (unless I'm mistaken) that the FieldCacheImpl.Entry class should have an additional property, .locale, to ensure that different locales get different comparators.

---

Patch (well, most of one) to follow immediately."
"LUCENE-544","RFE","IMPROVEMENT","MultiFieldQueryParser field boost multiplier","Allows specific boosting per field, e.g. +(name:foo^1 description:foo^0.1).

Went from String[] field to MultiFieldQueryParser.FieldSetting[] field in constructor. "
"LUCENE-3246","REFACTORING","IMPROVEMENT","Invert IR.getDelDocs -> IR.getLiveDocs","Spinoff from LUCENE-1536, where we need to fix the low level filtering
we do for deleted docs to ""match"" Filters (ie, a set bit means the doc
is accepted) so that filters can be pushed all the way down to the
enums when possible/appropriate.

This change also inverts the meaning first arg to
TermsEnum.docs/AndPositions (renames from skipDocs to liveDocs).
"
"LUCENE-1840","BUG","IMPROVEMENT","QueryUtils should check that equals properly handles null","Its part of the equals contract, but many classes currently violate"
"LUCENE-901","BUG","BUG","DefaultSimilarity.queryNorm() should never return Infinity","Currently DefaultSimilarity.queryNorm() returns Infinity if sumOfSquaredWeights=0.
This can result in a score of NaN (e. g. in TermScorer) if boost=0.0f.

A simple fix would be to return 1.0f in case zero is passed in.

See LUCENE-698 for discussions about this."
"LUCENE-324","BUG","BUG","org.apache.lucene.analysis.cn.ChineseTokenizer missing offset decrement","Apparently, in ChineseTokenizer, offset should be decremented like bufferIndex
when Character is OTHER_LETTER.  This directly affects startOffset and endOffset
values.

This is critical to have Highlighter working correctly because Highlighter marks
matching text based on these offset values."
"LUCENE-518","BUG","BUG","document field lengths count analyzer synonym overlays","Using a synonym expansion analyzer to add tokens with zero offset from the substituted token should not extend the length of the field in the document (for scoring purposes)"
"LUCENE-3695","DESIGN_DEFECT","BUG","FST Builder methods need fixing,documentation,or improved type safety","Its confusing the way an FST Builder has 4 add() methods, and you get assertion errors (what happens if assertions are disabled?) if you use the wrong one:

For reference we have 3 FST input types:
* BYTE1 (byte)
* BYTE2 (char)
* BYTE4 (int)

For the builder add() method signatures we have:
* add(BytesRef)
* add(char[], int offset, int len)
* add(CharSequence)
* add(IntsRef)

But certain methods only work with certain FST input types, and these mappings are not the ones you think. 

For example, you would think that if you have a char-based FST you should use add(char[]) or add(CharSequence), but this is not the case: those add methods actually only work with int-based FST (they use codePointAt() to extract codepoints). Instead, you have to use add(IntsRef) for the char-based one.

The worst is if you use the wrong one, you get an assertion error, but i'm not sure what happens if assertions are disabled.

Maybe the ultimate solution is to parameterize FST's generics on input too (FST<input,output>) and just require BytesRef/CharsRef/IntsRef as the parameter? Then you could just have add(), and this might clean up FSTEnum too (it would no longer need that InputOutput class but maybe could use Map.Entry<input,output> or something?
 
I think the documentation is improving but i still notice add(BytesRef) has no javadoc at all, and it only works with BYTE1, so I think we still have some work to do even if we want to just pursue a documentation fix.
"
"LUCENE-2204","DESIGN_DEFECT","IMPROVEMENT","FastVectorHighlighter: some classes and members should be publicly accessible to implement FragmentsBuilder","I intended to design custom FragmentsBuilder can be written and pluggable, though, when I tried to write it out of the FVH package, it came out that some classes and members should be publicly accessible."
"LUCENE-3488","REFACTORING","IMPROVEMENT","Factor out SearcherManager from NRTManager","Currently we have NRTManager and SearcherManager while NRTManager contains a big piece of the code that is already in SearcherManager. Users are kind of forced to use NRTManager if they want to have SearcherManager goodness with NRT. The integration into NRTManager also forces you to maintain two instances even if you know you always want deletes. To me NRTManager tries to do more than necessary and mixes lots of responsibilities ie. handling searchers and handling indexing generations. NRTManager should use a SearcherManager by aggregation rather than duplicate a lot of logic. SearcherManager should have a NRT and Directory based implementation users can simply choose from."
"LUCENE-2649","RFE","IMPROVEMENT","FieldCache should include a BitSet for matching docs","The FieldCache returns an array representing the values for each doc.  However there is no way to know if the doc actually has a value.

This should be changed to return an object representing the values *and* a BitSet for all valid docs."
"LUCENE-2720","DESIGN_DEFECT","BUG","IndexWriter should throw IndexFormatTooOldExc on open, not later during optimize/getReader/close","Spinoff of LUCENE-2618 and also related to the original issue LUCENE-2523...

If you open IW on a too-old index, you don't find out until much later that the index is too old.

This is because IW does not go and open segment readers on all segments.  It only does so when it's time to apply deletes, do merges, open an NRT reader, etc.

This is a serious bug because you can in fact succeed in committing with the new major version of Lucene against your too-old index, which is catastrophic because suddenly the old Lucene version will no longer open the index, and so your index becomes unusable."
"LUCENE-3077","BUG","BUG","DWPT doesn't see changes to DW#infoStream","DW does not push infostream changes to DWPT since DWPT#infoStream is final and initialized on DWPTPool initialization (at least for initial DWPT) we should push changes to infostream to DWPT too"
"LUCENE-1395","DOCUMENTATION","IMPROVEMENT","Javadoc mistake in SegmentMerger",""
"LUCENE-707","DOCUMENTATION","IMPROVEMENT","Lucene Java Site docs","It would be really nice if the Java site docs where consistent with the rest of the Lucene family (namely, with navigation tabs, etc.) so that one can easily go between Nutch, Hadoop, etc."
"LUCENE-791","DOCUMENTATION","IMPROVEMENT","Update the Wiki","The wiki needs updating.  For starters, the URL is still Jakarta.  I think infrastructure needs to be contacted to do this move.  If someone is so inclined, it might be useful to go through and cleanup/organize what is there."
"LUCENE-933","IMPROVEMENT","BUG","QueryParser can produce empty sub BooleanQueries when Analyzer proudces no tokens for input","as triggered by SOLR-261, if you have a query like this...

   +foo:BBB  +(yak:AAA  baz:CCC)

...where the analyzer produces no tokens for the ""yak:AAA"" or ""baz:CCC"" portions of the query (posisbly because they are stop words) the resulting query produced by the QueryParser will be...

  +foo:BBB +()

...that is a BooleanQuery with two required clauses, one of which is an empty BooleanQuery with no clauses.

this does not appear to be ""good"" behavior.

In general, QueryParser should be smarter about what it does when parsing encountering parens whose contents result in an empty BooleanQuery -- but what exactly it should do in the following situations...

 a)  +foo:BBB +()
 b)  +foo:BBB ()
 c)  +foo:BBB -()

...is up for interpretation.  I would think situation (b) clearly lends itself to dropping the sub-BooleanQuery completely.  situation (c) may also lend itself to that solution, since semanticly it means ""don't allow a match on any queries in the empty set of queries"".  .... I have no idea what the ""right"" thing to do for situation (a) is."
"LUCENE-3535","REFACTORING","TASK","move PerFieldCodecWrapper into codecs package","PerFieldCodecWrapper is a codec, but its 'hardwired' as lucene's only codec currently (except for PreFlex/3.x case)

it lets you choose a format for the postings lists per-field.

I think we should move this to the codecs package as a start... just a rote refactor."
"LUCENE-230","BUG","BUG","Search with sort fails when a document has a missing value","Testing on version: lucene-1.4-rc2

Call in question: IndexSearcher.search(Query query, Filter filter, int nDocs, 
Sort sort) 

Description: I'm making a call to search with a sort field - in my case I'm 
sorting by date. If any document in the results set (Hits) has a missing value 
in the sort field, the entire call throws an [uncaught] exception during the 
sorting process with no results returned. 

This is an undesireable result, and the prospects for patching this problem 
outside the search classes are ugly, e.g. trying to patch the index itself.

This is actually a critical function in my application. Thank you for 
addressing it.

-Dan"
"LUCENE-3106","BUG","BUG","commongrams filter calls incrementToken() after it returns false","In LUCENE-3064, we beefed up MockTokenizer with assertions, and I started cutting over some analysis tests to use MockTokenizer for better coverage.

The commongrams tests fail, because they call incrementToken() after it already returns false. 

In general its my understanding consumers should not do this (and i know of a few tokenizers that will actually throw exceptions if you do this, just like java iterators and such)."
"LUCENE-857","IMPROVEMENT","IMPROVEMENT","Remove BitSet caching from QueryFilter","Since caching is built into the public BitSet bits(IndexReader reader)  method, I don't see a way to deprecate that, which means I'll just cut it out and document it in CHANGES.txt.  Anyone who wants QueryFilter caching will be able to get the caching back by wrapping the QueryFilter in the CachingWrapperFilter."
"LUCENE-1821","RFE","BUG","Weight.scorer() not passed doc offset for ""sub reader""","Now that searching is done on a per segment basis, there is no way for a Scorer to know the ""actual"" doc id for the document's it matches (only the relative doc offset into the segment)

If using caches in your scorer that are based on the ""entire"" index (all segments), there is now no way to index into them properly from inside a Scorer because the scorer is not passed the needed offset to calculate the ""real"" docid

suggest having Weight.scorer() method also take a integer for the doc offset

Abstract Weight class should have a constructor that takes this offset as well as a method to get the offset
All Weights that have ""sub"" weights must pass this offset down to created ""sub"" weights


Details on workaround:
In order to work around this, you must do the following:
* Subclass IndexSearcher
* Add ""int getIndexReaderBase(IndexReader)"" method to your subclass
* during Weight creation, the Weight must hold onto a reference to the passed in Searcher (casted to your sub class)
* during Scorer creation, the Scorer must be passed the result of YourSearcher.getIndexReaderBase(reader)
* Scorer can now rebase any collected docids using this offset

Example implementation of getIndexReaderBase():
{code}
// NOTE: more efficient implementation can be done if you cache the result if gatherSubReaders in your constructor
public int getIndexReaderBase(IndexReader reader) {
  if (reader == getReader()) {
    return 0;
  } else {
    List readers = new ArrayList();
    gatherSubReaders(readers);
    Iterator iter = readers.iterator();
    int maxDoc = 0;
    while (iter.hasNext()) {
      IndexReader r = (IndexReader)iter.next();
      if (r == reader) {
        return maxDoc;
      } 
      maxDoc += r.maxDoc();
    } 
  }
  return -1; // reader not in searcher
}
{code}

Notes:
* This workaround makes it so you cannot serialize your custom Weight implementation
"
"LUCENE-1371","RFE","IMPROVEMENT","Add Searcher.search(Query, int)","Now that we've deprecated Hits (LUCENE-1290), I think we should add this trivial convenience method to Searcher, which is just sugar for Searcher.search(Query, null, int) ie null filter, returning a TopDocs.

This way there is a simple API for users to retrieve the top N results for a Query.
"
"LUCENE-1811","TEST","BUG","TestIndexReaderReopen nightly build failure","An interesting failure in last night's build (http://hudson.zones.apache.org/hudson/job/Lucene-trunk/920).

I think the root cause wast he AIOOB exception... all the ""lock obtain timed out"" exceptions look like they cascaded.

{code}
    [junit] Testsuite: org.apache.lucene.index.TestIndexReaderReopen
    [junit] Lock obtain timed out: org.apache.lucene.store.SingleInstanceLock@6ac615: write.lock)
    [junit] Tests run: 15, Failures: 1, Errors: 0, Time elapsed: 31.087 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] java.lang.ArrayIndexOutOfBoundsException: Array index out of range: 148
    [junit] 	at org.apache.lucene.util.BitVector.getAndSet(BitVector.java:74)
    [junit] 	at org.apache.lucene.index.SegmentReader.doDelete(SegmentReader.java:908)
    [junit] 	at org.apache.lucene.index.IndexReader.deleteDocument(IndexReader.java:1122)
    [junit] 	at org.apache.lucene.index.DirectoryReader.doDelete(DirectoryReader.java:521)
    [junit] 	at org.apache.lucene.index.IndexReader.deleteDocument(IndexReader.java:1122)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$8.modifyIndex(TestIndexReaderReopen.java:638)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.refreshReader(TestIndexReaderReopen.java:840)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.access$400(TestIndexReaderReopen.java:47)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$9.run(TestIndexReaderReopen.java:681)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$ReaderThread.run(TestIndexReaderReopen.java:822)
    [junit] org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: org.apache.lucene.store.SingleInstanceLock@88d319: write.lock
    [junit] 	at org.apache.lucene.store.Lock.obtain(Lock.java:85)
    [junit] 	at org.apache.lucene.index.DirectoryReader.acquireWriteLock(DirectoryReader.java:666)
    [junit] 	at org.apache.lucene.index.IndexReader.setNorm(IndexReader.java:994)
    [junit] 	at org.apache.lucene.index.IndexReader.setNorm(IndexReader.java:1020)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$8.modifyIndex(TestIndexReaderReopen.java:634)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.refreshReader(TestIndexReaderReopen.java:840)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.access$400(TestIndexReaderReopen.java:47)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$9.run(TestIndexReaderReopen.java:681)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$ReaderThread.run(TestIndexReaderReopen.java:822)
    ...
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testThreadSafety(org.apache.lucene.index.TestIndexReaderReopen):	FAILED
    [junit] Error occurred in thread Thread-36:
    [junit] Lock obtain timed out: org.apache.lucene.store.SingleInstanceLock@6ac615: write.lock
    [junit] junit.framework.AssertionFailedError: Error occurred in thread Thread-36:
    [junit] Lock obtain timed out: org.apache.lucene.store.SingleInstanceLock@6ac615: write.lock
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.testThreadSafety(TestIndexReaderReopen.java:764)
    [junit] 
    [junit] 
{code}"
"LUCENE-2599","CLEANUP","IMPROVEMENT","Deprecate Spatial Contrib","The spatial contrib is blighted by bugs.  The latest series, found by Grant and discussed [here|http://search.lucidimagination.com/search/document/c32e81783642df47/spatial_rethinking_cartesian_tiers_implementation] shows that we need to re-think the cartesian tier implementation.

Given the need to create a spatial module containing code taken from both lucene and Solr, it makes sense to deprecate the spatial contrib, and start from scratch in the new module.


"
"LUCENE-279","BUG","IMPROVEMENT","[PATCH] Javadoc improvements and minor fixes","Javadoc improvements for Scorer.java and Weight.java. 
This also fixes some recent changes introduced minor warnings when building 
the javadocs and adds a small comment in Similarity.java. 
The individual patches will be attached."
"LUCENE-812","BUG","BUG","Unable to set LockFactory implementation via ${org.apache.lucene.store.FSDirectoryLockFactoryClass}","While trying to move from Lucene 2.0 to Lucene 2.1 I noticed a problem with the LockFactory instantiation code.
During previous tests we successfully specified the LockFactory implementation by setting the property
${org.apache.lucene.store.FSDirectoryLockFactoryClass} to ""org.apache.lucene.store.NativeFSLockFactory"".
This does no longer work due to a bug in the FSDirectory class. The problem is caused from the fact that this
class tries to invoke the default constructor of the specified LockFactory class. However neither NativeFSLockFactory
nor SimpleFSLockFactory do have a default constructor.

FSDirectory, Line 285:
          try {
            lockFactory = (LockFactory) c.newInstance();          
          } catch (IllegalAccessException e) {
            throw new IOException(""IllegalAccessException when instantiating LockClass "" + lockClassName);
          } catch (InstantiationException e) {
            throw new IOException(""InstantiationException when instantiating LockClass "" + lockClassName);
          } catch (ClassCastException e) {
            throw new IOException(""unable to cast LockClass "" + lockClassName + "" instance to a LockFactory"");
          }

A possible workaround is to not set the property at all and call FSDirectory.setLockFactory(...) instead. "
"LUCENE-3903","CLEANUP","BUG","javadocs very very ugly if you generate with java7","Java7 changes its javadocs to look much nicer, but this involves different CSS styles.

Lucene overrides the CSS with stylesheet+prettify.css which is a combination of java5/6 stylesheet + google prettify:
but there are problems because java7 has totally different styles.

So if you generate javadocs with java7, its like you have no stylesheet at all.

A solution might be to make stylesheet7+prettify.css and conditionalize a property in ant based on java version."
"LUCENE-2197","IMPROVEMENT","BUG","StopFilter should not create a new CharArraySet if the given set is already an instance of CharArraySet","With LUCENE-2094 a new CharArraySet is created no matter what type of set is passed to StopFilter. This does not behave as  documented and could introduce serious performance problems. Yet, according to the javadoc, the instance of CharArraySet should be passed to CharArraySet.copy (which is very fast for CharArraySet instances) instead of ""copied"" via ""new CharArraySet()"""
"LUCENE-2783","BUG","BUG","Deadlock in IndexWriter","If autoCommit == true a merge usually triggers a commit. A commit (prepareCommit) can trigger a merge vi the flush method. There is a synchronization mechanism for commit (commitLock) and a separate synchronization mechanism for merging (ConcurrentMergeScheduler.wait). If one thread holds the commitLock monitor and another one holds the ConcurrentMergeScheduler monitor we have a deadlock."
"LUCENE-975","RFE","RFE","Position based TermVectorMapper","As part of the new TermVectorMapper approach to TermVectors, the ensuing patch loads term vectors and stores the term info by position.  This should let people directly index into a term vector given a position.  Actually, it does it through Maps, b/c the array based bookkeeping is a pain given the way positions are stored.  

The map looks like:
Map<String,   Map<Integer, TVPositionInfo>>

where the String is the field name, the integer is the position, and TVPositionInfo is a storage mechanism for the terms and offsets that occur at a position.  It _should_ handle multiple terms per position (which is always my downfall! )

I have not tested performance of this approach.
"
"LUCENE-973","BUG","BUG","Token of  """" returns in CJKTokenizer + new TestCJKTokenizer","The """" string returns as Token in the boundary of two byte character and one byte character. 

There is no problem in CJKAnalyzer. 
When CJKTokenizer is used with the unit, it becomes a problem. (Use it with 
Solr etc.)"
"LUCENE-3847","RFE","IMPROVEMENT","LuceneTestCase should check for modifications on System properties","- fail the test if changes have been detected.
- revert the state of system properties before the suite.
- cleanup after the suite."
"LUCENE-1378","DOCUMENTATION","TASK","Remove remaining @author references","$ find . -name \*.java | xargs grep '@author' | cut -d':' -f1 | xargs perl -pi -e 's/ \@author.*//'
"
"LUCENE-1734","BUG","BUG","CharReader should delegate reset/mark/markSupported","The final class CharReader should delegate reset/mark/markSupported to its wrapped reader. Otherwise clients will get ""reset() not supported"" exception."
"LUCENE-1509","BUG","BUG","IndexCommit.getFileNames() should not return dups","If the index was created with autoCommit false, and more than 1
segment was flushed during the IndexWriter session, then the shared
doc-store files are incorrectly duplicated in
IndexCommit.getFileNames().  This is because that method is walking
through each SegmentInfo, appending its files to a list.  Since
multiple SegmentInfo's may share the doc store files, this causes dups.

To fix this, I've added a SegmentInfos.files(...) method, and
refactored all places that were computing their files one SegmentInfo
at a time to use this new method instead.
"
"LUCENE-1060","DOCUMENTATION","IMPROVEMENT","Correct 2 minor javadoc mistakes in core, javadoc.access=private","Patches Token.java and TermVectorsReader.java"
"LUCENE-2549","BUG","BUG","TimeLimitingCollector's TimeExceededException contains useless relative docid","We found another bug with the RandomIndexWriter: When TimeLimitingCollector breaks collection after timeout, it records the last/next collected docid. It does this without rebasing, so the docid is useless. TestTimeLimitingCollector checks the docid, but correctly rebases it (as only this makes sense). Because the RandomIndexWriter uses different merge settings, the index is now sometimes not optimized and so the test fails (which is correct, as the docid is useless for non-optimized index).

Attached is a patch that fixes this. Please tell me if I should backport to 2.9 and 3.0!"
"LUCENE-483","BUG","BUG","QueryParser.getFieldQuery(String,String) doesn't set default slop on MultiPhraseQuery","there seems to have been an oversight in calling mph.setSlop(phraseSlop) in QueryParser.getFieldQuery(String,String).  The result being that in some cases, the ""default slop"" value doesnt' get set right (sometimes, ... see below).

when i tried amending TestMultiAnalyzer to demonstrate the problem, I discovered that the grammer aparently always calls getFieldQuery(String,String,int) -- even if no ""~slop"" was specified in the text being parsed, in which case it passes the default as if it were specified.
(just to clarify: i haven't comfirmed this from a detailed reading of the grammer/code, it's just what i've deduced based on observation of the test)

The problem isn't entirely obvious unless you have a subclasses of QueryParser and try to call getFieldQuery(String,String) directly.   

In my case, I had overridden getFieldQuery(String,String) to call super.getFieldQuery(String,String) and wrap the result in a DisjunctionMaxQuery ... I don't care about supporting the ~slop syntax, but i do care about the default slop and i wasn't getting lucky the way QueryParser does, because getFieldQuery(String,String,int) wasn't getting back something it could call setSlop() with the (default) value it got from the javacc generated code.

My description may not make much sense, but hopefull the test patch i'm about to attach will.  The fix is also in the patch, and is fairly trivial.

(disclaimer: i don't have javacc installed, so I tested this patch by manually making the change to both QueryParser.java ... it should only be commited by someone with javacc who can regen the java file and confirm that my jj change doesn't have some weird bug in it)



"
"LUCENE-1467","CLEANUP","TASK","Consolidate Solr's and Lucene's OpenBitSet classes","See SOLR-875 for details."
"LUCENE-356","BUG","BUG","ArrayIndexOutOfBoundsException when using MultiFieldQueryParser","We get the following exception:

Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.Vector.elementAt(Vector.java:434)
        at org.apache.lucene.queryParser.QueryParser.addClause(QueryParser.java:181)
        at org.apache.lucene.queryParser.QueryParser.Query(QueryParser.java:529)
        at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:108)
        at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:87)
        at
org.apache.lucene.queryParser.MultiFieldQueryParser.parse(MultiFieldQueryParser.java:77)
        at idx.Mquery.main(Mquery.java:64)


We are using a query with 'AND' like 'bla AND blo' on 5 fields.
One of the fields has a Tokenizer which returns no token
at all on this query, and this together with the AND
triggers the exception."
"LUCENE-524","IMPROVEMENT","IMPROVEMENT","Current implementation of fuzzy and wildcard queries inappropriately implemented as Boolean query rewrites","The implementation of MultiTermQuery in terms of BooleanQuery introduces several problems:

1) Collisions with maximum clause limit on boolean queries which throws an exception.  This is most problematic because it is difficult to ascertain in advance how many terms a fuzzy query or wildcard query might involve.

2) The boolean disjunctive scoring is not appropriate for either fuzzy or wildcard queries.  In effect the score is divided by the number of terms in the query which has nothing to do with the relevancy of the results.

3) Performance of disjunctive boolean queries for large term sets is quite sub-optimal"
"LUCENE-291","IMPROVEMENT","IMPROVEMENT","[PATCH] FSDirectory create() method deletes all files","hi all,

the current implementation of FSDirectory.create(...) silently deletes all files
(even empty directories) within the index directory when setting up a new index
with create option enabled. Lucene doesn't care when deleting files in the index
directory if they  belong to lucene or not. I don't think that this is a real
bug, but it can be a pain if somebody whants to store some private information
in the lucene index directory, e.g some configuration files.

Therefore i implemented a FileFilter which knows about the internal lucene file
extensions, so that all other files would never get touched when creating a new
index. The current patch is an enhancement in FSDirectory only. I don't think
that there is a need to make it available in the Directory class and change all
it's depending classes.

regards
Bernhard"
"LUCENE-453","BUG","BUG","Using MultiSearcher and ParallelMultiSearcher can change the sort order.","When using multiple sort criteria the first criterium that indicates a difference should be used.
When a field does not exist for a given document, special rules apply.
From what I see in the code, it is sorted as 0 for integer and float fields, and null Strings are sorted before others.

This works correctly in both Lucene 1.4.3 and in trunk as long as you use a single IndexSearcher (except perhaps in special cases, see other bug reports like LUCENE-374).

However, in MultiSearcher and ParallelMultiSearcher, the results of the separate IndexSearchers are merged and there an error occurs.
The bug is located in FieldDocSortedHitQueue.

It can even be demonstrated by passing a single indexSearcher to a MultiSearcher.

TestCase and patch follow."
"LUCENE-2876","CLEANUP","TASK","Remove Scorer.getSimilarity()","Originally this was part of the patch for per-field Similarity (LUCENE-2236), but I pulled it 
out here as its own issue as its really mostly unrelated. I also like it as a separate issue 
to apply the deprecation to branch_3x to just make less surprises/migration hassles for 4.0 users.

Currently Scorer takes a confusing number of ctors, either a Similarity, or a Weight + Similarity.
Also, lots of scorers don't use the Similarity at all, and its not really needed in Scorer itself.
Additionally, the Weight argument is often null. The Weight makes sense to be here in Scorer, 
its the parent that created the scorer, and used by Scorer itself to support LUCENE-2590's features.
But I dont think all queries work with this feature correctly right now, because they pass null.

Finally the situation gets confusing if you start to consider delegators like ScoreCachingWrapperScorer,
which arent really delegating correctly so I'm unsure features like LUCENE-2590 aren't working with this.

So I think we should remove the getSimilarity, if your scorer uses a Similarity its already coming
to you via your ctor from your Weight and you can manage this yourself.

Also, all scorers should pass the Weight (parent) that created them, and this should be Scorer's only ctor.
I fixed all core/contrib/solr Scorers (even the internal ones) to pass their parent Weight, just for consistency
of this visitor interface. The only one that passes null is Solr's ValueSourceScorer.

I set fix-for 3.1, not because i want to backport anything, only to mark the getSimilarity deprecated there.
"
"LUCENE-3202","RFE","TASK","Add DataInput/DataOutput subclasses that delegate to an InputStream/OutputStream.","Such classes would be handy for FST serialization/deserialization."
"LUCENE-3029","BUG","BUG","MultiPhraseQuery assigns different scores to identical docs when using 0 pos-incr","If you have two identical docs with tokens a b c all zero pos-incr (ie
they occur on the same position), and you run a MultiPhraseQuery with
[a, b] and [c] (all pos incr 0)... then the two docs will get
different scores despite being identical.

Admittedly it's a strange query... but I think the scorer ought to
count the phrase as having tf=1 for each doc.

The problem is that we are missing a tie-breaker for the PhraseQuery
used by ExactPhraseScorer, and so the PQ ends up flip/flopping such
that every other document gets the same score.  Ie, even docIDs all
get one score and odd docIDs all get another score.

Once I added the hard tie-breaker (ord) the scores are the same.

However... there's a separate bug, that can over-count the tf, such
that if I create the MPQ like this:
{noformat}
  mpq.add(new Term[] {new Term(""field"", ""a"")}, 0);
  mpq.add(new Term[] {new Term(""field"", ""b""), new Term(""field"", ""c"")}, 0);
{noformat}

I get tf=2 per doc, but if I create it like this:

{noformat}
  mpq.add(new Term[] {new Term(""field"", ""b""), new Term(""field"", ""c"")}, 0);
  mpq.add(new Term[] {new Term(""field"", ""a"")}, 0);
{noformat}

I get tf=1 (which I think is correct?).

This happens because MultipleTermPositions freely returns the same
position more than once: it just unions the positions of the two
streams, so when both have their term at pos=0, you'll get pos=0
twice, which is not good and leads to over-counting tf.

Unfortunately, I don't see a performant way to fix that... and I'm not
sure that it really matters that much in practice.




"
"LUCENE-2017","IMPROVEMENT","IMPROVEMENT","CloseableThreadLocal is now obsolete","Since Lucene 3 depends on Java 5, we can use ThreadLocal#remove() to take care or resource management."
"LUCENE-2003","BUG","BUG","Highlighter has problems when you use StandardAnalyzer with LUCENE_29 or simplier StopFilter with stopWordsPosIncr mode switched on","This is a followup on LUCENE-1987:

If you set in HighligterTest the constant static final Version TEST_VERSION = Version.LUCENE_24 to LUCENE_29 or LUCENE_CURRENT, the test testSimpleQueryScorerPhraseHighlighting fails. Please note, that currently (before LUCENE-2002 is fixed), you must also set the QueryParser to respect posIncr."
"LUCENE-2318","TEST","TEST","Add System.getProperty(""tempDir"") as final static to LuceneTestCase(J4)","Almost every test calls System.getProperty(""tempDir"") and some of them check the return value for null. In other cases the test simply fails from within eclipse.

We should add this to LuceneTestCase(J4) as a static final constant. For enabling tests run in eclipse, we can add a fallback to ""."", if the Sysprop is not defined."
"LUCENE-1466","RFE","RFE","CharFilter - normalize characters before tokenizer","This proposes to import CharFilter that has been introduced in Solr 1.4.

Please see for the details:
- SOLR-822
- http://www.nabble.com/Proposal-for-introducing-CharFilter-to20327007.html"
"LUCENE-1798","IMPROVEMENT","IMPROVEMENT","FieldCacheSanityChecker called directly by FieldCache.get*","As suggested by McCandless in LUCENE-1749, we can make FieldCacheImpl a client of the FieldCacheSanityChecker and have it sanity check itself each time it creates a new cache entry, and log a warning if it thinks there is a problem.  (although we'd probably only want to do this if the caller has set some sort of infoStream/warningStream type property on the FieldCache object."
"LUCENE-2731","SPEC","BUG","HyphenationCompoundWordTokenFilter fails to load DTD in Crimson parser (JDK 1.4)","HyphenationCompoundWordTokenFilter loads the DTD in its XML parser from memory by supplying EntityResolver. In Java 1.4 (affects Lucene 2.9, but also later versions if not Apache Xerces is used as XML parser) this does not work, because Cromson does not even ask the entity resolver, if no base URI is known. As the hyphenation file is loaded from Reader/InputStream no base URI is known. Crimson needs at least a non-null systemId to proceed.

This patch (Lucene 2.9 only)  fakes this by supplying a fake systemId to the InputSource."
"LUCENE-2867","DESIGN_DEFECT","IMPROVEMENT","Change contrib QP API that uses CharSequence as string identifier","There are some API methods on contrib queryparser that expects CharSequence as identifier. This is wrong, since it may lead to incorrect or mislead behavior, as shown on LUCENE-2855. To avoid this problem, these APIs will be changed and enforce the use of String instead of CharSequence on version 4. This patch already deprecate the old API methods and add new substitute methods that uses only String."
"LUCENE-386","BUILD_SYSTEM","BUG","rev. 169301: wrong directory name in build.xml","build.xml mentions non-existing directory contrib/WordNet/ which should read
contrib/wordnet in line 418.

below the result of svn diff against the corrected and working version of build.xml

--- build.xml   (revision 169301)
+++ build.xml   (working copy)
@@ -415,7 +415,7 @@
         <!-- TODO: find a dynamic way to do include multiple source roots -->
         <packageset dir=""src/java""/>
         <packageset dir=""contrib/analyzers/src/java""/>
-        <packageset dir=""contrib/WordNet/src/java""/>
+        <packageset dir=""contrib/wordnet/src/java""/>
         <packageset dir=""contrib/highlighter/src/java""/>
         <packageset dir=""contrib/similarity/src/java""/>
         <packageset dir=""contrib/spellchecker/src/java""/>"
"LUCENE-3168","TEST","IMPROVEMENT","Enable Throttling only during nightly builds","Some of my tests take forever even on a big :) machine. In order to speed up our tests we should default the IO throttling to NEVER and only run in during nightly.

"
"LUCENE-1400","BUILD_SYSTEM","IMPROVEMENT","Add Apache RAT (Release Audit Tool) target to build.xml","
Apache RAT is a useful tool to check for common mistakes in our source code (eg missing copyright headers):

    http://incubator.apache.org/rat/

I'm just copying the patch Grant worked out for Solr (SOLR-762).  I plan to commit to 2.4 & 2.9."
"LUCENE-2875","BUG","BUG","NumericTokenStream.NumericTermAttribute does not support cloning -> Solr analysis request handlers fail","During converting Solr's AnalysisRequestHandlers (LUCENE-2374) I noticed, that the current implementation of NumericTokenStream fails on cloneAttributes(), which is needed to buffer the tokens for structured display.

This issue should fix this by refactoring the inner class."
"LUCENE-1130","IMPROVEMENT","BUG","Hitting disk full during DocumentWriter.ThreadState.init(...) can cause hang","More testing of RC2 ...

I found one case, if you hit disk full during init() in
DocumentsWriter.ThreadState, when we first create the term vectors &
fields writer, such that subsequent calls to
IndexWriter.add/updateDocument will then hang forever.

What's happening in this case is we are incrementing nextDocID even
though we never call finishDocument (because we ""thought"" init did not
succeed).  Then, when we finish the next document, it will never
actually write because missing finishDocument call never happens.
"
"LUCENE-1814","TEST","BUG","Some Lucene tests try and use a Junit Assert in new threads","There are a few cases in Lucene tests where JUnit Asserts are used inside a new threads run method - this won't work because Junit throws an exception when a call to Assert fails - that will kill the thread, but the exception will not propagate to JUnit - so unless a failure is caused later from the thread termination, the Asserts are invalid.

TestThreadSafe
TestStressIndexing2
TestStringIntern"
"LUCENE-1036","BUG","BUG","Unreleased 2.3 version of IndexWriter.optimize()  consistly throws java.lang.IllegalArgumentException out-of-the-box","Since the upcoming 2.3 version of Lucene has support for the setRAMBufferSizeMB() method in Index Writer,  I thought I would test its performance.   So, using my application that was built upon (and worked with) Lucene 2.2,  I downloaded the nightly build 2007-10-26_03-16-46 and rebuilt my application with new code setting setRAMBufferSizeMB() from a properties file.   My test data resides in a database table of 30 columns holding 1.25 million records.   The good news is that performance is superior to Lucene 2.2.  The indexing completes in roughly 1/3 the time.   The bad news is the Index Writer.optimize() step now throws an java.lang.IllegalArgumentException.
I also run tests against various other tables.  Indexing smaller amounts of data did not throw the exception.  Indexing largers amounts of data did throw the exception.  Note, I also tested nightly builds dating back to 2007-10-05.

...
INFO:  SEIndexThread.commitCheck...
INFO:    ----Commit point reached:  1200000
INFO:  SEIndexThread.commitCheck...
INFO:    ----Commit point reached:  1225000
INFO:  SEIndexThread.commitCheck...
INFO:    ----Commit point reached:  1250000
INFO:  SEIndexThread.closeIndex()...
INFO:    ----commit point reached:  1250659
INFO:    ----optimize index
INFO: SEIndexThread():  java.lang.IllegalArgumentException

java.lang.IllegalArgumentException
        at java.lang.Thread.setPriority(Thread.java(Compiled Code))
        at org.apache.lucene.index.ConcurrentMergeScheduler.merge(ConcurrentMerg
eScheduler.java(Compiled Code))
        at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1750)
        at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:1686)
        at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:1652)
        at LuceneSearchEngine.optimizeIndex(LuceneSearchEngine.java:643)
        at LuceneSearchEngine.optimizeIndex(LuceneSearchEngine.java:636)
        at SEIndexThread.closeIndex(SEIndexThread.java:674)
        at SEIndexThread.processSearchObject(SEIndexThread.java:487)
        at SEIndexThread.prepareIndex(SEIndexThread.java:391)
        at SEIndexThread.run(SEIndexThread.java:41)

"
"LUCENE-2741","BUG","BUG","Several Codecs use the same files - PerFieldCodecWrapper can not hold two codec using the same files","Currently we have a rather simple file naming scheme which prevents us from using more than one codec in a segment that relies on the same file.  For instance pulsing and standard codec can not be used together since they both need the .frq .tii .tis etc. To make this work we either need to write distinct per codec files or set a per field / codec file ID. While the first solution seems to be quiet verbose the second one seems to be more flexible too.

One possibility to do that would be to assign a unique id to each SegmentsWriteState when opening the FieldsConsumer and write the IDs into the segments file to eventually load it once the segment is opened. Otherwise our PerFieldCodec feature will not be really flexible nor useful though.  "
"LUCENE-3092","IMPROVEMENT","IMPROVEMENT","NRTCachingDirectory, to buffer small segments in a RAMDir","I created this simply Directory impl, whose goal is reduce IO
contention in a frequent reopen NRT use case.

The idea is, when reopening quickly, but not indexing that much
content, you wind up with many small files created with time, that can
possibly stress the IO system eg if merges, searching are also
fighting for IO.

So, NRTCachingDirectory puts these newly created files into a RAMDir,
and only when they are merged into a too-large segment, does it then
write-through to the real (delegate) directory.

This lets you spend some RAM to reduce I0.
"
"LUCENE-1985","REFACTORING","IMPROVEMENT","DisjunctionMaxQuery -  Iterator code to  for ( A  a : container ) construct","For better readability  - converting the Iterable<T> to  for ( A  a : container ) constructs that is more intuitive to read. "
"LUCENE-2046","BUG","BUG","IndexReader.isCurrent incorrectly returns false after writer.prepareCommit has been called","Spinoff from thread ""2 phase commit with external data"" on java-user.

The IndexReader should not see the index as changed, after a prepareCommit has been called but before commit is called."
"LUCENE-2980","IMPROVEMENT","BUG","Benchmark's ContentSource should not rely on file suffixes to be lower cased when detecting file type (gzip/bzip2/text)","file.gz is correctly handled as gzip, but file.GZ handled as text which is wrong.
"
"LUCENE-1521","BUG","BUG","""fdx size mismatch"" exception in StoredFieldsWriter.closeDocStore() when closing index with 500M documents","When closing index that contains 500,000,000 randomly generated documents, an exception is thrown:

java.lang.RuntimeException: after flush: fdx size mismatch: 500000000 docs vs 4000000004 length in bytes of _0.fdx
	at org.apache.lucene.index.StoredFieldsWriter.closeDocStore(StoredFieldsWriter.java:94)
	at org.apache.lucene.index.DocFieldConsumers.closeDocStore(DocFieldConsumers.java:83)
	at org.apache.lucene.index.DocFieldProcessor.closeDocStore(DocFieldProcessor.java:47)
	at org.apache.lucene.index.DocumentsWriter.closeDocStore(DocumentsWriter.java:367)
	at org.apache.lucene.index.IndexWriter.flushDocStores(IndexWriter.java:1688)
	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3518)
	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3442)
	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1623)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1588)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1562)
        ...

This appears to be a bug at StoredFieldsWriter.java:93:

      if (4+state.numDocsInStore*8 != state.directory.fileLength(state.docStoreSegmentName + ""."" + IndexFileNames.FIELDS_INDEX_EXTENSION))

where the multiplication by 8 is causing integer overflow. The fix would be to cast state.numDocsInStore to long before multiplying.

It appears that this is another instance of the mistake that caused bug LUCENE-1519. I did a cursory seach for \*8 against the code to see if there might be yet more instances of the same mistake, but found none. 

"
"LUCENE-1340","RFE","RFE","Make it posible not to include TF information in index","Term Frequency is typically not needed  for all fields, some CPU (reading one VInt less and one X>>>1...) and IO can be spared by making pure boolen fields possible in Lucene. This topic has already been discussed and accepted as a part of Flexible Indexing... This issue tries to push things a bit faster forward as I have some concrete customer demands.

benefits can be expected for fields that are typical candidates for Filters, enumerations, user rights, IDs or very short ""texts"", phone  numbers, zip codes, names...

Status: just passed standard test (compatibility), commited for early review, I have not tried new feature, missing some asserts and one two unit tests

Complexity: simpler than expected

can be used via omitTf() (who used omitNorms() will know where to find it :)  "
"LUCENE-2426","IMPROVEMENT","IMPROVEMENT","change sort order to binary order","Since flexible indexing, terms are now represented as byte[], but for backwards compatibility reasons, they are not sorted as byte[], but instead as if they were char[].

I think its time to look at sorting terms as byte[]... this would yield the following improvements:
* terms are more opaque by default, they are byte[] and sort as byte[]. I think this would make lucene friendlier to customizations.
* numerics and collation are then free to use their own encoding (full byte) rather than avoiding the use of certain bits to remain compatible with char[] sort order.
* automaton gets simpler because as in LUCENE-2265, it uses byte[] too, and has special hacks because terms are sorted as char[]
"
"LUCENE-696","BUG","BUG","Scorer.skipTo() doesn't always work if called before next()","skipTo() doesn't work for all scorers if called before next()."
"LUCENE-1135","DOCUMENTATION","TASK","Mark contrib/wikipedia as experimental","I am going to add javadocs to trunk and 2_3 branch that mark the WikipediaTokenizer as experimental.  I think it is fine to release, but I want people to know that the grammar may change in the next release (although I will try to keep it the same)"
"LUCENE-1762","CLEANUP","IMPROVEMENT","Slightly more readable code in Token/TermAttributeImpl","No big deal. 

growTermBuffer(int newSize) was using correct, but slightly hard to follow code. 

the method was returning null as a hint that the current termBuffer has enough space to the upstream code or reallocated buffer.

this patch simplifies logic   making this method to only reallocate buffer, nothing more.  
It reduces number of if(null) checks in a few methods and reduces amount of code. 
all tests pass.

This also adds tests for the new basic attribute impls (copies of the Token tests)."
"LUCENE-2772","IMPROVEMENT","","Make SlowMultiReaderWrapper wrap always so close() is safe","The overhead when wrapping an atomic reader using SlowMultiReaderWrapper is very low, the work done in the static wrap method is much higher (instantiate ArrayList, recusively went through all subreaders), just to check the number of readers than simply always wrapping.

MultiFields already is optimized when called by one-segment or atomic readers, so there is no overhead at all. So this patch removes the static wrap method and you simply wrap like a TokenFilter with ctor: new SlowMultiReaderWrapper(reader)

When this is done, there is also no risk to close a SegmentReader (which you should not do), when wrap() returns a single SegmentReader. This help in parent issue with cleaning up the case in close().

The patch also removes the now useless mainReader/reader variables and simply closes the wrapper."
"LUCENE-1527","BUG","BUG","Benchmark deletes.alg fails","Benchmark deletes.alg fails because the index reader defaults to open readonly.  "
"LUCENE-1782","REFACTORING","IMPROVEMENT","Rename OriginalQueryParserHelper","We should rename the new QueryParser so it's clearer that it's
Lucene's default QueryParser, going forward, and not just a temporary
""bridge"" to a future new QueryParser.

How about we rename oal.queryParser.original -->
oal.queryParser.standard (can't use ""default"": it's a Java keyword)?
Then, leave the OriginalQueryParserHelper under that package, but
simply rename it to QueryParser?

This way if we create other sub-packages in the future, eg
ComplexPhraseQueryParser, they too can have a QueryParser class under
them, to make it clear that's the ""top"" class you use to parse
queries.
"
"LUCENE-1583","BUG","BUG","SpanOrQuery skipTo() doesn't always move forwards","In SpanOrQuery the skipTo() method is improperly implemented if the target doc is less than or equal to the current doc, since skipTo() may not be called for any of the clauses' spans:

    public boolean skipTo(int target) throws IOException {
          if (queue == null) {
            return initSpanQueue(target);
          }

          while (queue.size() != 0 && top().doc() < target) {
            if (top().skipTo(target)) {
              queue.adjustTop();
            } else {
              queue.pop();
            }
          }
          
        	return queue.size() != 0;
        }

This violates the correct behavior (as described in the Spans interface documentation), that skipTo() should always move forwards, in other words the correct implementation would be:

    public boolean skipTo(int target) throws IOException {
          if (queue == null) {
            return initSpanQueue(target);
          }

          boolean skipCalled = false;
          while (queue.size() != 0 && top().doc() < target) {
            if (top().skipTo(target)) {
              queue.adjustTop();
            } else {
              queue.pop();
            }
            skipCalled = true;
          }
          
          if (skipCalled) {
        	return queue.size() != 0;
          }
          return next();
        }"
"LUCENE-570","DESIGN_DEFECT","IMPROVEMENT","Expose directory on IndexReader","It would be really useful to expose the index directory on the IndexReader class."
"LUCENE-1912","BUG","BUG","FastVectorHighlighter: latter terms cannot be highlighted if two or more terms are concatenated","My customer found a bug in FastVectorHighlighter. I'm working for the fix. I'll post it as soon as possible. We hope the fix in 2.9."
"LUCENE-224","CLEANUP","BUG","[PATCH] FilteredTermEnum code cleanup","FilteredTermEnum's constructor takes two parameters but doesn't use them. This 
patch changes that and thus makes the code easier readable. Maybe the old 
constructor should be kept (as deprecated)? I'm not sure, this version seems 
cleaner to me."
"LUCENE-2242","RFE","TASK","Contrib CharTokenizer classes should be instantiated using their new Version based ctors","Contrib CharTokenizer classes should be instantiated using their new Version based ctors introduced by LUCENE-2183 and LUCENE-2240"
"LUCENE-486","TEST","TEST","Core Test should not have dependencies on the Demo code","The TestDoc.java Test file has a dependency on the Demo FileDocument code.  Some of us don't keep the Demo code around after downloading, so this breaks the build.

Patch will be along shortly"
"LUCENE-2002","RFE","BUG","Add oal.util.Version ctor to QueryParser","This is a followup of LUCENE-1987:

If somebody uses StandardAnalyzer with Version.LUCENE_CURRENT and then uses QueryParser, phrase queries will not work, because the StopFilter enables position Increments for stop words, but QueryParser ignores them per default. The user has to explicitely enable them.

This issue would add a ctor taking the Version constant and automatically enable this setting. The same applies to the contrib queryparser. Eventually also StopAnalyzer should add this version ctor.

To be able to remove the default ctor for 3.0 (to remove a possible trap for users of QueryParser), it must be deprecated and the new one also added to 2.9.1."
"LUCENE-2414","RFE","RFE","add icu-based tokenizer for unicode text segmentation","I pulled out the last part of LUCENE-1488, the tokenizer itself and cleaned it up some.

The idea is simple:
* First step is to divide text into writing system boundaries (scripts)
* You supply an ICUTokenizerConfig (or just use the default) which lets you tailor segmentation on a per-writing system basis.
* This tailoring can be any BreakIterator, so rule-based or dictionary-based or your own.

The default implementation (if you do not customize) is just to do UAX#29, but with tailorings for stuff with no clear word division:
* Thai (uses dictionary-based word breaking)
* Khmer, Myanmar, Lao (uses custom rules for syllabification)

Additionally as more of an example i have a tailoring for hebrew that treats the punctuation special. (People have asked before
for ways to make standardanalyzer treat dashes differently, etc)
"
"LUCENE-1978","CLEANUP","TASK","Remove HitCollector","Remove the rest of HitCollectors"
"LUCENE-2157","IMPROVEMENT","IMPROVEMENT","DelimitedPayloadTokenFilter copies the bufer over itsself. Instead it should only set the length. Also optimize logic.","This is a small improvement I found when looking around. It is also a bad idea to copy a array over itsself.

All tests pass, will commit later!"
"LUCENE-1702","BUG","BUG","Thai token type() bug","While adding tests for offsets & type to ThaiAnalyzer, i discovered it does not type Thai numeric digits correctly.
ThaiAnalyzer uses StandardTokenizer, and this is really an issue with the grammar, which adds the entire [:Thai:] block to ALPHANUM.

i propose that alphanum be described a little bit differently in the grammar.
Instead, [:letter:] should be allowed to have diacritics/signs/combining marks attached to it.

this would allow the [:thai:] hack to be completely removed, would allow StandardTokenizer to parse complex writing systems such as Indian languages, and would fix LUCENE-1545.
"
"LUCENE-2449","TEST","TEST","Improve random testing","We have quite a few random tests, but there's no way to ""crank"" them.

The idea here is to add a multiplier which can be increased by a sysprop. For example, we could set this to something higher than 1 for hudson."
"LUCENE-467","IMPROVEMENT","IMPROVEMENT","Use Float.floatToRawIntBits over Float.floatToIntBits","Copied From my Email:
  Float.floatToRawIntBits (in Java1.4) gives the raw float bits without
normalization (like *(int*)&floatvar would in C).  Since it doesn't do
normalization of NaN values, it's faster (and hopefully optimized to a
simple inline machine instruction by the JVM).

On my Pentium4, using floatToRawIntBits is over 5 times as fast as
floatToIntBits.
That can really add up in something like Similarity.floatToByte() for
encoding norms, especially if used as a way to compress an array of
float during query time as suggested by Doug."
"LUCENE-1078","CLEANUP","IMPROVEMENT","Cleanup some unused and unnecessary code","Several classes in trunk have some unused and unnecessary code. This includes unused fields, unused automatic variables, unused imports and unnecessary assignments. Attached it a patch to clean these up."
"LUCENE-2894","DOCUMENTATION","IMPROVEMENT","Use of google-code-prettify for Lucene/Solr Javadoc","My company, RONDHUIT uses google-code-prettify (Apache License 2.0) in Javadoc for syntax highlighting:

http://www.rondhuit-demo.com/RCSS/api/com/rondhuit/solr/analysis/JaReadingSynonymFilterFactory.html

I think we can use it for Lucene javadoc (java sample code in overview.html etc) and Solr javadoc (Analyzer Factories etc) to improve or simplify our life."
"LUCENE-1299","BUG","BUG","Spell Checker suggestSimilar throws NPE when IndexReader is not null and field is null","The SpellChecker.suggestSimilar(String word, int numSug, IndexReader ir,   String field, boolean morePopular) throws a NullPointerException when the IndexReader is not null, but the Field is.  The Javadocs say that it is fine to have the field be null, but doesn't comment on the fact that the IndexReader also needs to be null in that case.

"
"LUCENE-3749","CLEANUP","TASK","Similarity.java javadocs and simplifications for 4.0","As part of adding additional scoring systems to lucene, we made a lower-level Similarity
and the existing stuff became e.g. TFIDFSimilarity which extends it.

However, I always feel bad about the complexity introduced here (though I do feel there
are some ""excuses"", that its a difficult challenge).

In order to try to mitigate this, we also exposed an easier API (SimilarityBase) on top of 
it that makes some assumptions (and trades off some performance) to try to provide something 
consumable for e.g. experiments.

Still, we can cleanup a few things with the low-level api: fix outdated documentation and
shoot for better/clearer naming etc.
"
"LUCENE-3852","REFACTORING","IMPROVEMENT","Rename BaseMultiReader class to BaseCompositeReader and make public","Currently the abstract DirectoryReader and MultiReader and ParallelCompositeReader extend a package private class. Users that want to implement a composite reader, should be able to subclass this pkg-private class, as it implements lots of abstract methods, useful for own implementations. In fact MultiReader is a shallow subclass only implementing correct closing&refCounting.

By making it public after the rename, the generics problems (type parameter R is not correctly displayed) in the JavaDocs are solved, too."
"LUCENE-3238","BUG","BUG","SpanMultiTermQueryWrapper with Prefix Query issue","If we try to do a search with SpanQuery and a PrefixQuery this message is returned:

""You can only use SpanMultiTermQueryWrapper with a suitable SpanRewriteMethod.""

The problem is in the WildcardQuery rewrite function.

If the wildcard query is a prefix, a new prefix query is created, the rewrite method is set with the SpanRewriteMethod and the prefix query is returned.

But, that's the rewritten prefix query which should be returned:

-      return rewritten;
+      return rewritten.rewrite(reader);

I will attach a patch with a unit test included.



"
"LUCENE-1434","RFE","RFE","IndexableBinaryStringTools: convert arbitrary byte sequences into Strings that can be used as index terms, and vice versa","Provides support for converting byte sequences to Strings that can be used as index terms, and back again. The resulting Strings preserve the original byte sequences' sort order (assuming the bytes are interpreted as unsigned).

The Strings are constructed using a Base 8000h encoding of the original binary data - each char of an encoded String represents a 15-bit chunk from the byte sequence.  Base 8000h was chosen because it allows for all lower 15 bits of char to be used without restriction; the surrogate range [U+D800-U+DFFF] does not represent valid chars, and would require complicated handling to avoid them and allow use of char's high bit.

This class is intended to serve as a mechanism to allow CollationKeys to serve as index terms."
"LUCENE-1282","IMPROVEMENT","BUG","Sun hotspot compiler bug in 1.6.0_04/05 affects Lucene","This is not a Lucene bug.  It's an as-yet not fully characterized Sun
JRE bug, as best I can tell.  I'm opening this to gather all things we
know, and to work around it in Lucene if possible, and maybe open an
issue with Sun if we can reduce it to a compact test case.

It's hit at least 3 users:

  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200803.mbox/%3c8c4e68610803180438x39737565q9f97b4802ed774a5@mail.gmail.com%3e
  http://mail-archives.apache.org/mod_mbox/lucene-solr-user/200804.mbox/%3c4807654E.7050900@virginia.edu%3e
  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200805.mbox/%3c733777220805060156t7fdb8fectf0bc984fbfe48a22@mail.gmail.com%3e

It's specific to at least JRE 1.6.0_04 and 1.6.0_05, that affects
Lucene.  Whereas 1.6.0_03 works OK and it's unknown whether 1.6.0_06
shows it.

The bug affects bulk merging of stored fields.  When it strikes, the
segment produced by a merge is corrupt because its fdx file (stored
fields index file) is missing one document.  After iterating many
times with the first user that hit this, adding diagnostics &
assertions, its seems that a call to fieldsWriter.addDocument some
either fails to run entirely, or, fails to invoke its call to
indexStream.writeLong.  It's as if when hotspot compiles a method,
there's some sort of race condition in cutting over to the compiled
code whereby a single method call fails to be invoked (speculation).

Unfortunately, this corruption is silent when it occurs and only later
detected when a merge tries to merge the bad segment, or an
IndexReader tries to open it.  Here's a typical merge exception:

{code}
Exception in thread ""Thread-10"" 
org.apache.lucene.index.MergePolicy$MergeException: 
org.apache.lucene.index.CorruptIndexException:
    doc counts differ for segment _3gh: fieldsReader shows 15999 but segmentInfo shows 16000
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:271)
Caused by: org.apache.lucene.index.CorruptIndexException: doc counts differ for segment _3gh: fieldsReader shows 15999 but segmentInfo shows 16000
        at org.apache.lucene.index.SegmentReader.initialize(SegmentReader.java:313)
        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:262)
        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:221)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3099)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2834)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:240)
{code}

and here's a typical exception hit when opening a searcher:

{code}
org.apache.lucene.index.CorruptIndexException: doc counts differ for segment _kk: fieldsReader shows 72670 but segmentInfo shows 72671
        at org.apache.lucene.index.SegmentReader.initialize(SegmentReader.java:313)
        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:262)
        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:230)
        at org.apache.lucene.index.DirectoryIndexReader$1.doBody(DirectoryIndexReader.java:73)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:636)
        at org.apache.lucene.index.DirectoryIndexReader.open(DirectoryIndexReader.java:63)
        at org.apache.lucene.index.IndexReader.open(IndexReader.java:209)
        at org.apache.lucene.index.IndexReader.open(IndexReader.java:173)
        at org.apache.lucene.search.IndexSearcher.<init>(IndexSearcher.java:48)
{code}

Sometimes, adding -Xbatch (forces up front compilation) or -Xint
(disables compilation) to the java command line works around the
issue.

Here are some of the OS's we've seen the failure on:

{code}
SuSE 10.0
Linux phoebe 2.6.13-15-smp #1 SMP Tue Sep 13 14:56:15 UTC 2005 x86_64 
x86_64 x86_64 GNU/Linux 

SuSE 8.2
Linux phobos 2.4.20-64GB-SMP #1 SMP Mon Mar 17 17:56:03 UTC 2003 i686 
unknown unknown GNU/Linux 

Red Hat Enterprise Linux Server release 5.1 (Tikanga)
Linux lab8.betech.virginia.edu 2.6.18-53.1.14.el5 #1 SMP Tue Feb 19 
07:18:21 EST 2008 i686 i686 i386 GNU/Linux
{code}

I've already added assertions to Lucene to detect when this bug
strikes, but since assertions are not usually enabled, I plan to add a
real check to catch when this bug strikes *before* we commit the merge
to the index.  This way we can detect & quarantine the failure and
prevent corruption from entering the index.

"
"LUCENE-1988","CLEANUP","IMPROVEMENT","CharacterCache - references deleted ","CharacterCache is deprecated by Character.valueOf(c) . Hence the latter is chosen over the former. "
"LUCENE-3638","IMPROVEMENT","IMPROVEMENT","IndexReader.document always return a doc with all the stored fields loaded. And this can be slow for the indexed document contain huge fields","when generating digest for some documents with huge fields, it should be unnecessary to load the field but just interesting part of the field with the offset information. but indexreader always return the whole field content. afterward, the customized storedfieldsreader will got a repeated loading"
"LUCENE-3339","BUG","BUG","TestNRTThreads hangs in nightly 3.x builds","Maybe we have a problem, maybe its a bug in the test.

But its strange that lately the 3.x nightlies have been hanging here."
"LUCENE-1946","CLEANUP","TASK","Remove deprecated TokenStream API","I looked into clover analysis: It seems to be no longer used since I removed the tests yesterday - I am happy!"
"LUCENE-2042","RFE","IMPROVEMENT","Allow controllable printing of the hits","Adds ""print.hits.field"" property to the alg.  If set, then the hits retrieved by Search* tasks are printed, along with the value of the specified field, for each doc."
"LUCENE-892","IMPROVEMENT","IMPROVEMENT","CompoundFileReader's openInput produces streams that may do an extra buffer copy","Spinoff of LUCENE-888.

The class for reading from a compound file (CompoundFileReader) has a
primary stream which is a BufferedIndexInput when that stream is from
an FSDirectory (which is the norm).  That is one layer of buffering.

Then, when its openInput is called, a CSIndexInput is created which
also subclasses from BufferedIndexInput.  That's a second layer of
buffering.

When a consumer actually uses that CSIndexInput to read, and a call to
readByte or readBytes runs out of what's in the first buffer, it will
go to refill its buffer.  But that refill calls the first
BufferedIndexInput which in turn may refill its buffer (a double
copy) by reading the underlying stream.

Not sure how to fix it yet but we should change things to not do the
extra buffer copy.
"
"LUCENE-431","IMPROVEMENT","IMPROVEMENT","RAMInputStream and RAMOutputStream without further buffering","From java-dev, Doug's reply of 12 Sep 2005 
on Delaying buffer allocation in BufferedIndexInput: 
 
Paul Elschot wrote: 
... 
> I noticed that RAMIndexInput extends BufferedIndexInput. 
> It has all data in buffers already, so why is there another 
> layer of buffering? 
 
No good reason: it's historical. 
 
To avoid this either: (a) the BufferedIndexInput API would need to be  
modified to permit subclasses to supply the buffer; or (b)  
RAMInputStream could subclass IndexInput directly, using its own  
buffers.  The latter would probably be simpler. 
 
End of quote. 
 
I made version (b) of RAMInputStream. 
Using this RAMInputStream, TestTermVectorsReader failed as the only 
failing test."
"LUCENE-1797","BUG","BUG","new QueryParser over-increment position for MultiPhraseQuery","If the new QP is parsing a phrase, and when the analyzer runs on the text within the phrase it produces some tokens with posIncr=0, a MultiPhraseQuery is produced.  But, the positions of the added terms are over-incremented, and don't match what the current QueryParser does."
"LUCENE-1335","IMPROVEMENT","BUG","Correctly handle concurrent calls to addIndexes, optimize, commit","Spinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200807.mbox/%3Cc7b302c50807111018j58b6d08djd56b5889f6b3780d@mail.gmail.com%3E"
"LUCENE-2112","RFE","BUG","Flex on non-flex emulation of TermsEnum incorrectly seeks/nexts beyond current field","Spinoff of LUCENE-2111, where Uwe found this issue with the flex on non-flex emulation."
"LUCENE-1457","BUG","BUG","There are a few binary search implmentations in lucene that suffer from a now well known overflow bug","http://googleresearch.blogspot.com/2006/06/extra-extra-read-all-about-it-nearly.html

The places I see it are:

MultiSearcher.subSearcher(int)
TermInfosReader.getIndexOffset(Term)
MultiSegmentReader.readerIndex(int, int[], int)
MergeDocIDRemapper.remap(int)

I havn't taken much time to consider how likely any of these are to overflow. The values being averaged would have to be very large. That would rule out possible problems for at least a couple of these, but how about something like the MergeDocIDRemapper? Is there a document number that could be reached that has a chance of triggering this bug? If not we can close this and have a record of looking into it."
"LUCENE-1188","IMPROVEMENT","IMPROVEMENT","equals and hashCode implementation in org.apache.lucene.search.* package","I would like to talk about the implementation of equals and hashCode method  in org.apache.lucene.search.* package. 

Example One:

org.apache.lucene.search.spans.SpanTermQuery (Super Class)
	<- org.apache.lucene.search.payloads.BoostingTermQuery (Sub Class)

Observation:

* BoostingTermQuery defines equals but inherits hashCode from SpanTermQuery. Definition of equals is a code clone of SpanTermQuery with a change in class name. 

Intention:

I believe the intention of equals redefinition in BoostingTermQuery is not to make the objects of SpanTermQuery and BoostingTermQuery comparable. ie. spanTermQuery.equals(boostingTermQuery) == false && boostingTermQuery.equals(spanTermQuery) == false.


Problem:

With current implementation, the intention might not be respected as a result of symmetric property violation of equals contract i.e.
spanTermQuery.equals(boostingTermQuery) == true (can be) && boostingTermQuery.equals(spanTermQuery) == false. (always)
(Note: Provided their state variables are equal)

Solution:

Change implementation of equals in SpanTermQuery from:

{code:title=SpanTermQuery.java|borderStyle=solid}
  public boolean equals(Object o) {
    if (!(o instanceof SpanTermQuery))
      return false;
    SpanTermQuery other = (SpanTermQuery)o;
    return (this.getBoost() == other.getBoost())
      && this.term.equals(other.term);
  }
{code}

To:
{code:title=SpanTermQuery.java|borderStyle=solid}
  public boolean equals(Object o) {
  	if(o == this) return true;
  	if(o == null || o.getClass() != this.getClass()) return false;
//    if (!(o instanceof SpanTermQuery))
//      return false;
    SpanTermQuery other = (SpanTermQuery)o;
    return (this.getBoost() == other.getBoost())
      && this.term.equals(other.term);
  }
{code}

Advantage:

* BoostingTermQuery.equals and BoostingTermQuery.hashCode is not needed while still preserving the same intention as before.
 
* Any further subclassing that does not add new state variables in the extended classes of SpanTermQuery, does not have to redefine equals and hashCode. 

* Even if a new state variable is added in a subclass, the symmetric property of equals contract will still be respected irrespective of implementation (i.e. instanceof / getClass) of equals and hashCode in the subclasses.


Example Two:


org.apache.lucene.search.CachingWrapperFilter (Super Class)
	<- org.apache.lucene.search.CachingWrapperFilterHelper (Sub Class)

Observation:
Same as Example One.

Problem:
Same as Example one.

Solution:
Change equals in CachingWrapperFilter from:
{code:title=CachingWrapperFilter.java|borderStyle=solid}
  public boolean equals(Object o) {
    if (!(o instanceof CachingWrapperFilter)) return false;
    return this.filter.equals(((CachingWrapperFilter)o).filter);
  }
{code}

To:
{code:title=CachingWrapperFilter.java|borderStyle=solid}
  public boolean equals(Object o) {
//    if (!(o instanceof CachingWrapperFilter)) return false;
    if(o == this) return true;
    if(o == null || o.getClass() != this.getClass()) return false;
    return this.filter.equals(((CachingWrapperFilter)o).filter);
  }
{code}

Advantage:
Same as Example One. Here, CachingWrapperFilterHelper.equals and CachingWrapperFilterHelper.hashCode is not needed.


Example Three:

org.apache.lucene.search.MultiTermQuery (Abstract Parent)
	<- org.apache.lucene.search.FuzzyQuery (Concrete Sub)
	<- org.apache.lucene.search.WildcardQuery (Concrete Sub)

Observation (Not a problem):

* WildcardQuery defines equals but inherits hashCode from MultiTermQuery.
Definition of equals contains just super.equals invocation. 

* FuzzyQuery has few state variables added that are referenced in its equals and hashCode.
Intention:

I believe the intention here is not to make objects of FuzzyQuery and WildcardQuery comparable. ie. fuzzyQuery.equals(wildCardQuery) == false && wildCardQuery.equals(fuzzyQuery) == false.

Proposed Implementation:
How about changing the implementation of equals in MultiTermQuery from:

{code:title=MultiTermQuery.java|borderStyle=solid}
    public boolean equals(Object o) {
      if (this == o) return true;
      if (!(o instanceof MultiTermQuery)) return false;

      final MultiTermQuery multiTermQuery = (MultiTermQuery) o;

      if (!term.equals(multiTermQuery.term)) return false;

      return getBoost() == multiTermQuery.getBoost();
    }
{code}

To:
{code:title=MultiTermQuery.java|borderStyle=solid}
    public boolean equals(Object o) {
      if (this == o) return true;
//      if (!(o instanceof MultiTermQuery)) return false;
      if(o == null || o.getClass() != this.getClass()) return false;

      final MultiTermQuery multiTermQuery = (MultiTermQuery) o;

      if (!term.equals(multiTermQuery.term)) return false;

      return getBoost() == multiTermQuery.getBoost();
    }
{code}

Advantage:

Same as above. Here, WildcardQuery.equals is not needed as it does not define any new state. (FuzzyQuery.equals is still needed because FuzzyQuery defines new state.) 
"
"LUCENE-2892","RFE","IMPROVEMENT","Add QueryParser.newFieldQuery","Note: this patch changes no behavior, just makes QP more subclassable.

Currently we have Query getFieldQuery(String field, String queryText, boolean quoted)
This contains very hairy methods for producing a query from QP's analyzer.

I propose we factor this into newFieldQuery(Analyzer analyzer, String field, String queryText, boolean quoted)
Then getFieldQuery just calls newFieldQuery(this.analyzer, field, queryText, quoted);

The reasoning is: it can be quite useful to consider the double quote as more than phrases, but a ""more exact"" search.
In the case the user quoted the terms, you might want to analyze the text with an alternate analyzer that:
doesn't produce synonyms, doesnt decompose compounds, doesn't use WordDelimiterFilter 
(you would need to be using preserveOriginal=true at index time for the WDF one), etc etc.

This is similar to the way google's double quote operator works, its not defined as phrase but ""this exact wording or phrase"".
For example compare results to a query of tests versus ""tests"".

Currently you can do this without heavy code duplication, but really only if you make a separate field (which is wasteful),
and make your custom QP lie about its field... in the examples I listed above you can do this with a single field, yet still
have a more exact phrase search.
"
"LUCENE-2472","IMPROVEMENT","IMPROVEMENT","The terms index divisor in IW should be set via IWC not via getReader","The getReader call gives a false sense of security... since if deletions have already been applied (and IW is pooling) the readers have already been loaded with a divisor of 1.

Better to set the divisor up front in IWC."
"LUCENE-2404","IMPROVEMENT","BUG","Improve speed of ThaiWordFilter by CharacterIterator, factor out LowerCasing and also fix some bugs (empty tokens stop iteration)","The ThaiWordFilter creates new Strings out of term buffer before passing to The BreakIterator., But BreakIterator can take a CharacterIterator and directly process on it without buffer copying.
As Java itsself does not provide a CharacterIterator implementation in java.text, we can use the javax.swing.text.Segment class, that operates on a char[] and is even reuseable! This class is very strange but it works and is in JDK 1.4+ and not deprecated.

The filter also had a bug: It stopped iterating tokens when an empty token occurred. Also the lowercasing for non-thai words was removed and put into the Analyzer by adding LowerCaseFilter."
"LUCENE-883","BUG","BUG","make spell checker test case work again","See attached path which makes the spellchecker test case work again. The problem without the patch is that consecutive calls to indexDictionary() will create a spelling index with duplicate words. Does anybody see a problem with this patch? I see that the spellchecker code is now used in Solr, isn't it? I didn't have time to test this patch inside Solr.

Also see http://issues.apache.org/jira/browse/LUCENE-632, but the null check is included in this patch so the NPE described there cannot happen anymore.
"
"LUCENE-2931","DOCUMENTATION","IMPROVEMENT","Improved javadocs for PriorityQueue#lessThan","It kills me that I have to inspect the code every time I implement a PriorityQueue. :)"
"LUCENE-1062","IMPROVEMENT","BUG","Improved Payloads API","We want to make some optimizations to the Payloads API.

See following thread for related discussions:
http://www.gossamer-threads.com/lists/lucene/java-dev/54708"
"LUCENE-1096","BUG","BUG","Deleting docs of all returned Hits during search causes ArrayIndexOutOfBoundsException","For background user discussion:
http://www.nabble.com/document-deletion-problem-to14414351.html

{code}
Hits h = m_indexSearcher.search(q); // Returns 11475 documents 
for(int i = 0; i < h.length(); i++) 
{ 
  int doc = h.id(i); 
  m_indexSearcher.getIndexReader().deleteDocument(doc);  <-- causes ArrayIndexOutOfBoundsException when i = 6400
} 
{code}
"
"LUCENE-1398","RFE","RFE","Add ReverseStringFilter","add ReverseStringFilter and ReverseStringAnalyzer that can be used for backword much. For Example, ""*ry"", ""*ing"", ""*ber""."
"LUCENE-2812","BUG","BUG","IndexReader.indexExists sometimes returns true when an index isn't present","If you open a writer on a new dir and prepareCommit but don't finish the commit, IndexReader.indexExists incorrectly returns true, because it just checks for whether a segments_N file is present and not whether it can be successfully read."
"LUCENE-2421","IMPROVEMENT","IMPROVEMENT","Hardening of NativeFSLock","NativeFSLock create a test lock file which its name might collide w/ another JVM that is running. Very unlikely, but still it happened a couple of times already, since the tests were parallelized. This may result in a false exception thrown from release(), when the lock file's delete() is called and returns false, because the file does not exist (deleted by another JVM already). In addition, release() should give a second attempt to delete() if it fails, since the file may be held temporarily by another process (like AntiVirus) before it fails. The proposed changes are:

1) Use ManagementFactory.getRuntimeMXBean().getName() as part of the test lock name (should include the process Id)
2) In release(), if delete() fails, check if the file indeed exists. If it is, let's attempt a re-delete() few ms later.
3) If (3) still fails, throw an exception. Alternatively, we can attempt a deleteOnExit.

I'll post a patch later today."
"LUCENE-687","IMPROVEMENT","IMPROVEMENT","Performance improvement: Lazy skipping on proximity file","Hello,

I'm proposing a patch here that changes org.apache.lucene.index.SegmentTermPositions to avoid unnecessary skips and reads on the proximity stream. Currently a call of next() or seek(), which causes a movement to a document in the freq file also moves the prox pointer to the posting list of that document.  But this is only necessary if actual positions have to be retrieved for that particular document. 

Consider for example a phrase query with two terms: the freq pointer for term 1 has to move to document x to answer the question if the term occurs in that document. But *only* if term 2 also matches document x, the positions have to be read to figure out if term 1 and term 2 appear next to each other in document x and thus satisfy the query. 

A move to the posting list of a document can be quite expensive. It has to be skipped to the last skip point before that document and then the documents between the skip point and the desired document have to be scanned, which means that the VInts of all positions of those documents have to be read and decoded. 

An improvement is to move the prox pointer lazily to a document only if nextPosition() is called. This will become even more important in the future when the size of the proximity file increases (e. g. by adding payloads to the posting lists).

My patch implements this lazy skipping. All unit tests pass. 


I also attach a new unit test that works as follows:
Using a RamDirectory an index is created and test docs are added. Then the index is optimized to make sure it only has a single segment. This is important, because IndexReader.open() returns an instance of SegmentReader if there is only one segment in the index. The proxStream instance of SegmentReader is package protected, so it is possible to set proxStream to a different object. I am using a class called SeeksCountingStream that extends IndexInput in a way that it is able to count the number of invocations of seek(). 

Then the testcase searches the index using a PhraseQuery ""term1 term2"". It is known how many documents match that query and the testcase can verify that seek() on the proxStream is not called more often than number of search hits.

Example:
Number of docs in the index: 500
Number of docs that match the query ""term1 term2"": 5

Invocations of seek on prox stream (old code): 29
Invocations of seek on prox stream (patched version): 5

- Michael
"
"LUCENE-1479","BUG","BUG","TrecDocMaker skips over documents when ""Date"" is missing from documents","TrecDocMaker skips over Trec documents if they do not have a ""Date"" line. When such a document is encountered, the code may skip over several documents until the next tag that is searched for is found.
The result is, instead of reading ~25M documents from the GOV2 collection, the code reads only ~23M (don't remember the actual numbers).

The fix adds a terminatingTag to read() such that the code looks for prefix, but only until terminatingTag is found. Appropriate changes were made in getNextDocData().

Patch to follow"
"LUCENE-1904","REFACTORING","IMPROVEMENT","move wordnet based synonym code out of contrib/memory and into contrib/wordnet (or somewhere else)","see LUCENE-387 ... some synonym related code has been living in contrib/memory for a very long time ... it should be refactored out."
"LUCENE-311","DESIGN_DEFECT","BUG","[PATCH] queryParser.setOperator(int) should be made typesafe","There are AND and DEFAULT_OPERATOR_AND in QueryParser, so calling 
setOperator(QueryParser.AND) looks okay and compiles, but it's not correct. 
I'll attach a patch that uses a typesafe enum to avoid this problem. As 
there's also a getOperator method I had to change the name of the new method 
to get/setDefaultOperator. I don't like that, but it seems to be the only way 
to avoid compile errors for people who switch to a new version of Lucene. 
 
Okay to commit?"
"LUCENE-2912","REFACTORING","IMPROVEMENT","remove field param from computeNorm, scorePayload ; remove UOE'd lengthNorm, switch SweetSpot to per-field ","In LUCENE-2236 we switched sim to per field (SimilarityProvider returns a per-field similarity).

But we didn't completely cleanup there... I think we should now do this:
* SweetSpotSimilarity loses all its hashmaps. Instead, just configure one per field and return it in your SimilarityProvider. this means for example, all its TF factors can now be configured per-field too, not just the length normalization factors.
* computeNorm and scorePayload lose their field parameter, as its redundant and confusing.
* the UOE'd obselete lengthNorm is removed. I also updated javadocs that were pointing to it (this is bad!).

"
"LUCENE-1298","BUG","BUG","MoreLikeThis ignores custom similarity","MoreLikeThis only allows the use of the DefaultSimilarity.  Patch shortly"
"LUCENE-1212","REFACTORING","IMPROVEMENT","Basic refactoring of DocumentsWriter","As a starting point for making DocumentsWriter more understandable,
I've fixed its inner classes to be static, and then broke the classes
out into separate sources, all in org.apache.lucene.index package.

"
"LUCENE-3542","BUG","BUG","StandardQueryParser ignores AND operator for tokenized query terms","The standard query parser uses the default query operator for query clauses that are created from tokenization in the query parser instead of the actual operator for the source term.

here is an example:
{code}
StandardQueryParser parser = new StandardQueryParser(new StandardAnalyzer(Version.LUCENE_34));
parser.setDefaultOperator(Operator.OR);
System.out.println(((BooleanQuery)parser.parse(""_deleted:true AND title:"", ""f"")));
{code}

this should yield:
+_deleted:true +(title: title:)

as our former core query parser does but actually yields:
+_deleted:true title: title:

seems like a bug to me, looking at the tests seems we don't test for this kind of queries in the standard query parser tests too.
"
"LUCENE-3519","BUG","BUG","BlockJoinCollector only allows retrieving groups for only one BlockJoinQuery","Spinoff from Mark Harwood's email (subject ""BlockJoin concerns"") to
dev list.

It's fine to use multiple nested joins in a single query, and
BlockJoinCollector should let you retrieve the top groups for all of
them.

But currently it always returns null after the first query's groups
have been retrieved, because of a silly bug.
"
"LUCENE-3626","RFE","IMPROVEMENT","Make PKIndexSplitter and MultiPassIndexSplitter work per segment","Spinoff from LUCENE-3624: DocValuesw merger throws exception on IW.addIndexes(SlowMultiReaderWrapper) as string-index like docvalues cannot provide asSortedSource."
"LUCENE-508","BUG","BUG","SegmentTermEnum.next() doesn't maintain prevBuffer at end","When you're iterating a SegmentTermEnum and you go past the end of the docs, you end up with a state where the nextBuffer = null and the prevBuffer is the penultimate term, not the last term.  This patch fixes it.  (It's also required for my Prefetching bug [LUCENE-506])

Index: java/org/apache/lucene/index/SegmentTermEnum.java
===================================================================
--- java/org/apache/lucene/index/SegmentTermEnum.java	(revision 382121)
+++ java/org/apache/lucene/index/SegmentTermEnum.java	(working copy)
@@ -109,6 +109,7 @@
   /** Increments the enumeration to the next element.  True if one exists.*/
   public final boolean next() throws IOException {
     if (position++ >= size - 1) {
+      prevBuffer.set(termBuffer);
       termBuffer.reset();
       return false;
     }
"
"LUCENE-1911","IMPROVEMENT","BUG","When using QueryWrapperFilter with CachingWrapperFilter, QueryWrapperFilter returns a DocIdSet that creates a Scorer, which gets cached rather than a bit set","there is a large performance cost to this.

The old impl for this type of thing, QueryFilter, recommends :

@deprecated use a CachingWrapperFilter with QueryWrapperFilter

The deprecated QueryFilter itself also suffers from the problem because its now implemented using a CachingWrapperFilter and QueryWrapperFilter.

see http://search.lucidimagination.com/search/document/7f54715f14b8b7a/lucene_2_9_0rc4_slower_than_2_4_1"
"LUCENE-3258","BUG","BUG","File leak when IOException occurs during index optimization.","I am not sure if this issue requires a fix due to the nature of its occurrence, or if it exists in other versions of Lucene.

I am using Lucene Java 3.0.3 on a SUSE Linux machine with Java 6 and have noticed there are a number of file handles that are not being released from my java application. There are IOExceptions in my log regarding disk full, which causes a merge and the optimization to fail. The index is not currupt upon encountering the IOException. I am using CFS for my index format, so 3X my largest index size during optimization certainly consumes all of my available disk. 

I realize that I need to add more disk space to my machine, but I investigated how to clean up the leaking file handles. After failing to find a misuse of Lucene's IndexWriter in the code I have wrapping Lucene, I did a quick search for close() being invoked in the Lucene Jave source code. I found a number of source files that attempt to close more than one object within the same close() method. I think a try/catch should be put around each of these close() attempts to avoid skipping a subsequent closes. The catch may be able to ignore a caught exception to avoid masking the original exception like done in SimpleFSDirectory.close().

Locations in Lucene Java source where I suggest a try/catch should be used:
- org.apache.lucene.index.FormatPostingFieldsWriter.finish()
- org.apache.lucene.index.TermInfosWriter.close()
- org.apache.lucene.index.SegmentTermPositions.close()
- org.apache.lucene.index.SegmentMergeInfo.close()
- org.apache.lucene.index.SegmentMerger.mergeTerms() (The finally block)
- org.apache.lucene.index.DirectoryReader.close()
- org.apache.lucene.index.FieldsReader.close()
- org.apache.lucene.index.MultiLevelSkipListReader.close()
- org.apache.lucene.index.MultipleTermPositions.close()
- org.apache.lucene.index.SegmentMergeQueue.close()
- org.apache.lucene.index.SegmentMergeDocs.close()
- org.apache.lucene.index.TermInfosReader.close()"
"LUCENE-2952","BUILD_SYSTEM","IMPROVEMENT","Make license checking/maintenance easier/automated","Instead of waiting until release to check licenses are valid, we should make it a part of our build process to ensure that all dependencies have proper licenses, etc."
"LUCENE-2274","TEST","TEST","Catch exceptions in Threads created by JUnit tasks","On hudson we had several assertions failed in TestRAMDirectory, that were never caught by the error reportier in JUnit (as the test itsself did not fail). This patch adds a handler for uncaught exceptions to LuceneTestCase(J4) that let the test fail in tearDown()."
"LUCENE-3746","BUG","BUG","suggest.fst.Sort.BufferSize should not automatically fail just because of freeMemory()","Follow up op dev thread: [FSTCompletionTest failure ""At least 0.5MB RAM buffer is needed"" | http://markmail.org/message/d7ugfo5xof4h5jeh]"
"LUCENE-3011","RFE","RFE","FST serialization and deserialization from plain DataInput/DataOutput streams.","Currently the automaton can be saved only to a Directory instance (IndexInput/ IndexOutput)."
"LUCENE-1008","BUG","BUG","document with no term vector fields after documents with term vector fields corrupts the index","If a document with no term-vector-enabled fields is added after
document(s) that did have term vectors, as part of a single set of
buffered docs, then the term-vector documents file is corrupted
because we fail to write a ""0"" vInt.

Thanks to Grant for spotting this!

Spinoff from this thread:

    http://www.gossamer-threads.com/lists/lucene/java-dev/53306
"
"LUCENE-3387","CLEANUP","","Get javadoc for the similarities package in shape","1. Create a package.html in the similarities package.
2. Update the javadoc of the search package (package.html mentions Similarity)?
3. Compile the javadoc to see if there are any warnings."
"LUCENE-2763","REFACTORING","IMPROVEMENT","Swap URL+Email recognizing StandardTokenizer and UAX29Tokenizer","Currently, in addition to implementing the UAX#29 word boundary rules, StandardTokenizer recognizes email adresses and URLs, but doesn't provide a way to turn this behavior off and/or provide overlapping tokens with the components (username from email address, hostname from URL, etc.).

UAX29Tokenizer should become StandardTokenizer, and current StandardTokenizer should be renamed to something like UAX29TokenizerPlusPlus (or something like that).

For rationale, see [the discussion at the reopened LUCENE-2167|https://issues.apache.org/jira/browse/LUCENE-2167?focusedCommentId=12929325&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12929325]."
"LUCENE-1510","BUG","BUG","InstantiatedIndexReader throws NullPointerException in norms() when used with a MultiReader","
When using InstantiatedIndexReader under a MultiReader where the other Reader contains documents, a NullPointerException is thrown here;

 public void norms(String field, byte[] bytes, int offset) throws IOException {
    byte[] norms = getIndex().getNormsByFieldNameAndDocumentNumber().get(field);
    System.arraycopy(norms, 0, bytes, offset, norms.length);
  }

the 'norms' variable is null. Performing the copy only when norms is not null does work, though I'm sure it's not the right fix.

java.lang.NullPointerException
	at org.apache.lucene.store.instantiated.InstantiatedIndexReader.norms(InstantiatedIndexReader.java:297)
	at org.apache.lucene.index.MultiReader.norms(MultiReader.java:273)
	at org.apache.lucene.search.TermQuery$TermWeight.scorer(TermQuery.java:70)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:131)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:112)
	at org.apache.lucene.search.Searcher.search(Searcher.java:136)
	at org.apache.lucene.search.Searcher.search(Searcher.java:146)
	at org.apache.lucene.store.instantiated.TestWithMultiReader.test(TestWithMultiReader.java:41)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:164)
	at junit.framework.TestCase.runBare(TestCase.java:130)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:120)
	at junit.framework.TestSuite.runTest(TestSuite.java:230)
	at junit.framework.TestSuite.run(TestSuite.java:225)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)

"
"LUCENE-2790","IMPROVEMENT","IMPROVEMENT","IndexWriter should call MP.useCompoundFile and not LogMP.getUseCompoundFile","Spin off from here: http://www.gossamer-threads.com/lists/lucene/java-dev/112311.

I will attach a patch shortly that addresses the issue on trunk."
"LUCENE-2031","REFACTORING","TASK","Move PatternAnalyzer out of contrib/memory to contrib/analyzers","in the memory index contrib there is a PatternAnalyzer.
i think this analyzer belongs in contrib/analyzers instead, it has no relation to memory index."
"LUCENE-3870","BUG","BUG","VarDerefBytesImpl doc values prefix length may fall across two pages","The VarDerefBytesImpl doc values encodes the unique byte[] with prefix (1 or 2 bytes) first, followed by bytes, so that it can use PagedBytes.fillSliceWithPrefix.

It does this itself rather than using PagedBytes.copyUsingLengthPrefix...

The problem is, it can write an invalid 2 byte prefix spanning two blocks (ie, last byte of block N and first byte of block N+1), which fillSliceWithPrefix won't decode correctly.

"
"LUCENE-3624","DESIGN_DEFECT","TASK","Throw exception for ""Multi-SortedSource"" instead of returning null","Spinoff of LUCENE-3623: currently if you addIndexes(FIR) or similar, you get a NPE deep within codecs during merge.

I think the NPE is confusing, it looks like a bug but a clearer exception would be an improvement."
"LUCENE-2945","BUG","BUG","Surround Query doesn't properly handle equals/hashcode","In looking at using the surround queries with Solr, I am hitting issues caused by collisions due to equals/hashcode not being implemented on the anonymous inner classes that are created by things like DistanceQuery (branch 3.x, near line 76)"
"LUCENE-3448","RFE","","Add FixedBitSet.and(other/DISI), andNot(other/DISI)","For the parent issue, and() and andNot() on DISIs and other FixedBitSets are missing. This issue will add those methods.

The DISI methods (also the already existing or(DISI)) method will check for OpenBitSetIterator and do an inplace operation using the bits as optimization."
"LUCENE-1571","BUG","BUG","DistanceFilter problem with deleted documents","I know this is the locallucene lib, but wanted to make sure we don't get this bug when it gets into lucene contrib.

I suspect that the issue is that deleted documents are trying to be evaluated by the filter.  I did some debugging and I confirmed that it is bombing on a document that is marked as deleted (using Luke).


Thanks!

Using the locallucene library 1.51, I get a NullPointerException at line 123 of DistanceFilter
The method is 	public BitSet bits(IndexReader reader) 
The line is double x = NumberUtils.SortableStr2double(sx);

The stack trace is:
java.lang.NullPointerException
	at org.apache.solr.util.NumberUtils.SortableStr2long(NumberUtils.java:149)
	at org.apache.solr.util.NumberUtils.SortableStr2double(NumberUtils.java:104)
	at com.pjaol.search.geo.utils.DistanceFilter.bits(DistanceFilter.java:123)
	at org.apache.lucene.search.Filter.getDocIdSet(Filter.java:49)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:140)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:112)
	at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:113)
	at org.apache.lucene.search.Hits.<init>(Hits.java:90)
	at org.apache.lucene.search.Searcher.search(Searcher.java:72)"
"LUCENE-1990","IMPROVEMENT","IMPROVEMENT","Add unsigned packed int impls in oal.util","There are various places in Lucene that could take advantage of an
efficient packed unsigned int/long impl.  EG the terms dict index in
the standard codec in LUCENE-1458 could subsantially reduce it's RAM
usage.  FieldCache.StringIndex could as well.  And I think ""load into
RAM"" codecs like the one in TestExternalCodecs could use this too.

I'm picturing something very basic like:
{code}
interface PackedUnsignedLongs  {
  long get(long index);
  void set(long index, long value);
}
{code}

Plus maybe an iterator for getting and maybe also for setting.  If it
helps, most of the usages of this inside Lucene will be ""write once""
so eg the set could make that an assumption/requirement.

And a factory somewhere:

{code}
  PackedUnsignedLongs create(int count, long maxValue);
{code}

I think we should simply autogen the code (we can start from the
autogen code in LUCENE-1410), or, if there is an good existing impl
that has a compatible license that'd be great.

I don't have time near-term to do this... so if anyone has the itch,
please jump!
"
"LUCENE-1894","BUG","BUG","Spatial checks for a string in an int,double map","{code}
  private Map<Integer,Double> distances;
{code}

{code}
    if (precise != null) {
      double xLat = getPrecision(lat, precise);
      double xLng = getPrecision(lng, precise);
      
      String k = new Double(xLat).toString() +"",""+ new Double(xLng).toString();
    
      Double d = (distances.get(k));
      if (d != null){
        return d.doubleValue();
      }
    }
{code}

Something is off here eh?"
"LUCENE-3372","BUG","BUG","TestIndexWriterOnDiskFull.testAddDocumentOnDiskFull seed failure","version: trunk r1155278
reproduce-able: always

{code}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterOnDiskFull
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.847 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterOnDiskFull -Dtestmethod=testAddDocumentOnDiskFull -Dtests.seed=-3cc23002ebad518d:70ae722281b31c9f:57406021f8789a22
    [junit] NOTE: test params are: codec=RandomCodecProvider: {content=MockFixedIntBlock(blockSize=1081)}, locale=hr_HR, timezone=Atlantic/Jan_Mayen
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestIndexWriterOnDiskFull]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_26 (64-bit)/cpus=8,threads=1,free=85252968,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAddDocumentOnDiskFull(org.apache.lucene.index.TestIndexWriterOnDiskFull):     Caused an ERROR
    [junit] no segments* file found in MockDirWrapper(org.apache.lucene.store.RAMDirectory@65dcc2a3 lockFactory=MockLockFactoryWrapper(org.apache.lucene.store.SingleInstanceLockFactory@6e8f94)): files: [_1.cfs, _1.cfe, _0.cfe, _0.cfs]
    [junit] org.apache.lucene.index.IndexNotFoundException: no segments* file found in MockDirWrapper(org.apache.lucene.store.RAMDirectory@65dcc2a3 lockFactory=MockLockFactoryWrapper(org.apache.lucene.store.SingleInstanceLockFactory@6e8f94)): files: [_1.cfs, _1.cfe, _0.cfe, _0.cfs]
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:657)
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:534)
    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:284)
    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:277)
    [junit]     at org.apache.lucene.index.TestIndexWriter.assertNoUnreferencedFiles(TestIndexWriter.java:158)
    [junit]     at org.apache.lucene.index.TestIndexWriterOnDiskFull.testAddDocumentOnDiskFull(TestIndexWriterOnDiskFull.java:114)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1526)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1428)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestIndexWriterOnDiskFull FAILED
{code}

"
"LUCENE-3477","CLEANUP","IMPROVEMENT","Fix JFlex tokenizer compiler warnings","We get lots of distracting fallthrough warnings running ""ant compile""
in modules/analysis, from the tokenizers generated from JFlex.

Digging a bit, they actually do look spooky.

So I managed to edit the JFlex inputs to insert a bunch of break
statements in our rules, but I have no idea if this is
right/dangerous, and it seems a bit weird having to do such insertions
of ""naked"" breaks.

But, this does fix all the warnings, and all tests pass...
"
"LUCENE-1474","BUG","BUG","Incorrect SegmentInfo.delCount when IndexReader.flush() is used","When deleted documents are flushed using IndexReader.flush() the delCount in SegmentInfo is updated based on the current value and SegmentReader.pendingDeleteCount (introduced by LUCENE-1267). It seems that pendingDeleteCount is not reset after the commit, which means after a second flush() or close() of an index reader the delCount in SegmentInfo is incorrect. A subsequent IndexReader.open() call will fail with an error when assertions are enabled. E.g.:

java.lang.AssertionError: delete count mismatch: info=3 vs BitVector=2
	at org.apache.lucene.index.SegmentReader.loadDeletedDocs(SegmentReader.java:405)
[...]"
"LUCENE-2820","BUG","BUG","CMS fails to cleanly stop threads","When you close IW, it waits for (or aborts and then waits for) all running merges.

However, it's wait criteria is wrong -- it waits for the threads to be done w/ their merges, not for the threads to actually die.

CMS already has a sync() method, to wait for running threads, which we can call from CMS.close.  However it has a thread hazard because a MergeThread removes itself from mergeThreads before it actually exits.  So sync() is able to return even while a merge thread is still running.

This was uncovered by LUCENE-2819 on the test case TestCustomScoreQuery.testCustomExternalQuery, though I expect other test cases would show it."
"LUCENE-3002","TEST","TEST","Add tests.iter.min to improve controlling tests.iter's behavior","As discussed here: http://lucene.472066.n3.nabble.com/Stop-iterating-if-testsFailed-td2747426.html, this issue proposes to add tests.iter.min in order to allow one better control over how many iterations are run:

* Keep tests.iter as it is today
* Add tests.iter.min (default to tests.iter) to denote that at least N instances of the test should run until there's either a failure or tests.iter is reached.

If one wants to run until the first failure, he can set tests.iter.min=1 and tests.iter=X -- up to X instances of the test will run, until the first failure.

Similarly, one can set tests.iter=N to denote that at least N instances should run, regardless if there were failures, but if after N runs a failure occurred, the test should stop.

Note: unlike what's proposed on the thread, tests.iter.max is dropped from this proposal as it's exactly like tests.iter, so no point in having two similar parameters.

I will work on a patch tomorrow."
"LUCENE-663","RFE","RFE","New feature rich higlighter for Lucene.","Well, I refactored (took) some code from two previous highlighters.
This highlighter:
+ use TermPositionVector where available
+ use Analyzer if no TermPositionVector found or is forced to use it.
+ support for all lucene queries (Term, Phrase with slops, Prefix, Wildcard, Range) except Fuzzy Query (can be implemented easly)

- has no support for scoring (yet)
- use same prefix,postfix for accepted terms (yet)

? It's written in Java5

In next release I'd like to add support for Fuzzy, ""coloring"" f.e. diffrent color for terms btw. phrase terms (slops), scoring of fragments

It's apache licensed - I hope so :-) I put licene statement in every file
"
"LUCENE-982","RFE","IMPROVEMENT","Create new method optimize(int maxNumSegments) in IndexWriter","Spinning this out from the discussion in LUCENE-847.

I think having a way to ""slightly optimize"" your index would be useful
for many applications.

The current optimize() call is very expensive for large indices
because it always optimizes fully down to 1 segment.  If we add a new
method which instead is allowed to stop optimizing once it has <=
maxNumSegments segments in the index, this would allow applications to
eg optimize down to say <= 10 segments after doing a bunch of updates.
This should be a nice compromise of gaining good speedups of searching
while not spending the full (and typically very high) cost of
optimizing down to a single segment.

Since LUCENE-847 is now formalizing an API for decoupling merge policy
from IndexWriter, if we want to add this new optimize method we need
to take it into account in LUCENE-847.
"
"LUCENE-2239","RFE","TASK","Revise NIOFSDirectory and its usage due to NIO limitations on Thread.interrupt","I created this issue as a spin off from http://mail-archives.apache.org/mod_mbox/lucene-java-dev/201001.mbox/%3Cf18c9dde1001280051w4af2bc50u1cfd55f85e50914f@mail.gmail.com%3E

We should decide what to do with NIOFSDirectory, if we want to keep it as the default on none-windows platforms and how we want to document this.

"
"LUCENE-3295","BUG","BUG","BitVector never skips fully populated bytes when writing ClearedDgaps","When writing cleared DGaps in BitVector we compare a byte against 0xFF (255) yet the byte is casted into an int (-1) and the comparison will never succeed. We should mask the byte with 0xFF before comparing or compare against -1"
"LUCENE-2234","RFE","RFE","Hindi Analyzer","An analyzer for hindi.

below are MAP values on the FIRE 2008 test collection.
QE means expansion with morelikethis, all defaults, on top 5 docs.

||setup||T||T(QE)||TD||TD(QE)||TDN||TDN(QE)||
|words only|0.1646|0.1979|0.2241|0.2513|0.2468|0.2735|
|HindiAnalyzer|0.2875|0.3071|0.3387|*0.3791**|0.3837|0.3810|
|improvement|74.67%|55.18%|51.14%|50.86%|55.47%|39.31%|

* TD was the official measurement, highest score for this collection in FIRE 2008 was 0.3487: http://www.isical.ac.in/~fire/paper/mcnamee-jhu-fire2008.pdf

needs a bit of cleanup and more tests"
"LUCENE-3872","BUG","BUG","Index changes are lost if you call prepareCommit() then close()","You are supposed to call commit() after calling prepareCommit(), but... if you forget, and call close() after prepareCommit() without calling commit(), then any changes done after the prepareCommit() are silently lost (including adding/deleting docs, but also any completed merges).

Spinoff from java-user thread ""lots of .cfs (compound files) in the index directory"" from Tim Bogaert.

I think to fix this, IW.close should throw an IllegalStateException if prepareCommit() was called with no matching call to commit()."
"LUCENE-2710","TEST","BUG","""reproduce with"" on test failure isn't right if you manually overrided anything","If you run a test with eg -Dtests.codec=SimpleText...

If it fails, the ""reproduce with"" fails to include that manual override (-Dtests.codec=SimpleText), ie it only includes the seed / test class / test method.  So it won't actually reproduce the fail, in general.

We just need to fix the ""reproduce with"" to add any manual overrides...."
"LUCENE-1507","RFE","IMPROVEMENT","adding EmptyDocIdSet/Iterator","Adding convenience classes for EmptyDocIdSet and EmptyDocIdSetIterator"
"LUCENE-623","DOCUMENTATION","IMPROVEMENT","RAMDirectory.close() should have a comment about not releasing any resources","I wrongly assumed that calling RAMDirectory.close() would free up the memory occupied by the RAMDirectory.
It might be helpful to add a javadoc comment that warns users that RAMDirectory.close() is a no-op, since it might be a common assumption that close() would release resources."
"LUCENE-2422","IMPROVEMENT","BUG","don't reuse byte[] in IndexInput/Output for read/writeString","IndexInput now holds a private ""byte[] bytes"", which it re-uses for reading strings.  Likewise, IndexOutput holds a UTF8Result (which holds ""byte[] bytes""), re-used for writing strings.

These are both dangerous, since on reading or writing immense strings, we never free this storage.

We don't use read/writeString in very perf sensitive parts of the code, so, I think we should not reuse the byte[] at all.

I think this is likely the cause of the recent ""IndexWriter and memory usage"" thread, started by Ross Woolf on java-user@."
"LUCENE-3874","BUG","BUG","bogus positions create a corrumpt index","Its pretty common that positionIncrement can overflow, this happens really easily 
if people write analyzers that don't clearAttributes().

It used to be the case that if this happened (and perhaps still is in 3.x, i didnt check),
that IW would throw an exception.

But i couldnt find the code checking this, I wrote a test and it makes a corrumpt index..."
"LUCENE-2257","IMPROVEMENT","IMPROVEMENT","relax the per-segment max unique term limit","Lucene can't handle more than 2.1B (limit of signed 32 bit int) unique terms in a single segment.

But I think we can improve this to termIndexInterval (default 128) * 2.1B.  There is one place (internal API only) where Lucene uses an int but should use a long."
"LUCENE-475","IMPROVEMENT","IMPROVEMENT"," RAMDirectory(Directory dir, boolean closeDir)  constructor uses memory inefficiently.","recently I found that  RAMDirectory(Directory dir, boolean closeDir)  constructor uses memory inefficiently.
files from source index are read entirely intro memory as single byte array which is after all is thrown away. And if I want to load my 200M optimized, compound format index to memory for faster search I should give JVM at least 400Mb memory limit. For larger indexes this can be an issue.

I've attached patch how to solve this problem."
"LUCENE-1868","DOCUMENTATION","TASK","update NOTICE.txt","From the java-dev discussion, NOTICE.txt should be up-to-date.

One thing I know, is that the persian stopwords file (analyzers/fa) came from the same source as the arabic stopwords file, and is BSD-licensed. 

There might be others (I think ICU has already been added)"
"LUCENE-1137","RFE","RFE","Token type as BitSet: typeBits()","It is sometimes useful to have a more compact, easy to parse, type representation for Token than the current type() String.  This patch adds a BitSet onto Token, defaulting to null, with accessors for setting bit flags on a Token.  This is useful for communicating information about a token to TokenFilters further down the chain.  

For example, in the WikipediaTokenizer, the possibility exists that a token could be both a category and bold (or many other variations), yet it is difficult to communicate this without adding in a lot of different Strings for type.  Unlike using the payload information (which could serve this purpose), the BitSet does not get added to the index (although one could easily convert it to a payload.)"
"LUCENE-3148","BUG","BUG","TestIndexWriterExceptions reproducible AOOBE in MockVariableIntBlockCodec","{code}
  [junit] Testsuite: org.apache.lucene.index.TestIndexWriterExceptions
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.739 sec
    [junit]
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testDocumentsWriterAbort -Dtests.seed=4579947455
682149564:-7960989923752018504
    [junit] NOTE: test params are: codec=RandomCodecProvider: {content=MockVariableIntBlock(baseBlockSize=32)}, locale=bg_BG, timezone=Brazil
/Acre
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestIndexWriterExceptions]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=94363216,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testDocumentsWriterAbort(org.apache.lucene.index.TestIndexWriterExceptions):      Caused an ERROR
    [junit] 66
    [junit] java.lang.ArrayIndexOutOfBoundsException: 66
    [junit]     at org.apache.lucene.index.codecs.mockintblock.MockVariableIntBlockCodec$MockIntFactory$2.add(MockVariableIntBlockCodec.java:
114)
    [junit]     at org.apache.lucene.index.codecs.intblock.VariableIntBlockIndexOutput.close(VariableIntBlockIndexOutput.java:118)
    [junit]     at org.apache.lucene.index.codecs.sep.SepPostingsWriterImpl.close(SepPostingsWriterImpl.java:320)
    [junit]     at org.apache.lucene.index.codecs.BlockTermsWriter.close(BlockTermsWriter.java:137)
    [junit]     at org.apache.lucene.index.PerFieldCodecWrapper$FieldsWriter.close(PerFieldCodecWrapper.java:81)
    [junit]     at org.apache.lucene.index.FreqProxTermsWriter.flush(FreqProxTermsWriter.java:103)
    [junit]     at org.apache.lucene.index.TermsHash.flush(TermsHash.java:118)
    [junit]     at org.apache.lucene.index.DocInverter.flush(DocInverter.java:80)
    [junit]     at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:75)
    [junit]     at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:457)
    [junit]     at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:417)
    [junit]     at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:309)
    [junit]     at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:381)
    [junit]     at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1469)
    [junit]     at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1229)
    [junit]     at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1210)
    [junit]     at org.apache.lucene.index.TestIndexWriterExceptions.testDocumentsWriterAbort(TestIndexWriterExceptions.java:555)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1333)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1251)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.index.TestIndexWriterExceptions FAILED
{code}

trunk: r1127871"
"LUCENE-2409","RFE","RFE","add a tokenfilter for icu transforms","I pulled the ICUTransformFilter out of LUCENE-1488 and create an issue for it here.

This is a tokenfilter that applies an ICU Transliterator, which is a context-sensitive way
to transform text. 

These are typically rule-based and you can use ones included with ICU (such as Traditional-Simplified)
or you can make your own from your own set of rules.

User's Guide: http://userguide.icu-project.org/transforms/general
Rule Tutorial: http://userguide.icu-project.org/transforms/general/rules
"
"LUCENE-2474","RFE","IMPROVEMENT","Allow to plug in a Cache Eviction Listener to IndexReader to eagerly clean custom caches that use the IndexReader (getFieldCacheKey)","Allow to plug in a Cache Eviction Listener to IndexReader to eagerly clean custom caches that use the IndexReader (getFieldCacheKey).

A spin of: https://issues.apache.org/jira/browse/LUCENE-2468. Basically, its make a lot of sense to cache things based on IndexReader#getFieldCacheKey, even Lucene itself uses it, for example, with the CachingWrapperFilter. FieldCache enjoys being called explicitly to purge its cache when possible (which is tricky to know from the ""outside"", especially when using NRT - reader attack of the clones).

The provided patch allows to plug a CacheEvictionListener which will be called when the cache should be purged for an IndexReader."
"LUCENE-1623","BACKPORT","BUG","Back-compat break with non-ascii field names","If a field name contains non-ascii characters in a 2.3.x index, then
on upgrade to 2.4.x unexpected problems are hit.  It's possible to hit
a ""read past EOF"" IOException; it's also possible to not hit an
exception but get an incorrect field name.

This was caused by LUCENE-510, because the FieldInfos (*.fnm) file is
not properly versioned.

Spinoff from http://www.nabble.com/Read-past-EOF-td23276171.html
"
"LUCENE-2977","IMPROVEMENT","IMPROVEMENT","WriteLineDocTask should write gzip/bzip2/txt according to the extension of specified output file name","Since the readers behave this way it would be nice and handy if also this line writer would."
"LUCENE-462","BUG","BUG","bad normalization in sorted search returning TopDocs","FieldSortedHitQueue.maxscore is maintained in the lessThan method (which never gets called if a single document is added to the queue).

I've checked in a test to TestSort.testTopDocsScores() with the final assertion commented out."
"LUCENE-3059","BUG","BUG","PulsingTermState.clone leaks memory","I looked at the heap dump from the OOME this morning (thank you Uwe
for turning this on!), and I think it's a real memory leak.

Well, not really a leak; rather, the cloned PulsingTermState, which we
cache in the terms dict cache, is hanging onto large byte[]
unnecessarily.
"
"LUCENE-2617","BUG","BUG","coord should still apply to missing terms/clauses","Missing terms in a boolean query ""disappear"" (i.e. they don't even affect the coord factor)."
"LUCENE-3111","BUG","BUG","TestFSTs.testRandomWords failure","Was running some while(1) tests on the docvalues branch (r1103705) and the following test failed:

{code}
    [junit] Testsuite: org.apache.lucene.util.automaton.fst.TestFSTs
    [junit] Testcase: testRandomWords(org.apache.lucene.util.automaton.fst.TestFSTs):	FAILED
    [junit] expected:<771> but was:<TwoLongs:771,771>
    [junit] junit.framework.AssertionFailedError: expected:<771> but was:<TwoLongs:771,771>
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs$FSTTester.verifyUnPruned(TestFSTs.java:540)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs$FSTTester.doTest(TestFSTs.java:496)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs$FSTTester.doTest(TestFSTs.java:359)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs.doTest(TestFSTs.java:319)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs.testRandomWords(TestFSTs.java:940)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs.testRandomWords(TestFSTs.java:915)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1282)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1211)
    [junit] 
    [junit] 
    [junit] Tests run: 7, Failures: 1, Errors: 0, Time elapsed: 7.628 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: Ignoring nightly-only test method 'testBigSet'
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestFSTs -Dtestmethod=testRandomWords -Dtests.seed=-269475578956012681:0
    [junit] NOTE: test params are: codec=PreFlex, locale=ar, timezone=America/Blanc-Sablon
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestToken, TestCodecs, TestIndexReaderReopen, TestIndexWriterMerging, TestNoDeletionPolicy, TestParallelReaderEmptyIndex, TestParallelTermEnum, TestPerSegmentDeletes, TestSegmentReader, TestSegmentTermDocs, TestStressAdvance, TestTermVectorsReader, TestSurrogates, TestMultiFieldQueryParser, TestAutomatonQuery, TestBooleanScorer, TestFuzzyQuery, TestMultiTermConstantScore, TestNumericRangeQuery64, TestPositiveScoresOnlyCollector, TestPrefixFilter, TestQueryTermVector, TestScorerPerf, TestSloppyPhraseQuery, TestSpansAdvanced, TestWindowsMMap, TestRamUsageEstimator, TestSmallFloat, TestUnicodeUtil, TestFSTs]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=137329960,total=208207872
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.util.automaton.fst.TestFSTs FAILED
{code}

I am not able to reproduce"
"LUCENE-3137","BUG","BUG","Benchmark's ExtractReuters creates its temp dir wrongly if provided out-dir param ends by slash","See LUCENE-929 for context.
As result, it might fail to create the temp dir at all."
"LUCENE-736","BUG","BUG","Sloppy Phrase Scorer matches the doc ""A B C D E"" for query = ""B C B""~2","This is an extension of https://issues.apache.org/jira/browse/LUCENE-697

In addition to abnormalities Yonik pointed out in 697, there seem to be other issues with slopy phrase search and scoring.

1) A phrase with a repeated word would be detected in a document although it is not there.
I.e. document = A B D C E , query = ""B C B"" would not find this document (as expected), but query ""B C B""~2 would find it. 
I think that no matter how large the slop is, this document should not be a match.

2) A document containing both orders of a query, symmetrically, would score differently for the queru and for its reveresed form.
I.e. document = A B C B A would score differently for queries ""B C""~2 and ""C B""~2, although it is symmetric to both.

I will attach test cases that show both these problems and the one reported by Yonik in 697. "
"LUCENE-2855","IMPROVEMENT","BUG","Contrib queryparser should not use CharSequence as Map key","Today, contrib query parser uses Map<CharSequence,...> in many different places, which may lead to problems, since CharSequence interface does not enforce the implementation of hashcode and equals methods. Today, it's causing a problem with QueryTreeBuilder.setBuilder(CharSequence,QueryBuilder) method, that does not works as expected."
"LUCENE-2196","DESIGN_DEFECT","IMPROVEMENT","Spellchecker should implement java.io.Closable","As the most of the lucene classes implement Closable (IndexWriter) Spellchecker should do too. "
"LUCENE-1825","BUG","IMPROVEMENT","Incorrect usage of AttributeSource.addAttribute/getAttribute leads to failures when onlyUseNewAPI=true","when seting ""use only new API"" for TokenStream, i received the following exception:

{code}
   [junit] Caused by: java.lang.IllegalArgumentException: This AttributeSource does not have the attribute 'interface org.apache.lucene.analysis.tokenattributes.TermAttribute'.
    [junit] 	at org.apache.lucene.util.AttributeSource.getAttribute(AttributeSource.java:249)
    [junit] 	at org.apache.lucene.index.TermsHashPerField.start(TermsHashPerField.java:252)
    [junit] 	at org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:145)
    [junit] 	at org.apache.lucene.index.DocFieldProcessorPerThread.processDocument(DocFieldProcessorPerThread.java:244)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:772)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:755)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:2613)
{code}

However, i can't actually see the culprit that caused this exception

suggest that the IllegalArgumentException include ""getClass().getName()"" in order to be able to identify which TokenStream implementation actually caused this
"
"LUCENE-1059","IMPROVEMENT","IMPROVEMENT","bad java practices which affect performance (result of code inspection)","IntelliJ IDEA found the following issues in the Lucense source code and tests:

1) explicit for loops where calls to System.arraycopy() should have been
2) calls to Boolean constructor (in stead of the appropriate static method/field)
3) instantiation of unnecessary Integer instances for toString, instead of calling the static one
4) String concatenation using + inside a call to StringBuffer.append(), in stead of chaining the append calls

all minor issues. patch is forthcoming.
"
"LUCENE-1128","RFE","IMPROVEMENT","Add Highlighting benchmark support to contrib/benchmark","I would like to be able to test the performance (speed, initially) of the Highlighter in a standard way.  Patch to follow that adds the Highlighter as a dependency benchmark and adds in tasks extending the ReadTask to perform highlighting on retrieved documents."
"LUCENE-3706","RFE","RFE","add offsets into lucene40 postings","LUCENE-3684 added support for IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS, but
only SimpleText implements it.

I think we should implement it in the other 4.0 codecs (starting with Lucene40PostingsFormat)."
"LUCENE-1481","DESIGN_DEFECT","BUG","Sort and SortField does not have equals() and hashCode()","During developing for my project panFMP I had the following issue:
I have a cache for queries (like Solr has, too)  for query results. This cache also uses the Sort/SortField as key into the cache. The problem is, because Sort/SortField does not implement equals() and hashCode(), you cannot store them as cache keys. To workaround, currently I use Sort.toString() as cache key, but this is not so nice.

In corelation with issue LUCENE-1478, I could fix this there in one patch together with the other improvements."
"LUCENE-2259","IMPROVEMENT","IMPROVEMENT","add IndexWriter.removeUnferencedFiles, so apps can more immediately delete index files when readers are closed","This has come up several times on the user's list.

On Windows, which prevents deletion of still-open files, IndexWriter cannot remove files that are in-use by open IndexReaders.  This is fine, and IndexWriter periodically retries the delete, but it doesn't retry very often (only on open, on flushing a new segment, and on committing a merge).  So it lacks immediacy.

With this expert method, apps that want faster deletion can call this method."
"LUCENE-2581","RFE","IMPROVEMENT","FastVectorHighlighter: Make FragmentsBuilder use Encoder","Make FragmentsBuilder use Encoder, as Highlighter does."
"LUCENE-1614","RFE","IMPROVEMENT","Add next() and skipTo() variants to DocIdSetIterator that return the current doc, instead of boolean","See http://www.nabble.com/Another-possible-optimization---now-in-DocIdSetIterator-p23223319.html for the full discussion. The basic idea is to add variants to those two methods that return the current doc they are at, to save successive calls to doc(). If there are no more docs, return -1. A summary of what was discussed so far:
# Deprecate those two methods.
# Add nextDoc() and skipToDoc(int) that return doc, with default impl in DISI (calls next() and skipTo() respectively, and will be changed to abstract in 3.0).
#* I actually would like to propose an alternative to the names: advance() and advance(int) - the first advances by one, the second advances to target.
# Wherever these are used, do something like '(doc = advance()) >= 0' instead of comparing to -1 for improved performance.

I will post a patch shortly"
"LUCENE-3653","IMPROVEMENT","IMPROVEMENT","Lucene Search not scalling","I've noticed that when doing thousands of searches in a single thread the average time is quite low i.e. a few milliseconds. When adding more concurrent searches doing exactly the same search the average time increases drastically. 
I've profiled the search classes and found that the whole of lucene blocks on 

org.apache.lucene.index.SegmentCoreReaders.getTermsReader
org.apache.lucene.util.VirtualMethod
  public synchronized int getImplementationDistance 
org.apache.lucene.util.AttributeSourcew.getAttributeInterfaces

These cause search times to increase from a few milliseconds to up to 2 seconds when doing 500 concurrent searches on the same in memory index. Note: That the index is not being updates at all, so not refresh methods are called at any stage.


Some questions:
  Why do we need synchronization here?
  There must be a non-lockable solution for these, they basically cause lucene to be ok for single thread applications but disastrous for any concurrent implementation.

I'll do some experiments by removing the synchronization from the methods of these classes."
"LUCENE-3593","RFE","RFE","Add a filter returning all document without a value in a field","In some situations it would be useful to have a Filter that simply returns all document that either have at least one or no value in a certain field. We don't have something like that out of the box and adding it seems straight forward."
"LUCENE-564","REFACTORING","BUG","Class DisjunctionSumScorer does not need to be public.","See title, patch follows."
"LUCENE-3708","DESIGN_DEFECT","BUG","codec postings api (finishDoc) is inconsistent","finishDoc says:

{noformat}
  /** Called when we are done adding positions & payloads
   *  for each doc.  Not called  when the field omits term
   *  freq and positions. */
   public abstract void finishDoc() throws IOException;
{noformat}

But this is confusing (because a field can omit just positions, is it called then?!),
and wrong (because merging calls it always, even if freq+positions is omitted).

I think we should fix the javadoc and fix FreqProxTermsWriter to always call finish()
"
"LUCENE-1784","DESIGN_DEFECT","IMPROVEMENT","Make BooleanWeight and DisjunctionMaxWeight protected","Currently, BooleanWeight is private, yet it has 2 protected members (similarity, weights) which are unaccessible from custom code

i have some use cases where it would be very useful to crawl a BooleanWeight to get at the sub Weight objects

however, since BooleanWeight is private, i have no way of doing this

If BooleanWeight is protected, then i can subclass BooleanQuery to hook in and wrap BooleanWeight with a subclass to facilitate this walking of the Weight objects

Would also want DisjunctionMaxWeight to be protected, along with its ""weights"" member

Would be even better if these Weights were made public with accessors to their sub ""weights"" objects (then no subclassing would be necessary on my part)

this should be really trivial and would be great if it can get into 2.9

more generally, it would be nice if all Weight classes were public with nice accessors to relevant ""sub weights""/etc so custom code can get its hooks in where and when desired"
"LUCENE-1504","IMPROVEMENT","IMPROVEMENT","Contrib-Spatial should use DocSet API rather then deprecated BitSet API","Contrib-Spatial should be rewritten to use the new DocIdSet Filter API with OpenBitSets instead of j.u.BitSets. FilteredDocIdSet can be used to replace (I)SerialChainFilter."
"LUCENE-793","DOCUMENTATION","BUG","Javadocs should explain possible causes for IOExceptions","
Most methods in Lucene reserve the right to throw an IOException.  This can occur for nearly all methods from low level problems like wrong permissions, transient IO errors, bad hard drive or corrupted file system, corrupted index, etc, but for some methods there are also more interesting causes that we should try to document.

Spinoff of this thread:

    http://www.gossamer-threads.com/lists/lucene/java-user/44929"
"LUCENE-2289","IMPROVEMENT","IMPROVEMENT","Calls to SegmentInfos.message should be wrapped w/ infoStream != null checks","To avoid the expensive message creation (which involves the '+' operator on strings, calls to message should be wrapped w/ infoStream != null check, rather than inside message(). I'll attach a patch which does that."
"LUCENE-684","DOCUMENTATION","IMPROVEMENT","Refrase javadoc 1st sentence for IndexReader.deleteDocuments",""
"LUCENE-2669","BUG","IMPROVEMENT","NumericRangeQuery.NumericRangeTermsEnum sometimes seeks backwards","Subclasses of FilteredTermsEnum are ""supposed to"" seek forwards only (this gives better performance, typically).

However, we don't check for this, so I added an assert to do that (while digging into testing the SimpleText codec) and NumericRangeQuery trips the assert!

Other MTQs seem not to trip it.

I think I know what's happening -- say NRQ has term ranges a-c, e-f to seek to, but then while it's .next()'ing through the first range, the first term after c is f.  At this point NRQ sees the range a-c is done, and then tries to seek to term e which is before f.  Maybe NRQ's accept method should detect this case (where you've accidentally .next()'d into or possibly beyond the next one or more seek ranges)?"
"LUCENE-2285","CLEANUP","IMPROVEMENT","Code cleanup from all sorts of (trivial) warnings","I would like to do some code cleanup and remove all sorts of trivial warnings, like unnecessary casts, problems w/ javadocs, unused variables, redundant null checks, unnecessary semicolon etc. These are all very trivial and should not pose any problem.

I'll create another issue for getting rid of deprecated code usage, like LuceneTestCase and all sorts of deprecated constructors. That's also trivial because it only affects Lucene code, but it's a different type of change.

Another issue I'd like to create is about introducing more generics in the code, where it's missing today - not changing existing API. There are many places in the code like that.

So, with you permission, I'll start with the trivial ones first, and then move on to the others."
"LUCENE-1393","BUILD_SYSTEM","BUG","Lucene's nightly Hudson builds don't have svn version in MANIFEST.MF","Solr had the same issue but apparently made a configuration change to the Hudson configuration to get it working:

    https://issues.apache.org/jira/browse/SOLR-684

Also I opened this INFRA issue:

    https://issues.apache.org/jira/browse/INFRA-1721

which says the svnversion exe is located in this path:

    /opt/subversion-current/bin

In that INRA issue, /etc/init.d/tomcat was also fixed in theory so that svnversion would be on the path the next time Hudson is restarted.  Still, in case that doesn't work, or it changes in the future, it seems a good idea to make the same change that Solr made to Lucene's Hudson configuration.

Hoss can you detail what you needed to do for Solr?  Or maybe just do it also for Lucene ;)  Thanks!"
"LUCENE-1223","BUG","BUG","lazy fields don't enforce binary vs string value","If you have a binary field, and load it lazy, and then ask that field
for its stringValue, it will incorrectly give you a String back (and
then will refuse to give a binaryValue).  And, vice-versa."
"LUCENE-2382","BUG","BUG","Merging implemented by codecs must catch aborted merges","This is a regression (we lost functionality on landing flex).

When you close IW with ""false"" (meaning abort all running merges), IW asks the merge threads to abort.  The threads are supposed to periodically check if they are aborted and throw an exception if so.

But on the cutover to flex, where the codec can override how merging is done (but a default impl is in the base enum classes), we lost this."
"LUCENE-961","BUG","BUG","RegexCapabilities is not Serializable","The class RegexQuery is marked Serializable by its super class, but it contains a RegexCapabilities which is not Serializable. Thus attempting to serialize the query results in an exception. 

Making RegexCapabilities serializable should be no problem since its subclasses contain only serializable classes (java.util.regex.Pattern and org.apache.regexp.RE)."
"LUCENE-1277","CLEANUP","BUG","Remove System.out left in SpanHighlighter code","A System.out debug was left in the code when a Query is not supported by the SpanHighlighter. This issue simply removes it."
"LUCENE-2659","TEST","TEST","lucenetestcase ease of use improvements","I started working on this in LUCENE-2658, here is the finished patch.

There are some problems with LuceneTestCase:
* a tests beforeClass, or the test itself (its @befores and its method), might have some
  random behavior, but only the latter can be reproduced with -Dtests.seed
* if you want to do things in beforeClass, you have to use a different API: newDirectory(random)
  instead of newDirectory, etc.
* for a new user, the current output can be verbose, confusing and overwhelming.

So, I refactored this class to address these problems. 
A class still needs 2 seeds internally, as the beforeClass will only run once, 
but the methods or setUp() might run many times, especially when increasing iterations.

but lucenetestcase deals with this, and the ""seed"" is 128-bit (UUID): 
the MSB is initialized in beforeClass, the LSB varied for each method run.
if you provide a seed with a -D, they are both fixed to the UUID you provided.

I fixed the API to be consistent, so you should be able to migrate a test from 
setUp() to beforeClass() [junit3 to junit4] without changing parameters.

The codec, locale, timezone is only printed once at the end if any tests fail, 
as its per-class anyway (setup in beforeClass)

finally, when a test fails, you get a single ""reproduce with"" command line you can copy and paste to reproduce.
this way you dont have to spend time trying to figure out what the command line should be.

{noformat}
    [junit] Tests run: 2, Failures: 2, Errors: 0, Time elapsed: 0.197 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestExample -Dtestmethod=testMethodA 
              -Dtests.seed=a51e707b-6550-7800-9f8c-72622d14bf5f
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestExample -Dtestmethod=testMethodB 
              -Dtests.seed=a51e707b-6550-7800-f7eb-2efca3820738
    [junit] NOTE: test params are: codec=PreFlex, locale=ar_LY, timezone=Etc/UCT
    [junit] ------------- ---------------- ---------------
    [junit] Test org.apache.lucene.util.TestExample FAILED
{noformat}
"
"LUCENE-1535","TEST","TEST","Make tests using java.util.Random reproducible on failure","This is a patch for LuceneTestCase to support logging of the Random seed used in randomized tests. The patch also includes an example implementation in TestTrieRangeQuery.

It overrides the protected method runTest() and inserts a try-catch around the super.runTest() call. Two new methods newRandom() and newRandom(long) are available for the test case. As each test case is run in an own TestCase object instance (so 5 test methods in a class instantiate 5 instances each method working in separate), the random seed is saved on newRandom() and when the test fails with any Throwable, a message with the seed (if not null) is printed out. If newRandom was never called no message will be printed.

This patch has only one problem: If a single test method calls newRandom() more than once, only the last seed is saved and printed out. But each test method in a Testcase should call newRandom() exactly once for usage during the execution of this test method. And it is not thread save (no sync, no volatile), but for tests it's unimportant.

I forgot to mention: If a test fails, the message using the seed is printed to stdout. The developer can then change the test temporarily:

{code}LuceneTestCase.newRandom() -> LuceneTestCase.newRandom(long){code}

using the seed from the failed test printout.

*Reference:*
{quote}
: By allowing Random to randomly seed itself, we effectively test a much
: much larger space, ie every time we all run the test, it's different.  We can
: potentially cast a much larger net than a fixed seed.

i guess i'm just in favor of less randomness and more iterations.

: Fixing the bug is the ""easy"" part; discovering a bug is present is where
: we need all the help we can get ;)

yes, but knowing a bug is there w/o having any idea what it is or how to 
trigger it can be very frustrating.

it would be enough for tests to pick a random number, log it, and then use 
it as the seed ... that way if you get a failure you at least know what 
seed was used and you can then hardcode it temporarily to reproduce/debug

-Hoss
{quote}"
"LUCENE-852","RFE","IMPROVEMENT","spellchecker: make hard-coded values configurable","the class org.apache.lucene.search.spell.SpellChecker uses the following hard-coded values in its method
indexDictionary:
        writer.setMergeFactor(300);
        writer.setMaxBufferedDocs(150);
this poses problems when the spellcheck index is created on systems with certain limits, i.e. in unix
environments where the ulimit settings are restricted for the user (http://www.gossamer-threads.com/lists/lucene/java-dev/47428#47428).

there are several ways to circumvent this:
1. add another indexDictionary method with additional parameters:
    public void indexDictionary (Dictionary dict, int mergeFactor, int maxBufferedDocs) throws IOException
    
2. add setter methods for mergeFactor and maxBufferedDocs 
    (see code in http://www.gossamer-threads.com/lists/lucene/java-dev/47428#47428 )

3. Make SpellChecker subclassing easier as suggested by Chris Hostetter 
   (see reply  http://www.gossamer-threads.com/lists/lucene/java-dev/47463#47463)

thanx,
karin
"
"LUCENE-823","BUG","BUG","Lucene fails to close file handles under certain situations","As a followon to LUCENE-820, I've added a further check in
MockRAMDirectory to assert that there are no open files when the
directory is closed.

That check caused a few unit tests to fail, and in digging into the
reason I uncovered these cases where Lucene fails to close file
handles:

  * TermInfosReader.close() was setting its ThreadLocal enumerators to
    null without first closing the SegmentTermEnum in there.  It looks
    like this was part of the fix for LUCENE-436.  I just added the
    call to close.

    This is somewhat severe since we could leak many file handles for
    use cases that burn through threads and/or indexes.  Though,
    FSIndexInput does have a finalize() to close itself.

  * Flushing of deletes in IndexWriter opens SegmentReader to do the
    flushing, and it correctly calls close() to close the reader.  But
    if an exception is hit during commit and before actually closing,
    it will leave open those handles.  I fixed this first calling
    doCommit() and then doClose() in a finally.  The ""disk full"" tests
    we now have were hitting this.

  * IndexWriter's addIndexes(IndexReader[]) method was opening a
    reader but not closing it with a try/finally.  I just put a
    try/finally in.

I've also changed some unit tests to use MockRAMDirectory instead of
RAMDirectory to increase testing coverage of ""leaking open file
handles"".
"
"LUCENE-885","BUILD_SYSTEM","IMPROVEMENT","clean up build files so contrib tests are run more easily","Per mailing list discussion...

http://www.nabble.com/Tests%2C-Contribs%2C-and-Releases-tf3768924.html#a10655448

Tests for contribs should be run when ""ant test"" is used,  existing ""test"" target renamed to ""test-core""
"
"LUCENE-2760","IMPROVEMENT","IMPROVEMENT","optimize spanfirstquery, spanpositionrangequery","SpanFirstQuery and SpanPositionRangeQuery (SpanFirst is just a special case of this), are currently inefficient.

Take this worst case example: SpanFirstQuery(""the"").
Currently the code reads all the positions for the term ""the"".

But when enumerating spans, once we have passed the allowable range we should move on to the next document (skipTo)
 "
"LUCENE-1638","BUG","BUG","Thread safety issue can cause index corruption when autoCommit=true and multiple threads are committing","This is only present in 2.9 trunk, but has been there since
LUCENE-1516 was committed I believe.

It's rare to hit: it only happens if multiple calls to commit() are in
flight (from different threads) and where at least one of those calls
is due to a merge calling commit (because autoCommit is true).

When it strikes, it leaves the index corrupt because it incorrectly
removes an active segment.  It causes exceptions like this:
{code}
java.io.FileNotFoundException: _1e.fnm
	at org.apache.lucene.store.MockRAMDirectory.openInput(MockRAMDirectory.java:246)
	at org.apache.lucene.index.FieldInfos.<init>(FieldInfos.java:67)
	at org.apache.lucene.index.SegmentReader.initialize(SegmentReader.java:536)
	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:468)
	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:414)
	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:641)
	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:627)
	at org.apache.lucene.index.DocumentsWriter.applyDeletes(DocumentsWriter.java:923)
	at org.apache.lucene.index.IndexWriter.applyDeletes(IndexWriter.java:4987)
	at org.apache.lucene.index.IndexWriter.doFlushInternal(IndexWriter.java:4165)
	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:4025)
	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:4016)
	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:2077)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:2040)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:2004)
	at org.apache.lucene.index.TestStressIndexing2.indexRandom(TestStressIndexing2.java:210)
	at org.apache.lucene.index.TestStressIndexing2.testMultiConfig(TestStressIndexing2.java:104)
{code}

It's caused by failing to increment changeCount inside the same
synchronized block where segmentInfos was changed, in commitMerge.
The fix is simple -- I plan to commit shortly.
"
"LUCENE-1760","DOCUMENTATION","TASK","TokenStream API javadoc improvements","- Change or remove experimental warnings of new TokenStream API
- Improve javadocs for deprecated Token constructors
- javadocs for TeeSinkTokenStream.SinkFilter"
"LUCENE-3466","REFACTORING","","Rename Analyzer.reusableTokenStream() to tokenStream()","All Analysis consumers now use reusableTokenStream().  To finally make reuse mandatory, lets rename resuableTokenStream() to tokenStream() (removing the old tokenStream() method)."
"LUCENE-3641","BUG","BUG","MultiReader does not propagate readerFinishedListeners to clones/reopened readers","While working on refactoring MultiReader/DirectoryReader in trunk, I found out that MultiReader does not correctly pass readerFinishedListeners to its clones and reopened readers."
"LUCENE-675","TEST","IMPROVEMENT","Lucene benchmark: objective performance test for Lucene","We need an objective way to measure the performance of Lucene, both indexing and querying, on a known corpus. This issue is intended to collect comments and patches implementing a suite of such benchmarking tests.

Regarding the corpus: one of the widely used and freely available corpora is the original Reuters collection, available from http://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz or http://people.csail.mit.edu/u/j/jrennie/public_html/20Newsgroups/20news-18828.tar.gz. I propose to use this corpus as a base for benchmarks. The benchmarking suite could automatically retrieve it from known locations, and cache it locally."
"LUCENE-2981","CLEANUP","IMPROVEMENT","Review and potentially remove unused/unsupported Contribs","Some of our contribs appear to be lacking for development/support or are missing tests.  We should review whether they are even pertinent these days and potentially deprecate and remove them.

One of the things we did in Mahout when bringing in Colt code was to mark all code that didn't have tests as @deprecated and then we removed the deprecation once tests were added.  Those that didn't get tests added over about a 6 mos. period of time were removed.

I would suggest taking a hard look at:
ant
db
lucli
swing

(spatial should be gutted to some extent and moved to modules)"
"LUCENE-3098","RFE","RFE","Grouped total count","When grouping currently you can get two counts:
* Total hit count. Which counts all documents that matched the query.
* Total grouped hit count. Which counts all documents that have been grouped in the top N groups.

Since the end user gets groups in his search result instead of plain documents with grouping. The total number of groups as total count makes more sense in many situations. "
"LUCENE-1617","BUILD_SYSTEM","IMPROVEMENT","Add ""testpackage"" to common-build.xml","One can define ""testcase"" to execute just one test class, which is convenient. However, I didn't notice any equivalent for testing a whole package. I find it convenient to be able to test packages rather than test cases because often it is not so clear which test class to run.

Following patch allows one to ""ant test -Dtestpackage=search"" (for example) and run all tests under the \*/search/\* packages in core, contrib and tags, or do ""ant test-core -Dtestpackage=search"" and execute similarly just for core, or do ""ant test-core -Dtestpacakge=lucene/search/function"" and run all the tests under \*/lucene/search/function/\* (just in case there is another o.a.l.something.search.function package out there which we want to exclude."
"LUCENE-2113","RFE","TASK","singletermsenum","singletermsenum for flex (like the existing singletermenum, it is a filteredtermSenum that only matches one term, to preserve multitermquery semantics)"
"LUCENE-3125","BUG","BUG","TestDocValuesIndexing.testAddIndexes failures on docvalues branch","doc values branch r1124825, reproducible 
{code}
    [junit] Testsuite: org.apache.lucene.index.values.TestDocValuesIndexing
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.716 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestDocValuesIndexing -Dtestmethod=testAddIndexes -Dtests.seed=5939035003978436534:-6429764582682717131
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=MockRandom, BYTES_VAR_DEREF=MockRandom, INTS=Pulsing(freqCutoff=13)}, locale=da_DK, timezone=Asia/Macao
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDocValuesIndexing]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=88582432,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAddIndexes(org.apache.lucene.index.values.TestDocValuesIndexing):     Caused an ERROR
    [junit] null
    [junit] java.nio.channels.ClosedChannelException
    [junit]     at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)
    [junit]     at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:603)
    [junit]     at org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:161)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:222)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)
    [junit]     at org.apache.lucene.store.DataInput.readInt(DataInput.java:73)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readInt(BufferedIndexInput.java:162)
    [junit]     at org.apache.lucene.store.DataInput.readLong(DataInput.java:115)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readLong(BufferedIndexInput.java:175)
    [junit]     at org.apache.lucene.store.MockIndexInputWrapper.readLong(MockIndexInputWrapper.java:136)
    [junit]     at org.apache.lucene.index.values.PackedIntsImpl$IntsEnumImpl.<init>(PackedIntsImpl.java:263)
    [junit]     at org.apache.lucene.index.values.PackedIntsImpl$IntsEnumImpl.<init>(PackedIntsImpl.java:249)
    [junit]     at org.apache.lucene.index.values.PackedIntsImpl$IntsReader.getEnum(PackedIntsImpl.java:239)
    [junit]     at org.apache.lucene.index.values.DocValues.getEnum(DocValues.java:54)
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.getValuesEnum(TestDocValuesIndexing.java:484)
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.testAddIndexes(TestDocValuesIndexing.java:202)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1304)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1233)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.values.TestDocValuesIndexing FAILED
{code}

and

{code}

    [junit] Testsuite: org.apache.lucene.index.values.TestDocValuesIndexing
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.94 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestDocValuesIndexing -Dtestmethod=testAddIndexes -Dtests.seed=-3677966427932339626:-4746638811786223564
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=Standard, BYTES_FIXED_DEREF=MockSep, FLOAT_64=SimpleText}, locale=ca, timezone=Asia/Novosibirsk
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDocValuesIndexing]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=88596152,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAddIndexes(org.apache.lucene.index.values.TestDocValuesIndexing):     Caused an ERROR
    [junit] Bad file descriptor
    [junit] java.io.IOException: Bad file descriptor
    [junit]     at java.io.RandomAccessFile.seek(Native Method)
    [junit]     at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput.readInternal(SimpleFSDirectory.java:101)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:222)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)
    [junit]     at org.apache.lucene.store.MockIndexInputWrapper.readByte(MockIndexInputWrapper.java:105)
    [junit]     at org.apache.lucene.index.values.Floats$FloatsReader.load(Floats.java:281)
    [junit]     at org.apache.lucene.index.values.SourceCache$DirectSourceCache.load(SourceCache.java:101)
    [junit]     at org.apache.lucene.index.values.DocValues.getSource(DocValues.java:101)
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.getSource(TestDocValuesIndexing.java:472)
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.getValuesEnum(TestDocValuesIndexing.java:482)
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.testAddIndexes(TestDocValuesIndexing.java:203)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1304)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1233)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.values.TestDocValuesIndexing FAILED
{code}"
"LUCENE-2129","IMPROVEMENT","BUG","standard codec's terms dict seek should only scan if new term is in same index block","TermInfosReader in trunk already optimizes for this case... just need to do the same on flex."
"LUCENE-1075","BUG","BUG","Possible thread hazard in IndexWriter.close(false)","Spinoff from this thread:

  http://www.gossamer-threads.com/lists/lucene/java-dev/55391

On reviewing the code I found one case where an aborted merge (from
calling close(false)) could write to files that a newly opened
IndexWriter would also try to write to.

I strengthened an existing test case in TestConcurrentMergeScheduler
to tickle this case, and also modified MockRAMDirectory to throw an
IOException if ever a file besides segments.gen is overwritten.

However, strangely, I can't get an unhandled exception to occur during
the test and I'm not sure why.  Still I think this is a good defensive
check so we should commit it.
"
"LUCENE-3761","RFE","IMPROVEMENT","Generalize SearcherManager","I'd like to generalize SearcherManager to a class which can manage instances of a certain type of interfaces. The reason is that today SearcherManager knows how to handle IndexSearcher instances. I have a SearcherManager which manages a pair of IndexSearcher and TaxonomyReader pair.

Recently, few concurrency bugs were fixed in SearcherManager, and I realized that I need to apply them to my version as well. Which led me to think why can't we have an SM version which is generic enough so that both my version and Lucene's can benefit from?

The way I see SearcherManager, it can be divided into two parts: (1) the part that manages the logic of acquire/release/maybeReopen (i.e., ensureOpen, protect from concurrency stuff etc.), and (2) the part which handles IndexSearcher, or my SearcherTaxoPair. I'm thinking that if we'll have an interface with incRef/decRef/tryIncRef/maybeRefresh, we can make SearcherManager a generic class which handles this interface.

I will post a patch with the initial idea, and we can continue from there."
"LUCENE-388","IMPROVEMENT","BUG","[PATCH] IndexWriter.maybeMergeSegments() takes lots of CPU resources","Note: I believe this to be the same situation with 1.4.3 as with SVN HEAD.

Analysis using hprof utility shows that during index creation with many
documents highlights that the CPU spends a large portion of it's time in
IndexWriter.maybeMergeSegments(), which seems to be a 'waste' compared with
other valuable CPU intensive operations such as tokenization etc.

Using the following test snippet to retrieve some rows from the db and create an
index:

        Analyzer a = new StandardAnalyzer();
        writer = new IndexWriter(indexDir, a, true);
        writer.setMergeFactor(1000);
        writer.setMaxBufferedDocs(10000);
        writer.setUseCompoundFile(false);
        connection = DriverManager.getConnection(
                ""jdbc:inetdae7:tower.aconex.com?database=<somedb>"", ""secret"",
                ""squirrel"");
        String sql = ""select userid, userfirstname, userlastname, email from userx"";
        LOG.info(""sql="" + sql);
        Statement statement = connection.createStatement();
        statement.setFetchSize(5000);
        LOG.info(""Executing sql"");
        ResultSet rs = statement.executeQuery(sql);
        LOG.info(""ResultSet retrieved"");
        int row = 0;

        LOG.info(""Indexing users"");
        long begin = System.currentTimeMillis();
        while (rs.next()) {
            int userid = rs.getInt(1);
            String firstname = rs.getString(2);
            String lastname = rs.getString(3);
            String email = rs.getString(4);
            String fullName = firstname + "" "" + lastname;
            Document doc = new Document();
            doc.add(Field.Keyword(""userid"", userid+""""));
            doc.add(Field.Keyword(""firstname"", firstname.toLowerCase()));
            doc.add(Field.Keyword(""lastname"", lastname.toLowerCase()));
            doc.add(Field.Text(""name"", fullName.toLowerCase()));
            doc.add(Field.Keyword(""email"", email.toLowerCase()));
            writer.addDocument(doc);
            row++;
            if((row % 100)==0){
                LOG.info(row + "" indexed"");
            }
        }
        double end = System.currentTimeMillis();
        double diff = (end-begin)/1000;
        double rate = row/diff;
        LOG.info(""rate:"" +rate);

On my 1.5GHz PowerBook with 1.5Gb RAM and a 5400 RPM drive, my CPU is maxed out,
and I end up getting a rate of indexing between 490-515 documents/second run
over 10 times in succession.  

By applying a simple patch to IndexWriter (see attached shortly), which defers
the calling of maybeMergeSegments() so that it is only called every 2000
times(an arbitrary figure), I appear to get a new rate of between 945-970
documents/second.  Using Luke to look inside each index created between these 2
there does not appear to be any difference.  Same number of Documents, same
number of Terms.

I'm not suggesting one should apply this patch, I'm just highlighting the
difference in performance that this sort of change gives you.  

We are about to use Lucene to index 4 million construction document records, and
so speeding up the indexing process is in our best interest! :)  If one
considers the amount of CPU time spent in maybeMergeSegments over the initial
index creation of 4 million documents, I think one could see how it would be
ideal to try to speed this area up (at least move the bottleneck to IO). 

I woul appreciate anyone taking a moment to comment on this."
"LUCENE-2576","BUG","BUG","Intermittent failure in TestIndexWriter.testCommitThreadSafety","Mark's while(1) hudson box found this failure (and I can repro it too):

{noformat}
Error Message

MockRAMDirectory: cannot close: there are still open files: {_1m.cfs=1,
_1k.cfs=1, _14.cfs=1, _1g.cfs=1, _1h.cfs=1, _1f.cfs=1, _1n.cfs=1,
_1i.cfs=1, _1j.cfs=1, _1l.cfs=1}

Stacktrace

java.lang.RuntimeException: MockRAMDirectory: cannot close: there are
still open files: {_1m.cfs=1, _1k.cfs=1, _14.cfs=1, _1g.cfs=1,
_1h.cfs=1, _1f.cfs=1, _1n.cfs=1, _1i.cfs=1, _1j.cfs=1, _1l.cfs=1}
       at
org.apache.lucene.store.MockRAMDirectory.close(MockRAMDirectory.java:282)
       at
org.apache.lucene.index.TestIndexWriter.testCommitThreadSafety(TestIndexWriter.java:4616)
       at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:328)

Standard Output

NOTE: random codec of testcase 'testCommitThreadSafety' was: Sep

Standard Error

The following exceptions were thrown by threads:
*** Thread: Thread-1784 ***
java.lang.RuntimeException: junit.framework.AssertionFailedError: null
       at org.apache.lucene.index.TestIndexWriter$9.run(TestIndexWriter.java:4606)
Caused by: junit.framework.AssertionFailedError: null
       at junit.framework.Assert.fail(Assert.java:47)
       at junit.framework.Assert.assertTrue(Assert.java:20)
       at junit.framework.Assert.assertTrue(Assert.java:27)
       at org.apache.lucene.index.TestIndexWriter$9.run(TestIndexWriter.java:4597)
{noformat}"
"LUCENE-3855","BUG","BUG","TestStressNRT failures (reproducible)","Build server logs. Reproduces on at least two machines.

{noformat}
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestStressNRT -Dtestmethod=test -Dtests.seed=69468941c1bbf693:19e66d58475da929:69e9d2f81769b6d0 -Dargs=""-Dfile.encoding=UTF-8""
    [junit] NOTE: test params are: codec=Lucene3x, sim=RandomSimilarityProvider(queryNorm=true,coord=false): {}, locale=ro, timezone=Etc/GMT+1
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestStressNRT]
    [junit] NOTE: Linux 3.0.0-16-generic amd64/Sun Microsystems Inc. 1.6.0_27 (64-bit)/cpus=2,threads=1,free=74960064,total=135987200
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: test(org.apache.lucene.index.TestStressNRT):	Caused an ERROR
    [junit] MockDirectoryWrapper: cannot close: there are still open files: {_ng.cfs=8}
    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_ng.cfs=8}
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:555)
    [junit] 	at org.apache.lucene.index.TestStressNRT.test(TestStressNRT.java:385)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$SubclassSetupTeardownRule$1.evaluate(LuceneTestCase.java:743)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:639)
    [junit] 	at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:538)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:600)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:164)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 	at org.apache.lucene.util.StoreClassNameRule$1.evaluate(StoreClassNameRule.java:21)
    [junit] 	at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
    [junit] Caused by: java.lang.RuntimeException: unclosed IndexInput: _ng.cfs
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.addFileHandle(MockDirectoryWrapper.java:479)
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper$1.openSlice(MockDirectoryWrapper.java:777)
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.openInput(CompoundFileDirectory.java:221)
    [junit] 	at org.apache.lucene.codecs.lucene3x.TermInfosReader.<init>(TermInfosReader.java:112)
    [junit] 	at org.apache.lucene.codecs.lucene3x.Lucene3xFields.<init>(Lucene3xFields.java:84)
    [junit] 	at org.apache.lucene.codecs.lucene3x.PreFlexRWPostingsFormat$1.<init>(PreFlexRWPostingsFormat.java:51)
    [junit] 	at org.apache.lucene.codecs.lucene3x.PreFlexRWPostingsFormat.fieldsProducer(PreFlexRWPostingsFormat.java:51)
    [junit] 	at org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:108)
    [junit] 	at org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:51)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReadersAndLiveDocs.getMergeReader(IndexWriter.java:521)
    [junit] 	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3587)
    [junit] 	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3257)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:382)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:451)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestStressNRT FAILED
{noformat}"
"LUCENE-787","BUILD_SYSTEM","BUG","ant test won't run in 'out of the box' installation","one possible solution would be to remove 'lib' from the junit.classpath"
"LUCENE-1242","IMPROVEMENT","IMPROVEMENT","small speedups to bulk merging","The bulk merging code, for stored fields & term vectors, was calling isDeleted twice for each deleted doc.

Patch also changes DocumentsWriter to use IndexWriter.message for its infoStream messages."
"LUCENE-2328","BUG","BUG","IndexWriter.synced  field accumulates data leading to a Memory Leak","I am running into a strange OutOfMemoryError. My small test application does
index and delete some few files. This is repeated for 60k times. Optimization
is run from every 2k times a file is indexed. Index size is 50KB. I did analyze
the HeapDumpFile and realized that IndexWriter.synced field occupied more than
half of the heap. That field is a private HashSet without a getter. Its task is
to hold files which have been synced already.

There are two calls to addAll and one call to add on synced but no remove or
clear throughout the lifecycle of the IndexWriter instance.

According to the Eclipse Memory Analyzer synced contains 32618 entries which
look like file names ""_e065_1.del"" or ""_e067.cfs""

The index directory contains 10 files only.

I guess synced is holding obsolete data "
"LUCENE-824","CLEANUP","IMPROVEMENT","IndexWriter#addIndexesNoOptimize has redundent try/catch","With the new transaction code, the try/catch clause at the beginning of IndexWriter#addIndexesNoOptimize is redundant."
"LUCENE-2751","TEST","TEST","add LuceneTestCase.newSearcher()","Most tests in the search package don't care about what kind of searcher they use.

we should randomly use MultiSearcher or ParallelMultiSearcher sometimes in tests."
"LUCENE-1492","RFE","IMPROVEMENT","Allow readOnly OpenReader task","I'd like to change OpenReader in contrib/benchmark to open a readOnly reader by default, and take readOnly optional param if for some reason a ""writable IndexReader"" becomes necessary in the future."
"LUCENE-3140","BACKPORT","IMPROVEMENT","Backport FSTs to 3.x",""
"LUCENE-962","BUG","BUG","I/O exception in DocsWriter add or updateDocument may not delete unreferenced files","If an I/O exception is thrown in DocumentsWriter#addDocument or #updateDocument, the stored fields files may not be cleaned up."
"LUCENE-2451","TEST","IMPROVEMENT","remove dead code from oal.util.cache","We have dead cache impls in oal.util.cache*; we only use DBLRUCache.

These are internal APIs; I'd like to remove all but DBLRUcache."
"LUCENE-1919","BUG","BUG","Analysis back compat break","Old and new style token streams don't mix well.
"
"LUCENE-1072","BUG","BUG","NullPointerException during indexing in DocumentsWriter$ThreadState$FieldData.addPosition","In my case during indexing sometimes appear documents with unusually large ""words"" - text-encoded images in fact.
Attempt to add document that contains field with such token produces java.lang.IllegalArgumentException:
java.lang.IllegalArgumentException: term length 37944 exceeds max term length 16383
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.addPosition(DocumentsWriter.java:1492)
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.invertField(DocumentsWriter.java:1321)
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.processField(DocumentsWriter.java:1247)
        at org.apache.lucene.index.DocumentsWriter$ThreadState.processDocument(DocumentsWriter.java:972)
        at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:2202)
        at org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:2186)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1432)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1411)

This is expected, exception is caught and ignored. The problem is that after this IndexWriter becomes somewhat corrupted and subsequent attempts to add documents to the index fail as well, this time with NPE:
java.lang.NullPointerException
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.addPosition(DocumentsWriter.java:1497)
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.invertField(DocumentsWriter.java:1321)
        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.processField(DocumentsWriter.java:1247)
        at org.apache.lucene.index.DocumentsWriter$ThreadState.processDocument(DocumentsWriter.java:972)
        at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:2202)
        at org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:2186)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1432)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1411)

This is 100% reproducible."
"LUCENE-3209","IMPROVEMENT","IMPROVEMENT","Memory codec","This codec stores all terms/postings in RAM.  It uses an
FST<BytesRef>.  This is useful on a primary key field to ensure
lookups don't need to hit disk, to keep NRT reopen time fast even
under IO contention.
"
"LUCENE-503","RFE","RFE","Contrib: ThaiAnalyzer to enable Thai full-text search in Lucene","Thai text don't have space between words. Usually, a dictionary-based algorithm is used to break string into words. For Lucene to be usable for Thai, an Analyzer that know how to break Thai words is needed.

I've implemented such Analyzer, ThaiAnalyzer, using ICU4j DictionaryBasedBreakIterator for word breaking. I'll upload the code later.

I'm normally a C++ programmer and very new to Java. Please review the code for any problem. One possible problem is that it requires ICU4j. I don't know whether this is OK."
"LUCENE-451","BUG","BUG","BooleanQuery explain with boost==0","BooleanWeight.explain() uses the returned score of subweights to determine if a clause matched.
If any required clause has boost==0, the returned score will be zero and the explain for the entire BooleanWeight will be simply  Explanation(0.0f, ""match required"").

I'm not sure what the correct fix is here.  I don't think it can be done based on score alone, since that isn't how scorers work.   Perhaps we need a new method ""boolean Explain.matched()"" that returns true on a match, regardless of what the score may be? 

Related to the problem above, even if no boosts are zero, it it sometimes nice to know *why* a particular query failed to match.  It would mean a longer explanation, but maybe we should include non matching explains too?"
"LUCENE-3647","BUG","BUG","DocValues merging is not associative, leading to different results depending upon how merges execute","recently I cranked up TestDuelingCodecs to actually test docvalues (previously it wasn't testing it at all).

This test is simple, it indexes the same random content with 2 different indexwriters, it just allows them
to use different codecs with different indexwriterconfigs.

then it asserts the indexes are equal.

Sometimes, always on BYTES_FIXED_DEREF type, we end out with one reader that has a zero-filled byte[] for a doc,
but that same document in the other reader has no docvalues at all.
"
"LUCENE-3904","CLEANUP","BUG","Similarity javadocs look ugly if created with java7's javadoc","The captions used to illustrate the formulas are tables here:
in jdk 5/6 the table is centered nicely.

But with java7's javadocs (I think due to some css styles changes?),
the table is not centered but instead stretched.

I think we just need to center this table with a different technique?

Have a look at http://people.apache.org/~rmuir/java7-style-javadocs/org/apache/lucene/search/Similarity.html to see what I mean.

NOTE: these javadocs are under TFIDFSimilarity.java in trunk."
"LUCENE-2507","RFE","RFE","automaton spellchecker","The current spellchecker makes an n-gram index of your terms, and queries this for spellchecking.
The terms that come back from the n-gram query are then re-ranked by an algorithm such as Levenshtein.

Alternatively, we could just do a levenshtein query directly against the index, then we wouldn't need
a separate index to rebuild.
"
"LUCENE-2898","IMPROVEMENT","BUG","CMS merge throttling is not aggressive enough","I hit this crab while working on the NRT benchmarker (in luceneutil).

CMS today forcefully idles any incoming threads, when there are too many merges pending.

This is the last line of defense that it has, since it also juggles thread priorities (and forcefully idles the biggest merges) to try to reduce the outstanding merge count.

But when it cannot keep up it has no choice but to stall those threads responsible for making new segments.

However, the logic is in the wrong place now -- the stalling happens after pulling the next merge from IW.  This is poor because it means if you have N indexing threads, you allow max + N outstanding merges.

I have a simple fix, which is to just move the stall logic to before we pull the next merge from IW."
"LUCENE-1598","BUG","BUG","While you could use a custom Sort Comparator source with remote searchable before, you can no longer do so with FieldComparatorSource","FieldComparatorSource is not serializable, but can live on a SortField"
"LUCENE-3103","TEST","TEST","create a simple test that indexes and searches byte[] terms","Currently, the only good test that does this is Test2BTerms (disabled by default)

I think we should test this capability, and also have a simpler example for how to do this.
"
"LUCENE-3640","CLEANUP","IMPROVEMENT","remove IndexSearcher.close","Now that IS is never ""heavy"" (since you have to pass in your own IR), IS.close is truly a no-op... I think we should remove it.

"
"LUCENE-277","BUG","BUG","Sorting produces duplicates","If you run the code below the exception will be thrown. I believe that it isn't 
correct behaviour (the duplicities, of course), index id of hits should be 
unique as it is without sort.

Lucene versions:
1.4-final
1.4.1
CVS 1.5-rc1-dev


import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.document.Field;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.queryParser.ParseException;
import org.apache.lucene.queryParser.QueryParser;
import org.apache.lucene.search.Hits;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.Searcher;
import org.apache.lucene.search.Sort;
import org.apache.lucene.search.SortField;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.RAMDirectory;

import java.io.IOException;
import java.util.HashSet;
import java.util.LinkedList;
import java.util.ListIterator;
import java.util.Set;

/**
 * Run this test with Lucene 1.4 final or 1.4.1
 */
public class DuplicityTest
{
    public static void main(String[] args) throws IOException, ParseException
    {
        Directory directory = create_index();

        search_index(directory);
    }

    private static void search_index(Directory directory) throws IOException, 
ParseException
    {
        IndexReader reader = IndexReader.open(directory);
        Searcher searcher = new IndexSearcher(reader);

        Sort sort = new Sort(new SortField(""co"", SortField.INT, false));

        Query q = QueryParser.parse(""sword"", ""text"", new StandardAnalyzer());

        find_duplicity(searcher.search(q), ""no sort"");

        find_duplicity(searcher.search(q, sort), ""using sort"");

        searcher.close();
        reader.close();
    }

    private static void find_duplicity(Hits hits, String message) throws 
IOException
    {
        System.out.println(message + "" hits size: "" + hits.length());

        Set set = new HashSet();
        for (int i = 0; i < hits.length(); i++) {
//            System.out.println(hits.id(i) + "": "" + hits.doc(i).toString());
            Integer id = new Integer(hits.id(i));
            if (!set.contains(id))
                set.add(id);
            else
                throw new RuntimeException(""duplicity found, index id: "" + id);
        }
        System.out.println(""no duplicity found"");
    }

    private static LinkedList words;

    static {
        words = new LinkedList();

        words.add(""word"");
        words.add(""sword"");
        words.add(""dwarf"");
        words.add(""whale"");
        words.add(""male"");
    }

    private static Directory create_index() throws IOException
    {
        Directory directory = new RAMDirectory();

        ListIterator e_words1 = words.listIterator();
        ListIterator e_words2 = words.listIterator(words.size());

        IndexWriter writer = new IndexWriter(directory, new StandardAnalyzer(), 
true);

        int co = 1;

        for (int i = 0; i < 300; i++) {

            if (!e_words1.hasNext()) {
                e_words1 = words.listIterator();
                e_words1.hasNext();
            }
            String word1 = (String)e_words1.next();
            if (!e_words2.hasPrevious()) {
                e_words2 = words.listIterator(words.size());
                e_words2.hasPrevious();
            }
            String word2 = (String)e_words2.previous();

            Document doc = new Document();

            doc.add(Field.Keyword(""co"", String.valueOf(co)));
            doc.add(Field.Text(""text"", word1 + "" "" + word2));
            writer.addDocument(doc);

            if (i % 20 == 0)
                co++;
        }
        writer.optimize();
        System.err.println(""index size: "" + writer.docCount());
        writer.close();

        return directory;
    }
}"
"LUCENE-1389","BUG","BUG","SimpleSpanFragmenter can create very short fragments","Line 74 of SimpleSpanFragmenter returns true when the current token is the start of a hit on a span or phrase, thus starting a new fragment. Two problems occur:

- The previous fragment may be very short, but if it contains a hit it will be combined with the new fragment later so this disappears.
- If the token is close to a natural fragment boundary the new fragment will end up very short; possibly even as short as just the span or phrase itself. This is the result of creating a new fragment without incrementing currentNumFrags.

To fix, remove or comment out line 74. The result is that fragments average to the fragment size unless a span or phrase hit is towards the end of the fragment - that fragment is made larger and the following fragment shorter to accommodate the hit."
"LUCENE-3490","REFACTORING","IMPROVEMENT","Restructure codec hierarchy","Spinoff of LUCENE-2621. (Hoping we can do some of the renaming etc here in a rote way to make progress).

Currently Codec.java only represents a portion of the index, but there are other parts of the index 
(stored fields, term vectors, fieldinfos, ...) that we want under codec control. There is also some 
inconsistency about what a Codec is currently, for example Memory and Pulsing are really just 
PostingsFormats, you might just apply them to a specific field. On the other hand, PreFlex actually
is a Codec: it represents the Lucene 3.x index format (just not all parts yet). I imagine we would
like SimpleText to be the same way.

So, I propose restructuring the classes so that we have something like:
* CodecProvider <-- dead, replaced by java ServiceProvider mechanism. All indexes are 'readable' if codecs are in classpath.
* Codec <-- represents the index format (PostingsFormat + FieldsFormat + ...)
* PostingsFormat: this is what Codec controls today, and Codec will return one of these for a field.
* FieldsFormat: Stored Fields + Term Vectors + FieldInfos?

I think for PreFlex, it doesnt make sense to expose its PostingsFormat as a 'public' class, because preflex
can never be per-field so there is no use in allowing you to configure PreFlex for a specific field.
Similarly, I think in the future we should do the same thing for SimpleText. Nobody needs SimpleText for production, it should
just be a Codec where we try to make as much of the index as plain text and simple as possible for debugging/learning/etc.
So we don't need to expose its PostingsFormat. On the other hand, I don't think we need Pulsing or Memory codecs,
because its pretty silly to make your entire index use one of their PostingsFormats. To parallel with analysis:
PostingsFormat is like Tokenizer and Codec is like Analyzer, and we don't need Analyzers to ""show off"" every Tokenizer.

we can also move the baked in PerFieldCodecWrapper out (it would basically be PerFieldPostingsFormat). Privately it would
write the ids to the file like it does today. in the future, all 3.x hairy backwards code would move to PreflexCodec. 
SimpleTextCodec would get a plain text fieldinfos impl, etc."
"LUCENE-2677","TEST","TEST","Tests failing when run with tests.iter > 1","TestMultiLevelSkipList and TestsFieldReader are falling if run with -Dtests.iter > 1 - not all values are reset though
I will attach a patch in a second."
"LUCENE-1812","RFE","RFE","Static index pruning by in-document term frequency (Carmel pruning)","This module provides tools to produce a subset of input indexes by removing postings data for those terms where their in-document frequency is below a specified threshold. The net effect of this processing is a much smaller index that for common types of queries returns nearly identical top-N results as compared with the original index, but with increased performance. 

Optionally, stored values and term vectors can also be removed. This functionality is largely independent, so it can be used without term pruning (when term freq. threshold is set to 1).

As the threshold value increases, the total size of the index decreases, search performance increases, and recall decreases (i.e. search quality deteriorates). NOTE: especially phrase recall deteriorates significantly at higher threshold values. 

Primary purpose of this class is to produce small first-tier indexes that fit completely in RAM, and store these indexes using IndexWriter.addIndexes(IndexReader[]). Usually the performance of this class will not be sufficient to use the resulting index view for on-the-fly pruning and searching. 

NOTE: If the input index is optimized (i.e. doesn't contain deletions) then the index produced via IndexWriter.addIndexes(IndexReader[]) will preserve internal document id-s so that they are in sync with the original index. This means that all other auxiliary information not necessary for first-tier processing, such as some stored fields, can also be removed, to be quickly retrieved on-demand from the original index using the same internal document id. 

Threshold values can be specified globally (for terms in all fields) using defaultThreshold parameter, and can be overriden using per-field or per-term values supplied in a thresholds map. Keys in this map are either field names, or terms in field:text format. The precedence of these values is the following: first a per-term threshold is used if present, then per-field threshold if present, and finally the default threshold.

A command-line tool (PruningTool) is provided for convenience. At this moment it doesn't support all functionality available through API."
"LUCENE-1884","DOCUMENTATION","TASK","javadocs cleanup","basic cleanup in core/contrib: typos, apache license header as javadoc, missing periods that screw up package summary, etc.
"
"LUCENE-2889","CLEANUP","TASK","Remove @lucene.experimental from Numeric*","NumericRangeQuery and NumericField are now there since 2.9. It is still marked as experimental. The API stabilized and there are no changes in the public parts (even in Lucene trunk no changes). Also lot's of people ask, if ""experimental"" means ""unstable"" in general, but it means only ""unstable API"".

I will remove the @lucene.experimental from Numeric* classes. NumericUtils* stays with @lucene.internal, as it is not intended for public use. Some people use it to make ""TermQuery"" on a numeric field, but this should be done using a NRQ with upper==lower and included=true, which does not affect scoring (applies also to Solr)."
"LUCENE-1551","RFE","IMPROVEMENT","Add reopen(IndexCommit) methods to IndexReader","Add reopen(IndexCommit) methods to IndexReader to be able to reopen an index on any previously saved commit points with all advantages of LUCENE-1483.

Similar to open(IndexCommit) & company available in 2.4.0.
"
"LUCENE-843","IMPROVEMENT","IMPROVEMENT","improve how IndexWriter uses RAM to buffer added documents","I'm working on a new class (MultiDocumentWriter) that writes more than
one document directly into a single Lucene segment, more efficiently
than the current approach.

This only affects the creation of an initial segment from added
documents.  I haven't changed anything after that, eg how segments are
merged.

The basic ideas are:

  * Write stored fields and term vectors directly to disk (don't
    use up RAM for these).

  * Gather posting lists & term infos in RAM, but periodically do
    in-RAM merges.  Once RAM is full, flush buffers to disk (and
    merge them later when it's time to make a real segment).

  * Recycle objects/buffers to reduce time/stress in GC.

  * Other various optimizations.

Some of these changes are similar to how KinoSearch builds a segment.
But, I haven't made any changes to Lucene's file format nor added
requirements for a global fields schema.

So far the only externally visible change is a new method
""setRAMBufferSize"" in IndexWriter (and setMaxBufferedDocs is
deprecated) so that it flushes according to RAM usage and not a fixed
number documents added.
"
"LUCENE-2316","RFE","IMPROVEMENT","Define clear semantics for Directory.fileLength","On this thread: http://mail-archives.apache.org/mod_mbox/lucene-java-dev/201003.mbox/%3C126142c1003121525v24499625u1589bbef4c0792e7@mail.gmail.com%3E it was mentioned that Directory's fileLength behavior is not consistent between Directory implementations if the given file name does not exist. FSDirectory returns a 0 length while RAMDirectory throws FNFE.

The problem is that the semantics of fileLength() are not defined. As proposed in the thread, we'll define the following semantics:

* Returns the length of the file denoted by <code>name</code> if the file exists. The return value may be anything between 0 and Long.MAX_VALUE.
* Throws FileNotFoundException if the file does not exist. Note that you can call dir.fileExists(name) if you are not sure whether the file exists or not.

For backwards we'll create a new method w/ clear semantics. Something like:

{code}
/**
 * @deprecated the method will become abstract when #fileLength(name) has been removed.
 */
public long getFileLength(String name) throws IOException {
  long len = fileLength(name);
  if (len == 0 && !fileExists(name)) {
    throw new FileNotFoundException(name);
  }
  return len;
}
{code}

The first line just calls the current impl. If it throws exception for a non-existing file, we're ok. The second line verifies whether a 0 length is for an existing file or not and throws an exception appropriately."
"LUCENE-3423","RFE","RFE","add Terms.docCount","spinoff from LUCENE-3290, where yonik mentioned:

{noformat}
Is there currently a way to get the number of documents that have a value in the field?
Then one could compute the average length of a (sparse) field via sumTotalTermFreq(field)/docsWithField(field)
docsWithField(field) would be useful in other contexts that want to know how sparse a field is (automatically selecting faceting algorithms, etc).
{noformat}

I think this is a useful stat to add, in case you have sparse fields for heuristics or scoring."
"LUCENE-1066","DOCUMENTATION","IMPROVEMENT","better explain output","Very simple patch that slightly improves output of idf: show both docFreq and numDocs."
"LUCENE-2598","TEST","TEST","allow tests to use different Directory impls","Now that all tests use MockRAMDirectory instead of RAMDirectory, they are all picky like windows and force our tests to
close readers etc before closing the directory.

I think we should do the following:
# change new MockRAMDIrectory() in tests to .newDirectory(random)
# LuceneTestCase[J4] tracks if all dirs are closed at tearDown and also cleans up temp dirs like solr.
# factor out the Mockish stuff from MockRAMDirectory into MockDirectoryWrapper
# allow a -Dtests.directoryImpl or simpler to specify the default Directory to use for tests: default being ""random""

i think theres a chance we might find some bugs that havent yet surfaced because they are easier to trigger with FSDir
Furthermore, this would be beneficial to Directory-implementors as they could run the entire testsuite against their Directory impl, just like codec-implementors can do now.
"
"LUCENE-894","BUILD_SYSTEM","BUG","Custom build.xml for binary distributions","The binary files of a distribution come with the demo sources
and a build.xml file. However, the build.xml doesn't work for
the binary distribution, so it can't be used to build the 
demos.

This problem was notices the first time when release 2.1 was
made. Before we ship 2.2 we should fix this."
"LUCENE-3531","IMPROVEMENT","IMPROVEMENT","Improve CachingWrapperFilter to optionally also cache acceptDocs, if identical to liveDocs","Spinoff from LUCENE-1536: This issue removed the different cache modes completely and always applies the acceptDocs using BitsFilteredDocIdSet.wrap(), the cache only contains raw DocIdSet without any deletions/acceptDocs. For IndexReaders that are seldom reopened, this might not be as performant as it could be. If the acceptDocs==IR.liveDocs, those DocIdSet could also be cached with liveDocs applied."
"LUCENE-3905","TEST","IMPROVEMENT","BaseTokenStreamTestCase should test analyzers on real-ish content","We already have LineFileDocs, that pulls content generated from europarl or wikipedia... I think sometimes BTSTC should test the analyzers on that as well."
"LUCENE-1347","BUG","BUG","IndexWriter.rollback can hang if a previous call hit an exception","IW.rollback has logic to make sure only one thread actually gets to do
the rollback whenever multiple threads are calling it at the same
time, by setting the ""boolean closing"" to true in the thread that got
there first.

Other threads wait for that variable to become false again before
returning from abort.

But, we are not restoring closing to false in a try/finally in
rollback(), which means on hitting an exception in rollback, a
subsequent call to rollback() will hang forever.

close() has the same logic, but there is already a try/finally there
to restore closing to false on exception.

The fix is straightforward.
"
"LUCENE-1211","IMPROVEMENT","BUG","Small speedups to DocumentsWriter's quickSort","In working on LUCENE-510 I found that DocumentsWriter's quickSort can
be further optimized to handle the common case of sorting only 2
values.

I ran with this alg:

  analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
  
  doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker
  
  docs.file=/Volumes/External/lucene/wiki.txt
  doc.stored = true
  doc.term.vector = true
  doc.add.log.step=2000
  doc.maker.forever = false
  
  directory=FSDirectory
  autocommit=false
  compound=false
  
  ram.flush.mb=64
  
  { ""Rounds""
    ResetSystemErase
    { ""BuildIndex""
      CreateIndex
      { ""AddDocs"" AddDoc > : 200000
      - CloseIndex
    }
    NewRound
  } : 5
  
  RepSumByPrefRound BuildIndex

Best of 5 was 857.3 docs/sec before the optimization and 881.6 after =
2.8% speedup, on a quad-core Mac Pro with 4-drive RAID 0 array.

The fix is trivial.  I will commit shortly.

"
"LUCENE-2142","BUG","BUG","FieldCache.getStringIndex should not throw exception if term count exceeds doc count","Spinoff of LUCENE-2133/LUCENE-831.

Currently FieldCache cannot handle more than one value per field.
We may someday want to fix that... but until that day:

FieldCache.getStringIndex currently does a simplistic check to try to
catch when you've accidentally allowed more than one term per field,
by testing if the number of unique terms exceeds the number of
documents.

The problem is, this is not a perfect check, in that it allows false
negatives (you could have more than one term per field for some docs
and the check won't catch you).

Further, the exception thrown is the unchecked RuntimeException.

So this means... you could happily think all is good, until some day,
well into production, once you've updated enough docs, suddenly the
check will catch you and throw an unhandled exception, stopping all
searches [that need to sort by this string field] in their tracks.
It's not gracefully degrading.

I think we should simply remove the test, ie, if you have more terms
than docs then the terms simply overwrite one another.
"
"LUCENE-2753","DESIGN_DEFECT","IMPROVEMENT","IndexReader.listCommits should return a List and not an abstract Collection","Spinoff from here: http://www.mail-archive.com/dev@lucene.apache.org/msg07509.html"
"LUCENE-3099","RFE","IMPROVEMENT","Grouping module should allow subclasses to set the group key per document","The new grouping module can only group by a single-valued indexed field.

But, if we make the 'getGroupKey' a method that a subclass could override, then I think we could refactor Solr over to the module, because it could do function queries and normal queries via subclass (I think).

This also makes the impl more extensible to apps that might have their own interesting group values per document."
"LUCENE-3859","REFACTORING","TASK","nuke/clean up AtomicReader.hasNorms","implementations already have to return fieldInfos() [which can tell you this], and normValues() [which can also tell you this].

So if we want to keep it, I think it should just have a final implementation and not be required for FilterReaders, etc.

Or we can just nuke it... do we really need 3 ways to do the same thing?"
"LUCENE-1055","BUILD_SYSTEM","TASK","Remove GData from trunk ","GData doesn't seem to be maintained anymore. We're going to remove it before we cut the 2.3 release unless there are negative votes.

In case someones jumps in in the future and starts to maintain it, we can re-add it to the trunk.

If anyone is using GData and needs it to be in 2.3 please let us know soon!"
"LUCENE-3244","RFE","BUG","Contrib/Module-uptodate assume name matches path and jar","With adding a new 'queries' module, I am trying to change the project name of contrib/queries to queries-contrib.  However currently the contrib-uptodate assumes that the name property is used in the path and in the jar name.

By using the name in the path, I must set the value to 'queries' (since the path is contrib/queries).  However because the project name is now queries-contrib, the actual jar file will be lucene-queries-contrib-${version}.jar, not lucene-queries-${version}.jar, as is expected.

Consequently I think we need to separate the path name from the jar name properties.  For simplicity I think adding a new jar-name property will suffice, which can be optional and if omitted, is filled in with the name property."
"LUCENE-629","IMPROVEMENT","IMPROVEMENT","Performance improvement for merging stored, compressed fields","Hello everyone,

currently the merging of stored, compressed fields is not optimal for the following reason: every time a stored, compressed field is being merged, the FieldsReader uncompresses the data, hence the FieldsWriter has to compress it again when it writes the merged fields data (.fdt) file. The uncompress/compress step is unneccessary and slows down the merge performance significantly.

This patch improves the merge performance by avoiding the uncompress/compress step. In the following I give an overview of the changes I made:
   * Added a new FieldSelectorResult constant named ""LOAD_FOR_MERGE"" to org.apache.lucene.document.FieldSelectorResult
   * SegmentMerger now uses an FieldSelector to get stored fields from the FieldsReader. This FieldSelector's accept() method returns the FieldSelectorResult ""LOAD_FOR_MERGE"" for every field.
   * Added a new inner class to FieldsReader named ""FieldForMerge"", which extends  org.apache.lucene.document.AbstractField. This class holds the field properties and its data. If a field has the FieldSelectorResult ""LOAD_FOR_MERGE"", then the FieldsReader creates an instance of ""FieldForMerge"" and does not uncompress the field's data.
   * FieldsWriter checks if the field it is about to write is an instanceof FieldsReader.FieldForMerge. If true, then it does not compress the field data.


To test the performance I index about 350,000 text files and store the raw text in a stored, compressed field in the lucene index. I use a merge factor of 10. The final index has a size of 366MB. After building the index, I optimize it to measure the pure merge performance.

Here are the performance results:

old version:
   * Time for Indexing:  36.7 minutes
   * Time for Optimizing: 4.6 minutes

patched version:
   * Time for Indexing:  20.8 minutes
   * Time for Optimizing: 0.5 minutes

The results show that the index build time improved by about 43%, and the optimizing step is more than 8x faster. 

A diff of the final indexes (old and patched version) shows, that they are identical. Furthermore, all junit testcases succeeded with the patched version. 

Regards,
  Michael Busch"
"LUCENE-269","DOCUMENTATION","BUG","[PATCH] demo HTML parser corrupts foreign characters","We are using HTML parser for parsing English and other NL documents in 
Eclipse.  Post Lucene 1.2 there has been a regression in the parser.  
Characters coming from Reader (obtained from getReader() ) are corrupted.  
Only the characters that can be encoded using the default machine encoding go 
through correctly.  For example, parsing Chinese document on an English 
machine results with all characters, except the few English words, corrupted."
"LUCENE-2283","RFE","BUG","Possible Memory Leak in StoredFieldsWriter","StoredFieldsWriter creates a pool of PerDoc instances

this pool will grow but never be reclaimed by any mechanism

furthermore, each PerDoc instance contains a RAMFile.
this RAMFile will also never be truncated (and will only ever grow) (as far as i can tell)

When feeding documents with large number of stored fields (or one large dominating stored field) this can result in memory being consumed in the RAMFile but never reclaimed. Eventually, each pooled PerDoc could grow very large, even if large documents are rare.

Seems like there should be some attempt to reclaim memory from the PerDoc[] instance pool (or otherwise limit the size of RAMFiles that are cached) etc
"
"LUCENE-3350","BUG","BUG","trunk:  TestDocumentsWriterDeleteQueue.testStressDeleteQueue seed failure","fails 100% of the time for me, trunk r1152089

{code}
    [junit] Testsuite: org.apache.lucene.index.TestDocumentsWriterDeleteQueue
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.585 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestDocumentsWriterDeleteQueue -Dtestmethod=testStressDeleteQueue -Dtests.seed=724635056932528964:-56
53725200660632980
    [junit] NOTE: test params are: codec=RandomCodecProvider: {}, locale=en_US, timezone=Pacific/Port_Moresby
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDocumentsWriterDeleteQueue]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_26 (64-bit)/cpus=8,threads=1,free=86067624,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testStressDeleteQueue(org.apache.lucene.index.TestDocumentsWriterDeleteQueue):    FAILED
{code}"
"LUCENE-3278","REFACTORING","","Rename contrib/queryparser project to queryparser-contrib","Much like with contrib/queries, we should differentiate the contrib/queryparser from the queryparser module.  No directory structure changes will be made, just ant and maven."
"LUCENE-888","IMPROVEMENT","IMPROVEMENT","Improve indexing performance by increasing internal buffer sizes","In working on LUCENE-843, I noticed that two buffer sizes have a
substantial impact on overall indexing performance.

First is BufferedIndexOutput.BUFFER_SIZE (also used by
BufferedIndexInput).  Second is CompoundFileWriter's buffer used to
actually build the compound file.  Both are now 1 KB (1024 bytes).

I ran the same indexing test I'm using for LUCENE-843.  I'm indexing
~5,500 byte plain text docs derived from the Europarl corpus
(English).  I index 200,000 docs with compound file enabled and term
vector positions & offsets stored plus stored fields.  I flush
documents at 16 MB RAM usage, and I set maxBufferedDocs carefully to
not hit LUCENE-845.  The resulting index is 1.7 GB.  The index is not
optimized in the end and I left mergeFactor @ 10.

I ran the tests on a quad-core OS X 10 machine with 4-drive RAID 0 IO
system.

At 1 KB (current Lucene trunk) it takes 622 sec to build the index; if
I increase both buffers to 8 KB it takes 554 sec to build the index,
which is an 11% overall gain!

I will run more tests to see if there is a natural knee in the curve
(buffer size above which we don't really gain much more performance).

I'm guessing we should leave BufferedIndexInput's default BUFFER_SIZE
at 1024, at least for now.  During searching there can be quite a few
of this class instantiated, and likely a larger buffer size for the
freq/prox streams could actually hurt search performance for those
searches that use skipping.

The CompoundFileWriter buffer is created only briefly, so I think we
can use a fairly large (32 KB?) buffer there.  And there should not be
too many BufferedIndexOutputs alive at once so I think a large-ish
buffer (16 KB?) should be OK.
"
"LUCENE-2087","IMPROVEMENT","IMPROVEMENT","Remove recursion in NumericRangeTermEnum","The current FilteredTermEnum in NRQ uses setEnum() which itsself calls next(). This may lead to a recursion that can overflow stack, if the index is empty and a large range with low precStep is used. With 64 bit numbers and precStep == 1 there may be 127 recursions, as each sub-range would hit no term on empty index and the setEnum call would then call next() which itsself calls setEnum again. This leads to recursion depth of 256.

Attached is a patch that converts to iterative approach. setEnum is now unused and throws UOE (like endEnum())."
"LUCENE-2565","TEST","BUG","TestUTF32ToUTF8 can run forever","Stress testing this particular test uncovered that the testRandomRanges testcase can run forever, depending on the random numbers picked..."
"LUCENE-3193","RFE","RFE","TwoPhaseCommit interface","I would like to propose a TwoPhaseCommit interface which declares the methods necessary to implement a 2-phase commit algorithm:
* prepareCommit()
* commit()
* rollback()

The prepare/commit ones have variants that take a (Map<String,String> commitData) following the ones we have in IndexWriter.

In addition, a TwoPhaseCommitTool which implements a 2-phase commit amongst several TPCs.

Having IndexWriter implement that interface will allow running the 2-phase commit algorithm on multiple IWs or IW + any other object that implements the interface.

We should mark the interface @lucene.internal so as to not block ourselves in the future. This is pretty advanced stuff anyway.

Will post a patch soon"
"LUCENE-2864","RFE","RFE","add maxtf to fieldinvertstate","the maximum within-document TF is a very useful scoring value, 
we should expose it so that people can use it in scoring

consider the following sim:
{code}
@Override
public float idf(int docFreq, int numDocs) {
  return 1.0F; /* not used */
}

@Override
public float computeNorm(String field, FieldInvertState state) {
  return state.getBoost() / (float) Math.sqrt(state.getMaxTF());
}
{code}

which is surprisingly effective, but more interesting for practical reasons.

"
"LUCENE-3190","BUG","BUG","TestStressIndexing2 testMultiConfig failure","trunk: r1134311

reproducible

{code}
    [junit] Testsuite: org.apache.lucene.index.TestStressIndexing2
    [junit] Tests run: 1, Failures: 2, Errors: 0, Time elapsed: 0.882 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] java.lang.AssertionError: ram was 460908 expected: 408216 flush mem: 395100 active: 65808
    [junit]     at org.apache.lucene.index.DocumentsWriterFlushControl.assertMemory(DocumentsWriterFlushControl.java:102)
    [junit]     at org.apache.lucene.index.DocumentsWriterFlushControl.doAfterDocument(DocumentsWriterFlushControl.java:164)
    [junit]     at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:380)
    [junit]     at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1473)
    [junit]     at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1445)
    [junit]     at org.apache.lucene.index.TestStressIndexing2$IndexingThread.indexDoc(TestStressIndexing2.java:723)
    [junit]     at org.apache.lucene.index.TestStressIndexing2$IndexingThread.run(TestStressIndexing2.java:757)
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestStressIndexing2 -Dtestmethod=testMultiConfig -Dtests.seed=2571834029692482827:-8116419692655152763
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestStressIndexing2 -Dtestmethod=testMultiConfig -Dtests.seed=2571834029692482827:-8116419692655152763
    [junit] The following exceptions were thrown by threads:
    [junit] *** Thread: Thread-0 ***
    [junit] junit.framework.AssertionFailedError: java.lang.AssertionError: ram was 460908 expected: 408216 flush mem: 395100 active: 65808
    [junit]     at junit.framework.Assert.fail(Assert.java:47)
    [junit]     at org.apache.lucene.index.TestStressIndexing2$IndexingThread.run(TestStressIndexing2.java:762)
    [junit] NOTE: test params are: codec=RandomCodecProvider: {f33=Standard, f57=MockFixedIntBlock(blockSize=649), f11=Standard, f41=MockRandom, f40=Standard, f62=MockRandom, f75=Standard, f73=MockSep, f29=MockFixedIntBlock(blockSize=649), f83=MockRandom, f66=MockSep, f49=MockVariableIntBlock(baseBlockSize=9), f72=Pulsing(freqCutoff=7), f54=Standard, id=MockFixedIntBlock(blockSize=649), f80=MockRandom, f94=MockSep, f93=Pulsing(freqCutoff=7), f95=Standard}, locale=en_SG, timezone=Pacific/Palau
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestStressIndexing2]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=133324528,total=158400512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testMultiConfig(org.apache.lucene.index.TestStressIndexing2):     FAILED
    [junit] r1.numDocs()=17 vs r2.numDocs()=16
    [junit] junit.framework.AssertionFailedError: r1.numDocs()=17 vs r2.numDocs()=16
    [junit]     at org.apache.lucene.index.TestStressIndexing2.verifyEquals(TestStressIndexing2.java:308)
    [junit]     at org.apache.lucene.index.TestStressIndexing2.verifyEquals(TestStressIndexing2.java:278)
    [junit]     at org.apache.lucene.index.TestStressIndexing2.testMultiConfig(TestStressIndexing2.java:124)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1403)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1321)
    [junit] 
    [junit] 
    [junit] Testcase: testMultiConfig(org.apache.lucene.index.TestStressIndexing2):     FAILED
    [junit] Some threads threw uncaught exceptions!
    [junit] junit.framework.AssertionFailedError: Some threads threw uncaught exceptions!
    [junit]     at org.apache.lucene.util.LuceneTestCase.tearDown(LuceneTestCase.java:603)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1403)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1321)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestStressIndexing2 FAILED
{code}"
"LUCENE-3305","RFE","RFE","Kuromoji code donation - a new Japanese morphological analyzer","Atilika Inc. () would like to donate the Kuromoji Japanese morphological analyzer to the Apache Software Foundation in the hope that it will be useful to Lucene and Solr users in Japan and elsewhere.

The project was started in 2010 since we couldn't find any high-quality, actively maintained and easy-to-use Java-based Japanese morphological analyzers, and these become many of our design goals for Kuromoji.

Kuromoji also has a segmentation mode that is particularly useful for search, which we hope will interest Lucene and Solr users.  Compound-nouns, such as  (Kansai International Airport) and  (Nikkei Newspaper), are segmented as one token with most analyzers.  As a result, a search for  (airport) or  (newspaper) will not give you a for in these words.  Kuromoji can segment these words into    and   , which is generally what you would want for search and you'll get a hit.

We also wanted to make sure the technology has a license that makes it compatible with other Apache Software Foundation software to maximize its usefulness.  Kuromoji has an Apache License 2.0 and all code is currently owned by Atilika Inc.  The software has been developed by my good friend and ex-colleague Masaru Hasegawa and myself.

Kuromoji uses the so-called IPADIC for its dictionary/statistical model and its license terms are described in NOTICE.txt.

I'll upload code distributions and their corresponding hashes and I'd very much like to start the code grant process.  I'm also happy to provide patches to integrate Kuromoji into the codebase, if you prefer that.

Please advise on how you'd like me to proceed with this.  Thank you.
"
"LUCENE-2044","RFE","IMPROVEMENT","Allow random seed to be set in DeleteByPercentTask","Need this to make index identical on multiple runs.  "
"LUCENE-227","DOCUMENTATION","BUG","[PATCH] documentation typo","Just a small patch that fixes a typo and changes the first sentence, as that 
one is used by Javadoc as a kind of summary so it should be something more 
useful than ""The Jakarta Lucene API is divided into several packages."""
"LUCENE-649","DOCUMENTATION","IMPROVEMENT","Fixed Spelling mailinglist.xml","Just fixed some spelling in the mailinglist.xml in /java/trunk/xdocs



"
"LUCENE-1621","CLEANUP","IMPROVEMENT","deprecate term and getTerm in MultiTermQuery","This means moving getTerm and term up to sub classes as appropriate and reimplementing equals, hashcode as appropriate in sub classes."
"LUCENE-924","DOCUMENTATION","","IndexWriter has incomplete Javadocs","A couple of getter methods in IndexWriter have no javadocs."
"LUCENE-1166","RFE","RFE","A tokenfilter to decompose compound words","A tokenfilter to decompose compound words you find in many germanic languages (like German, Swedish, ...) into single tokens.

An example: Donaudampfschiff would be decomposed to Donau, dampf, schiff so that you can find the word even when you only enter ""Schiff"".

I use the hyphenation code from the Apache XML project FOP (http://xmlgraphics.apache.org/fop/) to do the first step of decomposition. Currently I use the FOP jars directly. I only use a handful of classes from the FOP project.

My question now:
Would it be OK to copy this classes over to the Lucene project (renaming the packages of course) or should I stick with the dependency to the FOP jars? The FOP code uses the ASF V2 license as well.

What do you think?"
"LUCENE-3503","BUG","BUG","DisjunctionSumScorer gives slightly (float iotas) different scores when you .nextDoc vs .advance","Spinoff from LUCENE-1536.

I dug into why we hit a score diff when using luceneutil to benchmark
the patch.

At first I thought it was BS1/BS2 difference, but because of a bug in
the patch it was still using BS2 (but should be BS1) -- Robert's last
patch fixes that.

But it's actually a diff in BS2 itself, whether you next or advance
through the docs.

It's because DisjunctionSumScorer, when summing the float scores for a
given doc that matches multiple sub-scorers, might sum in a different
order, when you had .nextDoc'd to that doc than when you had .advance'd
to it.

This in turn is because the PQ used by that scorer (ScorerDocQueue)
makes no effort to break ties.  So, when the top N scorers are on the
same doc, the PQ doesn't care what order they are in.

Fixing ScorerDocQueue to break ties will likely be a non-trivial perf
hit, though, so I'm not sure whether we should do anything here..."
"LUCENE-1214","BUG","BUG","Possible hidden exception on SegmentInfos commit","I am not sure if this is that big of a deal, but I just ran into it and thought I might mention it.

SegmentInfos.commit removes the Segments File if it hits an exception. If it cannot remove the Segments file (because its not there or on Windows something has a hold of it), another Exception is thrown about not being able to delete the Segments file. Because of this, you lose the first exception, which might have useful info, including why the segments file might not be there to delete.

- Mark"
"LUCENE-1010","BUG","BUG","Document with no term vectors mixed with ones that have term vectors cause EOFException during merge","Another spinoff from here:

  http://www.gossamer-threads.com/lists/lucene/java-dev/53306

Thank you to Andi Vajda for capturing the issue in a compact test!

This is the same logical error from LUCENE-1008, but in this case the
bug is in TermVectorsWriter: we are failing to write the ""0"" field
count to the tvd file when the document has no vectors.  I have a unit
test showing the issue & simple fix.
"
"LUCENE-1139","IMPROVEMENT","IMPROVEMENT","Various small improvements to contrib/benchmark","I've worked out a few small improvements to contrib/benchmark:

  * Refactored the common code in Open/CreateIndexTask that sets the
    configuration for the IndexWriter.  This also fixes a bug in
    OpenIndexTasks that prevented you from disabling flushing by RAM.

  * Added a new config property for LineDocMaker:

      doc.reuse.fields=true|false

    which turns on/off reusing of Field/Document by LineDocMaker.
    This lets us measure performance impact of sharing Field/Document
    vs not, and also turn it off when necessary (eg if you have your
    own consumer that uses private threads).

  * Added merge.scheduler & merge.policy config options.

  * Added param for OptimizeTask, which expects an int and calls
    optimize(maxNumSegments) with that param.

  * Added param for CloseIndex(true|false) -- if you pass false that
    means close the index, aborting any running merges
"
"LUCENE-2258","IMPROVEMENT","IMPROVEMENT","Remove ""synchonized"" from FuzzyTermEnum#similarity(final String target)","The similarity method in FuzzyTermEnum is synchronized which is stupid because of:
- TermEnums are the iterator pattern and so are single-thread per definition
- The method is private, so nobody could ever create a fake FuzzyTermEnum just to have this method and use it multithreaded.
- The method is not static and has no static fields - so instances do not affect each other

The root of this comes from LUCENE-296, but was never reviewd and simply committed. The argument for making it synchronized is wrong."
"LUCENE-482","RFE","RFE","JE Directory Implementation","I've created a port of DbDirectory to JE"
"LUCENE-1076","RFE","IMPROVEMENT","Allow MergePolicy to select non-contiguous merges","I started work on this but with LUCENE-1044 I won't make much progress
on it for a while, so I want to checkpoint my current state/patch.

For backwards compatibility we must leave the default MergePolicy as
selecting contiguous merges.  This is necessary because some
applications rely on ""temporal monotonicity"" of doc IDs, which means
even though merges can re-number documents, the renumbering will
always reflect the order in which the documents were added to the
index.

Still, for those apps that do not rely on this, we should offer a
MergePolicy that is free to select the best merges regardless of
whether they are continuguous.  This requires fixing IndexWriter to
accept such a merge, and, fixing LogMergePolicy to optionally allow
it the freedom to do so.
"
"LUCENE-717","BUILD_SYSTEM","BUG","src builds fail because of no ""lib"" directory","I just downloaded http://mirrors.ibiblio.org/pub/mirrors/apache/lucene/java/lucene-2.0.0-src.tar.gz and noticed that you can't compile and run the tests from that src build because it doesn't inlcude the lib dir (and the build file won't attempt to make it if it doesn't exist) ...

hossman@coaster:~/tmp/l2$ tar -xzvf lucene-2.0.0-src.tar.gz
  ...
hossman@coaster:~/tmp/l2$ cd lucene-2.0.0/
hossman@coaster:~/tmp/l2/lucene-2.0.0$ ant test
  ...
test:
    [mkdir] Created dir: /home/hossman/tmp/l2/lucene-2.0.0/build/test

BUILD FAILED
/home/hossman/tmp/l2/lucene-2.0.0/common-build.xml:169: /home/hossman/tmp/l2/lucene-2.0.0/lib not found.

(it's refrenced in junit.classpath, but i'm not relaly sure why)

"
"LUCENE-2249","BUG","BUG","ParallelMultiSearcher should shut down thread pool on close","ParallelMultiSearcher does not shut down its internal thread pool on close. As a result, programs that create multiple instances of this class over their lifetime end up ""leaking"" threads."
"LUCENE-862","BUG","BUG","Contrib query org.apache.lucene.search.BoostingQuery sets boost on constructor Query, not cloned copy","BoostingQuery sets the boost value on the passed context Query

    public BoostingQuery(Query match, Query context, float boost) {
      this.match = match;
      this.context = (Query)context.clone();        // clone before boost
      this.boost = boost;

      context.setBoost(0.0f);                      // ignore context-only matches
    }

This should be 
      this.context.setBoost(0.0f);                      // ignore context-only matches

Also, boost value of 0.0 may have wrong effect - see discussion at

http://www.mail-archive.com/java-user@lucene.apache.org/msg12243.html 

"
"LUCENE-949","BUG","BUG","AnalyzingQueryParser can't work with leading wildcards.","The getWildcardQuery mehtod in AnalyzingQueryParser.java need the following changes to accept leading wildcards:

	protected Query getWildcardQuery(String field, String termStr) throws ParseException
	{
		String useTermStr = termStr;
		String leadingWildcard = null;
		if (""*"".equals(field))
		{
			if (""*"".equals(useTermStr))
				return new MatchAllDocsQuery();
		}
		boolean hasLeadingWildcard = (useTermStr.startsWith(""*"") || useTermStr.startsWith(""?"")) ? true : false;

		if (!getAllowLeadingWildcard() && hasLeadingWildcard)
			throw new ParseException(""'*' or '?' not allowed as first character in WildcardQuery"");

		if (getLowercaseExpandedTerms())
		{
			useTermStr = useTermStr.toLowerCase();
		}

		if (hasLeadingWildcard)
		{
			leadingWildcard = useTermStr.substring(0, 1);
			useTermStr = useTermStr.substring(1);
		}

		List tlist = new ArrayList();
		List wlist = new ArrayList();
		/*
		 * somewhat a hack: find/store wildcard chars in order to put them back
		 * after analyzing
		 */
		boolean isWithinToken = (!useTermStr.startsWith(""?"") && !useTermStr.startsWith(""*""));
		isWithinToken = true;
		StringBuffer tmpBuffer = new StringBuffer();
		char[] chars = useTermStr.toCharArray();
		for (int i = 0; i < useTermStr.length(); i++)
		{
			if (chars[i] == '?' || chars[i] == '*')
			{
				if (isWithinToken)
				{
					tlist.add(tmpBuffer.toString());
					tmpBuffer.setLength(0);
				}
				isWithinToken = false;
			}
			else
			{
				if (!isWithinToken)
				{
					wlist.add(tmpBuffer.toString());
					tmpBuffer.setLength(0);
				}
				isWithinToken = true;
			}
			tmpBuffer.append(chars[i]);
		}
		if (isWithinToken)
		{
			tlist.add(tmpBuffer.toString());
		}
		else
		{
			wlist.add(tmpBuffer.toString());
		}

		// get Analyzer from superclass and tokenize the term
		TokenStream source = getAnalyzer().tokenStream(field, new StringReader(useTermStr));
		org.apache.lucene.analysis.Token t;

		int countTokens = 0;
		while (true)
		{
			try
			{
				t = source.next();
			}
			catch (IOException e)
			{
				t = null;
			}
			if (t == null)
			{
				break;
			}
			if (!"""".equals(t.termText()))
			{
				try
				{
					tlist.set(countTokens++, t.termText());
				}
				catch (IndexOutOfBoundsException ioobe)
				{
					countTokens = -1;
				}
			}
		}
		try
		{
			source.close();
		}
		catch (IOException e)
		{
			// ignore
		}

		if (countTokens != tlist.size())
		{
			/*
			 * this means that the analyzer used either added or consumed
			 * (common for a stemmer) tokens, and we can't build a WildcardQuery
			 */
			throw new ParseException(""Cannot build WildcardQuery with analyzer "" + getAnalyzer().getClass()
					+ "" - tokens added or lost"");
		}

		if (tlist.size() == 0)
		{
			return null;
		}
		else if (tlist.size() == 1)
		{
			if (wlist.size() == 1)
			{
				/*
				 * if wlist contains one wildcard, it must be at the end,
				 * because: 1) wildcards at 1st position of a term by
				 * QueryParser where truncated 2) if wildcard was *not* in end,
				 * there would be *two* or more tokens
				 */
				StringBuffer sb = new StringBuffer();
				if (hasLeadingWildcard)
				{
					// adding leadingWildcard
					sb.append(leadingWildcard);
				}
				sb.append((String) tlist.get(0));
				sb.append(wlist.get(0).toString());
				return super.getWildcardQuery(field, sb.toString());
			}
			else if (wlist.size() == 0 && hasLeadingWildcard)
			{
				/*
				 * if wlist contains no wildcard, it must be at 1st position
				 */
				StringBuffer sb = new StringBuffer();
				if (hasLeadingWildcard)
				{
					// adding leadingWildcard
					sb.append(leadingWildcard);
				}
				sb.append((String) tlist.get(0));
				sb.append(wlist.get(0).toString());
				return super.getWildcardQuery(field, sb.toString());
			}
			else
			{
				/*
				 * we should never get here! if so, this method was called with
				 * a termStr containing no wildcard ...
				 */
				throw new IllegalArgumentException(""getWildcardQuery called without wildcard"");
			}
		}
		else
		{
			/*
			 * the term was tokenized, let's rebuild to one token with wildcards
			 * put back in postion
			 */
			StringBuffer sb = new StringBuffer();
			if (hasLeadingWildcard)
			{
				// adding leadingWildcard
				sb.append(leadingWildcard);
			}
			for (int i = 0; i < tlist.size(); i++)
			{
				sb.append((String) tlist.get(i));
				if (wlist != null && wlist.size() > i)
				{
					sb.append((String) wlist.get(i));
				}
			}
			return super.getWildcardQuery(field, sb.toString());
		}
	}
"
"LUCENE-245","IMPROVEMENT","BUG","FieldCacheImpl cache gets rebuilt every time","FieldCacheImpl uses WeakHashMap to store the cached objects, but since 
there is no other reference to this cache it is getting released every time."
"LUCENE-2320","RFE","IMPROVEMENT","Add MergePolicy to IndexWriterConfig","Now that IndexWriterConfig is in place, I'd like to move MergePolicy to it as well. The change is not straightforward and so I've kept it for a separate issue. MergePolicy requires in its ctor an IndexWriter, however none can be passed to it before an IndexWriter actually exists. And today IW may create an MP just for it to be overridden by the application one line afterwards. I don't want to make iw member of MP non-final, or settable by extending classes, however it needs to remain protected so they can access it directly. So the proposed changes are:

* Add a SetOnce object (to o.a.l.util), or Immutable, which can only be set once (hence its name). It'll have the signature SetOnce<T> w/ *synchronized set<T>* and *T get()*. T will be declared volatile, so that get() won't be synchronized.
* MP will define a *protected final SetOnce<IndexWriter> writer* instead of the current writer. *NOTE: this is a bw break*. any suggestions are welcomed.
* MP will offer a public default ctor, together with a set(IndexWriter).
* IndexWriter will set itself on MP using set(this). Note that if set will be called more than once, it will throw an exception (AlreadySetException - or does someone have a better suggestion, preferably an already existing Java exception?).

That's the core idea. I'd like to post a patch soon, so I'd appreciate your review and proposals."
"LUCENE-3672","BUG","BUG","IndexCommit.equals() bug","IndexCommit.equals() checks for equality of Directories and versions, but it doesn't check IMHO the more important generation numbers. It looks like commits are really identified by a combination of directory and segments_XXX, which means the generation number, because that's what the DirectoryReader.open() checks for.

This bug leads to an unexpected behavior when the only change to be committed is in userData - we get two commits then that are declared equal, they have the same version but they have different generation numbers. I have no idea how this situation is treated in a few dozen references to IndexCommit.equals() across Lucene...

On the surface the fix is trivial - either add the gen number to equals(), or use gen number instead of version. However, it's puzzling why these two would ever get out of sync??? and if they are always supposed to be in sync then maybe we don't need both of them at all, maybe just generation or version is sufficient?"
"LUCENE-3736","RFE","","ParallelReader is now atomic, rename to ParallelAtomicReader and also add a ParallelCompositeReader (that requires LogDocMergePolicy to have identical subreader structure)","The plan is:
- Move all subreaders to ctor (builder-like API. First build reader-set, then call build)
- Rename ParallelReader to ParallelAtomicReader
- Add a ParallelCompositeReader with same builder API, but taking any CompositeReader-set and checks them that they are aligned (docStarts identical). The subreaders are ParallelAtomicReaders."
"LUCENE-2275","IMPROVEMENT","IMPROVEMENT","DocumentsWriter.applyDeletes should not create TermDocs or IndexSearcher if not needed","DocumentsWriter.applyDeletes(IndexReader, int) always creates TermDocs and IndexSearcher, even if there were no deletes by Term or by Query. The attached patch wraps those creations w/ checks on whether there were any deletes by these two. Additionally, the searcher wasn't closed in a finally block, so I fixed that as well.

I'll attach a patch shortly."
"LUCENE-1855","REFACTORING","IMPROVEMENT","Change AttributeSource API to use generics","The AttributeSource API will be easier to use with JDK 1.5 generics.

Uwe, if you started working on a patch for this already feel free to assign this to you."
"LUCENE-1576","BUG","BUG","Brazilian Analyzer doesn't remove stopwords when uppercase is given","The order of filters matter here, just need to apply lowercase token filter before removing stopwords

	result = new StopFilter( result, stoptable );
		result = new BrazilianStemFilter( result, excltable );
		// Convert to lowercase after stemming!
		result = new LowerCaseFilter( result );

Lowercase must come before BrazilianStemFilter

At the end of day I'll attach a patch, it's straightforward"
"LUCENE-319","BUG","BUG","[PATCH] Loosing first matching document in BooleanQuery","This patch fixes loosing of first matching document when BooleanQuery
with BooleanClause.Occur.SHOULD is added to another BooleanQuery."
"LUCENE-1068","BUG","BUG","Invalid behavior of StandardTokenizerImpl","The following code prints the output of StandardAnalyzer:

        Analyzer analyzer = new StandardAnalyzer();
        TokenStream ts = analyzer.tokenStream(""content"", new StringReader(""<some text>""));
        Token t;
        while ((t = ts.next()) != null) {
            System.out.println(t);
        }

If you pass ""www.abc.com"", the output is (www.abc.com,0,11,type=<HOST>) (which is correct in my opinion).
However, if you pass ""www.abc.com."" (notice the extra '.' at the end), the output is (wwwabccom,0,12,type=<ACRONYM>).

I think the behavior in the second case is incorrect for several reasons:
1. It recognizes the string incorrectly (no argue on that).
2. It kind of prevents you from putting URLs at the end of a sentence, which is perfectly legal.
3. An ACRONYM, at least to the best of my understanding, is of the form A.B.C. and not ABC.DEF.

I looked at StandardTokenizerImpl.jflex and I think the problem comes from this definition:
// acronyms: U.S.A., I.B.M., etc.
// use a post-filter to remove dots
ACRONYM    =  {ALPHA} ""."" ({ALPHA} ""."")+

Notice how the comment relates to acronym as U.S.A., I.B.M. and not something else. I changed the definition to
ACRONYM    =  {LETTER} ""."" ({LETTER} ""."")+
and it solved the problem.

This was also reported here:
http://www.nabble.com/Inconsistent-StandardTokenizer-behaviour-tf596059.html#a1593383
http://www.nabble.com/Standard-Analyzer---Host-and-Acronym-tf3620533.html#a10109926
"
"LUCENE-1082","BUG","BUG","IndexReader.lastModified - throws NPE","IndexReader.lastModified(String dir) or its variants always return NPE on 2.3, perhaps something to do with SegmentInfo."
"LUCENE-1294","BUILD_SYSTEM","BUG","Jar manifest should not contain ${user.name} of the person building","Not sure if it is a big deal, but I don't particularly like that my user id for my build machine is in the manifest of the JAR that I constructed.  It's a stretch, security-wise, I know, but I don't see how it serves any useful purpose.  We have signatures/logs/SVN tags so we know who built the particular item w/o needing to know what their local user account name is.

The fix is:

{code}
Index: common-build.xml
===================================================================
--- common-build.xml    (revision 661027)
+++ common-build.xml    (working copy)
@@ -281,7 +281,7 @@
                <attribute name=""Implementation-Title"" value=""org.apache.lucene""/>
                <!-- impl version can be any string -->
                <attribute name=""Implementation-Version""
-                          value=""${version} ${svnversion} - ${user.name} - ${DSTAMP} ${TSTAMP}""/>
+                          value=""${version} ${svnversion} - ${DSTAMP} ${TSTAMP}""/>
                <attribute name=""Implementation-Vendor""
                           value=""The Apache Software Foundation""/>
                <attribute name=""X-Compile-Source-JDK"" 
{code} "
"LUCENE-2084","IMPROVEMENT","IMPROVEMENT","remove Byte/CharBuffer wrapping for collation key generation","We can remove the overhead of ByteBuffer and CharBuffer wrapping in CollationKeyFilter and ICUCollationKeyFilter.

this patch moves the logic in IndexableBinaryStringTools into char[],int,int and byte[],int,int based methods, with the previous Byte/CharBuffer methods delegating to these.
Previously, the Byte/CharBuffer methods required a backing array anyway.
"
"LUCENE-995","RFE","IMPROVEMENT","Add open ended range query syntax to QueryParser","The QueryParser fails to generate open ended range queries.
Parsing e.g. ""date:[1990 TO *]""  gives zero results,
but
ConstantRangeQuery(""date"",""1990"",null,true,true)
does produce the expected results.

""date:[* TO 1990]"" gives the same results as ConstantRangeQuery(""date"",null,""1990"",true,true)."
"LUCENE-2118","BUG","BUG","Intermittent failure in TestIndexWriterMergePolicy.testMaxBufferedDocsChange","Last night's build failed from it: http://hudson.zones.apache.org/hudson/job/Lucene-trunk/1019/changes

Here's the exc:

{code}
    [junit] Testcase: testMaxBufferedDocsChange(org.apache.lucene.index.TestIndexWriterMergePolicy):	FAILED
    [junit] maxMergeDocs=2147483647; numSegments=11; upperBound=10; mergeFactor=10
    [junit] junit.framework.AssertionFailedError: maxMergeDocs=2147483647; numSegments=11; upperBound=10; mergeFactor=10
    [junit] 	at org.apache.lucene.index.TestIndexWriterMergePolicy.checkInvariants(TestIndexWriterMergePolicy.java:234)
    [junit] 	at org.apache.lucene.index.TestIndexWriterMergePolicy.testMaxBufferedDocsChange(TestIndexWriterMergePolicy.java:164)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:208)
{code}

Test doesn't fail if I run on opensolaris nor os X machines..."
"LUCENE-2366","BUG","BUG","LLRect.createBox returned box does not contains all points in (center,distance) disc","LLRect,createBox computation of a bouding box for a disc given center and distance doest not contains all the point in the distance.

Example : the point north by distance doest not have Lat inferior of Lat of the UpperRight corner of the returned box"
"LUCENE-3510","RFE","IMPROVEMENT","BooleanScorer should not limit number of prohibited clauses","Today it's limited to 32, because it uses a separate bit in the mask
for each clause.

But I don't understand why it does this; I think all prohibited
clauses can share a single boolean/bit?  Any match on a prohibited
clause sets this bit and the doc is not collected; we don't need each
prohibited clause to have a dedicated bit?

We also use the mask for required clauses, but this code is now
commented out (we always use BS2 if there are any required clauses);
if we re-enable this code (and I think we should, at least in certain
cases: I suspect it'd be faster than BS2 in many cases), I think we
can cutover to an int count instead of bit masks, and then have no
limit on the required clauses sent to BooleanScorer also.

Separately I cleaned a few things up about BooleanScorer: all of the
embedded scorer methods (nextDoc, docID, advance, score) now throw
UOE; pre-allocate the buckets instead of doing it lazily
per-sub-collect.
"
"LUCENE-1351","RFE","IMPROVEMENT","Add some ligatures (ff, fi, fl, ft, st) to ISOLatin1AccentFilter","ISOLatin1AccentFilter removes common diacritics and some ligatures. This patch adds support for additional common ligatures: ff, fi, fl, ft, st."
"LUCENE-3028","BUG","BUG","IW.getReader() returns inconsistent reader on RT Branch","I extended the testcase TestRollingUpdates#testUpdateSameDoc to pull a NRT reader after each update and asserted that is always sees only one document. Yet, this fails with current branch since there is a problem in how we flush in the getReader() case. What happens here is that we flush all threads and then release the lock (letting other flushes which came in after we entered the flushAllThread context, continue) so that we could concurrently get a new segment that transports global deletes without the corresponding add. They sneak in while we continue to open the NRT reader which in turn sees inconsistent results.

I will upload a patch soon"
"LUCENE-3645","REFACTORING","IMPROVEMENT","Remove unnecessary array wrapping when calling varargs methods","varargs method callers don't have to wrap args in arrays"
"LUCENE-2795","RFE","IMPROVEMENT","Genericize DirectIOLinuxDir -> UnixDir","Today DirectIOLinuxDir is tricky/dangerous to use, because you only want to use it for indexWriter and not IndexReader (searching).  It's a trap.

But, once we do LUCENE-2793, we can make it fully general purpose because then a single native Dir impl can be used.

I'd also like to make it generic to other Unices, if we can, so that it becomes UnixDirectory."
"LUCENE-1850","DOCUMENTATION","TASK","Update overview example code","See http://lucene.apache.org/java/2_4_1/api/core/overview-summary.html - need to update for non-deprecated best-practices/recommended API usage.

Also, double-check that the demo app works as documented."
"LUCENE-702","IMPROVEMENT","BUG","Disk full during addIndexes(Directory[]) can corrupt index","This is a spinoff of LUCENE-555

If the disk fills up during this call then the committed segments file can reference segments that were not written.  Then the whole index becomes unusable.

Does anyone know of any other cases where disk full could corrupt the index?

I think disk full should worse lose the documents that were ""in flight"" at the time.  It shouldn't corrupt the index."
"LUCENE-1519","BUG","BUG","Change Primitive Data Types from int to long in class SegmentMerger.java","Hi

We are getting an exception while optimize. We are getting this exception ""mergeFields produced an invalid result: docCount is 385282378 but fdx file size is 3082259028; now aborting this merge to prevent index corruption""
 
I have  checked the code for class SegmentMerger.java and found this check 

***********************************************************************************************************************************************************************
if (4+docCount*8 != fdxFileLength)
        // This is most likely a bug in Sun JRE 1.6.0_04/_05;
        // we detect that the bug has struck, here, and
        // throw an exception to prevent the corruption from
        // entering the index.  See LUCENE-1282 for
        // details.
        throw new RuntimeException(""mergeFields produced an invalid result: docCount is "" + docCount + "" but fdx file size is "" + fdxFileLength + ""; now aborting this merge to prevent index corruption"");
}
***********************************************************************************************************************************************************************

In our case docCount is 385282378 and fdxFileLength size is 3082259028, even though 4+385282378*8 is equal to 3082259028, the above code will not work because number 3082259028 is out of int range. So type of variable docCount needs to be changed to long

I have written a small test for this 

************************************************************************************************************************************************************************

public class SegmentMergerTest {
public static void main(String[] args) {
int docCount = 385282378; 
long fdxFileLength = 3082259028L; 
if(4+docCount*8 != fdxFileLength) 
System.out.println(""No Match"" + (4+docCount*8));
else 
System.out.println(""Match"" + (4+docCount*8));
}
}

************************************************************************************************************************************************************************

Above test will print No Match but if you change the data type of docCount to long, it will print Match

Can you please advise us if this issue will be fixed in next release?

Regards
Deepak







 



"
"LUCENE-2086","IMPROVEMENT","IMPROVEMENT","When resolving deletes, IW should resolve in term sort order","See java-dev thread ""IndexWriter.updateDocument performance improvement""."
"LUCENE-3037","BUG","BUG","TestFSTs.testRealTerms produces a corrupt index","seems to be prox/skip related: the test passes, but the checkindex upon closing fails.

ant test-core -Dtestcase=TestFSTs -Dtests.seed=-4012305283315171209:0 -Dtests.multiplier=3 -Dtests.nightly=true -Dtests.linedocsfile=c:/data/enwiki.random.lines.txt.gz

Note: to get the enwiki.random.lines.txt.gz you have to fetch it from hudson (warning 1 gigabyte file).
you also have to run the test a few times to trigger it.

ill upload the index this thing makes to this issue.
"
"LUCENE-3365","BUG","BUG","Create or Append mode determined before obtaining write lock","If an IndexWriter(""writer1"") is opened in CREATE_OR_APPEND mode, it determines whether to CREATE or APPEND before obtaining the write lock.  When another IndexWriter(""writer2"") is in the process of creating the index, this can result in writer1 entering create mode and then waiting to obtain the lock.  When writer2 commits and releases the lock, writer1 is already in create mode and overwrites the index created by write2.

This bug was probably effected by LUCENE-2386 as prior to that Lucene generated an empty commit when a new index was created.  I think the issue could still have occurred prior to that but the two IndexWriters would have needed to be opened nearly simultaneously and the first IndexWriter would need to release the lock before the second timed out."
"LUCENE-2179","RFE","IMPROVEMENT","CharArraySet.clear()","I needed CharArraySet.clear() for something I was working on in Solr in a tokenstream.

instead I ended up using CharArrayMap<Boolean> because it supported .clear()

it would be better to use a set though, currently it will throw UOE for .clear() because AbstractSet will call iterator.remove() which throws UOE.

In Solr, the very similar CharArrayMap.clear() looks like this:
{code}
  @Override
  public void clear() {
    count = 0;
    Arrays.fill(keys,null);
    Arrays.fill(values,null);
  }
{code}

I think we can do a similar thing as long as we throw UOE for the UnmodifiableCharArraySet

will submit a patch later tonight (unless someone is bored and has nothing better to do)"
"LUCENE-3434","RFE","IMPROVEMENT","Make ShingleAnalyzerWrapper and PerFieldAnalyzerWrapper immutable","Both ShingleAnalyzerWrapper and PerFieldAnalyzerWrapper have setters which change some state which impacts their analysis stack.  If these are going to become reusable, then the state must be immutable as changing it will have no effect.

Process will be similar to QueryAutoStopWordAnalyzer, I will remove in trunk and deprecate in 3x."
"LUCENE-2764","TEST","TEST","Allow tests to use random codec per field","Since we now have a real per field codec support we should enable to run the tests with a random codec per field. When I change something related to codecs internally I would like to ensure that whatever combination of codecs (except of preflex) I use the code works just fine. I created a RandomCodecProvider in LuceneTestCase that randomly selects the codec for fields when it sees them the first time. I disabled the test by default to leave the old randomize codec support in as it was / is."
"LUCENE-2802","BUG","BUG","DirectoryReader ignores NRT SegmentInfos in #isOptimized()","DirectoryReader  only takes shared (with IW) SegmentInfos into account in DirectoryReader#isOptimized(). This can return true even if the actual realtime reader sees more than one segments. 

{code}
public boolean isOptimized() {
    ensureOpen();
   // if segmentsInfos changes in IW this can return false positive
    return segmentInfos.size() == 1 && !hasDeletions();
  }
{code}

DirectoryReader should check if this reader has a non-nul segmentInfosStart and use that instead"
"LUCENE-1616","REFACTORING","IMPROVEMENT","add one setter for start and end offset to OffsetAttribute","add OffsetAttribute. setOffset(startOffset, endOffset);

trivial change, no JUnit needed

Changed CharTokenizer to use it"
"LUCENE-3717","TEST","TASK","Add fake charfilter to BaseTokenStreamTestCase to find offsets bugs","Recently lots of issues have been fixed about broken offsets, but it would be nice to improve the
test coverage and test that they work across the board (especially with charfilters).

in BaseTokenStreamTestCase.checkRandomData, we can sometimes pass the analyzer a reader wrapped
in a ""MockCharFilter"" (the one in the patch sometimes doubles characters). If the analyzer does
not call correctOffsets or does incorrect ""offset math"" (LUCENE-3642, etc) then eventually
this will create offsets and the test will fail.

Other than tests bugs, this found 2 real bugs: ICUTokenizer did not call correctOffset() in its end(),
and ThaiWordFilter did incorrect offset math."
"LUCENE-1788","CLEANUP","TASK","Cleanup highlighter test class","cleanup highlighter test class - did some of this in another issue, but there is a bit more to do"
"LUCENE-2829","IMPROVEMENT","IMPROVEMENT","improve termquery ""pk lookup"" performance","For things that are like primary keys and don't exist in some segments (worst case is primary/unique key that only exists in 1)
we do wasted seeks.

While LUCENE-2694 tries to solve some of this issue with TermState, I'm concerned we could every backport that to 3.1 for example.

This is a simpler solution here just to solve this one problem in termquery... we could just revert it in trunk when we resolve LUCENE-2694,
but I don't think we should leave things as they are in 3.x
"
"LUCENE-1599","BUG","BUG","SpanRegexQuery and SpanNearQuery is not working with MultiSearcher","MultiSearcher is using:
queries[i] = searchables[i].rewrite(original);
to rewrite query and then use combine to combine them.

But SpanRegexQuery's rewrite is different from others.
After you call it on the same query, it always return the same rewritten queries.

As a result, only search on the first IndexSearcher work. All others are using the first IndexSearcher's rewrite queries.
So many terms are missing and return unexpected result.

Billow"
"LUCENE-2958","IMPROVEMENT","IMPROVEMENT","WriteLineDocTask improvements","Make WriteLineDocTask and LineDocSource more flexible/extendable:
* allow to emit lines also for empty docs (keep current behavior as default)
* allow more/less/other fields"
"LUCENE-395","RFE","IMPROVEMENT","CoordConstrainedBooleanQuery + QueryParser support","Attached 2 new classes:

1) CoordConstrainedBooleanQuery
A boolean query that only matches if a specified number of the contained clauses
match. An example use might be a query that returns a list of books where ANY 2
people from a list of people were co-authors, eg:
""Lucene In Action"" would match (""Erik Hatcher"" ""Otis Gospodneti&#263;"" ""Mark Harwood""
""Doug Cutting"") with a minRequiredOverlap of 2 because Otis and Erik wrote that.
The book ""Java Development with Ant"" would not match because only 1 element in
the list (Erik) was selected.

2) CustomQueryParserExample
A customised QueryParser that allows definition of
CoordConstrainedBooleanQueries. The solution (mis)uses fieldnames to pass
parameters to the custom query."
"LUCENE-1672","CLEANUP","TASK","Deprecate all String/File ctors/opens in IndexReader/IndexWriter/IndexSearcher","During investigation of LUCENE-1658, I found out, that even LUCENE-1453 is not completely fixed.
As 1658 deprecates all FSDirectory.getDirectory() static factories, we should not use them anymore. As the user is now free to choose the correct directory implementation using direct instantiation or using FSDir.open() he should no longer use all ctors/methods in IndexWriter/IndexReader/IndexSearcher & Co. that simply take path names as String or File and always instantiate the Directory himself.

LUCENE-1453 currently works for the cached directory implementations from FSDir.getDirectory, but not with uncached, non refcounting FSDirs. Sometime reopen() closes the directory (as far as I see, when a SegmentReader changes to a MultiSegmentReader and/or deletes apply). This is hard to track. In Lucene 3.0 we then can remove the whole bunch of closeDirectory parameters/fields in these classes and simply do not care anymore about closing directories.

To remove this closeDirectory parameter now (before 3.0) and also fix 1453 correctly, an additional idea would be to change these factories that take the File/String to return the IndexReader wrapped by a FilteredIndexReader, that keeps track on closing the underlying directory after close and reopen. This is simplier than passing this boolean between different DirectoryIndexReader instances. The small performance impact by wrapping with FilterIndexReader should not be so bad, because the method is deprecated and we can state, that it is better to user the factory method with Directory parameter."
"LUCENE-1683","BUG","IMPROVEMENT","RegexQuery matches terms the input regex doesn't actually match","I was writing some unit tests for our own wrapper around the Lucene regex classes, and got tripped up by something interesting.

The regex ""cat."" will match ""cats"" but also anything with ""cat"" and 1+ following letters (e.g. ""cathy"", ""catcher"", ...)  It is as if there is an implicit .* always added to the end of the regex.

Here's a unit test for the behaviour I would expect myself:

    @Test
    public void testNecessity() throws Exception {
        File dir = new File(new File(System.getProperty(""java.io.tmpdir"")), ""index"");
        IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), true);
        try {
            Document doc = new Document();
            doc.add(new Field(""field"", ""cat cats cathy"", Field.Store.YES, Field.Index.TOKENIZED));
            writer.addDocument(doc);
        } finally {
            writer.close();
        }

        IndexReader reader = IndexReader.open(dir);
        try {
            TermEnum terms = new RegexQuery(new Term(""field"", ""cat."")).getEnum(reader);
            assertEquals(""Wrong term"", ""cats"", terms.term());
            assertFalse(""Should have only been one term"", terms.next());
        } finally {
            reader.close();
        }
    }

This test fails on the term check with terms.term() equal to ""cathy"".

Our workaround is to mangle the query like this:

    String fixed = String.format(""(?:%s)$"", original);
"
"LUCENE-1112","IMPROVEMENT","BUG","Document is partially indexed on an unhandled exception","With LUCENE-843, it's now possible for a subset of a document's
fields/terms to be indexed or stored when an exception is hit.  This
was not the case in the past (it was ""all or none"").

I plan to make it ""all or none"" again by immediately marking a
document as deleted if any exception is hit while indexing it.

Discussion leading up to this:

  http://www.gossamer-threads.com/lists/lucene/java-dev/56103
"
"LUCENE-1468","BUG","BUG","FSDirectory.list() is inconsistent","LUCENE-638 added a check to the FSDirectory.list() method to only return files that are Lucene related. I think this change made the FSDirectory implementation inconsistent with all other methods in Directory. E.g. you can create a file with an arbitrary name using FSDirectory, fileExists() will report that it is there, deleteFile() will remove it, but the array returned by list() will not contain the file.

The actual issue that was reported in LUCENE-638 was about sub directories. Those should clearly not be listed, but IMO it is not the responsibility of a Directory implementation to decide what kind of files can be created or listed. The Directory class is an abstraction of a directory and it should't to more than that.
"
"LUCENE-2625","BUG","BUG","IndexReader.termDocs() retrieves no documents","TermDocs object returned by indexReader.termDocs() retrieves no documents, howerver, the documents are retrieved correctly when using indexReader.termDocs(Term), indexReader.termDocs(null) and indexSearcher.search(Query)."
"LUCENE-383","BUG","BUG","ConstantScoreRangeQuery - fixes ""too many clauses"" exception","ConstantScoreQuery wraps a filter (representing a set of documents) and returns
a constant score for each document in the set.

ConstantScoreRangeQuery implements a RangeQuery that works for any number of
terms in the range.  It rewrites to a ConstantScoreQuery that wraps a RangeFilter.

Still needed:
  - unit tests (these classes have been tested and work fine in-house, but the
current tests rely on too much application specific code)
  - code review of Weight() implementation (I'm unsure If I got all the score
normalization stuff right)
  - explain() implementation

NOTE: requires Java 1.4 for BitSet.nextSetBit()"
"LUCENE-2100","DESIGN_DEFECT","IMPROVEMENT","Make contrib analyzers final","The analyzers in contrib/analyzers should all be marked final. None of the Analyzers should ever be subclassed - users should build their own analyzers if a different combination of filters and Tokenizers is desired."
"LUCENE-2956","BUG","BUG","Support updateDocument() with DWPTs","With separate DocumentsWriterPerThreads (DWPT) it can currently happen that the delete part of an updateDocument() is flushed and committed separately from the corresponding new document.

We need to make sure that updateDocument() is always an atomic operation from a IW.commit() and IW.getReader() perspective.  See LUCENE-2324 for more details."
"LUCENE-2209","DOCUMENTATION","TASK","add @experimental javadocs tag","There are a lot of things marked experimental, api subject to change, etc. in lucene.

this patch simply adds a @experimental tag to common-build.xml so that we can use it, for more consistency.
"
"LUCENE-2165","RFE","BUG","SnowballAnalyzer lacks a constructor that takes a Set of Stop Words","As discussed on the java-user list, the SnowballAnalyzer has been updated to use a Set of stop words. However, there is no constructor which accepts a Set, there's only the original String[] one

This is an issue, because most of the common sources of stop words (eg StopAnalyzer) have deprecated their String[] stop word lists, and moved over to Sets (eg StopAnalyzer.ENGLISH_STOP_WORDS_SET). So, for now, you either have to use a deprecated field on StopAnalyzer, or manually turn the Set into an array so you can pass it to the SnowballAnalyzer

I would suggest that a constructor is added to SnowballAnalyzer which accepts a Set. Not sure if the old String[] one should be deprecated or not.

A sample patch against 2.9.1 to add the constructor is:


--- SnowballAnalyzer.java.orig  2009-12-15 11:14:08.000000000 +0000
+++ SnowballAnalyzer.java       2009-12-14 12:58:37.000000000 +0000
@@ -67,6 +67,12 @@
     stopSet = StopFilter.makeStopSet(stopWords);
   }
 
+  /** Builds the named analyzer with the given stop words. */
+  public SnowballAnalyzer(Version matchVersion, String name, Set stopWordsSet) {
+    this(matchVersion, name);
+    stopSet = stopWordsSet;
+  }
+
"
"LUCENE-1699","RFE","IMPROVEMENT","Field tokenStream should be usable with stored fields.","Field.tokenStream should be usable for indexing even for stored values.  Useful for many types of pre-analyzed values (text/numbers, etc)
http://search.lucidimagination.com/search/document/902bad4eae20bdb8/field_tokenstreamvalue"
"LUCENE-591","RFE","IMPROVEMENT","Add meta keywords to HTMLParser","
It would be good if the HTMLParser could give us the keywords specified in the meta tags, so that we can index them.

In HTMLParser.jj:

  void addMetaTag() {
      metaTags.setProperty(currentMetaTag, currentMetaContent);
      currentMetaTag = null;
      currentMetaContent = null;
      return;
  }

One way to do it:

  void addMetaTag() throws IOException {
      metaTags.setProperty(currentMetaTag, currentMetaContent);
      if (currentMetaTag.equalsIgnoreCase(""keywords"")) {
          pipeOut.write(currentMetaContent);
      }
      currentMetaTag = null;
      currentMetaContent = null;
      return;
  }
"
"LUCENE-2388","DOCUMENTATION","BUG","the unversioned site points to a dead trunk","The unversioned site needs to point to the new merged trunk.
Currently it points to the closed-off dead trunk in two different places.
"
"LUCENE-3729","RFE","IMPROVEMENT","Allow using FST to hold terms data in DocValues.BYTES_*_SORTED",""
"LUCENE-1654","RFE","IMPROVEMENT","Include diagnostics per-segment when writing a new segment","It would be very helpful if each segment in an index included
diagnostic information, such as the current version of Lucene.

EG, in LUCENE-1474 this would be very helpful to see if certain
segments were written under 2.4.0.

We can start with just the current version.

We could also consider making this extensible, so you could provide
your own arbitrary diagnostics, but SegmentInfo/s is not public so I
think such an API would be ""one-way"" in that you'd have to use
CheckIndex to check on it later.  Or we could wait on such extensibility
until we provide some consistent way to access per-segment details
in the index.
"
"LUCENE-2602","RFE","IMPROVEMENT","Default merge policy should take deletions into account","LUCENE-1634 added a calibrateSizeByDeletes; we had a TODO to default this to true as of 3.0 but we missed it.  I'll fix it now for 3.1 and 4.0.  While this is technically a change in back-compat (for 3.x), I think it's fine to make an exception here; this should be a big win for indices that have high doc turnover with time."
"LUCENE-262","BUG","BUG","SegmentReader.hasSeparateNorms always returns false","The loop in that method looks like this: 
 
for(int i = 0; i < 0; i++){ 
 
I guess ""i < 0"" should be replaced by ""i < result.length""?"
"LUCENE-3648","IMPROVEMENT","IMPROVEMENT","Speed up SegementDocsEnum by making it more friendly for JIT optimizations","Since we moved the bulk reading into the codec ie. make all  bulk reading codec private in LUCENE-3584 we have seen some performance [regression|http://people.apache.org/~mikemccand/lucenebench/Term.html] on different CPUs. I tried to optimize the implementation to make it more eligible for runtime optimizations, tried to make loops JIT friendly by moving out branches where I can, minimize member access in all loops, use final members where possible and specialize the two common cases With & Without LiveDocs.

I will attache a patch and my benchmark results in a minute."
"LUCENE-3090","BUG","BUG","DWFlushControl does not take active DWPT out of the loop on fullFlush","We have seen several OOM on TestNRTThreads and all of them are caused by DWFlushControl missing DWPT that are set as flushPending but can't full due to a full flush going on. Yet that means that those DWPT are filling up in the background while they should actually be checked out and blocked until the full flush finishes. Even further we currently stall on the maxNumThreadStates while we should stall on the num of active thread states. I will attach a patch tomorrow."
"LUCENE-1162","DESIGN_DEFECT","IMPROVEMENT","Improve architecture of FieldSortedHitQueue","Per the discussion (quite some time ago) on issue LUCENE-806, I'd like to propose an architecture change to the way FieldSortedHitQueue works, and in particular the way it creates SortComparatorSources. I think (I hope) that anyone who looks at the FSHQ code will agree that the class does a lot and much of it's repetitive stuff that really has no business being in that class.

I am about to attach a patch which, in and of itself, doesn't really achieve much that's concrete but does tidy things up a great deal and makes it easier to plug in different behaviours. I then have a subsequent patch which provides a fairly simple and flexible example of how you might replace an implementation, in this case the field-local-String-comparator version from LUCENE-806.

The downside to this patch is that it involved changing the signature of SortComparatorSource.newComparator to take a Locale. There would be ways around this (letting FieldSortedHitQueue take in either a SortComparatorSource or some new, improved interface which takes a Locale (and possibly extends SortComparatorSource). I'm open to this but personally I think that the Locale version makes sense and would suggest that the code would be nicer by breaking the API (and hence targeting this to, presumably, 3.0 at a minimum).

This code does not include specific tests (I will add these, if people like the general idea I'm proposing here) but all current tests pass with this change.

Patch to follow."
"LUCENE-3469","REFACTORING","TASK","move DocumentStoredFieldsVisitor to o.a.l.document","when examining the changes to the field/document API, i noticed this class was in o.a.l.index

I think it should be in o.a.l.document, its more intuitive packaging"
"LUCENE-3579","DESIGN_DEFECT","IMPROVEMENT","DirectoryTaxonomyWriter should throw a proper exception if it was closed","DirTaxoWriter may throw random exceptions (NPE, Already Closed - depend on what API you call) after it has been closed/rollback. We should detect up front that it is already closed, and throw AlreadyClosedException.

Also, on LUCENE-3573 Doron pointed out a problem with DTW.rollback -- it should call close() rather than refreshReader. I will fix that as well in this issue."
"LUCENE-3097","RFE","RFE","Post grouping faceting","This issues focuses on implementing post grouping faceting.
* How to handle multivalued fields. What field value to show with the facet.
* Where the facet counts should be based on
** Facet counts can be based on the normal documents. Ungrouped counts. 
** Facet counts can be based on the groups. Grouped counts.
** Facet counts can be based on the combination of group value and facet value. Matrix counts.   

And properly more implementation options.

The first two methods are implemented in the SOLR-236 patch. For the first option it calculates a DocSet based on the individual documents from the query result. For the second option it calculates a DocSet for all the most relevant documents of a group. Once the DocSet is computed the FacetComponent and StatsComponent use one the DocSet to create facets and statistics.  

This last one is a bit more complex. I think it is best explained with an example. Lets say we search on travel offers:
|||hotel||departure_airport||duration||
|Hotel a|AMS|5
|Hotel a|DUS|10
|Hotel b|AMS|5
|Hotel b|AMS|10

If we group by hotel and have a facet for airport. Most end users expect (according to my experience off course) the following airport facet:
AMS: 2
DUS: 1

The above result can't be achieved by the first two methods. You either get counts AMS:3 and DUS:1 or 1 for both airports."
"LUCENE-2076","RFE","","Add org.apache.lucene.store.FSDirectory.getDirectory()","On the Apache Lucene.Net side, we have done some clean up with the upcoming 2.9.1 such that we are now depreciating improperly use of parameter type for some public APIs.  When we release 3.0, those depreciated code will be removed.

One area where we had difficulty with required us to add a new method like so: Lucene.Net.Store.FSDirectory.GetDirectory().  This method does the same thing as Lucene.Net.Store.FSDirectory.GetFile().  This was necessary because we switched over from using System.IO.FileInfo to System.IO.DirectoryInfo.  Why?  In the .NET world, a file and a directory are two different things.

Why did we have to add Lucene.Net.Store.FSDirectory.GetDirectory()?  Because we can't change the return type of Lucene.Net.Store.FSDirectory.GetFile() and still remain backward compatible (API wise) to be depreciated with the next release.

Why ask for Java Lucene to add org.apache.lucene.store.FSDirectory.getDirectory()?  To keep the APIs 1-to-1 in par with Java Lucene and Lucene.Net."
"LUCENE-3412","BUG","BUG","SloppyPhraseScorer returns non-deterministic results for queries with many repeats","Proximity queries with many repeats (four or more, based on my testing) return non-deterministic results. I run the same query multiple times with the same data set and get different results.

So far I've reproduced this with Solr 1.4.1, 3.1, 3.2, 3.3, and latest 4.0 trunk.

Steps to reproduce (using the Solr example):
1) In solrconfig.xml, set queryResultCache size to 0.
2) Add some documents with text ""dog dog dog"" and ""dog dog dog dog"". http://localhost:8983/solr/update?stream.body=%3Cadd%3E%3Cdoc%3E%3Cfield%20name=%22id%22%3E1%3C/field%3E%3Cfield%20name=%22text%22%3Edog%20dog%20dog%3C/field%3E%3C/doc%3E%3Cdoc%3E%3Cfield%20name=%22id%22%3E2%3C/field%3E%3Cfield%20name=%22text%22%3Edog%20dog%20dog%20dog%3C/field%3E%3C/doc%3E%3C/add%3E&commit=true
3) Do a ""dog dog dog dog""~1 query. http://localhost:8983/solr/select?q=%22dog%20dog%20dog%20dog%22~1
4) Repeat step 3 many times.

Expected results: The document with id 2 should be returned.

Actual results: The document with id 2 is always returned. The document with id 1 is sometimes returned.

Different proximity values show the same bug - ""dog dog dog dog""~5, ""dog dog dog dog""~100, etc show the same behavior.

So far I've traced it down to the ""repeats"" array in SloppyPhraseScorer.initPhrasePositions() - depending on the order of the elements in this array, the document may or may not match. I think the HashSet may be to blame, but I'm not sure - that at least seems to be where the non-determinism is coming from."
"LUCENE-1099","DESIGN_DEFECT","IMPROVEMENT","Making Tokenizer.reset(Reader) public","In order to implement reusableTokenStream and be able to reset a Tokenizer, Tokenizer defines a reset(Reader) method. The problem is that this method is protected. I need to call this reset(Reader) method without having to know in advance what will be the type of the Tokenizer (I plan to have several).
I noticed that almost all Tokenizer extensions define this method as public, and I wonder if this can be changed for Tokenizer also (I can't simply create my general Tokenizer extension and inherit from it because I want to use StandardTokenizer as well). "
"LUCENE-1186","IMPROVEMENT","IMPROVEMENT","[PATCH] Clear ThreadLocal instances in close()","As already found out in LUCENE-436, there seems to be a garbage collection problem with ThreadLocals at certain constellations, resulting in an OutOfMemoryError.
The resolution there was to remove the reference to the ThreadLocal value when calling the close() method of the affected classes (see FieldsReader and TermInfosReader).
For Java < 5.0, this can effectively be done by calling threadLocal.set(null); for Java >= 5.0, we would call threadLocal.remove()

Analogously, this should be done in *any* class which creates ThreadLocal values

Right now, two classes of the core API make use of ThreadLocals, but do not properly remove their references to the ThreadLocal value
1. org.apache.lucene.index.SegmentReader
2. org.apache.lucene.analysis.Analyzer

For SegmentReader, I have attached a simple patch.
For Analyzer, there currently is no patch because Analyzer does not provide a close() method (future to-do?)

"
"LUCENE-917","DOCUMENTATION","","Javadoc improvements for Payload class","Some methods in org.apache.lucene.index.Payload don't have javadocs"
"LUCENE-971","IMPROVEMENT","IMPROVEMENT","Create enwiki indexable data as line-per-article rather than file-per-article","Create a line per article rather than a file. Consume with indexLineFile task."
"LUCENE-1705","RFE","","Add deleteAllDocuments() method to IndexWriter","Ideally, there would be a deleteAllDocuments() or clear() method on the IndexWriter

This method should have the same performance and characteristics as:
* currentWriter.close()
* currentWriter = new IndexWriter(..., create=true,...)

This would greatly optimize a delete all documents case. Using deleteDocuments(new MatchAllDocsQuery()) could be expensive given a large existing index.

IndexWriter.deleteAllDocuments() should have the same semantics as a commit(), as far as index visibility goes (new IndexReader opening would get the empty index)

I see this was previously asked for in LUCENE-932, however it would be nice to finally see this added such that the IndexWriter would not need to be closed to perform the ""clear"" as this seems to be the general recommendation for working with an IndexWriter now

deleteAllDocuments() method should:
* abort any background merges (they are pointless once a deleteAll has been received)
* write new segments file referencing no segments

This method would remove one of the final reasons i would ever need to close an IndexWriter and reopen a new one 
"
"LUCENE-1168","BUG","BUG","TermVectors index files can become corrupt when autoCommit=false","Spinoff from this thread:

  http://www.gossamer-threads.com/lists/lucene/java-dev/55951

There are actually 2 separate cases here, both only happening when
autoCommit=false:

  * First issue was caused by LUCENE-843 (sigh): if you add a bunch of
    docs with no term vectors, such that 1 or more flushes happen;
    then you add docs that do have term vectors, the tvx file will not
    have enough entries (= corruption).

  * Second issue was caused by bulk merging of term vectors
    (LUCENE-1120 -- only in trunk) and bulk merging of stored fields
    (LUCENE-1043, in 2.3), and only shows when autoCommit=false, and,
    the bulk merging optimization runs.  In this case, the code that
    reads the rawDocs tries to read too far in the tvx/fdx files (it's
    not really index corruption but rather a bug in the rawDocs
    reading).

"
"LUCENE-794","RFE","IMPROVEMENT","Extend contrib Highlighter to properly support PhraseQuery, SpanQuery,  ConstantScoreRangeQuery","This patch adds a new Scorer class (SpanQueryScorer) to the Highlighter package that scores just like QueryScorer, but scores a 0 for Terms that did not cause the Query hit. This gives 'actual' hit highlighting for the range of SpanQuerys, PhraseQuery, and  ConstantScoreRangeQuery. New Query types are easy to add. There is also a new Fragmenter that attempts to fragment without breaking up Spans.

See http://issues.apache.org/jira/browse/LUCENE-403 for some background.

There is a dependency on MemoryIndex."
"LUCENE-1584","RFE","IMPROVEMENT","Callback for intercepting merging segments in IndexWriter","For things like merging field caches or bitsets, it's useful to
know which segments were merged to create a new segment.

"
"LUCENE-1217","IMPROVEMENT","IMPROVEMENT","use isBinary cached variable instead of instanceof in Field","Field class can hold three types of values, 
See: AbstractField.java  protected Object fieldsData = null; 

currently, mainly RTTI (instanceof) is used to determine the type of the value stored in particular instance of the Field, but for binary value we have mixed RTTI and cached variable ""boolean isBinary"" 

This patch makes consistent use of cached variable isBinary.

Benefit: consistent usage of method to determine run-time type for binary case  (reduces chance to get out of sync on cached variable). It should be slightly faster as well.

Thinking aloud: 
Would it not make sense to maintain type with some integer/byte""poor man's enum"" (Interface with a couple of constants)
code:java{
public static final interface Type{
public static final byte BOOLEAN = 0;
public static final byte STRING = 1;
public static final byte READER = 2;
....
}
}

and use that instead of isBinary + instanceof? "
"LUCENE-1491","BUG","BUG","EdgeNGramTokenFilter stops on tokens smaller then minimum gram size.","If a token is encountered in the stream that is shorter in length than the min gram size, the filter will stop processing the token stream.

Working up a unit test now, but may be a few days before I can provide it. Wanted to get it in the system."
"LUCENE-2021","IMPROVEMENT","IMPROVEMENT","French elision filter should use CharArraySet","French elision filter creates new strings, lowercases them, etc just to check against a Set<String>.
trivial patch to use chararrayset instead."
"LUCENE-1966","IMPROVEMENT","IMPROVEMENT","Arabic Analyzer: Stopwords list needs enhancement","The provided Arabic stopwords list needs some enhancements (e.g. it contains a lot of words that not stopwords, and some cleanup) . patch will be provided with this issue."
"LUCENE-3524","RFE","IMPROVEMENT","Add ""direct"" PackedInts.Reader impl, that reads directly from disk on each get","Spinoff from LUCENE-3518.

If we had a direct PackedInts.Reader impl we could use that instead of
the RandomAccessReaderIterator.
"
"LUCENE-848","RFE","RFE","Add supported for Wikipedia English as a corpus in the benchmarker stuff","Add support for using Wikipedia for benchmarking."
"LUCENE-3042","BUG","BUG","AttributeSource can have an invalid computed state","If you work a tokenstream, consume it, then reuse it and add an attribute to it, the computed state is wrong.
thus for example, clearAttributes() will not actually clear the attribute added.

So in some situations, addAttribute is not actually clearing the computed state when it should.
"
"LUCENE-3485","BUG","BUG","LuceneTaxonomyReader .decRef() may close the inner IR, renderring the LTR in a limbo.","TaxonomyReader which supports ref-counting, has a decRef() method which delegates to an inner IndexReader and calls its .decRef(). The latter may close the reader (if the ref is zeroes) but the taxonomy would remain 'open' which will fail many of its method calls.

Also, the LTR's .close() method does not work in the same manner as IndexReader's - which calls decRef(), and leaves the real closing logic to the decRef(). I believe this should be the right approach for the fix."
"LUCENE-3201","IMPROVEMENT","IMPROVEMENT","improved compound file handling","Currently CompoundFileReader could use some improvements, i see the following problems
* its CSIndexInput extends bufferedindexinput, which is stupid for directories like mmap.
* it seeks on every readInternal
* its not possible for a directory to override or improve the handling of compound files.

for example: it seems if you were impl'ing this thing from scratch, you would just wrap the II directly (not extend BufferedIndexInput,
and add compound file offset X to seek() calls, and override length(). But of course, then you couldnt throw read past EOF always when you should,
as a user could read into the next file and be left unaware.

however, some directories could handle this better. for example MMapDirectory could return an indexinput that simply mmaps the 'slice' of the CFS file.
its underlying bytebuffer etc naturally does bounds checks already etc, so it wouldnt need to be buffered, not even needing to add any offsets to seek(),
as its position would just work.

So I think we should try to refactor this so that a Directory can customize how compound files are handled, the simplest 
case for the least code change would be to add this to Directory.java:

{code}
  public Directory openCompoundInput(String filename) {
    return new CompoundFileReader(this, filename);
  }
{code}

Because most code depends upon the fact compound files are implemented as a Directory and transparent. at least then a subclass could override...
but the 'recursion' is a little ugly... we could still label it expert+internal+experimental or whatever.
"
"LUCENE-1800","IMPROVEMENT","IMPROVEMENT","QueryParser should use reusable token streams","Just like indexing, the query parser should use reusable token streams"
"LUCENE-891","DOCUMENTATION","TASK","A number of documentation fixes for the search package summary","Improves readability and clarity, corrects some basic English, makes some example text even more clear, and repairs typos."
"LUCENE-2000","RFE","TASK","Use covariant clone() return types","*Paul Cowan wrote in LUCENE-1257:*

OK, thought I'd jump in and help out here with one of my Java 5 favourites. Haven't seen anyone discuss this, and don't believe any of the patches address this, so thought I'd throw a patch out there (against SVN HEAD @ revision 827821) which uses Java 5 covariant return types for (almost) all of the Object#clone() implementations in core. 
i.e. this:

public Object clone() {
changes to:
public SpanNotQuery clone() {

which lets us get rid of a whole bunch of now-unnecessary casts, so e.g.

if (clone == null) clone = (SpanNotQuery) this.clone();
becomes
if (clone == null) clone = this.clone();

Almost everything has been done and all downcasts removed, in core, with the exception of

Some SpanQuery stuff, where it's assumed that it's safe to cast the clone() of a SpanQuery to a SpanQuery - this can't be made covariant without declaring ""abstract SpanQuery clone()"" in SpanQuery itself, which breaks those SpanQuerys that don't declare their own clone() 
Some IndexReaders, e.g. DirectoryReader - we can't be more specific than changing .clone() to return IndexReader, because it returns the result of IndexReader.clone(boolean). We could use covariant types for THAT, which would work fine, but that didn't follow the pattern of the others so that could be a later commit. 
Two changes were also made in contrib/, where not making the changes would have broken code by trying to widen IndexInput#clone() back out to returning Object, which is not permitted. contrib/ was otherwise left untouched.

Let me know what you think, or if you have any other questions."
"LUCENE-1007","RFE","IMPROVEMENT","Flexibility to turn on/off any flush triggers","See discussion at http://www.gossamer-threads.com/lists/lucene/java-dev/53186

Provide the flexibility to turn on/off any flush triggers - ramBufferSize, maxBufferedDocs and maxBufferedDeleteTerms. One of ramBufferSize and maxBufferedDocs must be enabled."
"LUCENE-943","RFE","RFE","ComparatorKey in Locale based sorting","This is a reply/follow-up on Chris Hostetter's message on Lucene developers list (aug 2006):
http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200608.mbox/%3cPine.LNX.4.58.0608211050330.5081@hal.rescomp.berkeley.edu%3e

> perhaps it would be worthwhile for comparatorStringLocale to convert the String[] it gets back from FieldCache.DEFAULT.getStrings to a new CollationKey[]? or maybe even for FieldCache.DEFAULT.getStrings to be deprecated, and replaced with a FieldCache.DEFAULT.getCollationKeys(reader,field,Collator)?

I think the best is to keep the default behavior as it is today. There is a cost of building caches for sort fields which I think not everyone wants. However for some international production environments there are indeed possible performance gains in comparing precalculated keys instead of comparing strings with rulebased collators.

Since Lucene's Sort architecture is pluggable it is easy to create a custom locale-based comparator, which utilizes the built-in caching/warming mechanism of FieldCache, and may be used in SortField constructor.

I'm not sure whether there should be classes for this in Lucene core or not, but it could be nice to have the option of performance vs. memory consumption in localized sorting without having to use additional jars.

"
"LUCENE-789","BUG","BUG","Custom similarity is ignored when using MultiSearcher","Symptoms:
I am using Searcher.setSimilarity() to provide a custom similarity that turns off tf() factor. However, somewhere along the way the custom similarity is ignored and the DefaultSimilarity is used. I am using MultiSearcher and BooleanQuery.

Problem analysis:
The problem seems to be in MultiSearcher.createWeight(Query) method. It creates an instance of CachedDfSource but does not set the similarity. As the result CachedDfSource provides DefaultSimilarity to queries that use it.

Potential solution:
Adding the following line:
    cacheSim.setSimilarity(getSimilarity());
after creating an instance of CacheDfSource (line 312) seems to fix the problem. However, I don't understand enough of the inner workings of this class to be absolutely sure that this is the right thing to do.

"
"LUCENE-3662","RFE","RFE","extend LevenshteinAutomata to support transpositions as primitive edits","This would be a nice improvement for spell correction: currently a transposition counts as 2 edits,
which means users of DirectSpellChecker must use larger values of n (e.g. 2 instead of 1) and 
larger priority queue sizes, plus some sort of re-ranking with another distance measure for good results.

Instead if we can integrate ""chapter 7"" of http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652 
then you can just build an alternative DFA where a transposition is only a single edit 
(http://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)

According to the benchmarks in the original paper, the performance for LevT looks to be very similar to Lev.

Support for this is now in moman (https://bitbucket.org/jpbarrette/moman/) thanks to Jean-Philippe 
Barrette-LaPierre.
"
"LUCENE-274","RFE","IMPROVEMENT","[PATCH] to store binary fields with compression","hi all,

as promised here is the enhancement for the binary field patch with optional
compression. The attachment includes all necessary diffs based on the latest
version from CVS. There is also a small junit test case to test the core
functionality for binary field compression. The base implementation for binary
fields where this patch relies on, can be found in patch #29370. The existing
unit tests pass fine.

For testing binary fields and compression, I'm creating an index from 2700 plain
text files (avg. 6kb per file) and store all file content within that index
without using compression. The test was created using the IndexFiles class from
the demo distribution. Setting up the index and storing all content without
compression took about 60 secs and the final index size was 21 MB. Running the
same test, switching compression on, the time to index increase to 75 secs, but
the final index size shrinks to 13 MB. This is less than the plain text files
them self need in the file system (15 MB)

Hopefully this patch helps people dealing with huge index and want to store more
than just 300 bytes per document to display a well formed summary.

regards
Bernhard"
"LUCENE-820","BUG","BUG","SegmentReader.setNorm can fail to remove separate norms file, on Windows","
While working through LUCENE-710 I hit this bug: on Windows
only, when SegmentReader.setNorm is called, but separate norms
(_X_N.sY) had already been previously saved, then, on closing the
reader, we will write the next gen separate norm file correctly
(_X_N+1.sY) but fail to delete the current one.

It's quite minor because the next writer to touch the index will
remove the stale file.

This is because the Norm class still holds the IndexInput open when
the reader commits."
"LUCENE-2973","BUILD_SYSTEM","IMPROVEMENT","Source distribution packaging targets should make a tarball from ""svn export""","Instead of picking and choosing which stuff to include from a local working copy, Lucene's dist-src/package-tgz-src target and Solr's package-src target should simply perform ""svn export"" with the same revision and URL as the local working copy."
"LUCENE-1133","RFE","IMPROVEMENT","WikipediaTokenizer needs a way of not tokenizing certain parts of the text","It would be nice if the WikipediaTokenizer had a way of, via a flag, leaving categories, links, etc. as single tokens (or at least some parts of them)

Thus, if we came across [[Category:My Big Idea]] there would be a way of outputting, as a single token ""My Big Idea"".  

Optionally, it would be good to output both ""My Big Idea"" and the individual tokens as well.

I am not sure of how to do this in JFlex, so any insight would be appreciated."
"LUCENE-2150","BUILD_SYSTEM","IMPROVEMENT","Build should enable unchecked warnings in javac","Just have to uncomment this:
{code}
        <!-- for generics in Java 1.5: -->
        <!--<compilerarg line=""-Xlint:unchecked""/>-->
{code}
in common-build.xml.  Test & core are clean, but contrib still has many warnings.  Either we fix contrib with this issue, or, conditionalize this (anyone anty who can do this?) so contrib is off until we can fix it."
"LUCENE-2630","BUILD_SYSTEM","TASK","make the build more friendly to apache harmony","as part of improved testing, i thought it would be a good idea to make the build (ant test) more friendly
to working under apache harmony.

i'm not suggesting we de-optimize code for sun jvms or anything crazy like that, only use it as a tool.

for example:
* bugs in tests/code: for example i found a test that expected ArrayIOOBE 
  when really the javadoc contract for the method is just IOOBE... it just happens to
  pass always on sun jvm because thats the implementation it always throws.
* better reproduction of bugs: for example [2 months out of the year|http://en.wikipedia.org/wiki/Unusual_software_bug#Phase_of_the_Moon_bug]
  it seems TestQueryParser fails with thai locale in a difficult-to-reproduce way.
  but i *always* get similar failures like this with harmony for this test class.
* better stability and portability: we should try (if reasonable) to avoid depending
  upon internal details. the same kinds of things that fail in harmony might suddenly
  fail in a future sun jdk. because its such a different impl, it brings out a lot of interesting stuff.

at the moment there are currently a lot of failures, I think a lot might be caused by this: http://permalink.gmane.org/gmane.comp.java.harmony.devel/39484
"
"LUCENE-1975","CLEANUP","TASK","Remove deprecated SpanQuery.getTerms() and generify Query.extractTerms(Set<Term>)",""
"LUCENE-2979","CLEANUP","IMPROVEMENT","Simplify configuration API of contrib Query Parser","The current configuration API is very complicated and inherit the concept used by Attribute API to store token information in token streams. However, the requirements for both (QP config and token stream) are not the same, so they shouldn't be using the same thing.

I propose to simplify QP config and make it less scary for people intending to use contrib QP. The task is not difficult, it will just require a lot of code change and figure out the best way to do it. That's why it's a good candidate for a GSoC project.

I would like to hear good proposals about how to make the API more friendly and less scaring :)"
"LUCENE-1119","IMPROVEMENT","IMPROVEMENT","Optimize TermInfosWriter.add","I found one more optimization, in how terms are written in
TermInfosWriter.  Previously, each term required a new Term() and a
new String().  Looking at the cpu time (using YourKit), I could see
this was adding a non-trivial cost to flush() when indexing Wikipedia.

I changed TermInfosWriter.add to accept char[] directly, instead.

I ran a quick test building first 200K docs of Wikipedia.  With this
fix it took 231.31 sec (best of 3) and without the fix it took 236.05
sec (best of 3) = ~2% speedup.
"
"LUCENE-2593","BUG","BUG","disk full can cause index corruption in certain cases","Robert uncovered this nasty bug, in adding more randomness to
oal.index tests...

I got a standalone test to show the issue; the corruption path is
as follows:

  * The merge hits an initial exception (eg disk full when merging the
    postings).

  * In handling this exception, IW closes all the sub-readers,
    suppressing any further exceptions.

  * If one of these sub-readers has pending deletions, which happens
    if readers are pooled in IW, it will flush them.  If that flush
    hits a 2nd exception (eg disk full), then SegmentReader
    [incorrectly] leaves the SegmentInfo's delGen advanced by 1,
    referencing a corrupt file, yet the SegmentReader is still
    forcefully closed.

  * If enough disk frees up such that a later IW.commit/close
    succeeds, the resulting segments file will reference an invalid
    deletions file.
"
"LUCENE-2900","RFE","IMPROVEMENT","make applying deletes optional when pulling a new NRT reader","Usually when you pull an NRT reader, you want all deletes to be applied.

But in some expert cases you may not need it (eg you just want to validate that the doc was indexed).  Since it's costly to apply deletes, and trivial to add this boolean (we already have a boolean internally), I think we should add it.

The deletes are still buffered, and you can always later pull another reader (for ""real"" searching) with deletes applied."
"LUCENE-3773","IMPROVEMENT","IMPROVEMENT","small improvements to DWPTThreadPool","While working on another issue I cleaned up DWTPThreadPool a little, fixed some naming issues and fixed some todos... patch is coming soon..."
"LUCENE-2136","IMPROVEMENT","IMPROVEMENT","MultiReader should not use PQ for its Term/sEnum if it has only 1 reader","Related to LUCENE-2130....

Even though we've switched to segment-based searching, there are still times when the Term/sEnum is used against the top-level reader.  I think Solr does this, and from LUCENE-2130, certain rewrite modes of MTQ will do this as well.

Currently, on an optimized index, MTQ is still using a PQ to present the terms, which is silly because this just adds a sizable amount of overhead.  In such cases we should simply delecate to the single segment.

Note that the single segment can have deletions, and we should still delegate.  Ie, the index need not be optimized, just have a single segment."
"LUCENE-2636","RFE","RFE","Create ChainingCollector","ChainingCollector allows chaining a bunch of Collectors, w/o them needing to know or care about each other, and be passed into Lucene's search API, since it is a Collector on its own. It is a convenient, yet useful, class. Will post a patch w/ it shortly."
"LUCENE-1561","DOCUMENTATION","IMPROVEMENT","Maybe rename Field.omitTf, and strengthen the javadocs","Spinoff from here:

  http://www.nabble.com/search-problem-when-indexed-using-Field.setOmitTf()-td22456141.html

Maybe rename omitTf to something like omitTermPositions, and make it clear what queries will silently fail to work as a result."
"LUCENE-1642","BUG","BUG","IndexWriter.addIndexesNoOptimize ignores the compound file setting of the destination index","IndexWriter.addIndexesNoOptimize(Directory[]) ignores the compound file setting of the destination index. It is using the compound file flags of segments in the source indexes.
This sometimes causes undesired increase of the number of files in the destination index when non-compound file indexes are added until merge kicks in."
"LUCENE-365","IMPROVEMENT","BUG","[PATCH] Performance improvement to DisjunctionSumScorer","A recent profile of the new BooleanScorer2 showed that 
quite a bit of CPU time is spent in the advanceAfterCurrent method 
of DisjunctionScorer, and in the PriorityQueue of scorers that 
is used there. 
 
This patch reduces the internal overhead of DisjunctionScorer 
to about 70% of the current one (ie. 30% saving in cpu time). 
It also reduces the number of calls to the subscorers, but 
that was not measured. 
 
To get this, it was necessary to specialize the PriorityQueue 
for a Scorer and to add move some code fragments from DisjunctionScorer 
to this specialized queue."
"LUCENE-1863","DOCUMENTATION","BUG","SnowballAnalyzer has a link to net.sf (a package that is empty and needs to be removed).","need to remove net.sf and points to org.tartarus.snowball.ext. Doesn't work as a link though, so I'll also remove the @link to lose the javadoc error and broken link."
"LUCENE-2132","DOCUMENTATION","BUG","the demo application does not work as of 3.0","the demo application does not work. QueryParser needs a Version argument.

While I am here, remove @author too"
"LUCENE-1001","RFE","RFE","Add Payload retrieval to Spans","It will be nice to have access to payloads when doing SpanQuerys.

See http://www.gossamer-threads.com/lists/lucene/java-dev/52270 and http://www.gossamer-threads.com/lists/lucene/java-dev/51134

Current API, added to Spans.java is below.  I will try to post a patch as soon as I can figure out how to make it work for unordered spans (I believe I have all the other cases working).

{noformat}
 /**
   * Returns the payload data for the current span.
   * This is invalid until {@link #next()} is called for
   * the first time.
   * This method must not be called more than once after each call
   * of {@link #next()}. However, payloads are loaded lazily,
   * so if the payload data for the current position is not needed,
   * this method may not be called at all for performance reasons.<br>
   * <br>
   * <p><font color=""#FF0000"">
   * WARNING: The status of the <b>Payloads</b> feature is experimental.
   * The APIs introduced here might change in the future and will not be
   * supported anymore in such a case.</font>
   *
   * @return a List of byte arrays containing the data of this payload
   * @throws IOException
   */
  // TODO: Remove warning after API has been finalized
  List/*<byte[]>*/ getPayload() throws IOException;

  /**
   * Checks if a payload can be loaded at this position.
   * <p/>
   * Payloads can only be loaded once per call to
   * {@link #next()}.
   * <p/>
   * <p><font color=""#FF0000"">
   * WARNING: The status of the <b>Payloads</b> feature is experimental.
   * The APIs introduced here might change in the future and will not be
   * supported anymore in such a case.</font>
   *
   * @return true if there is a payload available at this position that can be loaded
   */
  // TODO: Remove warning after API has been finalized
  public boolean isPayloadAvailable();
{noformat}"
"LUCENE-3299","REFACTORING","TASK","refactoring of Similarity.sloppyFreq() and Similarity.scorePayload","Currently these are top-level, but they only affect the SloppyDocScorer.
So it makes more sense to put these into the SloppyDocScorer api, this gives you additional flexibility
(e.g. combining payloads with CSF or whatever the hell you want to do), and is cleaner.

Furthermore, there are the following confusing existing issues:
* scorePayload should take bytesref
* PayloadTermScorer passes a *null* byte[] array to the sim if there are no payloads. I don't think it should do this, and its inconsistent with PayloadNearQuery, which does not do this. Its an undocumented conditional you need to have in the scoring algorithm which we should remove.
* there is an unused constant for scorepayload (NO_DOC_ID_PROVIDED), which is a documented, but never used anywhere. I think we should remove this conditional too, because its not possible to have a payload without a docid, and we shouldn't be passing fake document ids (-1) to our scoring APIs anyway.
"
"LUCENE-3871","TEST","BUG","Stack traces from failed tests are messed up on ANT 1.7.x",""
"LUCENE-1530","RFE","RFE","Support inclusive/exclusive for TrieRangeQuery/-Filter, remove default trie variant setters/getters","TrieRangeQuery/Filter is missing one thing: Ranges that have exclusive bounds. For TrieRangeQuery this may not be important for ranges on long or Date (==long) values (because [1..5] is the same like ]0..6[ or ]0..5]). This is not so simple for doubles because you must add/substract 1 from the trie encoded unsigned long.

To be conform with the other range queries, I will submit a patch that has two additional boolean parameters in the ctors to support inclusive/exclusive ranges for both ends. Internally it will be implemented using TrieUtils.incrementTrieCoded/decrementTrieCoded() but makes life simplier for double ranges (a simple exclusive replacement for the floating point range [0.0..1.0] is not possible without having the underlying unsigned long).

In December, when trie contrib was included (LUCENE-1470), 3 trie variants were supplied by TrieUtils. For new APIs a statically configureable default Trie variant does not conform to an API we want in Lucene (currently we want to deprecate all these static setters/getters). The important thing: It does not make code shorter or easier to understand, its more error prone. Before release of 2.9 it is a good time to remove the default trie variant and always force the parameter in TrieRangeQuery/Filter. It is better to choose the variant in the application and do not automatically manage it.

As Lucene 2.9 was not yet released, I will change the ctors and not preserve the old ones."
"LUCENE-1116","IMPROVEMENT","IMPROVEMENT","contrib.benchmark.quality package improvements","Few fixes and improvements for the search quality benchmark package:
- flush report and logger at the end (otherwise long submission reports might miss last lines).
- add run-tag-name to submission report (API change).
- add control over max-#queries to run (useful at debugging a quality evaluation setup).
- move control over max-docs-to-retrieve from benchmark constructor to a setter method (API change).
- add computation of Mean Reciprocal Rank (MRR) in QualityStats.
- QualityStats fixed to not fail if there are no results to average.
- Add a TREC queries reader adequate for the 1MQ track (track started 2007).

All tests pass, will commit this in 1-2 days if there is no objection.
"
"LUCENE-2104","BUG","BUG","IndexWriter.unlock does does nothing if NativeFSLockFactory is used","If NativeFSLockFactory is used, IndexWriter.unlock will return, silently doing nothing. The reason is that NativeFSLockFactory's makeLock always creates a new NativeFSLock. NativeFSLock's release first checks if its lock is not null. However, only if obtain() is called, that lock is not null. So release actually does nothing, and so IndexWriter.unlock does not delete the lock, or fail w/ exception.
This is only a problem in NativeFSLock, and not in other Lock implementations, at least as I was able to see.

Need to think first how to reproduce in a test, and then fix it. I'll work on it."
"LUCENE-3650","REFACTORING","TASK","move o.a.l.index.codecs.* -> o.a.l.codecs.*","These package names are getting pretty long, e.g.:

org.apache.lucene.index.codecs.lucene40.values.XXXXYYYY

I think we should move it to just the codecs package now while it won't cause anyone any trouble."
"LUCENE-2825","IMPROVEMENT","IMPROVEMENT","FSDirectory.open should return MMap on 64-bit Solaris","MMap is ~ 30% faster than NIOFS on this platform."
"LUCENE-2590","RFE","IMPROVEMENT","Enable access to the freq information in a Query's sub-scorers","The ability to gather more details than just the score, of how a given
doc matches the current query, has come up a number of times on the
user's lists.  (most recently in the thread ""Query Match Count"" by
Ryan McV on java-user).

EG if you have a simple TermQuery ""foo"", on each hit you'd like to
know how many times ""foo"" occurred in that doc; or a BooleanQuery +foo
+bar, being able to separately see the freq of foo and bar for the
current hit.

Lucene doesn't make this possible today, which is a shame because
Lucene in fact does compute exactly this information; it's just not
accessible from the Collector.
"
"LUCENE-2098","IMPROVEMENT","IMPROVEMENT","make BaseCharFilter more efficient in performance","Performance degradation in Solr 1.4 was reported. See:

http://www.lucidimagination.com/search/document/43c4bdaf5c9ec98d/html_stripping_slower_in_solr_1_4

The inefficiency has been pointed out in BaseCharFilter javadoc by Mike:

{panel}
NOTE: This class is not particularly efficient. For example, a new class instance is created for every call to addOffCorrectMap(int, int), which is then appended to a private list. 
{panel}
"
"LUCENE-986","REFACTORING","IMPROVEMENT","Refactor segmentInfos from IndexReader into its subclasses","References to segmentInfos in IndexReader cause different kinds of problems
for subclasses of IndexReader, like e. g. MultiReader.

Only subclasses of IndexReader that own the index directory, namely 
SegmentReader and MultiSegmentReader, should have a SegmentInfos object
and be able to access it.

Further information:
http://www.gossamer-threads.com/lists/lucene/java-dev/51808
http://www.gossamer-threads.com/lists/lucene/java-user/52460

A part of the refactoring work was already done in LUCENE-781"
"LUCENE-2674","IMPROVEMENT","IMPROVEMENT","improve how MTQs interact with the terms dict cache","Some small improvements:

  * Adds a TermsEnum.cacheCurrentTerm ""hint"" (codec can make this a no-op)

  * Removes the FTE.useTermsCache

  * Changes MTQ's TermCollector API to accept the TermsEnum so collectors can eg call .docFreq directly

  * Adds expert ctor to TermQuery allowing you to pass in the docFreq"
"LUCENE-713","DOCUMENTATION","BUG","File Formats Documentation is not correct for Term Vectors","From Samir Abdou on the dev mailing list:

Hi, 

There is an inconsistency between the files format page (from Lucene
website) and the source code. It concerns the positions and offsets of term
vectors. It seems that documentation (website) is not up to date. According
to the file format page, offsets and positions are not stored! Is that
correct?

Many thanks,

Samir
-----
Indeed, in the file formats term vectors section it doesn't talk about the storing of position and offset info.
"
"LUCENE-3760","REFACTORING","IMPROVEMENT","Cleanup DR.getCurrentVersion/DR.getUserData/DR.getIndexCommit().getUserData()","Spinoff from Ryan's dev thread ""DR.getCommitUserData() vs DR.getIndexCommit().getUserData()""... these methods are confusing/dups right now."
"LUCENE-1603","IMPROVEMENT","IMPROVEMENT","Changes for TrieRange in FilteredTermEnum and MultiTermQuery improvement","This is a patch, that is needed for the MultiTermQuery-rewrite of TrieRange (LUCENE-1602):
- Make the private members protected, to have access to them from the very special TrieRangeTermEnum 
- Fix a small inconsistency (docFreq() now only returns a value, if a valid term is existing)
- Improvement of MultiTermFilter.getDocIdSet to return DocIdSet.EMPTY_DOCIDSET, if the TermEnum is empty (less memory usage) and faster.
- Add the getLastNumberOfTerms() to MultiTermQuery for statistics on different multi term queries and how may terms they affect, using this new functionality, the improvement of TrieRange can be shown (extract from test case there, 10000 docs index, long values):
{code}
[junit] Average number of terms during random search on 'field8':
[junit]  Trie query: 244.2
[junit]  Classical query: 3136.94
[junit] Average number of terms during random search on 'field4':
[junit]  Trie query: 38.3
[junit]  Classical query: 3018.68
[junit] Average number of terms during random search on 'field2':
[junit]  Trie query: 18.04
[junit]  Classical query: 3539.42
{code}

All core tests pass.
"
"LUCENE-3233","RFE","IMPROVEMENT","HuperDuperSynonymsFilter","The current synonymsfilter uses a lot of ram and cpu, especially at build time.

I think yesterday I heard about ""huge synonyms files"" three times.

So, I think we should use an FST-based structure, sharing the inputs and outputs.
And we should be more efficient with the tokenStream api, e.g. using save/restoreState instead of cloneAttributes()
"
"LUCENE-1426","RFE","IMPROVEMENT","Next steps towards flexible indexing","In working on LUCENE-1410 (PFOR compression) I tried to prototype
switching the postings files to use PFOR instead of vInts for
encoding.

But it quickly became difficult.  EG we currently mux the skip data
into the .frq file, which messes up the int blocks.  We inline
payloads with positions which would also mess up the int blocks.
Skipping offsets and TermInfo offsets hardwire the file pointers of
frq & prox files yet I need to change these to block + offset, etc.

Separately this thread also started up, on how to customize how Lucene
stores positional information in the index:

  http://www.gossamer-threads.com/lists/lucene/java-user/66264

So I decided to make a bit more progress towards ""flexible indexing""
by first modularizing/isolating the classes that actually write the
index format.  The idea is to capture the logic of each (terms, freq,
positions/payloads) into separate interfaces and switch the flushing
of a new segment as well as writing the segment during merging to use
the same APIs.
"
"LUCENE-2502","CLEANUP","IMPROVEMENT","Remove some unused code in Surround query parser",""
"LUCENE-2183","RFE","IMPROVEMENT","Supplementary Character Handling in CharTokenizer","CharTokenizer is an abstract base class for all Tokenizers operating on a character level. Yet, those tokenizers still use char primitives instead of int codepoints. CharTokenizer should operate on codepoints and preserve bw compatibility. "
"LUCENE-3307","RFE","IMPROVEMENT","don't require an analyzer, if all fields are NOT_ANALYZED","This seems wierd, if you analyze only NOT_ANALYZED fields, you must have an analyzer (null will not work)
because documentsinverter wants it for things like offsetGap"
"LUCENE-2833","BUILD_SYSTEM","IMPROVEMENT","upgrade contrib/ant's tidy.jar","contrib/ant uses a Tidy.jar that also includes classes in org.w3c.dom, org.xml.sax, etc.

This is no problem if you are an ant user, but if you are an IDE user you need to carefully configure the order of your classpath or things will not compile, as these will override the ones in the Solr libs, for example.

The solution is to upgrade the tidy.jar to the newest one that only includes org.w3c.tidy and doesn't cause these problems."
"LUCENE-326","BUG","BUG","IndexWriter.addIndexes results in java.lang.OutOfMemoryError","I'm re-opening a bug I logged previously. My previous bug report has 
disappeared. 

Issue: IndexWriter.addIndexes results in java.lang.OutOfMemoryError for large 
merges.

Until this writing, I've been merging successfully only through repetition, 
i.e. I keep repeating merges until a success. As my index size has grown, my 
success rate has steadily declined. I've reached the point where merges now 
fail 100% of the time. I can't merge.

My tests indicate the threshold is ~30GB on P4/800MB VM with 6 indexes. I have 
repeated my tests on many different machines (not machine dependent). I have 
repeated my test using local and attached storage devices (not storage 
dependent).

For what its worth, I believe the exception occurs entirely during the optimize 
process which is called implicitly after the merge. I say this because each 
time it appears the correct amount of bytes are written to the new index. Is it 
possible to decouple the merge and optimize processes?


The code snippet follows. I can send you the class file and 120GB data set. Let 
me know how you want it.

>>>>> code sample >>>>>

Directory[] sources = new Directory[paths.length];
...

Directory dest = FSDirectory.getDirectory( path, true);
IndexWriter writer = new IndexWriter( dest, new TermAnalyzer( 
StopWords.SEARCH_MAP), true);

writer.addIndexes( sources);
writer.close();"
"LUCENE-3439","BUG","BUG","add checks/asserts if you search across a closed reader","if you try to search across a closed reader (and/or searcher too),
there are no checks, not even assertions statements.

this results in crazy scary stacktraces deep inside places like FSTs/various term dictionary implementations etc.

In some situations, depending on codec, you wont even get an error (i'm sure its fun when you try to retrieve the stored fields!)
"
"LUCENE-1441","BUG","BUG","KeywordTokenizer does not set start/end offset of the Token it produces","I think just adding these two lines in the next(Token) method is the right fix:

           reusableToken.setStartOffset(0);
           reusableToken.setEndOffset(upto);

I don't think this is a back compat issue because the start/end offset are now meaningless since they will inherit whatever the reusable token had previously been used for."
"LUCENE-2222","BUG","BUG","FixedIntBlockIndexInput.Reader does not initialise 'pending' int array","The FixedIntBlockIndexInput.Reader.pending int array is not initialised. As a consequence, the FixedIntBlockIndexInput.Reader#next() method returns always 0.

A call to FixedIntBlockIndexInput.Reader#blockReader.readBlock() during the Reader initialisation may solve the issue (to be tested)."
"LUCENE-399","RFE","IMPROVEMENT","WordListLoader.java should be able to read stopwords from a Reader","WordListLoader should be able to read the stopwords from a Reader.

This would (for example) allow stopword lists to be stored as a resource in the
jar file of a Lucene application.

Diff is attached."
"LUCENE-214","DOCUMENTATION","IMPROVEMENT","[PATCH] Field.toString could be more helpful","org.apache.lucene.document.Field.toString defaults to using Object.toString
for some sensible fields. e.g. !isStored && isIndexed && !isTokenized
fields. This makes debugging slightly more difficult than is really needed.

Please find pasted below possible alternative:

 /** Prints a Field for human consumption. */
  public final String toString() {
  	StringBuffer result = new StringBuffer();
  	if (isStored) {
  		if (isIndexed) {
  			if (isTokenized) {
  				result.append(""Text"");
  			} else {
  				result.append(""Keyword"");
  			}
  		} else {
			// XXX warn on tokenized not indexed?
  			result.append(""Unindexed"");
  		}
  	} else {
  		if (isIndexed) {
  			if (isTokenized) {
  				result.append(""Unstored"");
  			} else {
  				result.append(""UnstoredUntokenized"");
  			}
  		} else {
			result.append(""Nonsense_UnstoredUnindexed"");
  		}
  	}
  	
  	result.append('<');
  	result.append(name);
  	result.append(':');
  	if (readerValue != null) {
  		result.append(readerValue.toString());
  	} else {
  		result.append(stringValue);
  	}
  	result.append('>');
  	return result.toString();
  }


NB Im working against CVS HEAD"
"LUCENE-1695","REFACTORING","IMPROVEMENT","Update the Highlighter to use the new TokenStream API",""
"LUCENE-2771","CLEANUP","IMPROVEMENT","Remove norms() support from non-atomic IndexReaders","Spin-off from LUCENE-2769:
Currently all IndexReaders support norms(), but the core of Lucene never uses it and its even dangerous because of memory usage. We should do the same like with MultiFields and factor it out and throw UOE on non-atomic readers.

The SlowMultiReaderWrapper can then manage the norms. Also ParallelReader needs to be fixed."
"LUCENE-584","REFACTORING","IMPROVEMENT","Decouple Filter from BitSet","{code}
package org.apache.lucene.search;

public abstract class Filter implements java.io.Serializable 
{
  public abstract AbstractBitSet bits(IndexReader reader) throws IOException;
}

public interface AbstractBitSet 
{
  public boolean get(int index);
}

{code}

It would be useful if the method =Filter.bits()= returned an abstract interface, instead of =java.util.BitSet=.

Use case: there is a very large index, and, depending on the user's privileges, only a small portion of the index is actually visible.
Sparsely populated =java.util.BitSet=s are not efficient and waste lots of memory. It would be desirable to have an alternative BitSet implementation with smaller memory footprint.

Though it _is_ possibly to derive classes from =java.util.BitSet=, it was obviously not designed for that purpose.
That's why I propose to use an interface instead. The default implementation could still delegate to =java.util.BitSet=.

"
"LUCENE-3063","REFACTORING","TASK","factor CharTokenizer/CharacterUtils into analyzers module","Currently these analysis components are in the lucene core, but should really
be .util in the analyzers module.

Also, with MockTokenizer extending Tokenizer directly, we can add some additional
checks in the future to try to ensure our consumers are being good consumers (e.g. calling reset).

This is mentioned in http://wiki.apache.org/lucene-java/TestIdeas, I didn't implement it here yet,
this is just the factoring. I think we should try to do this before LUCENE-3040.
"
"LUCENE-1375","RFE","IMPROVEMENT","add IndexCommit.getTimestamp method","Convenience method for getDirectory().fileModified(getSegmentsFileName())."
"LUCENE-1543","IMPROVEMENT","IMPROVEMENT","Field specified norms in MatchAllDocumentsScorer ","This patch allows for optionally setting a field to use for norms factoring when scoring a MatchingAllDocumentsQuery.

From the test case:
{code:java}
.
    RAMDirectory dir = new RAMDirectory();
    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
    iw.setMaxBufferedDocs(2);  // force multi-segment
    addDoc(""one"", iw, 1f);
    addDoc(""two"", iw, 20f);
    addDoc(""three four"", iw, 300f);
    iw.close();

    IndexReader ir = IndexReader.open(dir);
    IndexSearcher is = new IndexSearcher(ir);
    ScoreDoc[] hits;

    // assert with norms scoring turned off

    hits = is.search(new MatchAllDocsQuery(), null, 1000).scoreDocs;
    assertEquals(3, hits.length);
    assertEquals(""one"", ir.document(hits[0].doc).get(""key""));
    assertEquals(""two"", ir.document(hits[1].doc).get(""key""));
    assertEquals(""three four"", ir.document(hits[2].doc).get(""key""));

    // assert with norms scoring turned on

    MatchAllDocsQuery normsQuery = new MatchAllDocsQuery(""key"");
    assertEquals(3, hits.length);
//    is.explain(normsQuery, hits[0].doc);
    hits = is.search(normsQuery, null, 1000).scoreDocs;

    assertEquals(""three four"", ir.document(hits[0].doc).get(""key""));    
    assertEquals(""two"", ir.document(hits[1].doc).get(""key""));
    assertEquals(""one"", ir.document(hits[2].doc).get(""key""));
{code}"
"LUCENE-1174","DOCUMENTATION","BUG","outdated information in Analyzer javadoc","I'm sure you find more ways to improve the javadoc, so feel free to change and extend my patch."
"LUCENE-476","RFE","IMPROVEMENT","BooleanQuery add public method that returns number of clauses this query","BooleanQuery add public method getClausesCount() that returns number of clauses this query.

current ways of getting clauses count are:
1).
 int clausesCount  = booleanQuery.getClauses().length;

or 

"
"LUCENE-1553","BUG","BUG","ConcurrentScheduleManager.addMyself() has wrong inted","This method has the wrong index for the 'size' variable, I think it should b allInstances.size.

{code:java}
private void addMyself() {
    synchronized(allInstances) {
      final int size=0;
      int upto = 0;
      for(int i=0;i<size;i++) {
        final ConcurrentMergeScheduler other = (ConcurrentMergeScheduler) allInstances.get(i);
        if (!(other.closed && 0 == other.mergeThreadCount()))
          // Keep this one for now: it still has threads or
          // may spawn new threads
          allInstances.set(upto++, other);
      }
      allInstances.subList(upto, allInstances.size()).clear();
      allInstances.add(this);
    }
  }
{code}"
"LUCENE-2030","DESIGN_DEFECT","IMPROVEMENT","CachingSpanFilter synchronizing on a none final protected object","CachingSpanFilter and CachingWrapperFilter expose their internal cache via a protected member which is lazily instantiated in the getDocSetId method. The current code yields the chance to double instantiate the cache and internally synchronizes on a protected none final member. My first guess is that this member was exposed for testing purposes so it should rather be changed to package private. 

This patch breaks backwards compat while I guess the cleanup is kind of worth breaking it."
"LUCENE-1791","TEST","TEST","Enhance QueryUtils and CheckHIts to wrap everything they check in MultiReader/MultiSearcher","methods in CheckHits & QueryUtils are in a good position to take any Searcher they are given and not only test it, but also test MultiReader & MultiSearcher constructs built around them"
"LUCENE-298","BUG","BUG","NullPointerExc. when indexing empty field with term vectors","Mark Harwood mentioned this on the user's list. Running the attached code 
you'll get this exception: 
 
Exception in thread ""main"" java.lang.NullPointerException 
	at 
org.apache.lucene.index.TermVectorsReader.clone(TermVectorsReader.java:303) 
	at 
org.apache.lucene.index.SegmentReader.getTermVectorsReader(SegmentReader.java:473) 
	at 
org.apache.lucene.index.SegmentReader.getTermFreqVectors(SegmentReader.java:507) 
	at 
org.apache.lucene.index.SegmentMerger.mergeVectors(SegmentMerger.java:204) 
	at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:94) 
	at 
org.apache.lucene.index.IndexWriter.mergeSegments(IndexWriter.java:618) 
	at 
org.apache.lucene.index.IndexWriter.flushRamSegments(IndexWriter.java:571) 
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:339) 
	at TVBug.main(TVBug.java:16)"
"LUCENE-590","DOCUMENTATION","BUG","Demo HTML parser gives incorrect summaries when title is repeated as a heading","If you have an html document where the title is repeated as a heading at the top of the document, the HTMLParser will return the title as the summary, ignoring everything else that was added to the summary. Instead, it should keep the rest of the summary and chop off the title part at the beginning (essentially the opposite). I don't see any benefit to repeating the title in the summary for any case.

In HTMLParser.jj's getSummary():

    String sum = summary.toString().trim();
    String tit = getTitle();
    if (sum.startsWith(tit) || sum.equals(""""))
      return tit;
    else
      return sum;

change it to: (* denotes a line that has changed)

    String sum = summary.toString().trim();
    String tit = getTitle();
*    if (sum.startsWith(tit))             // don't repeat title in summary
*      return sum.substring(tit.length()).trim();
    else
      return sum;
"
"LUCENE-1256","DOCUMENTATION","IMPROVEMENT","Changes.html formatting improvements","Some improvements to the Changes.html generated by the changes2html.pl script via the 'changes-to-html' ant task:

# Simplified the Simple stylesheet (removed monospace font specification) and made it the default.  The Fancy stylesheet is really hard for me to look at (yellow text on light blue background may provide high contrast with low eye strain, but IMHO it's ugly).
# Moved the monospace style from the Simple stylesheet to a new stylesheet named ""Fixed Width""
# Fixed syntax errors in the Fancy stylesheet, so that it displays as intended.
# Added <span style=""attrib"">  to change attributions.
# In the Fancy and Simple stylesheets, change attributions are colored dark green.
# Now properly handling change attributions in CHANGES.txt that have trailing periods.
# Clicking on an anchor to expand its children now changes the document location to show the children.
# Hovering over anchors now causes a tooltip to be displayed - either ""Click to expand"" or ""Click to collapse"" - the tooltip changes appropriately after a collapse or expansion."
"LUCENE-907","BUILD_SYSTEM","BUG","Demo and contrib jars should contain NOTICE.TXT and LICENSE.TXT","We should include NOTICE.TXT and LICENSE.TXT not only in the core jar but also
in the demo and contrib jars."
"LUCENE-470","TEST","TEST","Refactoring and slight extension of regex testing code.",""
"LUCENE-1675","DOCUMENTATION","IMPROVEMENT","Add a link to the release archive","It would be nice if the [Releases page|http://lucene.apache.org/java/docs/releases.html] contained a link to the release archive at http://archive.apache.org/dist/lucene/java/."
"LUCENE-3572","BUG","BUG","MultiIndexDocValues pretends it can merge sorted sources","Nightly build hit this failure:

{noformat}
ant test-core -Dtestcase=TestSort -Dtestmethod=testReverseSort -Dtests.seed=791b126576b0cfab:-48895c7243ecc5d0:743c683d1c9f7768 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1""

    [junit] Testcase: testReverseSort(org.apache.lucene.search.TestSort):	Caused an ERROR
    [junit] expected:<[CEGIA]> but was:<[ACEGI]>
    [junit] 	at org.apache.lucene.search.TestSort.assertMatches(TestSort.java:1248)
    [junit] 	at org.apache.lucene.search.TestSort.assertMatches(TestSort.java:1216)
    [junit] 	at org.apache.lucene.search.TestSort.testReverseSort(TestSort.java:759)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:523)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:149)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:51)
{noformat}

It's happening in the test for reverse-sort of a string field with DocValues, when the test had gotten SlowMultiReaderWrapper.

I committed a fix to the test to avoid testing this case, but we need a better fix to the underlying bug.

MultiIndexDocValues cannot merge sorted sources (I think?), yet somehow it's pretending it can (in the above test, the three subs had BYTES_FIXED_SORTED type, and the TypePromoter happily claims to merge these to BYTES_FIXED_SORTED; I think MultiIndexDocValues should return null for the sorted source in this case?"
"LUCENE-2480","CLEANUP","IMPROVEMENT","Remove support for pre-3.0 indexes","We should remove support for 2.x (and 1.9) indexes in 4.0. It seems that nothing can be done in 3x because there is no special code which handles 1.9, so we'll leave it there. This issue should cover:
# Remove the .zip indexes
# Remove the unnecessary code from SegmentInfo and SegmentInfos. Mike suggests we compare the version headers at the top of SegmentInfos, in 2.9.x vs 3.0.x, to see which ones can go.
# remove FORMAT_PRE from FieldInfos
# Remove old format from TermVectorsReader

If you know of other places where code can be removed, then please post a comment here.

I don't know when I'll have time to handle it, definitely not in the next few days. So if someone wants to take a stab at it, be my guest.
"
"LUCENE-1414","TEST","BUG","increase maxmemory for unit tests","We have some unit tests that require a fair amount of RAM.  But, sometimes the JRE does not give itself a very large max heap size, by default.  EG on a Mac Pro with 6 GB physical RAM, I see JRE 1.6.0 defaulting to max 80 GB and it always then hits this exception during testing:

    [junit] Testcase: testHugeFile(org.apache.lucene.store.TestHugeRamFile):	Caused an ERROR
    [junit] Java heap space
    [junit] java.lang.OutOfMemoryError: Java heap space
    [junit] 	at java.util.Arrays.copyOf(Arrays.java:2760)
    [junit] 	at java.util.Arrays.copyOf(Arrays.java:2734)
    [junit] 	at java.util.ArrayList.ensureCapacity(ArrayList.java:167)
    [junit] 	at java.util.ArrayList.add(ArrayList.java:351)
    [junit] 	at org.apache.lucene.store.RAMFile.addBuffer(RAMFile.java:69)
    [junit] 	at org.apache.lucene.store.RAMOutputStream.switchCurrentBuffer(RAMOutputStream.java:129)
    [junit] 	at org.apache.lucene.store.RAMOutputStream.writeBytes(RAMOutputStream.java:115)
    [junit] 	at org.apache.lucene.store.TestHugeRamFile.testHugeFile(TestHugeRamFile.java:68)

The fix is simple: add maxmemory=512M into common-build.xml.  I'll commit shortly."
"LUCENE-1158","BUG","BUG","DateTools UTC/GMT mismatch","Post from Antony Bowesman on java-user:

-----

I just noticed that although the Javadocs for Lucene 2.2 state that the dates 
for DateTools use UTC as a timezone, they are actually using GMT.

Should either the Javadocs be corrected or the code corrected to use UTC instead.

-----

I'm attaching a patch that changes the javadoc and will commit it, unless someone knows a reason the javadoc is correct and the code should be changed to UTC. To my understanding, there's no significant difference between UTC and GMT.
"
"LUCENE-904","BUILD_SYSTEM","IMPROVEMENT","Calculate MD5 checksums in target <dist-all>","Trivial patch that extends the ant target <dist-all> to calculate
the MD5 checksums for the dist files."
"LUCENE-3530","CLEANUP","","Remove deprecated methods in CompoundTokenFilters",""
"LUCENE-3019","RFE","IMPROVEMENT","FVH: uncontrollable color tags","The multi-colored tags is a feature of FVH. But it is uncontrollable (or more precisely, unexpected by users) that which color is used for each terms."
"LUCENE-616","BUG","BUG","CLONE -Merge error during add to index (IndexOutOfBoundsException)","I've been batch-building indexes, and I've build a couple hundred indexes with 
a total of around 150 million records.  This only happened once, so it's 
probably impossible to reproduce, but anyway... I was building an index with 
around 9.6 million records, and towards the end I got this:

java.lang.IndexOutOfBoundsException: Index: 54, Size: 24
        at java.util.ArrayList.RangeCheck(ArrayList.java:547)
        at java.util.ArrayList.get(ArrayList.java:322)
        at org.apache.lucene.index.FieldInfos.fieldInfo(FieldInfos.java:155)
        at org.apache.lucene.index.FieldInfos.fieldName(FieldInfos.java:151)
        at org.apache.lucene.index.SegmentTermEnum.readTerm(SegmentTermEnum.java
:149)
        at org.apache.lucene.index.SegmentTermEnum.next
(SegmentTermEnum.java:115)
        at org.apache.lucene.index.SegmentMergeInfo.next
(SegmentMergeInfo.java:52)
        at org.apache.lucene.index.SegmentMerger.mergeTermInfos
(SegmentMerger.java:294)
        at org.apache.lucene.index.SegmentMerger.mergeTerms
(SegmentMerger.java:254)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:93)
        at org.apache.lucene.index.IndexWriter.mergeSegments
(IndexWriter.java:487)
        at org.apache.lucene.index.IndexWriter.maybeMergeSegments
(IndexWriter.java:458)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:310)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:294)"
"LUCENE-1944","CLEANUP","TASK","Remove deprecated Directory stuff and IR/IW open/ctor hell","This patch removes primarily the deprecated Directory stuff. This also removes parts of the ctor/open hell in IR and IW. IndexModifier is completely removed as deprecated, too."
"LUCENE-2924","DOCUMENTATION","BUG","fix getting started / demo docs","Opening a new issue for this since there are a number of problems...:

  * We should get the versions right, eg when we explain how to do a src checkout it should point to the path for that release

  * Source checkout / build JARs instructions must be updated for the merger

  * Analyzers JAR must be on the classpath too

  * Demo sources are no longer shipped in a binary release

  * Fixup from LUCENE-2923 (remove web app, new command-line ops for IndexFiles, etc.)"
"LUCENE-1132","DOCUMENTATION","BUG","Highlighter Documentation updates","Various places in the Highlighter documentation refer to bytes (i.e. SimpleFragmenter) when it should be chars.  See http://www.gossamer-threads.com/lists/lucene/java-user/56986"
"LUCENE-2643","BUG","BUG","StringHelper#stringDifference is wrong about supplementary chars ","StringHelper#stringDifference does not take supplementary characters into account. Since this is not used internally at all we should think about removing it but I guess since it is not too complex we should just or fix it for bwcompat reasons. For released versions we should really fix it since folks might use it though. For trunk we could just drop it."
"LUCENE-2874","BUG","BUG","Highlighting overlapping tokens outputs doubled words","If for the text ""the fox did not jump"" we generate following tokens :
(the, 0, 0-3),({fox},0,0-7),(fox,1,4-7),(did,2,8-11),(not,3,12,15),(jump,4,16,18)

If TermVector for field is stored WITH_OFFSETS and not WITH_POSITIONS_OFFSETS, highlighing would output
""the<em>the fox</em> did not jump""

I join a patch with 2 additive JUnit tests and a fix of TokenSources class where token ordering by offset did'nt manage well overlapping tokens.
"
"LUCENE-3555","BACKPORT","BUG","Add support for distributed stats","(its a bug in a way, since we broke this, temporarily).

There is no way to do this now (distributed IDF, etc) with the new API.

But we should do it right:
* having the sim ask the searcher for docfreq of a term is wasteful and dangerous, 
  usually we have already seek'd to the term and already collected the 'raw' stuff.
* the situation is more than just docfreq, because you should be able to implement
  distributed scoring for all of the new sim models (or your own), that use any
  of Lucene's stats.
"
"LUCENE-1043","IMPROVEMENT","IMPROVEMENT","Speedup merging of stored fields when field mapping ""matches""","Robert Engels suggested the following idea, here:

  http://www.gossamer-threads.com/lists/lucene/java-dev/54217

When merging in the stored fields from a segment, if the field name ->
number mapping is identical then we can simply bulk copy the entire
entry for the document rather than re-interpreting and then re-writing
the actual stored fields.

I've pulled the code from the above thread and got it working on the
current trunk."
"LUCENE-2526","BUG","BUG",".toString on empty MultiPhraseQuery hits NPE","Ross Woolf hit this on java-user thread ""MultiPhraseQuery.toString() throws null pointer exception"".  It's still present on trunk..."
"LUCENE-1717","IMPROVEMENT","BUG","IndexWriter does not properly account for the RAM consumed by pending deletes","IndexWriter, with autoCommit false, is able to carry buffered deletes for quite some time before materializing them to docIDs (thus freeing up RAM used).

It's only on triggering a merge (or, commit/close) that the deletes are materialized and the RAM is freed.

I expect this in practice is a smallish amount of RAM, but we should still fix it.

I don't have a patch yet so if someone wants to grab this, feel free!!"
"LUCENE-1984","IMPROVEMENT","IMPROVEMENT","DisjunctionMaxQuery - Type safety  ","DisjunctionMaxQuery code has containers that are not type-safe . The comments indicate type-safety though. 

Better to express in the API and the internals the explicit type as opposed to type-less containers. 

Patch attached. 

Comments / backward compatibility concerns welcome.  "
"LUCENE-1861","BUILD_SYSTEM","","Add contrib libs to classpath for javadoc","I don't know Ant well enough to just do this easily, so I've labeled a wish - would be nice to get rid of all the errors/warnings that not finding these classes generates when building javadoc."
"LUCENE-1606","RFE","RFE","Automaton Query/Filter (scalable regex)","Attached is a patch for an AutomatonQuery/Filter (name can change if its not suitable).

Whereas the out-of-box contrib RegexQuery is nice, I have some very large indexes (100M+ unique tokens) where queries are quite slow, 2 minutes, etc. Additionally all of the existing RegexQuery implementations in Lucene are really slow if there is no constant prefix. This implementation does not depend upon constant prefix, and runs the same query in 640ms.

Some use cases I envision:
 1. lexicography/etc on large text corpora
 2. looking for things such as urls where the prefix is not constant (http:// or ftp://)

The Filter uses the BRICS package (http://www.brics.dk/automaton/) to convert regular expressions into a DFA. Then, the filter ""enumerates"" terms in a special way, by using the underlying state machine. Here is my short description from the comments:

     The algorithm here is pretty basic. Enumerate terms but instead of a binary accept/reject do:
      
     1. Look at the portion that is OK (did not enter a reject state in the DFA)
     2. Generate the next possible String and seek to that.

the Query simply wraps the filter with ConstantScoreQuery.

I did not include the automaton.jar inside the patch but it can be downloaded from http://www.brics.dk/automaton/ and is BSD-licensed."
"LUCENE-846","BUG","BUG","IOExeception can cause loss of data due to premature segment deletion","If you hit an IOException, e.g., disk full, while making a cfs from its constituent parts, you may not be able to rollback to the before-merge process. This happens via addIndexes.

I don't have a nice easy test for this; generating IOEs ain't so easy. But it does happen in the patch for the factored merge policy with the existing tests because the pseudo-randomly generated IOEs fall in a different place."
"LUCENE-350","DOCUMENTATION","BUG","counter field in segments file is not documented in fileformats.xml","The counter field in the current segments file format is not documented."
"LUCENE-1213","BUG","BUG","MultiFieldQueryParser ignores slop parameter","MultiFieldQueryParser.getFieldQuery(String, String, int) calls super.getFieldQuery(String, String), thus obliterating any slop parameter present in the query.

It should probably be changed to call super.getFieldQuery(String, String, int), except doing only that will result in a recursive loop which is a side-effect of what may be a deeper problem in MultiFieldQueryParser -- getFieldQuery(String, String, int) is documented as delegating to getFieldQuery(String, String), yet what it actually does is the exact opposite.  This also causes problems for subclasses which need to override getFieldQuery(String, String) to provide different behaviour.
"
"LUCENE-1396","IMPROVEMENT","IMPROVEMENT","Improve PhraseQuery.toString()","PhraseQuery.toString() is overly simplistic, in that it doesn't correctly show phrases with gaps or overlapping terms. This may be misleading when presenting phrase queries built using complex analyzers and filters."
"LUCENE-3665","RFE","IMPROVEMENT","Make WeightedSpanTermExtractor extensible to handle custom query implemenations","Currently if I have a custom query which subclasses query directly I can't use the QueryScorer for highlighting since it does explicit instanceof checks. In some cases its is possible to rewrite the query before passing it to the highlighter to obtain a primitive query. However I had the usecase where this was not possible ie. the original index was not available on the machine which highlights the results. To still use the highlighter I had to copy a bunch of code due to visibility issues in those classes. I think we can make this extensible with minor effort to allow this usecase without massive code duplication."
"LUCENE-2117","RFE","BUG","Fix SnowballAnalyzer casing behavior for Turkish Language","LUCENE-2102 added a new TokenFilter to handle Turkish unique casing behavior correctly. We should fix the casing behavior in SnowballAnalyzer too as it supports a TurkishStemmer."
"LUCENE-1279","IMPROVEMENT","IMPROVEMENT","RangeQuery and RangeFilter should use collation to check for range inclusion","See [this java-user discussion|http://www.nabble.com/lucene-farsi-problem-td16977096.html] of problems caused by Unicode code-point comparison, instead of collation, in RangeQuery.

RangeQuery could take in a Locale via a setter, which could be used with a java.text.Collator and/or CollationKey's, to handle ranges for languages which have alphabet orderings different from those in Unicode."
"LUCENE-3539","IMPROVEMENT","IMPROVEMENT","IndexFormatTooOld/NewExc should try to include fileName + directory when possible","(Spinoff from http://markmail.org/thread/t6s7nn3ve765nojc )

When we throw a too old/new exc we should try to include the full path to the offending file, if possible."
"LUCENE-2551","IMPROVEMENT","IMPROVEMENT","change jdk & icu collation to use byte[]","Now that term is byte[], we should switch collation to use byte[] instead of 'indexablebinarystring'.

This is faster and results in much smaller sort keys.

I figure we can work it out here, and fix termrangequery to use byte in parallel, but we can already test sorting etc now."
"LUCENE-1058","RFE","IMPROVEMENT","New Analyzer for buffering tokens","In some cases, it would be handy to have Analyzer/Tokenizer/TokenFilters that could siphon off certain tokens and store them in a buffer to be used later in the processing pipeline.

For example, if you want to have two fields, one lowercased and one not, but all the other analysis is the same, then you could save off the tokens to be output for a different field.

Patch to follow, but I am still not sure about a couple of things, mostly how it plays with the new reuse API.

See http://www.gossamer-threads.com/lists/lucene/java-dev/54397?search_string=BufferingAnalyzer;#54397"
"LUCENE-1358","BUG","BUG","Deadlock for some Query objects in the equals method (f.ex. PhraseQuery) in a concurrent environment","Some Query objects in lucene 2.3.2 (and previous versions) have internal variables using Vector.   These variables are used during the call to the equals method.   In a concurrent environment a deadlock might occur.    The attached code example shows this happening in lucene 2.3.2, but the patch in LUCENE-1346 fixes this issue (though that doesn't seem to be the intention of that patch according to the description :-)"
"LUCENE-437","BUG","BUG","SnowballFilter loses token position offset","SnowballFilter doesn't set the token position increment (and thus it defaults to 1).
This also affetcs SnowballAnalyzer since it uses SnowballFilter."
"LUCENE-1756","TEST","BUG","contrib/memory: PatternAnalyzerTest is a very, very, VERY, bad unit test","while working on something else i was started getting consistent IllegalStateExceptions from PatternAnalyzerTest -- but only when running the test from the top level.

Digging into the test, i've found numerous things that are very scary...
* instead of using assertions to test that tokens streams match, it throws an IllegalStateExceptions when they don't, and then logs a bunch of info about the token streams to System.out -- having assertion messages that tell you *exactly* what doens't match would make a lot more sense.
* it builds up a list of files to analyze using patsh thta it evaluates relative to the current working directory -- which means you get different files depending on wether you run the tests fro mthe contrib level, or from the top level build file
* the list of files it looks for include: ""../../*.txt"", ""../../*.html"", ""../../*.xml"" ... so not only do you get different results when you run the tests in the contrib vs at the top level, but different people runing the tests via the top level build file will get different results depending on what types of text, html, and xml files they happen to have two directories above where they checked out lucene.
* the test comments indicates that it's purpose is to show that PatternAnalyzer produces the same tokens as other analyzers - but points out this will fail for WhitespaceAnalyzer because of the 255 character token limit WhitespaceTokenizer imposes -- the test then proceeds to compare PaternAnalyzer to WhitespaceTokenizer, garunteeing a test failure for anyone who happens to have a text file containing more then 255 characters of non-whitespace in a row somewhere in ""../../"" (in my case: my bookmarks.html file, and the hex encoded favicon.gif images)
"
"LUCENE-1700","DESIGN_DEFECT","BUG","LogMergePolicy.findMergesToExpungeDeletes need to get deletes from the SegmentReader","With LUCENE-1516, deletes are carried over in the SegmentReaders
which means implementations of
MergePolicy.findMergesToExpungeDeletes (such as LogMergePolicy)
need to obtain deletion info from the SR (instead of from the
SegmentInfo which won't have the information)."
"LUCENE-2728","BUG","BUG","EnwikiContentSource does not properly identify the name/id of the Wikipedia article","The EnwikiContentSource does not properly identify the id (name in benchmark parlance) of the documents.  It currently produces assigns the id on the last <id> tag it sees in the document, as opposed to the id of the document.  Most documents have multiple <id> tags in them.  This prevents the ContentSource from being used effectively in producing documents for updating.

Example doc:
{quote}
<page>
    <title>AlgeriA</title>
    <id>5</id>
    <revision>
      <id>133452200</id>
      <timestamp>2007-05-25T17:11:48Z</timestamp>
      <contributor>
        <username>Gurch</username>
        <id>241822</id>
      </contributor>
      <minor />
      <comment>[[WP:AES|<86><90>]]Redirected page to [[Algeria]]</comment>
      <text xml:space=""preserve"">#REDIRECT [[Algeria]] {{R from CamelCase}}</text>
    </revision>
  </page>
{quote}

In this case, the getName() return 241822 instead of 5.  page/id is unique according to the schema at  http://www.mediawiki.org/xml/export-0.3.xsd, so we should just get that one."
"LUCENE-1367","RFE","IMPROVEMENT","Add a isDeleted method to IndexCommit","I wish to add a IndexCommit.isDeleted() method.

The use-case is that Solr will now support configurable IndexDeletionPolicy (SOLR-617). For the new replication (SOLR-561) to work, we need access to a list of IndexCommit instances which haven't been deleted yet. I can wrap the user specified IndexDeletionPolicy but since the IndexCommit does not have a isDeleted method, I may store a reference to an IndexCommit on which delete() has been called by the deletion policy. I can wrap the IndexCommit objects too just for having a isDeleted() method so a workaround exists. Not a big pain but if it can be managed on the lucene side easily, I'll appreciate it. It would save me from writing some delegate code."
"LUCENE-446","RFE","RFE","search.function - (1) score based on field value, (2) simple score customizability","FunctionQuery can return a score based on a field's value or on it's ordinal value.

FunctionFactory subclasses define the details of the function.  There is currently a LinearFloatFunction (a line specified by slope and intercept).

Field values are typically obtained from FieldValueSourceFactory.  Implementations include FloatFieldSource, IntFieldSource, and OrdFieldSource."
"LUCENE-1447","IMPROVEMENT","IMPROVEMENT","Improve payload error handling/reporting","If you try to load a payload more than once you get the exception:  IOException(""Payload cannot be loaded more than once for the same term position."");

You also get this exception if their is no payload to load, and its a bit confusing, as the message doesn't relate to the actual problem."
"LUCENE-1600","IMPROVEMENT","IMPROVEMENT","Reduce usage of String.intern(), performance is terrible","I profiled a simple MatchAllDocsQuery() against ~1.5 million documents (8 fields of short text, Field.Store.YES,Field.Index.NOT_ANALYZED_NO_NORMS), then retrieved all documents via searcher.doc(i, fs). String.intern() showed up as a top hotspot (see attached screenshot), so i implemented a small optimization to not intern() for every new Field(), instead forcing the intern in the FieldInfos class and adding a optional ""internName"" constructor to Field. This reduced execution time for searching and iterating through all documents by 35%. Results were similar for -server and -client.


TRUNK (2.9) w/out patch: matched 1435563 in 8884 ms/search
TRUNK (2.9) w/patch: matched 1435563 in 5786 ms/search"
"LUCENE-2062","RFE","RFE","Bulgarian Analyzer","someone asked about bulgarian analysis on solr-user today... http://www.lucidimagination.com/search/document/e1e7a5636edb1db2/non_english_languages
I was surprised we did not have anything.

This analyzer implements the algorithm specified here, http://members.unine.ch/jacques.savoy/Papers/BUIR.pdf

In the measurements there, this improves MAP approx 34%
"
"LUCENE-575","BUG","BUG","SpellChecker min score is increased by time","The minimum score, an instance variable, is modified in a search. That is wrong, since it makes it 1. thread unsafe and 2. not working. 

Lucky enought it is only used from the one and same method call, so I simply compied the instance variable to a local method variable.

        float min = this.min; 
"
"LUCENE-997","RFE","RFE","Add search timeout support to Lucene","This patch is based on Nutch-308. 

This patch adds support for a maximum search time limit. After this time is exceeded, the search thread is stopped, partial results (if any) are returned and the total number of results is estimated.

This patch tries to minimize the overhead related to time-keeping by using a version of safe unsynchronized timer.

This was also discussed in an e-mail thread.
http://www.nabble.com/search-timeout-tf3410206.html#a9501029"
"LUCENE-2691","CLEANUP","IMPROVEMENT","Consolidate Near Real Time and Reopen API semantics","We should consolidate the IndexWriter.getReader and the IndexReader.reopen semantics, since most people are already using the IR.reopen() method, we should simply add::
{code}
IR.reopen(IndexWriter)
{code}

Initially, it could just call the IW.getReader(), but it probably should switch to just using package private methods for sharing the internals"
"LUCENE-2600","RFE","BUG","don't try to cache a composite reader's MultiBits deletedDocs","MultiFields.getDeletedDocs now builds up a MultiBits instance (so that one can check if a top-level docID is deleted), but it now stuffs it into a private cache on IndexReader.

This is invalid when the composite reader is read/write, and can result in a MultiReader falsely claiming a doc was not deleted."
"LUCENE-1460","REFACTORING","TASK","Change all contrib TokenStreams/Filters to use the new TokenStream API","Now that we have the new TokenStream API (LUCENE-1422) we should change all contrib modules to use it."
"LUCENE-378","BUILD_SYSTEM","BUG","GCJ makefile hardcodes compiler commands","src/gcj/Makefile hardcodes the command names for gcj, gcjh, and g++. This makes it difficult to 
compile with a particular version of GCJ if multiple are installed with suffixes (eg, gcj-4.0)

Steps to reproduce:
1. Configure, compile, and install GCC/GCJ with something like --program-suffix=-4.0
2. cd ~/src/lucene && ant gcj

Expected results:
Somehow be able to specify my compiler.

Actual results:
Can't find 'gcj' executable, or worse runs wrong version. :)

Suggested fix: as is common with variable names like CC to force a C compiler, allow the builder to 
override the compiler commands used by setting optional environment variables GCJ etc.
Patch to be attached.

Additional info:
Building Lucene from SVN 2005-04-19."
"LUCENE-903","BUG","BUG","FilteredQuery explanation inaccuracy with boost","The value of explanation is different than the product of its part if boost > 1.
This is exposed after tightening the explanation check (part of LUCENE-446).

"
"LUCENE-2620","IMPROVEMENT","BUG","Queries with too many asterisks causing 100% CPU usage","If a search query has many adjacent asterisks (e.g. fo**************obar), I can get my webapp caught in a loop that does not seem to end in a reasonable amount of time and may in fact be infinite. For just a few asterisks the query eventually does return some results, but as I add more it takes a longer and longer amount of time. After about six or seven asterisks the query never seems to finish. Even if I abort the search, the thread handling the troublesome query continues running in the background and pinning a CPU.

I found the problem in src/java/org/apache/lucene/search/WildcardTermEnum.java on Lucene 3.0.1 and it looks like 3.0.2 ought to be affected as well. I'm not sure about trunk, though. I have a patch that fixes the problem for me in 3.0.1."
"LUCENE-3660","BUG","BUG","If indexwriter hits a non-ioexception from indexExists it leaks a write.lock","the rest of IW's ctor is careful about this.

IndexReader.indexExists catches any IOException and returns false, but the problem
occurs if some other exception (in my test, UnsupportedOperationException, but you
can imagine others are possible), when trying to e.g. read in the segments file.

I think we just need to move the IR.exists stuff inside the try / finally"
"LUCENE-3431","RFE","IMPROVEMENT","Make QueryAutoStopWordAnalyzer immutable and reusable","Currently QueryAutoStopWordAnalyzer allows its list of stop words to be changed after instantiation through its addStopWords() methods.  This stops the Analyzer from being reusable since it must instantiate its StopFilters every time.

Having these methods means that although the Analyzer can be instantiated once and reused between IndexReaders, the actual analysis stack is not reusable (which is probably the more expensive part).

So lets change the Analyzer so that its stop words are set at instantiation time, facilitating reuse."
"LUCENE-3636","RFE","TASK","make it possible to use searchermanager with distributed stats","LUCENE-3555 added explicit stats methods to indexsearcher, but you must
subclass to override this (e.g. populate with distributed stats).

Its also impossible to then do this with SearcherManager.

One idea is make this a factory method (or similar) on IndexSearcher instead,
so you don't need to subclass it to override.

Then you can initialize this in a SearcherWarmer, except there is currently
a lot of hair in what this warming should be. This is a prime example where
Searcher has different meaning from Reader, we should clean this up.

Otherwise, lets make NRT/SearcherManager subclassable in such a way that 
you can return a custom indexsearcher."
"LUCENE-2437","RFE","RFE","Indonesian Analyzer","This is an implementation of http://www.illc.uva.nl/Publications/ResearchReports/MoL-2003-02.text.pdf

The only change is that I added an option to disable derivational stemming, 
in case you want to just remove inflectional particles and possessive pronouns.

"
"LUCENE-636","RFE","IMPROVEMENT","[PATCH] Differently configured Lucene 'instances' in same JVM","Currently Lucene can be configured using system properties. When running multiple 'instances' of Lucene for different purposes in the same JVM, it is not possible to use different settings for each 'instance'.

I made changes to some Lucene classes so you can pass a configuration to that class. The Lucene 'instance' will use the settings from that configuration. The changes do not effect the API and/or the current behavior so are backwards compatible.

In addition to the changes above I also made the SegmentReader and SegmentTermDocs extensible outside of their package. I would appreciate the inclusion of these changes but don't mind creating a separate issue for them.

"
"LUCENE-2808","BUG","BUG","Intermitted failure on DocValues branch","I lately ran into two random failures on the CSF branch that seem not to be related to docValues but I can't reproduce them neither on docvalues branch nor on trunk.

{code}
jError Message

IndexFileDeleter doesn't know about file _1e.tvx
Stacktrace

junit.framework.AssertionFailedError: IndexFileDeleter doesn't know about file _1e.tvx
	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:979)
	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:917)
	at org.apache.lucene.index.IndexWriter.filesExist(IndexWriter.java:3633)
	at org.apache.lucene.index.IndexWriter.startCommit(IndexWriter.java:3699)
	at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2407)
	at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2478)
	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2460)
	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2444)
	at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:213)
Standard Output

NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testRandomExceptionsThreads -Dtests.seed=-6528669668419768890:4860241142852689334 -Dtests.codec=randomPerField -Dtests.multiplier=3
NOTE: test params are: codec=PreFlex, locale=sv, timezone=Atlantic/South_Georgia
Standard Error

NOTE: all tests run in this JVM:
[TestDemo, TestToken, TestBinaryDocument, TestCodecs, TestDirectoryReader, TestIndexInput, TestIndexWriterExceptions]
{code}

and

{code}

[junit] Testsuite: org.apache.lucene.index.TestIndexReaderReopen
    [junit] Testcase: testThreadSafety(org.apache.lucene.index.TestIndexReaderReopen):	Caused an ERROR
    [junit] MockDirectoryWrapper: cannot close: there are still open files: {_4_3.frq=1, _4_3.pos=1, _4_0.frq=1, _4_0.prx=1, _4.pst=1, _4_3.pyl=1, _4_3.skp=1, _4_0.tis=1, _4_3.doc=1, _4_3.tis=1}
    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_4_3.frq=1, _4_3.pos=1, _4_0.frq=1, _4_0.prx=1, _4.pst=1, _4_3.pyl=1, _4_3.skp=1, _4_0.tis=1, _4_3.doc=1, _4_3.tis=1}
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:387)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.testThreadSafety(TestIndexReaderReopen.java:859)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:979)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:917)
    [junit] Caused by: java.lang.RuntimeException: unclosed IndexInput
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:342)
    [junit] 	at org.apache.lucene.store.Directory.openInput(Directory.java:122)
    [junit] 	at org.apache.lucene.index.codecs.standard.StandardPostingsReader.<init>(StandardPostingsReader.java:49)
    [junit] 	at org.apache.lucene.index.codecs.standard.StandardCodec.fieldsProducer(StandardCodec.java:87)
    [junit] 	at org.apache.lucene.index.PerFieldCodecWrapper$FieldsReader.<init>(PerFieldCodecWrapper.java:119)
    [junit] 	at org.apache.lucene.index.PerFieldCodecWrapper.fieldsProducer(PerFieldCodecWrapper.java:211)
    [junit] 	at org.apache.lucene.index.SegmentReader$CoreReaders.<init>(SegmentReader.java:137)
    [junit] 	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:532)
    [junit] 	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:509)
    [junit] 	at org.apache.lucene.index.DirectoryReader.<init>(DirectoryReader.java:238)
    [junit] 	at org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:500)
    [junit] 	at org.apache.lucene.index.DirectoryReader.access$000(DirectoryReader.java:48)
    [junit] 	at org.apache.lucene.index.DirectoryReader$2.doBody(DirectoryReader.java:493)
    [junit] 	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:623)
    [junit] 	at org.apache.lucene.index.DirectoryReader.doReopenNoWriter(DirectoryReader.java:488)
    [junit] 	at org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:446)
    [junit] 	at org.apache.lucene.index.DirectoryReader.reopen(DirectoryReader.java:406)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$9.run(TestIndexReaderReopen.java:770)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$ReaderThread.run(TestIndexReaderReopen.java:897)
    [junit] 
    [junit] 
    [junit] Tests run: 17, Failures: 0, Errors: 1, Time elapsed: 13.766 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexReaderReopen -Dtestmethod=testThreadSafety -Dtests.seed=-5455993123574190959:-1935535300313439968 -Dtests.codec=randomPerField -Dtests.multiplier=3
    [junit] NOTE: test params are: codec=RandomCodecProvider: {field5=MockVariableIntBlock(baseBlockSize=29), id=Standard, fielda=Standard, field4=MockFixedIntBlock(blockSize=924), field3=Standard, field2=SimpleText, id2=Standard, field6=MockSep, field1=Pulsing(freqCutoff=8)}, locale=zh_CN, timezone=Asia/Hovd
{code}

I haven't seen those before - let me know if you have!"
"LUCENE-977","IMPROVEMENT","IMPROVEMENT","internal hashing improvements","Internal power-of-two closed hashtable traversal in DocumentsWriter and CharArraySet could be better.

Here is the current method of resolving collisions:
    if (text2 != null && !equals(text, len, text2)) {
      final int inc = code*1347|1;
      do {
        code += inc;
        pos = code & mask;
        text2 = entries[pos];
      } while (text2 != null && !equals(text, len, text2));

The problem is that two different hashCodes with the same lower bits will keep picking the same slots (the upper bits will be ignored).
This is because multiplication (*1347) only really shifts bits to the left... so given that the two codes already matched on the right, they will both pick the same increment, and this will keep them on the same path through the table (even though it's being added to numbers that differ on the left).  To resolve this, some bits need to be moved to the right when calculating the increment.

"
"LUCENE-3375","BUG","BUG","processing a synonym in a token stream will remove the following token from the stream","If you do a phrase search on a field derived from a fieldtype with the synonym filter which includes a synonym, the term following the synonym vanishes after synonym expansion.

e.g. http://host:port/solr/corename/select/?q=desc:%22xyzzy%20%20bbb%20pot%20of%20gold%22&version=2.2&start=0&rows=10&indent=on&debugQuery=true   (bbb is in the default synonyms file, desc is a ""text"" fieldtype)

outputs
....
<str name=""rawquerystring"">desc:""xyzzy  bbb pot of gold""</str>
<str name=""querystring"">desc:""xyzzy  bbb pot of gold""</str>
<str name=""parsedquery"">PhraseQuery(desc:""xyzzy bbbb 1 bbbb 2 of gold"")</str>
<str name=""parsedquery_toString"">desc:""xyzzy bbbb 1 bbbb 2 of gold""</str>
....

You can also see this behavior using the admin console analysis.jsp

Solr 3.3 behaves properly.
"
"LUCENE-3735","DESIGN_DEFECT","","Fix PayloadProcessorProvider to no longer use Directory for lookup, instead AtomicReader","The PayloadProcessorProvider has a broken API, this should be fixed. The current trunk mimics the old behaviour, but not 100%.

The PayloadProcessorProvider API should return a PayloadProcessor based on the AtomicReader instance that gets merged. As AtomicReader do no longer know the directory they are reside (they could be e.g. FilterIndexReaders, MemoryIndexes,...) a selection by Directory is no longer possible.

The current code in Lucene trunk mimics the old behavior by doing an instanceof SegmentReader check and then asking for a DirProvider. If something else is merged in, Payload processing is not supported. This should be changed, the old API could be kept backwards compatible by moving the instanceof check in a ""convenience class"" DirPayloadProcessorProvider, extending PayloadProcessorProvider."
"LUCENE-2413","REFACTORING","IMPROVEMENT","Consolidate all (Solr's & Lucene's) analyzers into modules/analysis","We've been wanting to do this for quite some time now...  I think, now that Solr/Lucene are merged, and we're looking at opening an unstable line of development for Solr/Lucene, now is the right time to do it.

A standalone module for all analyzers also empowers apps to separately version the analyzers from which version of Solr/Lucene they use, possibly enabling us to remove Version entirely from the analyzers.

We should also do LUCENE-2309 (decouple, as much as possible, indexer from the analysis API), but I don't think that issue needs to block this consolidation.

Once we do this, there is one place where our users can find all the analyzers that Solr/Lucene provide."
"LUCENE-1246","BUG","BUG","Missing a null check in BooleanQuery.toString(String)","Our queryParser/tokenizer in some situations creates null query and was added as a clause to Boolean query.
When we try to log the query, NPE is thrown from log(booleanQuery).

In BooleanQuery.toString(String), a simple null check is overlooked.
"
"LUCENE-1416","BUILD_SYSTEM","BUG","Ant contrib test can fail if there is a space in path to lucene project","A couple contrib ant tests get the path to test files through a URL object, and so the path is URL encoded. Normally fine, but if you have a space in your path (/svn stuff/lucene/contrib/ant) then it will have %20 for the space and (at least on my Ubuntu system) the test will fail with filenotfound. This patch simply replaces all %20 with "" "". Not sure if we want/need to take it any further."
"LUCENE-2509","IMPROVEMENT","IMPROVEMENT","Improve readability of StandardTermsDictWriter","One variable is named indexWriter, but it is a termsIndexWriter. Also some layout."
"LUCENE-385","RFE","BUG","[PATCH] don't delete all files in index directory on index creation","Many people use Lucene to index a part of their file system. The chance that  
you some day mix up index directory and document directory isn't that bad.  
Currently Lucene will delete *all* files in the index directory when the  
create paramater passed to IndexWriter is true, thus deleting your documents 
if you mixed up the parameters. I'll attach a patch that fixes  
this. Any objections?"
"LUCENE-3766","CLEANUP","IMPROVEMENT","Remove/deprecate Tokenizer's default ctor","I was working on a new Tokenizer... and I accidentally forgot to call super(input) (and super.reset(input) from my reset method)... which then meant my correctOffset() calls were silently a no-op; this is very trappy.

Fortunately the awesome BaseTokenStreamTestCase caught this (I hit failures because the offsets were not in fact being corrected).

One minimal thing we can do (but it sounds like from Robert there may be reasons why we can't) is add {{assert input != null}} in Tokenizer.correctOffset:

{noformat}
Index: lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java	(revision 1242316)
+++ lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java	(working copy)
@@ -82,6 +82,7 @@
    * @see CharStream#correctOffset
    */
   protected final int correctOffset(int currentOff) {
+    assert input != null: ""subclass failed to call super(Reader) or super.reset(Reader)"";
     return (input instanceof CharStream) ? ((CharStream) input).correctOffset(currentOff) : currentOff;
   }
{noformat}

But best would be to remove the default ctor that leaves input null..."
"LUCENE-2895","BUG","BUG","MockRandomCodec loads termsIndex even if termsIndexDivisor is set to -1","When working on LUCENE-2891 (on trunk), I found out that if MockRandomCodec is used, then setting IWC.readerTermsIndexDivisor to -1 allows seeking e.g., termDocs, when it shouldn't. Other Codecs fail to seek, as expected by the test. We need to find out why MockRandomCodec does not fail as expected.

To verify that, run ""ant test-core -Dtestcase=TestIndexWriterReader -Dtestmethod=testNoTermsIndex -Dtests.codec=MockRandom"", but comment out the line which adds MockRandom to the list of illegal codecs in the test."
"LUCENE-828","BUG","BUG","Term's equals() throws ClassCastException if passed something other than a Term","Term.equals(Object) does a cast to Term without checking if the other object is a Term.

It's unlikely that this would ever crop up but it violates the implied contract of Object.equals()."
"LUCENE-2314","RFE","IMPROVEMENT","Add AttributeSource.copyTo(AttributeSource)","One problem with AttributeSource at the moment is the missing ""insight"" into AttributeSource.State. If you want to create TokenStreams that inspect cpatured states, you have no chance. Making the contents of State public is a bad idea, as it does not help for inspecting (its a linked list, so you have to iterate).

AttributeSource currently contains a cloneAttributes() call, which returns a new AttrubuteSource with all current attributes cloned. This is the (more expensive) captureState. The problem is that you cannot copy back the cloned AS (which is the restoreState). To use this behaviour (by the way, ShingleMatrix can use it), one can alternatively use cloneAttributes and copyTo. You can easily change the cloned attributes and store them in lists and copy them back. The only problem is lower performance of these calls (as State is a very optimized class).

One use case could be:
{code}
AttributeSource state = cloneAttributes();
// .... do something ...
state.getAttribute(TermAttribute.class).setTermBuffer(foobar);
// ... more work
state.copyTo(this);
{code}"
"LUCENE-611","TEST","BUG","TestConstantScoreRangeQuery does not compile with ecj","TestConstantScoreRangeQuery has an assertEquals(String, Float, Float)
but most of the calls to assertEquals are (String, int, int).

ecj complains with the following error:
The method assertEquals(String, float, float) is ambiguous for the type TestConstantScoreRangeQuery

The simple solution is to supply an assertEquals(String, int, int) which calls Assert.assertEquals(String, int, int)

Patch to follow.
"
"LUCENE-799","BUG","BUG","Garbage data when reading a compressed, text field, lazily","lazy compressed text fields is a case that was neglected during lazy field implementation.  TestCase and patch provided.

"
"LUCENE-2295","RFE","IMPROVEMENT","Create a MaxFieldLengthAnalyzer to wrap any other Analyzer and provide the same functionality as MaxFieldLength provided on IndexWriter","A spinoff from LUCENE-2294. Instead of asking the user to specify on IndexWriter his requested MFL limit, we can get rid of this setting entirely by providing an Analyzer which will wrap any other Analyzer and its TokenStream with a TokenFilter that keeps track of the number of tokens produced and stop when the limit has reached.

This will remove any count tracking in IW's indexing, which is done even if I specified UNLIMITED for MFL.

Let's try to do it for 3.1."
"LUCENE-1962","CLEANUP","IMPROVEMENT","Persian Arabic Analyzer cleanup","While browsing through the code I found some places for minor improvements in the new Arabic / Persian Analyzer code. 

- prevent default stopwords from being loaded each time a default constructor is called
- replace if blocks with a single switch
- marking private members final where needed
- changed protected visibility to final in final class.

"
"LUCENE-1390","RFE","IMPROVEMENT","add ASCIIFoldingFilter and deprecate ISOLatin1AccentFilter","The ISOLatin1AccentFilter is removing accents from accented characters in the ISO Latin 1 character set.
It does what it does and there is no bug with it.

It would be nicer, though, if there was a more comprehensive version of this code that included not just ISO-Latin-1 (ISO-8859-1) but the entire Latin 1 and Latin Extended A unicode blocks.
See: http://en.wikipedia.org/wiki/Latin-1_Supplement_unicode_block
See: http://en.wikipedia.org/wiki/Latin_Extended-A_unicode_block

That way, all languages using roman characters are covered.
A new class, ISOLatinAccentFilter is attached. It is intended to supercede ISOLatin1AccentFilter which should get deprecated."
"LUCENE-726","CLEANUP","IMPROVEMENT","Remove usage of deprecated method Document.fields()","The classes DocumentWriter, FieldsWriter, and ParallelReader use the deprecated method Document.fields(). This simple patch changes these three classes to use Document.getFields() instead.

All unit tests pass."
"LUCENE-618","OTHER","","GData - Server wrong commit does not build","The last GData - Server commit  does not build due to a wrong commit.
Yonik did not commit all the files in the diff file. There are several sources and packages missing.
  
The diff - file with the date of 26.06.06 should be applied.
--> http://issues.apache.org/jira/browse/LUCENE-598
26.06.06.diff (644 kb)

could any of the lucene committers apply this patch. Yonik is on the way to Dublin.

Thanks Simon
"
"LUCENE-3205","CLEANUP","BUG","remove MultiTermQuery get/inc/clear totalNumberOfTerms","This method is not correct if the index has more than one segment.
Its also not thread safe, and it means calling query.rewrite() modifies
the original query. 

All of these things add up to confusion, I think we should remove this 
from multitermquery, the only thing that ""uses"" it is the NRQ tests, which 
conditionalizes all the asserts anyway.
"
"LUCENE-2167","RFE","RFE","Implement StandardTokenizer with the UAX#29 Standard","It would be really nice for StandardTokenizer to adhere straight to the standard as much as we can with jflex. Then its name would actually make sense.

Such a transition would involve renaming the old StandardTokenizer to EuropeanTokenizer, as its javadoc claims:

bq. This should be a good tokenizer for most European-language documents

The new StandardTokenizer could then say

bq. This should be a good tokenizer for most languages.

All the english/euro-centric stuff like the acronym/company/apostrophe stuff can stay with that EuropeanTokenizer, and it could be used by the european analyzers.
"
"LUCENE-837","REFACTORING","BUG","contrib/benchmark QueryMaker and Task Refactorings","Introduce an abstract QueryMaker implementation that shares much of the common code between the various QueryMaker implementations.

Add in a new QueryMaker for reading queries from a file that is specified in the properties.

Patch shortly, and if no concerns, will commit tomorrow or Wed."
"LUCENE-3076","RFE","IMPROVEMENT","add -Dtests.codecprovider","Currently to test a codec (or set of codecs) you have to add them to lucene's core and edit a couple of arrays here and there...

It would be nice if when using the test-framework you could instead specify a codecprovider by classname (possibly containing your own set of huper-duper codecs).

For example I made the following little codecprovider in contrib:
{noformat}
public class AppendingCodecProvider extends CodecProvider {
  public AppendingCodecProvider() {
    register(new AppendingCodec());
    register(new SimpleTextCodec());
  }
}
{noformat}

Then, I'm able to run tests with 'ant -lib build/contrib/misc/lucene-misc-4.0-SNAPSHOT.jar test-core -Dtests.codecprovider=org.apache.lucene.index.codecs.appending.AppendingCodecProvider', and it always picks from my set of  codecs (in this case Appending and SimpleText), and I can set -Dtests.codec=Appending if i want to set just one.

"
"LUCENE-2709","TEST","TEST","If test has methods with @Ignore, we should print out a notice","Currently these silently pass, but there is usually a reason they are @Ignore 
(sometimes good, sometimes really a TODO we should fix)

In my opinion we should add reasons for all these current @Ignores like Mike did with Test2BTerms.

Example output:
{noformat}
[junit] Testsuite: org.apache.lucene.index.Test2BTerms
[junit] Tests run: 0, Failures: 0, Errors: 0, Time elapsed: 0.184 sec
[junit]
[junit] ------------- Standard Error -----------------
[junit] NOTE: Ignoring test method 'test2BTerms' Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.
[junit] ------------- ---------------- ---------------

...

[junit] Testsuite: org.apache.solr.handler.dataimport.TestMailEntityProcessor
[junit] Tests run: 0, Failures: 0, Errors: 0, Time elapsed: 0.043 sec
[junit]
[junit] ------------- Standard Error -----------------
[junit] NOTE: Ignoring test method 'testConnection'
[junit] NOTE: Ignoring test method 'testRecursion'
[junit] NOTE: Ignoring test method 'testExclude'
[junit] NOTE: Ignoring test method 'testInclude'
[junit] NOTE: Ignoring test method 'testIncludeAndExclude'
[junit] NOTE: Ignoring test method 'testFetchTimeSince'
[junit] ------------- ---------------- ---------------
{noformat}"
"LUCENE-1172","IMPROVEMENT","IMPROVEMENT","Small speedups to DocumentsWriter","Some small fixes that I found while profiling indexing Wikipedia,
mainly using our own quickSort instead of Arrays.sort.

Testing first 200K docs of Wikipedia shows a speedup from 274.6
seconds to 270.2 seconds.

I'll commit in a day or two."
"LUCENE-3070","IMPROVEMENT","TASK","Enable DocValues by default for every Codec","Currently DocValues are enable with a wrapper Codec so each codec which needs DocValues must be wrapped by DocValuesCodec. The DocValues writer and reader should be moved to Codec to be enabled by default."
"LUCENE-2983","RFE","IMPROVEMENT","FieldInfos should be read-only if loaded from disk","Currently FieldInfos create a private FieldNumberBiMap when they are loaded from a directory which is necessary due to some limitation we need to face with IW#addIndexes(Dir). If we add an index via a directory to an existing index field number can conflict with the global field numbers in the IW receiving the directories. Those field number conflicts will remain until those segments are merged and we stabilize again based on the IW global field numbers. Yet, we unnecessarily creating a BiMap here where we actually should enforce read-only semantics since nobody should modify this FieldInfos instance we loaded from the directory. If somebody needs to get a modifiable copy they should simply create a new one and all all FieldInfo instances to it.

"
"LUCENE-2193","CLEANUP","TASK","Get rid of backwards tags","This is a followup on: [http://www.lucidimagination.com/search/document/bb6c23b6e87c0b63/back_compat_folders_in_tags_when_i_svn_update#3000a2232c678031]

Currently we use tags for specifying the revision number in the backwards branch that matches the current development branch revision (in common-build.xml). The idea is to just specify the corresponding revision no of the backwards branch in common-build.xml and the backwards-test target automatically handles up/down/co:

- We just give the rev number in common-build in common-build.xml as a property backwards-rev=""XXXX"". This property is used also in building the command line which is also a property backwards-svn-args=""-r $backwards-rev"". By that you can use ""ant -Dbackwards-svn-args=''"" to force test-backwards to checkout/update to head of branch (recommened for developers).

- we should rename target to ""test-backwards"" and keep a ""test-tag"" with dependency to that for compatibility

- The checkout on backwards creates a directory ""backwards/${backwards-branch}"" and uses ""svn co ${backwards-svn-args} 'http://svn.../${backwards-branch}' 'backwards/${backwards-branch}'"". The cool thing, the dir is checked out if non existent, but if the checkout already exists, svn co implicitely does an svn up to the given revision (it will also downgrade and merge if newer). So the test-backwards target always updates the checkout to the correct revision. I had not tried with local changes, but this should simply merge as an svn up.

The workflow for committing fixes to bw would be:

- First use ""svn up"" to upgrade the backwards working copy to HEAD.
- Commit the changes
- Copy and paste the message ""Committed revision XXXX"" to common-build.xml
- Commit the changes in trunk
"
"LUCENE-1370","RFE","IMPROVEMENT","Add ShingleFilter option to output unigrams if no shingles can be generated","Currently if ShingleFilter.outputUnigrams==false and the underlying token stream is only one token long, then ShingleFilter.next() won't return any tokens. This patch provides a new option, outputUnigramIfNoNgrams; if this option is set and the underlying stream is only one token long, then ShingleFilter will return that token, regardless of the setting of outputUnigrams.

My use case here is speeding up phrase queries. The technique is as follows:

First, doing index-time analysis using ShingleFilter (using outputUnigrams==true), thereby expanding things as follows:

""please divide this sentence into shingles"" ->
 ""please"", ""please divide""
 ""divide"", ""divide this""
 ""this"", ""this sentence""
 ""sentence"", ""sentence into""
 ""into"", ""into shingles""
 ""shingles""

Second, do query-time analysis using ShingleFilter (using outputUnigrams==false and outputUnigramIfNoNgrams==true). If the user enters a phrase query, it will get tokenized in the following manner:

""please divide this sentence into shingles"" ->
 ""please divide""
 ""divide this""
 ""this sentence""
 ""sentence into""
 ""into shingles""

By doing phrase queries with bigrams like this, I can gain a very considerable speedup. Without the outputUnigramIfNoNgrams option, then a single word query would tokenize like this:

""please"" ->
   [no tokens]

But thanks to outputUnigramIfNoNgrams, single words will now tokenize like this:

""please"" ->
  ""please""

****

The patch also adds a little to the pre-outputUnigramIfNoNgrams option tests.

****

I'm not sure if the patch in this state is useful to anyone else, but I thought I should throw it up here and try to find out.
"
"LUCENE-479","BUG","BUG","MultiReader.numDocs incorrect after undeleteAll","Calling MultiReader.undeleteAll does not clear cached numDocs value. So the subsequent numDocs() call returns a wrong value if there were deleted documents in the index. Following patch fixes the bug and adds a test showing the issue.


Index: src/test/org/apache/lucene/index/TestMultiReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestMultiReader.java       (revision 354923)
+++ src/test/org/apache/lucene/index/TestMultiReader.java       (working copy)
@@ -69,6 +69,18 @@
     assertTrue(vector != null);
     TestSegmentReader.checkNorms(reader);
   }
+
+  public void testUndeleteAll() throws IOException {
+    sis.read(dir);
+    MultiReader reader = new MultiReader(dir, sis, false, readers);
+    assertTrue(reader != null);
+    assertEquals( 2, reader.numDocs() );
+    reader.delete(0);
+    assertEquals( 1, reader.numDocs() );
+    reader.undeleteAll();
+    assertEquals( 2, reader.numDocs() );
+  }
+

   public void testTermVectors() {
     MultiReader reader = new MultiReader(dir, sis, false, readers);
Index: src/java/org/apache/lucene/index/MultiReader.java
===================================================================
--- src/java/org/apache/lucene/index/MultiReader.java   (revision 354923)
+++ src/java/org/apache/lucene/index/MultiReader.java   (working copy)
@@ -122,6 +122,7 @@
     for (int i = 0; i < subReaders.length; i++)
       subReaders[i].undeleteAll();
     hasDeletions = false;
+    numDocs = -1;      // invalidate cache
   }

   private int readerIndex(int n) {    // find reader for doc n:"
"LUCENE-3101","BUG","BUG","TestMinimize.testAgainstBrzozowski reproducible seed OOM","{code}
    [junit] Testsuite: org.apache.lucene.util.automaton.TestMinimize
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 3.792 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestMinimize -Dtestmethod=testAgainstBrzozowski -Dtests.seed=-7429820995201119781:1013305000165135537
    [junit] NOTE: test params are: codec=PreFlex, locale=ru, timezone=America/Pangnirtung
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestMinimize]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=294745976,total=310378496
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAgainstBrzozowski(org.apache.lucene.util.automaton.TestMinimize):     Caused an ERROR
    [junit] Java heap space
    [junit] java.lang.OutOfMemoryError: Java heap space
    [junit]     at java.util.BitSet.initWords(BitSet.java:144)
    [junit]     at java.util.BitSet.<init>(BitSet.java:139)
    [junit]     at org.apache.lucene.util.automaton.MinimizationOperations.minimizeHopcroft(MinimizationOperations.java:85)
    [junit]     at org.apache.lucene.util.automaton.MinimizationOperations.minimize(MinimizationOperations.java:52)
    [junit]     at org.apache.lucene.util.automaton.RegExp.toAutomaton(RegExp.java:502)
    [junit]     at org.apache.lucene.util.automaton.RegExp.toAutomatonAllowMutate(RegExp.java:478)
    [junit]     at org.apache.lucene.util.automaton.RegExp.toAutomaton(RegExp.java:428)
    [junit]     at org.apache.lucene.util.automaton.AutomatonTestUtil.randomAutomaton(AutomatonTestUtil.java:256)
    [junit]     at org.apache.lucene.util.automaton.TestMinimize.testAgainstBrzozowski(TestMinimize.java:43)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1282)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1211)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.util.automaton.TestMinimize FAILED


{code}"
"LUCENE-1512","RFE","RFE","Incorporate GeoHash in contrib/spatial","Based on comments from Yonik and Ryan in SOLR-773 
GeoHash provides the ability to store latitude / longitude values in a single field consistent hash field.
Which elements the need to maintain 2 field caches for latitude / longitude fields, reducing the size of an index
and the amount of memory needed for a spatial search."
"LUCENE-1052","IMPROVEMENT","IMPROVEMENT","Add an ""termInfosIndexDivisor"" to IndexReader","The termIndexInterval, set during indexing time, let's you tradeoff
how much RAM is used by a reader to load the indexed terms vs cost of
seeking to the specific term you want to load.

But the downside is you must set it at indexing time.

This issue adds an indexDivisor to TermInfosReader so that on opening
a reader you could further sub-sample the the termIndexInterval to use
less RAM.  EG a setting of 2 means every 2 * termIndexInterval is
loaded into RAM.

This is particularly useful if your index has a great many terms (eg
you accidentally indexed binary terms).

Spinoff from this thread:

  http://www.gossamer-threads.com/lists/lucene/java-dev/54371

"
"LUCENE-2779","IMPROVEMENT","IMPROVEMENT","Use ConcurrentHashMap in RAMDirectory","RAMDirectory synchronizes on its instance in many places to protect access to map of RAMFiles, in addition to updating the sizeInBytes member. In many places the sync is done for 'read' purposes, while only in few places we need 'write' access. This looks like a perfect use case for ConcurrentHashMap

Also, syncing around sizeInBytes is unnecessary IMO, since it's an AtomicLong ...

I'll post a patch shortly."
"LUCENE-1846","BUG","BUG","More Locale problems in Lucene","This is a followup to LUCENE-1836: I found some more Locale problems in Lucene with Date Formats. Even for simple date formats only consisting of numbers (like ISO dates), you should always give the US locale. Because the dates in DateTools should sort according to String.compare(), it is important, that the decimal digits are western ones. In some strange locales, this may be different. Whenever you want to format dates for internal formats you exspect to behave somehow, you should at least set the locale to US, which uses ASCII. Dates entered by users and displayed to users, should be formatted according to the default or a custom specified locale.
I also looked for DecimalFormat (especially used for padding numbers), but found no problems."
"LUCENE-2610","IMPROVEMENT","IMPROVEMENT","addIndexes(Directory...) should not trigger merge on flush()","IndexWriter.addIndexes(Directory..) calls flush() w/ triggerMerge=true. This beats the purpose of the changes done to addIndexes to not merge any segments and leave it as the application's choice. The change is very simple - pass false instead of true. I don't plan to post a patch, however opened an issue in case some want to comment about it."
"LUCENE-2178","RFE","IMPROVEMENT","Benchmark contrib should allow multiple locations in ext.classpath","When {{ant run-task}} is invoked with the  {{-Dbenchmark.ext.classpath=...}} option, only a single location may be specified.  If a classpath with more than one location is specified, none of the locations is put on the classpath for the invoked JVM."
"LUCENE-1324","RFE","BUG","TokenFilter should implement reset()","TokenFilter maintains a private member of TokenStream.
It should implement reset() and call its member TokenStream's reset() method. Otherwise, that TokenStream never gets reset.
Patch applied."
"LUCENE-3771","TEST","","Rename some remaining tests for new IndexReader class hierarchy",""
"LUCENE-1856","CLEANUP","TASK","Remove Hits","LUCENE-1290 removed all references to Hits from core.

Most work to be done here is to remove all references from the contrib modules and some new ones that crept into core after 1290."
"LUCENE-3723","BUG","BUG","Remove FieldMaskingSpanQuery (or fix its scoring)","In Lucene 4.0 we added new scoring mechanisms, but FieldMaskingSpanQuery is a serious problem:

Because it lies about the fields of its terms, this sometimes results in totally bogus
statistics, cases where a single terms totalTermFreq exceeds sumTotalTermFreq for the entire field (since its lying about it).

Such lying could result in NaN/Inf/Negative scores, exceptions, divide by zero, and other problems,
because the statistics are impossibly bogus."
"LUCENE-853","RFE","RFE","Caching does not work when using RMI","Filters and caching uses transient maps so that caching does not work if you are using RMI and a remote searcher 

I want to add a new RemoteCachededFilter that will make sure that the caching is done on the remote searcher side 
 "
"LUCENE-2207","BUG","BUG","CJKTokenizer generates tokens with incorrect offsets","If I index a Japanese *multi-valued* document with CJKTokenizer and highlight a term with FastVectorHighlighter, the output snippets have incorrect highlighted string. I'll attach a program that reproduces the problem soon."
"LUCENE-1722","DOCUMENTATION","IMPROVEMENT","SmartChineseAnalyzer javadoc improvement","Chinese -> English, and corrections to match reality (removes several javadoc warnings)"
"LUCENE-3149","IMPROVEMENT","TASK","upgrade icu to 4.8","we should upgrade from 4.6 to 4.8.

some internal methods became public, also a package-private reflection hack can be removed."
"LUCENE-3724","BUG","BUG","SimpleText sumTotalTermFreq is wrong if only positions are omitted","ant test -Dtestcase=TestOmitPositions -Dtestmethod=testBasic -Dtests.seed=-6c9bd4a6197b9463:-71d0d11bc2db9a15:697690b3dff2369 -Dargs=""-Dfile.encoding=UTF-8""
    [junit] java.lang.AssertionError: sumTotalTermFreq=0,sumDocFreq=400
    [junit] 	at org.apache.lucene.search.CollectionStatistics.<init>(CollectionStatistics.java:38)

This assert fails because #of positions for the field is < #of postings, which is impossible.

From memory i think SimpleText calculates sumTotalTermFreq ""one the fly"" by reading the positions from its text file...
In this case it should write the stat explicitly."
"LUCENE-1015","RFE","IMPROVEMENT","FieldCache should support longs and doubles","Would be nice if FieldCache supported longs and doubles"
"LUCENE-1864","DOCUMENTATION","BUG","bogus javadocs for FieldValueHitQuery.fillFields","FieldValueHitQuery.fillFields has javadocs that seem to be left over from a completely different method...

{code}
  /**
   * Given a FieldDoc object, stores the values used to sort the given document.
   * These values are not the raw values out of the index, but the internal
   * representation of them. This is so the given search hit can be collated by
   * a MultiSearcher with other search hits.
   * 
   * @param doc
   *          The FieldDoc to store sort values into.
   * @return The same FieldDoc passed in.
   * @see Searchable#search(Weight,Filter,int,Sort)
   */
  FieldDoc fillFields(final Entry entry) {
    final int n = comparators.length;
    final Comparable[] fields = new Comparable[n];
    for (int i = 0; i < n; ++i) {
      fields[i] = comparators[i].value(entry.slot);
    }
    //if (maxscore > 1.0f) doc.score /= maxscore;   // normalize scores
    return new FieldDoc(entry.docID, entry.score, fields);
  }

{code}"
"LUCENE-3386","RFE","","Integrate MockBM25Similarity and MockLMSimilarity into the framework","Steps:
1. Decide if {{MockLMSimilarity}} is needed at all (we have {{LMDirichletSimilarity}})
2. Move the classes to the similarities package
3. Move the similarities package to src/
4. Move all sims (inc. Similarity) to similarities
5. Make MockBM25Similarity a subclass of EasySimilarity?"
"LUCENE-3250","CLEANUP","TASK","remove contrib/misc and contrib/wordnet's dependencies on analyzers module","These contribs don't actually analyze any text.

After this patch, only the contrib/demo relies upon the analyzers module... we can separately try to figure that one out (I don't think any of these lucene contribs needs to reach back into modules/)

"
"LUCENE-3557","RFE","IMPROVEMENT","Spellchecker should take IndexWriterConfig... deprecate old methods?","When looking at LUCENE-3490, i realized there was no way to specify the codec for the spellchecker to use.

It has the following current methods:
* indexDictionary(Dictionary dict): this causes optimize!
* indexDictionary(Dictionary dict, int mergeFactory, int ramMB): this causes optimize!
* indexDictionary(Dictionary dict, int mergeFactor, int ramMB, boolean optimize)

But no way to specify an IndexwriterConfig. Additionally, I don't like that several of these ctors force an optimize in a tricky way,
even though it was like this all along.

So I think we should add indexDictionary(Dictionary dict, IndexWriterConfig config, boolean optimize).

We should either deprecate all the other ctors in 3.x and nuke in trunk, or at least add warnings to the ones that optimize."
"LUCENE-1436","REFACTORING","IMPROVEMENT","Make ReqExclScorer package private, and use DocIdSetIterator for excluded part.",""
"LUCENE-2359","BUG","BUG","CartesianPolyFilterBuilder doesn't handle edge case around the 180 meridian","Test case:  
Points all around the globe, plus two points at 0, 179.9 and 0,-179.9 (on each side of the meridian).  Then, do a Cartesian Tier filter on a point right near those two.  It will return all the points when it should just return those two.

The flawed logic is in the else clause below:
{code}
if (longX2 != 0.0) {
		//We are around the prime meridian
		if (longX == 0.0) {
			longX = longX2;
			longY = 0.0;
        	shape = getShapeLoop(shape,ctp,latX,longX,latY,longY);
		} else {//we are around the 180th longitude
			longX = longX2;
			longY = -180.0;
			shape = getShapeLoop(shape,ctp,latY,longY,latX,longX);
	}
{code}

Basically, the Y and X values are transposed.  This currently says go from longY (-180) all the way around  to longX which is the lower left longitude of the box formed.  Instead, it should go from the lower left long to -180."
"LUCENE-505","IMPROVEMENT","IMPROVEMENT","MultiReader.norm() takes up too much memory: norms byte[] should be made into an Object","MultiReader.norms() is very inefficient: it has to construct a byte array that's as long as all the documents in every segment.  This doubles the memory requirement for scoring MultiReaders vs. Segment Readers.  Although this is cached, it's still a baseline of memory that is unnecessary.

The problem is that the Normalization Factors are passed around as a byte[].  If it were instead replaced with an Object, you could perform a whole host of optimizations
a.  When reading, you wouldn't have to construct a ""fakeNorms"" array of all 1.0fs.  You could instead return a singleton object that would just return 1.0f.
b.  MultiReader could use an object that could delegate to NormFactors of the subreaders
c.  You could write an implementation that could use mmap to access the norm factors.  Or if the index isn't long lived, you could use an implementation that reads directly from the disk.

The patch provided here replaces the use of byte[] with a new abstract class called NormFactors.  
NormFactors has two methods on it
    public abstract byte getByte(int doc) throws IOException;  // Returns the byte[doc]
    public float getFactor(int doc) throws IOException;            // Calls Similarity.decodeNorm(getByte(doc))

There are four implementations of this abstract class
1.  NormFactors.EmptyNormFactors - This replaces the fakeNorms with a singleton that only returns 1.0
2.  NormFactors.ByteNormFactors - Converts a byte[] to a NormFactors for backwards compatibility in constructors.
3.  MultiNormFactors - Multiplexes the NormFactors in MultiReader to prevent the need to construct the gigantic norms array.
4.  SegmentReader.Norm - Same class, but now extends NormFactors to provide the same access.

In addition, Many of the Query and Scorer classes were changes to pass around NormFactors instead of byte[], and to call getFactor() instead of using the byte[].  I have kept around IndexReader.norms(String) for backwards compatibiltiy, but marked it as deprecated.  I believe that the use of ByteNormFactors in IndexReader.getNormFactors() will keep backward compatibility with other IndexReader implementations, but I don't know how to test that.
"
"LUCENE-3003","REFACTORING","IMPROVEMENT","Move UnInvertedField into Lucene core","Solr's UnInvertedField lets you quickly lookup all terms ords for a
given doc/field.

Like, FieldCache, it inverts the index to produce this, and creates a
RAM-resident data structure holding the bits; but, unlike FieldCache,
it can handle multiple values per doc, and, it does not hold the term
bytes in RAM.  Rather, it holds only term ords, and then uses
TermsEnum to resolve ord -> term.

This is great eg for faceting, where you want to use int ords for all
of your counting, and then only at the end you need to resolve the
""top N"" ords to their text.

I think this is a useful core functionality, and we should move most
of it into Lucene's core.  It's a good complement to FieldCache.  For
this first baby step, I just move it into core and refactor Solr's
usage of it.

After this, as separate issues, I think there are some things we could
explore/improve:

  * The first-pass that allocates lots of tiny byte[] looks like it
    could be inefficient.  Maybe we could use the byte slices from the
    indexer for this...

  * We can improve the RAM efficiency of the TermIndex: if the codec
    supports ords, and we are operating on one segment, we should just
    use it.  If not, we can use a more RAM-efficient data structure,
    eg an FST mapping to the ord.

  * We may be able to improve on the main byte[] representation by
    using packed ints instead of delta-vInt?

  * Eventually we should fold this ability into docvalues, ie we'd
    write the byte[] image at indexing time, and then loading would be
    fast, instead of uninverting
"
"LUCENE-1793","CLEANUP","IMPROVEMENT","remove custom encoding support in Greek/Russian Analyzers","The Greek and Russian analyzers support custom encodings such as KOI-8, they define things like Lowercase and tokenization for these.

I think that analyzers should support unicode and that conversion/handling of other charsets belongs somewhere else. 

I would like to deprecate/remove the support for these other encodings.
"
"LUCENE-3290","RFE","IMPROVEMENT","add FieldInvertState.numUniqueTerms, Terms.sumDocFreq","For scoring systems like lnu.ltc (http://trec.nist.gov/pubs/trec16/papers/ibm-haifa.mq.final.pdf), we need to supply 3 stats:
* average tf within d
* # of unique terms within d
* average number of unique terms across field

If we add FieldInvertState.numUniqueTerms, you can incorporate the first two into your norms/docvalues (once we cut over),
the average tf within d being length / numUniqueTerms.

to compute the average across the field, we can just write the sum of all terms' docfreqs into the terms dictionary header,
and you can then divide this by maxdoc to get the average.
"
"LUCENE-2389","IMPROVEMENT","IMPROVEMENT","Enforce TokenStream impl / Analyzer finalness by an assertion","As noted in LUCENE-1753 and other issues, TokenStream and Analyzers are based on the decorator pattern. At least all TokenStream and Analyzer implementations in Lucene and Solr should be final.

The attached patch adds an assertion to the ctors of both classes that does the corresponding checks:
- Analyzers must be final or private classes or anonymous inner classes
- TokenStreams must be final or private classes or anonymous inner classes or have a final incrementToken()

I will commit this after robert have fixed solr streams."
"LUCENE-2182","BUG","BUG","DEFAULT_ATTRIBUTE_FACTORY faills to load implementation class when iterface comes from different classloader","This is a followup for [http://www.lucidimagination.com/search/document/1724fcb3712bafba/using_the_new_tokenizer_api_from_a_jar_file]:

The DEFAULT_ATTRIBUTE_FACTORY should load the implementation class for a given attribute interface from the same classloader like the attribute interface. The current code loads it from the classloader of the lucene-core.jar file. In solr this fails when the interface is in a JAR file coming from the plugins folder. 

The interface is loaded correctly, because the addAttribute(FooAttribute.class) loads the FooAttribute.class from the plugin code and this with success. But as addAttribute tries to load the class from its local lucene-core.jar classloader it will not find the attribute.

The fix is to tell Class.forName to use the classloader of the corresponding interface, which is the correct way to handle it, as the impl and the attribute should always be in the same classloader and file.

I hope I can somehow add a test for that."
"LUCENE-3445","RFE","IMPROVEMENT","Add SearcherManager, to manage IndexSearcher usage across threads and reopens","This is a simple helper class I wrote for Lucene in Action 2nd ed.
I'd like to commit under Lucene (contrib/misc).

It simplifies using & reopening an IndexSearcher across multiple
threads, by using IndexReader's ref counts to know when it's safe
to close the reader.

In the process I also factored out a test base class for tests that
want to make lots of simultaneous indexing and searching threads, and
fixed TestNRTThreads (core), TestNRTManager (contrib/misc) and the new
TestSearcherManager (contrib/misc) to use this base class.
"
"LUCENE-1906","BUG","BUG","Backwards problems with CharStream and Tokenizers with custom reset(Reader) method","When reviewing the new CharStream code added to Tokenizers, I found a
serious problem with backwards compatibility and other Tokenizers, that do
not override reset(CharStream).

The problem is, that e.g. CharTokenizer only overrides reset(Reader):

{code}
  public void reset(Reader input) throws IOException {
    super.reset(input);
    bufferIndex = 0;
    offset = 0;
    dataLen = 0;
  }
{code}

If you reset such a Tokenizer with another CharStream (not a Reader), this
method will never be called and breaking the whole Tokenizer.

As CharStream extends Reader, I propose to remove this reset(CharStream
method) and simply do an instanceof check to detect if the supplied Reader
is no CharStream and wrap it. We could also remove the extra ctor (because
most Tokenizers have no support for passing CharStreams). If the ctor also
checks with instanceof and warps as needed the code is backwards compatible
and we do not need to add additional ctors in subclasses.

As this instanceof check is always done in CharReader.get() why not remove
ctor(CharStream) and reset(CharStream) completely?

Any thoughts?

I would like to fix this somehow before RC4, I'm, sorry :(
"
"LUCENE-886","CLEANUP","IMPROVEMENT","spellchecker cleanup","Some cleanup, attached here so it can be tracked if necessary: javadoc improvements; don't print exceptions to stderr but re-throw them; new constructor for a new test case. I will commit this soon."
"LUCENE-1103","RFE","IMPROVEMENT","WikipediaTokenizer","I have extended StandardTokenizer to recognize Wikipedia syntax and mark tokens with certain attributes.  It isn't necessarily complete, but it does a good enough job for it to be consumed and improved by others.

It sets the Token.type() value depending on the Wikipedia syntax (links, internal links, bold, italics, etc.) based on my pass at http://en.wikipedia.org/wiki/Wikipedia:Tutorial

I have only tested it with the benchmarking EnwikiDocMaker wikipedia stuff and it seems to do a decent job.

Caveats:  I am not sure how to best handle testing, since the content is licensed under GNU Free Doc License, I believe I can't copy and paste a whole document into the unit test.  I have hand coded one doc and have another one that just generally runs over the benchmark Wikipedia download.

One more question is where to put it.  It could go in analysis, but the tests at least will have a dependency on Benchmark.  I am thinking of adding a new contrib/wikipedia where this could grow to have other wikipedia things (perhaps we would move EnwikiDocMaker there????) and reverse the dependency on Benchmark.

I will post a patch over the next few days."
"LUCENE-2114","IMPROVEMENT","IMPROVEMENT","Improve org.apache.lucene.search.Filter Documentation and Tests to reflect per segment readers","Filter Javadoc does not mention that the Reader passed to getDocIDSet(Reader) could be on a per-segment basis.
This caused confusion on the users-list -- see http://lucene.markmail.org/message/6knz2mkqbpxjz5po?q=date:200912+list:org.apache.lucene.java-user&page=1
We should improve the javadoc and also add a testcase that reflects filtering on a per-segment basis."
"LUCENE-1288","RFE","IMPROVEMENT","Add getVersion method to IndexCommit","Returns the equivalent of IndexReader.getVersion for IndexCommit

{code}
public abstract long getVersion();
{code}"
"LUCENE-1013","BUG","BUG","IndexWriter.setMaxMergeDocs gives non-backwards-compatible exception ""out of the box""","Yonik hit this (see details in LUCENE-994): because we have switched
to LogByteSizeMergePolicy by default in IndexWriter, which uses MB to
limit max size of merges (setMaxMergeMB), when an existing app calls
setMaxMergeDocs (or getMaxMergeDocs) it will hit an
IllegalArgumentException on dropping in the new JAR.

I think the simplest solution is to fix LogByteSizeMergePolicy to
allow setting of the max by either MB or by doc count, just like how
in LUCENE-1007 allows flushing by either MB or docCount or both."
"LUCENE-627","BUG","BUG","highlighter problems with overlapping tokens","The lucene highlighter has problems when tokens that overlap are generated.

For example, if analysis of iPod generates the tokens ""i"", ""pod"", ""ipod"" (with pod and ipod in the same position),
then the highlighter will output this as iipod, regardless of if any of those tokens are highlighted.

Discovered via http://issues.apache.org/jira/browse/SOLR-24
"
"LUCENE-2769","BUG","BUG","FilterIndexReader in trunk does not implement getSequentialSubReaders() correctly","Since LUCENE-2459, getSequentialSubReaders() in FilterIndexReader returns null, so it returns an atomic reader. But If you call then any of the enum methods, it throws Exception because the underlying reader is not atomic.

We should move the null-returning method to SlowMultiReaderWrapper and fix FilterIndexReader's default to return in.getSequentialSubReaders(). Ideally an implementation must of course also wrap the sub-readers.

If we change this we have to look into other Impls like the MultiPassIndexSplitter if we need to add atomicity."
"LUCENE-792","BUG","BUG","PrecedenceQueryParser misinterprets queries starting with NOT","""NOT foo AND baz"" is parsed as ""-(+foo +baz)"" instead of ""-foo +bar"".

(I'm setting parser.setDefaultOperator(PrecedenceQueryParser.AND_OPERATOR) but the issue applies otherwise too.)
"
"LUCENE-3605","IMPROVEMENT","IMPROVEMENT","revisit segments.gen sleeping","in LUCENE-3601, i worked up a change where we intentionally crash() all un-fsynced files 
in tests to ensure that we are calling sync on files when we should.

I think this would be nice to do always (and with some fixes all tests pass).

But this is super-slow sometimes because when we corrupt the unsynced segments.gen, it causes
SIS.read to take 500ms each time (and in checkindex for some reason we do this twice, which seems wrong).

I can workaround this for now for tests (just do a partial crash that avoids corrupting the segments.gen),
but I wanted to create this issue for discussion about the sleeping/non-fsyncing of segments.gen, just
because i guess its possible someone could hit this slowness.
 "
"LUCENE-3608","BUG","BUG","MultiFields.getUniqueFieldCount is broken","this returns terms.size(), but terms is lazy-initted. So it wrongly returns 0.

Simplest fix would be to return -1."
"LUCENE-2372","CLEANUP","IMPROVEMENT","Replace deprecated TermAttribute by new CharTermAttribute","After LUCENE-2302 is merged to trunk with flex, we need to carry over all tokenizers and consumers of the TokenStreams to the new CharTermAttribute.

We should also think about adding a AttributeFactory that creates a subclass of CharTermAttributeImpl that returns collation keys in toBytesRef() accessor. CollationKeyFilter is then obsolete, instead you can simply convert every TokenStream to indexing only CollationKeys by changing the attribute implementation."
"LUCENE-3752","CLEANUP","TASK","move preflexrw to lucene3x package","Currently there are a lot of things made public in lucene3x codec, but all marked internal/experimental/deprecated.

A lot of this is just so our test codec (preflexrw) can subclass it. I think we should just move it to the same
package, then it call all be package-private."
"LUCENE-1455","BUG","BUG","org.apache.lucene.ant.HtmlDocument creates a FileInputStream in its constructor that it doesn't close","A look through the jtidy source code doesn't show a close that i can find in parse (seems to be standard that you close your own streams anyway), so this looks like a small descriptor leak to me."
"LUCENE-2668","BUG","BUG","offset gap should be added regardless of existence of tokens in DocInverterPerField","Problem: If a multiValued field which contains a stop word (e.g. ""will"" in the following sample) only value is analyzed by StopAnalyzer when indexing, the offsets of the subsequent tokens are not correct.

{code:title=indexing a multiValued field}
doc.add( new Field( F, ""Mike"", Store.YES, Index.ANALYZED, TermVector.WITH_OFFSETS ) );
doc.add( new Field( F, ""will"", Store.YES, Index.ANALYZED, TermVector.WITH_OFFSETS ) );
doc.add( new Field( F, ""use"", Store.YES, Index.ANALYZED, TermVector.WITH_OFFSETS ) );
doc.add( new Field( F, ""Lucene"", Store.YES, Index.ANALYZED, TermVector.WITH_OFFSETS ) );
{code}

In this program (soon to be attached), if you use WhitespaceAnalyzer, you'll get the offset(start,end) for ""use"" and ""Lucene"" will be use(10,13) and Lucene(14,20). But if you use StopAnalyzer, the offsets will be use(9,12) and lucene(13,19). When searching, since searcher cannot know what analyzer was used at indexing time, this problem causes out of alignment of FVH.

Cause of the problem: StopAnalyzer filters out ""will"", anyToken flag set to false then offset gap is not added in DocInverterPerField:

{code:title=DocInverterPerField.java}
if (anyToken)
  fieldState.offset += docState.analyzer.getOffsetGap(field);
{code}

I don't understand why the condition is there... If always the gap is added, I think things are simple."
"LUCENE-3113","BUG","BUG","fix analyzer bugs found by MockTokenizer","In LUCENE-3064, we beefed up MockTokenizer with assertions, and I've switched over the analysis tests to use MockTokenizer for better coverage.

However, this found a few bugs (one of which is LUCENE-3106):
* incrementToken() after it returns false in CommonGramsQueryFilter, HyphenatedWordsFilter, ShingleFilter, SynonymFilter
* missing end() implementation for PrefixAwareTokenFilter
* double reset() in QueryAutoStopWordAnalyzer and ReusableAnalyzerBase
* missing correctOffset()s in MockTokenizer itself.

I think it would be nice to just fix all the bugs on one issue... I've fixed everything except Shingle and Synonym"
"LUCENE-2313","TEST","IMPROVEMENT","Add VERBOSE to LuceneTestCase and LuceneTestCaseJ4","component-build.xml allows to define tests.verbose as a system property when running tests. Both LuceneTestCase and LuceneTestCaseJ4 don't read that property. It will be useful for overriding tests to access one place for this setting (I believe currently some tests do it on their own). Then (as a separate issue) we can move all tests that don't check the parameter to only print if VERBOSE is true.

I will post a patch soon."
"LUCENE-2533","BUG","BUG","FileSwitchDirectory should uniqueify the String file names returned by listAll","Right now we blindly concatenate what's returned from primary & secondary.

But a legit use of FSD is pointing to the same underlying FSDir but w/ different impls for opening the inputs/outputs.

I have simple patch that just uniqueifies using Set<String>."
"LUCENE-3269","TEST","TEST","Speed up Top-K sampling tests","speed up the top-k sampling tests (but make sure they are thorough on nightly etc still)

usually we would do this with use of atLeast(), but these tests are somewhat tricky,
so maybe a different approach is needed."
"LUCENE-3633","RFE","IMPROVEMENT","Remove code duplication in MultiReader/DirectoryReader, make everything inside final","After making IndexReader readOnly (LUCENE-3606) there is no need to have completely different DirectoryReader and MultiReader, the current code is heavy code duplication and violations against finalness patterns. There are only few differences in reopen and things like isCurrent/getDirectory/...

This issue will clean this up by introducing a hidden package-private base class for both and only handling reopen and incRef/decRef different. DirectoryReader is now final and all fields in BaseMultiReader, MultiReader and DirectoryReader are final now. DirectoryReader has now only static factories, no public ctor anymore."
"LUCENE-2776","IMPROVEMENT","BUG","indexwriter creates unwanted termvector info","I noticed today that when I build a big index in Solr, I get some unwanted termvector info, even though I didn't request any.
This does not happen on 3x - not sure when it started happening on trunk."
"LUCENE-2039","RFE","IMPROVEMENT","Regex support and beyond in JavaCC QueryParser","Since the early days the standard query parser was limited to the queries living in core, adding other queries or extending the parser in any way always forced people to change the grammar file and regenerate. Even if you change the grammar you have to be extremely careful how you modify the parser so that other parts of the standard parser are affected by customisation changes. Eventually you had to live with all the limitation the current parser has like tokenizing on whitespaces before a tokenizer / analyzer has the chance to look at the tokens. 
I was thinking about how to overcome the limitation and add regex support to the query parser without introducing any dependency to core. I added a new special character that basically prevents the parser from interpreting any of the characters enclosed in the new special characters. I choose the forward slash  '/' as the delimiter so that everything in between two forward slashes is basically escaped and ignored by the parser. All chars embedded within forward slashes are treated as one token even if it contains other special chars like * []?{} or whitespaces. This token is subsequently passed to a pluggable ""parser extension"" with builds a query from the embedded string. I do not interpret the embedded string in any way but leave all the subsequent work to the parser extension. Such an extension could be another full featured query parser itself or simply a ctor call for regex query. The interface remains quiet simple but makes the parser extendible in an easy way compared to modifying the javaCC sources.

The downsides of this patch is clearly that I introduce a new special char into the syntax but I guess that would not be that much of a deal as it is reflected in the escape method though. It would truly be nice to have more than once extension an have this even more flexible so treat this patch as a kickoff though.

Another way of solving the problem with RegexQuery would be to move the JDK version of regex into the core and simply have another method like:
{code}
protected Query newRegexQuery(Term t) {
  ... 
}
{code}

which I would like better as it would be more consistent with the idea of the query parser to be a very strict and defined parser.

I will upload a patch in a second which implements the extension based approach I guess I will add a second patch with regex in core soon too.

"
"LUCENE-1991","CLEANUP","IMPROVEMENT","Similarity#score deprecated method - javadoc reference + SimilarityDelegator","Old method  

  public float scorePayload(String fieldName, byte [] payload, int offset, int length)

has been deprecated by - 

  public float scorePayload(int docId, String fieldName, int start, int end, byte [] payload, int offset, int length)


References in PayLoadNearQuery (javadoc) changed. 

Also - SimilarityDelegator overrides the new method as opposed to the (deprecated) old one. "
"LUCENE-1575","REFACTORING","IMPROVEMENT","Refactoring Lucene collectors (HitCollector and extensions)","This issue is a result of a recent discussion we've had on the mailing list. You can read the thread [here|http://www.nabble.com/Is-TopDocCollector%27s-collect()-implementation-correct--td22557419.html].

We have agreed to do the following refactoring:
* Rename MultiReaderHitCollector to Collector, with the purpose that it will be the base class for all Collector implementations.
* Deprecate HitCollector in favor of the new Collector.
* Introduce new methods in IndexSearcher that accept Collector, and deprecate those that accept HitCollector.
** Create a final class HitCollectorWrapper, and use it in the deprecated methods in IndexSearcher, wrapping the given HitCollector.
** HitCollectorWrapper will be marked deprecated, so we can remove it in 3.0, when we remove HitCollector.
** It will remove any instanceof checks that currently exist in IndexSearcher code.
* Create a new (abstract) TopDocsCollector, which will:
** Leave collect and setNextReader unimplemented.
** Introduce protected members PriorityQueue and totalHits.
** Introduce a single protected constructor which accepts a PriorityQueue.
** Implement topDocs() and getTotalHits() using the PQ and totalHits members. These can be used as-are by extending classes, as well as be overridden.
** Introduce a new topDocs(start, howMany) method which will be used a convenience method when implementing a search application which allows paging through search results. It will also attempt to improve the memory allocation, by allocating a ScoreDoc[] of the requested size only.
* Change TopScoreDocCollector to extend TopDocsCollector, use the topDocs() and getTotalHits() implementations as they are from TopDocsCollector. The class will also be made final.
* Change TopFieldCollector to extend TopDocsCollector, and make the class final. Also implement topDocs(start, howMany).
* Change TopFieldDocCollector (deprecated) to extend TopDocsCollector, instead of TopScoreDocCollector. Implement topDocs(start, howMany)
* Review other places where HitCollector is used, such as in Scorer, deprecate those places and use Collector instead.

Additionally, the following proposal was made w.r.t. decoupling score from collect():
* Change collect to accecpt only a doc Id (unbased).
* Introduce a setScorer(Scorer) method.
* If during collect the implementation needs the score, it can call scorer.score().
If we do this, then we need to review all places in the code where collect(doc, score) is called, and assert whether Scorer can be passed. Also this raises few questions:
* What if during collect() Scorer is null? (i.e., not set) - is it even possible?
* I noticed that many (if not all) of the collect() implementations discard the document if its score is not greater than 0. Doesn't it mean that score is needed in collect() always?

Open issues:
* The name for Collector
* TopDocsCollector was mentioned on the thread as TopResultsCollector, but that was when we thought to call Colletor ResultsColletor. Since we decided (so far) on Collector, I think TopDocsCollector makes sense, because of its TopDocs output.
* Decoupling score from collect().

I will post a patch a bit later, as this is expected to be a very large patch. I will split it into 2: (1) code patch (2) test cases (moving to use Collector instead of HitCollector, as well as testing the new topDocs(start, howMany) method.
There might be even a 3rd patch which handles the setScorer thing in Collector (maybe even a different issue?)"
"LUCENE-934","TASK","TASK","Uploade Lucene 2.1 to ibiblio","Please uploaded Lucene (specifically lucene-core) 2.1.0 to ibiblio. I see 2.0.0 but not 2.1.0.

Thanks!"
"LUCENE-2839","DESIGN_DEFECT","BUG","Visibility of Scorer.score(Collector, int, int) is wrong","The method for scoring subsets in Scorer has wrong visibility, its marked protected, but protected methods should not be called from other classes. Protected methods are intended for methods that should be overridden by subclasses and are called by (often) final methods of the same class. They should never be called from foreign classes.

This method is called from another class out-of-scope: BooleanScorer(2) - so it must be public, but it's protected. This does not lead to a compiler error because BS(2) is in same package, but may lead to problems if subclasses from other packages override it. When implementing LUCENE-2838 I hit a trap, as I thought tis method should only be called from the class or Scorer itsself, but in fact its called from outside, leading to bugs, because I had not overridden it. As ConstantScorer did not use it I have overridden it with throw UOE and suddenly BooleanQuery was broken, which made it clear that it's called from outside (which is not the intention of protected methods).

We cannot fix this in 3.x, as it would break backwards for classes that overwrite this method, but we can fix visibility in trunk.
"
"LUCENE-2919","RFE","IMPROVEMENT","IndexSplitter that divides by primary key term","Index splitter that divides by primary key term.  The contrib MultiPassIndexSplitter we have divides by docid, however to guarantee external constraints it's sometimes necessary to split by a primary key term id.  I think this implementation is a fairly trivial change."
"LUCENE-374","BUG","BUG","You cannot sort on fields that don't exist","While it's possible to search for fields that don't exist (you'll get 0 hits),  
you'll get an exception if you try to sort by a field that has no values. The  
exception is this:  
  
if (termEnum.term() == null) {  
  throw new RuntimeException (""no terms in field "" + field);  
}  
  
I'll attach a change suggested by Yonik Seeley that removes this exception. 
 
Also, the if-condition above is incomplete anyway, so currently the exception 
is not always thrown (as termEnum .term() might well be != null but point to a 
term in a different field already)"
"LUCENE-2186","RFE","RFE","First cut at column-stride fields (index values storage)","I created an initial basic impl for storing ""index values"" (ie
column-stride value storage).  This is still a work in progress... but
the approach looks compelling.  I'm posting my current status/patch
here to get feedback/iterate, etc.

The code is standalone now, and lives under new package
oal.index.values (plus some util changes, refactorings) -- I have yet
to integrate into Lucene so eg you can mark that a given Field's value
should be stored into the index values, sorting will use these values
instead of field cache, etc.

It handles 3 types of values:

  * Six variants of byte[] per doc, all combinations of fixed vs
    variable length, and stored either ""straight"" (good for eg a
    ""title"" field), ""deref"" (good when many docs share the same value,
    but you won't do any sorting) or ""sorted"".

  * Integers (variable bit precision used as necessary, ie this can
    store byte/short/int/long, and all precisions in between)

  * Floats (4 or 8 byte precision)

String fields are stored as the UTF8 byte[].  This patch adds a
BytesRef, which does the same thing as flex's TermRef (we should merge
them).

This patch also adds basic initial impl of PackedInts (LUCENE-1990);
we can swap that out if/when we get a better impl.

This storage is dense (like field cache), so it's appropriate when the
field occurs in all/most docs.  It's just like field cache, except the
reading API is a get() method invocation, per document.

Next step is to do basic integration with Lucene, and then compare
sort performance of this vs field cache.

For the ""sort by String value"" case, I think RAM usage & GC load of
this index values API should be much better than field caache, since
it does not create object per document (instead shares big long[] and
byte[] across all docs), and because the values are stored in RAM as
their UTF8 bytes.

There are abstract Writer/Reader classes.  The current reader impls
are entirely RAM resident (like field cache), but the API is (I think)
agnostic, ie, one could make an MMAP impl instead.

I think this is the first baby step towards LUCENE-1231.  Ie, it
cannot yet update values, and the reading API is fully random-access
by docID (like field cache), not like a posting list, though I
do think we should add an iterator() api (to return flex's DocsEnum)
-- eg I think this would be a good way to track avg doc/field length
for BM25/lnu.ltc scoring.
"
"LUCENE-1900","DOCUMENTATION","BUG","Confusing Javadoc in Searchable.java","In Searchable.java, the javadoc for maxdoc() is:

  /** Expert: Returns one greater than the largest possible document number.
   * Called by search code to compute term weights.
   * @see org.apache.lucene.index.IndexReader#maxDoc()

The qualification ""expert"" and the statement ""called by search code to compute term weights"" is a bit confusing, It implies that maxdoc() somehow computes weights, which is obviously not true (what it does is explained in the other sentence). Maybe it is used as one factor of the weight, but do we really need to mention this here? "
"LUCENE-1587","BUG","BUG","RangeQuery equals method does not compare collator property fully","The equals method in the range query has the collator comparison implemented as:
(this.collator != null && ! this.collator.equals(other.collator))

When _this.collator = null_ and _other.collator = someCollator_  this method will incorrectly assume they are equal. 

So adding something like
|| (this.collator == null && other.collator != null)
would fix the problem
"
"LUCENE-1758","IMPROVEMENT","IMPROVEMENT","improve arabic analyzer: light8 -> light10","Someone mentioned on the java user list that the arabic analysis was not as good as they would like.

This patch adds the - prefix (light10 algorithm versus light8 algorithm).
In the light10 paper, this improves precision from .390 to .413
They mention this is not statistically significant, but it makes linguistic sense and at least has been shown not to hurt.

In the future, I hope openrelevance will allow us to try some more approaches. 
"
